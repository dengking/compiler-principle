{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"compiler-principle Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) is really an amazing book, as mentioned in Chapter 1 Introduction This book is is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages , machine architecture , language theory , algorithms , and software engineering . As mentioned in the paragraph above, the study of compiler is an interdiscipline involving computer science ( programming languages , machine architecture , algorithms , and software engineering ) and language theory , so the book may be hard to understand if lacking experience in programming, grasp of common data structure(especially tree which is mainly used in the front end, graph which is mainly used in the back end) and algorithms(especially recursion ). The language theory in the paragraph above mainly refers to the formal language theory , I have create a repository Automata-and-formal-language to record my knowledge of formal language theory and automata theory . A basic understanding of these theories will help understand the book. The data structure mentioned in the book can be found in my repository data-structure . The algorithm mentioned in the book can be found in my repository algorithm . Beside the book, this repository also contain some material supplemented to help understand.","title":"Home"},{"location":"#compiler-principle","text":"Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) is really an amazing book, as mentioned in Chapter 1 Introduction This book is is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages , machine architecture , language theory , algorithms , and software engineering . As mentioned in the paragraph above, the study of compiler is an interdiscipline involving computer science ( programming languages , machine architecture , algorithms , and software engineering ) and language theory , so the book may be hard to understand if lacking experience in programming, grasp of common data structure(especially tree which is mainly used in the front end, graph which is mainly used in the back end) and algorithms(especially recursion ). The language theory in the paragraph above mainly refers to the formal language theory , I have create a repository Automata-and-formal-language to record my knowledge of formal language theory and automata theory . A basic understanding of these theories will help understand the book. The data structure mentioned in the book can be found in my repository data-structure . The algorithm mentioned in the book can be found in my repository algorithm . Beside the book, this repository also contain some material supplemented to help understand.","title":"compiler-principle"},{"location":"1-Introduction/","text":"Chapter 1 Introduction How long have you been programming? Have you ever wondered what a programming language is? There are many ways to answer this question, here I will try to answer this question from some different perspectives. From formal language A programming language is a formal language , so it is equipped with 1\u3001 alphabet 2\u3001 words and lexical grammar to defining the syntax of tokens . 3\u3001 Syntax and formal grammar to describe its syntax 4\u3001 semantics From the perspectives of a compiler How dose compiler understand what the program mean? The table is a summary of the implementation of compiler. language phase grammar technique generator alphabet Lexical grammar Lexical analysis Regular expressions Finite-state machine Scanner generators, for example Lex (software) Syntax (programming languages) Syntax analysis Context-free grammar Parse tree , Abstract syntax tree Parser generators, for example Yacc Semantics (computer science) Semantic analysis (compilers) Lexical grammar , syntax and semantics define a programming language,.Only when specifying lexical grammar , syntax and semantics can the compiler understand what programmer have written and translate the language correctly. This book is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages, machine architecture, language theory, algorithms, and software engineering.","title":1},{"location":"1-Introduction/#chapter#1#introduction","text":"How long have you been programming? Have you ever wondered what a programming language is? There are many ways to answer this question, here I will try to answer this question from some different perspectives.","title":"Chapter 1 Introduction"},{"location":"1-Introduction/#from#formal#language","text":"A programming language is a formal language , so it is equipped with 1\u3001 alphabet 2\u3001 words and lexical grammar to defining the syntax of tokens . 3\u3001 Syntax and formal grammar to describe its syntax 4\u3001 semantics","title":"From  formal language"},{"location":"1-Introduction/#from#the#perspectives#of#a#compiler","text":"How dose compiler understand what the program mean? The table is a summary of the implementation of compiler. language phase grammar technique generator alphabet Lexical grammar Lexical analysis Regular expressions Finite-state machine Scanner generators, for example Lex (software) Syntax (programming languages) Syntax analysis Context-free grammar Parse tree , Abstract syntax tree Parser generators, for example Yacc Semantics (computer science) Semantic analysis (compilers) Lexical grammar , syntax and semantics define a programming language,.Only when specifying lexical grammar , syntax and semantics can the compiler understand what programmer have written and translate the language correctly. This book is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages, machine architecture, language theory, algorithms, and software engineering.","title":"From the perspectives of a compiler"},{"location":"1-Introduction/1.1-Language-Processors/","text":"1.1 Language Processors Compiler Simply stated, a compiler is a program that can read a program in one language--the source language--and translate it into an equivalent program in another language--the target language; see Fig. 1.1. An important role of the compiler is to report any errors in the source program that it detects during the translation process. See also: wikipedia Compiler If the target program is an executable machine-language program , it can then be called by the user to process inputs and produce outputs; see Fig. 1.2. See also: wikipedia Machine code wikipedia Executable NOTE: The compiler definition given here is very broad because target programs can take many forms, not just executable machine-language programs . As stated in article Is Python interpreted or compiled? Yes. : Compiling is a more general idea: take a program in one language (or form), and convert it into another language or form. Usually the source form is a higher-level language than the destination form, such as when converting from C to machine code. But converting from JavaScript 8 to JavaScript 5 is also a kind of compiling. In Python, the source is compiled into a much simpler form called bytecode . Interpreter An interpreter is another common kind of language processor. Instead of producing a target program as a translation, an interpreter appears to directly execute the operations specified in the source program on inputs supplied by the user, as shown in Fig. 1.3. The machine-language target program produced by a compiler is usually much faster than an interpreter at mapping inputs to outputs . An interpreter, however, can usually give better error diagnostics than a compiler, because it executes the source program statement by statement. NOTE: Executing the source program statement by statement is the feature of interpreter and it is impossible for advanced programming language to be so. Typical interpreter is shell: bash redis server's execution of command committed by the client can also be seen as an interpreter just as shell Hybrid compiler Java language processors combine compilation and interpretation , as shown in Fig. 1.4. A Java source program may first be compiled into an intermediate form called bytecode . The bytecodes are then interpreted by a virtual machine . A benefit of this arrangement is that bytecodes compiled on one machine can be interpreted on another machine, perhaps across a network. See also: Virtual machine In order to achieve faster processing of inputs to outputs, some Java compilers, called just-in-time compilers, translate the bytecodes into machine language immediately before they run the intermediate program to process the input. See also: Just-in-time compilation NOTE: language processor\uff1a compiler, such as c, c++ interpreter, such as shell script hybrid compiler, such as python and java NOTE: Python is similar to Java in combining compilation and interpretation , but there are difference between the two language. The following is an good article explaining python implementation: Is Python interpreted or compiled? Yes. This post is very informative and clear and can help understand the content in this chapter. In addition to a compiler, several other programs may be required to create an executable target program, as shown in Fig. 1.5. preprocessor assembler linker loader NOTE: This book focus only on compiler and the others is not included. NOTE: \u4ee5\u4e0b\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff0c\u5728\u4e00\u4e9b\u6587\u7ae0\u4e2d\u53ef\u80fd\u4f1a\u63d0\u5230\uff1a Dynamic compilation Dynamic vs Static Compiler (JavaScript) Why Static Compilation?","title":"Introduction"},{"location":"1-Introduction/1.1-Language-Processors/#11#language#processors","text":"","title":"1.1 Language Processors"},{"location":"1-Introduction/1.1-Language-Processors/#compiler","text":"Simply stated, a compiler is a program that can read a program in one language--the source language--and translate it into an equivalent program in another language--the target language; see Fig. 1.1. An important role of the compiler is to report any errors in the source program that it detects during the translation process. See also: wikipedia Compiler If the target program is an executable machine-language program , it can then be called by the user to process inputs and produce outputs; see Fig. 1.2. See also: wikipedia Machine code wikipedia Executable NOTE: The compiler definition given here is very broad because target programs can take many forms, not just executable machine-language programs . As stated in article Is Python interpreted or compiled? Yes. : Compiling is a more general idea: take a program in one language (or form), and convert it into another language or form. Usually the source form is a higher-level language than the destination form, such as when converting from C to machine code. But converting from JavaScript 8 to JavaScript 5 is also a kind of compiling. In Python, the source is compiled into a much simpler form called bytecode .","title":"Compiler"},{"location":"1-Introduction/1.1-Language-Processors/#interpreter","text":"An interpreter is another common kind of language processor. Instead of producing a target program as a translation, an interpreter appears to directly execute the operations specified in the source program on inputs supplied by the user, as shown in Fig. 1.3. The machine-language target program produced by a compiler is usually much faster than an interpreter at mapping inputs to outputs . An interpreter, however, can usually give better error diagnostics than a compiler, because it executes the source program statement by statement. NOTE: Executing the source program statement by statement is the feature of interpreter and it is impossible for advanced programming language to be so. Typical interpreter is shell: bash redis server's execution of command committed by the client can also be seen as an interpreter just as shell","title":"Interpreter"},{"location":"1-Introduction/1.1-Language-Processors/#hybrid#compiler","text":"Java language processors combine compilation and interpretation , as shown in Fig. 1.4. A Java source program may first be compiled into an intermediate form called bytecode . The bytecodes are then interpreted by a virtual machine . A benefit of this arrangement is that bytecodes compiled on one machine can be interpreted on another machine, perhaps across a network. See also: Virtual machine In order to achieve faster processing of inputs to outputs, some Java compilers, called just-in-time compilers, translate the bytecodes into machine language immediately before they run the intermediate program to process the input. See also: Just-in-time compilation NOTE: language processor\uff1a compiler, such as c, c++ interpreter, such as shell script hybrid compiler, such as python and java NOTE: Python is similar to Java in combining compilation and interpretation , but there are difference between the two language. The following is an good article explaining python implementation: Is Python interpreted or compiled? Yes. This post is very informative and clear and can help understand the content in this chapter. In addition to a compiler, several other programs may be required to create an executable target program, as shown in Fig. 1.5. preprocessor assembler linker loader NOTE: This book focus only on compiler and the others is not included. NOTE: \u4ee5\u4e0b\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff0c\u5728\u4e00\u4e9b\u6587\u7ae0\u4e2d\u53ef\u80fd\u4f1a\u63d0\u5230\uff1a Dynamic compilation Dynamic vs Static Compiler (JavaScript) Why Static Compilation?","title":"Hybrid compiler"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/","text":"1.2 The Structure of a Compiler Up to this point we have treated a compiler as a single box that maps a source program into a semantically equivalent target program. If we open up this box a little, we see that there are two parts to this mapping: 1\u3001analysis 2\u3001synthesis The analysis part breaks up the source program into constituent pieces and imposes a grammatical structure on them. It then uses this structure to create an intermediate representation of the source program. If the analysis part detects that the source program is either syntactically ill formed or semantically unsound, then it must provide informative messages, so the user can take corrective action. The analysis part also collects information about the source program and stores it in a data structure called a symbol table , which is passed along with the intermediate representation to the synthesis part . The synthesis part constructs the desired target program from the intermediate representation and the information in the symbol table. The analysis part is often called the front end of the compiler; the synthesis part is the back end . If we examine the compilation process in more detail, we see that it operates as a sequence of phases , each of which transforms one representation of the source program to another. A typical decomposition of a compiler into phases is shown in Fig. 1.6. In practice, several phases may be grouped together, and the intermediate representations between the grouped phases need not be constructed explicitly. The symbol table , which stores information about the entire source program, is used by all phases of the compiler. NOTE: \u7b26\u53f7\u8868\u4e5f\u662f\u4e00\u95e8\u79d1\u5b66\uff0c\u9700\u8981\u5bf9\u5b83\u4ed4\u7ec6\u7814\u7a76\uff0c\u672c\u4e66\u4e2d\u6d89\u53ca\u7b26\u53f7\u8868\u7684\u7ae0\u8282\u6709\uff1a 1.2.1 Lexical Analysis 1.2.7 Symbol-Table Management 2.7 Symbol Tables \u5173\u4e8esymbol table\u7684\u4e3b\u8981\u5185\u5bb9\u653e\u5230\u4e86 Chapter-2-A-Simple-Syntax-Directed-Translator\\2.7-Symbol-Tables \u4e2d\u3002 Some compilers have a machine-independent optimization phase between the front end and the back end . The purpose of this optimization phase is to perform transformations on the intermediate representation , so that the back end can produce a better target program than it would have otherwise produced from an unoptimized intermediate representation. Since optimization is optional, one or the other of the two optimization phases shown in Fig. 1.6 may be missing. NOTE: \u56fe1.6\u7ed9\u51fa\u4e86\u4e00\u4e2acompiler\u7684\u67b6\u6784\uff0c\u8fd9\u4e2a\u67b6\u6784\u662f\u6e05\u6670\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u601d\u8003\u4e00\u4e0b\u5982\u4f55\u6765\u5b9e\u73b0\u4e00\u4e2a\u7f16\u8bd1\u5668\uff0c\u5982\u4e0b\u662f\u4e24\u79cd\u7b56\u7565\uff1a \u5206\u5f00\u5b9e\u73b0\u5404\u4e2a\u90e8\u4ef6\u7136\u540e\u5c06\u5b83\u4eec\u7ec4\u88c5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u7f16\u8bd1\u5668\uff0c\u6bcf\u4e2a\u90e8\u4ef6\u53ef\u4ee5\u72ec\u7acb\u5de5\u4f5c\uff08\u8026\u5408\u5ea6\u4f4e\uff09 \u6240\u6709\u529f\u80fd\u96c6\u6210\u8d77\u6765\u5f62\u6210\u4e00\u4e2amonolithic compiler\uff0c\u4e0d\u80fd\u72ec\u7acb\u5730\u4f7f\u7528\u5176\u4e2d\u67d0\u4e2a\u90e8\u4ef6\uff08\u8026\u5408\u5ea6\u9ad8\uff09 \u76ee\u524dc\u7cfb\u8bed\u8a00\u4e2d\u6700\u6d41\u884c\u7684\u4e24\u6b3e\u7f16\u8bd1\u5668\uff1a gcc \u548c clang \u3002\u5728clang\u7684\u5b98\u65b9\u7f51\u7ad9\u7684\u6587\u7ae0 Clang vs Other Open Source Compilers \u4e2d\uff0c\u7ed9\u51fa\u4e86\u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u5bf9\u6bd4\uff08\u5404\u81ea\u7684\u957f\u5904\u4e0e\u77ed\u5904\uff09\uff0c\u4e0b\u9762\u662f\u4e24\u79cd\u7684\u5dee\u5f02\u4e4b\u4e00\uff1a Clang is designed as an API from its inception, allowing it to be reused by source analysis tools, refactoring, IDEs (etc) as well as for code generation. GCC is built as a monolithic static compiler, which makes it extremely difficult to use as an API and integrate into other tools. Further, its historic design and current policy makes it difficult to decouple the front-end from the rest of the compiler. \u663e\u7136\uff0cclang\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e00\u79cd\u7b56\u7565\uff0cgcc\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e8c\u79cd\u7b56\u7565\uff0c\u6b63\u5982\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4f5c\u8005\u63d0\u51fa\u7684\u89c2\u70b9 differences in goals lead to different strengths and weaknesses \u6211\u89c9\u5f97\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002\u5728clang\u7684\u5b98\u65b9\u6587\u6863\u4e2d\uff0c\u5c06clang\u7684\u8fd9\u79cd\u8bbe\u8ba1\u7b56\u7565\u79f0\u4e3a\uff1a Library Based Architecture \u3002 \u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u67b6\u6784\u6240\u91c7\u53d6\u7684\u622a\u7136\u4e0d\u540c\u7684\u4e24\u79cd\u7b56\u7565\u662f\u8f6f\u4ef6\u8bbe\u8ba1\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u5178\u578b\u7684\u95ee\u9898\u3002 \u629b\u5f00\u8f6f\u4ef6\u8bbe\u8ba1\u4e0d\u8c08\uff0c\u56de\u5f52\u672c\u4e66\u3002\u672c\u4e66\u6240\u8bb2\u8ff0\u7684\u662f\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u662f\u7eaf\u7406\u8bba\u4e0a\u7684\uff0c\u5982\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u6784\u6210\uff0c\u672c\u4e66\u4f1a\u5206\u7ae0\u8282\u6765\u4ecb\u7ecd\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u5206\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u4ece\u524d\u7aef\u5230\u540e\u7aef\u3002\u5b9e\u8df5\u662f\u6709\u52a9\u4e8e\u7406\u8bba\u7684\u7406\u89e3\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u5fc5\u987b\u8981\u4f7f\u7528\u4e00\u4e9b\u5de5\u5177\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u7406\u8bba\u3002clang\u4f5c\u4e3a\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u522b\u7684\u5b8c\u6574\u7684\u3001\u6210\u719f\u7684\u7f16\u8bd1\u5668\uff0c\u5b83\u5b9e\u73b0\u4e86\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u5404\u4e2a\u90e8\u5206\uff0c\u800c\u4e14\u5b83\u7684 Library Based Architecture \uff0c\u4f7f\u5f97\u57fa\u672c\u4e0a\u7f16\u8bd1\u5668\u7684\u6bcf\u4e2a\u90e8\u5206\u90fd\u5bf9\u5e94\u4e3a\u5b83\u7684\u4e00\u4e2a\u5e93\uff0c\u8fd9\u4e9b\u5e93\u6709\u7740\u7b80\u6d01\u7684API\uff0c\u5e76\u4e14\u6587\u6863\u8be6\u5c3d\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528clang\u6765\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u597d\u5de5\u5177\u3002 \u9664\u6b64\u4e4b\u5916\uff0c python \u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u539f\u7406\u7684\u975e\u5e38\u597d\u7684\u5de5\u5177\uff1a Design of CPython\u2019s Compiler \u00b6 \uff0c\u5176\u4e2d\u7ed9\u51facpython\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1 Python Language Services \u00b6 \uff0c\u8fd9\u662fpython\u6807\u51c6\u5e93\u63d0\u4f9b\u7684module\uff0cthese modules support tokenizing, parsing, syntax analysis, bytecode disassembly, and various other facilities. \u7ed3\u5408\u8fd9\u4e9b\u5de5\u5177\uff0c\u6211\u4eec\u80fd\u591f\u8be6\u5c3d\u5730\u89c2\u5bdf\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u5de5\u4f5c\u539f\u7406\u3002 1.2.1 Lexical Analysis The first phase of a compiler is called lexical analysis or scanning . The lexical analyzer reads the stream of characters making up the source program and groups the characters into meaningful sequences called lexemes . For each lexeme , the lexical analyzer produces as output a token of the form <token-name, attribute-value> that it passes on to the subsequent phase, syntax analysis . In the token, the first component token-name is an abstract symbol that is used during syntax analysis , and the second component attribute-value points to an entry in the symbol table for this token. Information from the symbol-table entry is needed for semantic analysis and code generation . For example, suppose a source program contains the assignment statement position = initial + rate * 60 (1.1) The characters in this assignment could be grouped into the following lexemes and mapped into the following tokens passed on to the syntax analyzer : 1\u3001 position is a lexeme that would be mapped into a token <id, 1> , where id is an abstract symbol standing for identifier and 1 points to the symbol-table entry for position . The symbol-table entry for an identifier holds information about the identifier, such as its name and type . 2\u3001The assignment symbol = is a lexeme that is mapped into the token <=> . Since this token needs no attribute-value, we have omitted the second component. We could have used any abstract symbol such as assign for the token-name, but for notational convenience we have chosen to use the lexeme itself as the name of the abstract symbol. 3\u3001 initial is a lexeme that is mapped into the token <id, 2> , where 2 points to the symbol-table entry for initial . 4\u3001 + is a lexeme that is mapped into the token <+> . 5\u3001 rate is a lexeme that is mapped into the token <id, 3> , where 3 points to the symbol-table entry for rate . 6\u3001 * is a lexeme that is mapped into the token <*> . 7\u3001 60 is a lexeme that is mapped into the token <60> . Technically speaking, for the lexeme 60 we should make up a token like <number, 4> , where 4 points to the symbol table for the internal representation of integer 60 but we shall defer the discussion of tokens for numbers until Chapter 2. Chapter 3 discusses techniques for building lexical analyzers. Blanks separating the lexemes would b e discarded by the lexical analyzer. Figure 1.7 shows the representation of the assignment statement (1.1) after lexical analysis as the sequence of tokens <id, 1> <=> <id, 2> <+> <id, 3> <?> <60> (1.2) NOTE: \u4e00\u3001Lexical Analysis\u4e3b\u8981\u5728Chapter 3 Lexical Analysis\u4e2d\u8bb2\u8ff0\u3002 \u4e8c\u3001 Clang \u4e2d\u7531 liblex \u6765\u5b9e\u73b0Lexical Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Lexer and Preprocessor Library \u00b6 1.2.2 Syntax Analysis The second phase of the compiler is syntax analysis or parsing . The parser uses the first components of the tokens produced by the lexical analyzer to create a tree-like intermediate representation that depicts(\u63cf\u8ff0) the grammatical structure of the token stream. A typical representation is a syntax tree in which each interior node represents an operation and the children of the node represent the arguments of the operation. A syntax tree for the token stream (1.2) is shown as the output of the syntactic analyzer in Fig. 1.7. This tree shows the order in which the operations in the assignment position = initial + rate * 60 are to be performed. The tree has an interior node labeled * with <id, 3> as its left child and the integer 60 as its right child. The node <id, 3> represents the identifier rate . The node labeled * makes it explicit that we must first multiply the value of rate by 60 . The node labeled + indicates that we must add the result of this multiplication to the value of initial. The root of the tree, labeled = , indicates that we must store the result of this addition into the location for the identifier position . This ordering of operations is consistent with the usual conventions of arithmetic which tell us that multiplication has higher precedence than addition, and hence that the multiplication is to be performed before the addition. The subsequent phases of the compiler use the grammatical structure to help analyze the source program and generate the target program. In Chapter 4 we shall use context-free grammars to specify the grammatical structure of programming languages and discuss algorithms for constructing efficient syntax analyzers automatically from certain classes of grammars. In Chapters 2 and 5 we shall see that syntax-directed definitions can help specify the translation of programming language constructs. NOTE: Clang \u4e2d\u7531 libparse \u6765\u5b9e\u73b0Syntax Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Parser Library \u00b6 1.2.3 Semantic Analysis The semantic analyzer uses the syntax tree and the information in the symbol table to check the source program for semantic consistency with the language definition. It also gathers type information and saves it in either the syntax tree or the symbol table , for subsequent use during intermediate-code generation. An important part of semantic analysis is type checking , where the compiler checks that each operator has matching operands. For example, many programming language definitions require an array index to be an integer; the compiler must report an error if a floating-point number is used to index an array. The language specification may permit some type conversions called coercions . For example, a binary arithmetic operator may be applied to either a pair of integers or to a pair of floating-point numbers. If the operator is applied to a floating-point number and an integer, the compiler may convert or coerce the integer into a floating-point number. In Fig. 1.7, notice that the output of the semantic analyzer has an extra node for the operator inttofloat , which explicitly converts its integer argument into a floating-point number. Type checking and semantic analysis are discussed in Chapter 6. NOTE: Clang \u4e2d\u7531 **libsema ** \u6765\u5b9e\u73b0Semantic Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Sema Library \u00b6 1.2.4 Intermediate Code Generation In the process of translating a source program into target code, a compiler may construct one or more intermediate representations , which can have a variety of forms. Syntax trees are a form of intermediate representation ; they are commonly used during syntax and semantic analysis. After syntax and semantic analysis of the source program, many compilers generate an explicit low-level or machine-like intermediate representation, which we can think of as a program for an abstract machine. This intermediate representation should have two important properties: 1\u3001it should be easy to produce 2\u3001it should be easy to translate into the target machine. In Chapter 6, we consider an intermediate form called three-address code, which consists of a sequence of assembly-like instructions with three operands per instruction. Each operand can act like a register. The output of the intermediate code generator in Fig. 1.7 consists of the three-address code sequence In Chapter 6, we cover the principal intermediate representations used in compilers. Chapter 5 introduces techniques for syntax-directed translation that are applied in Chapter 6 to type checking and intermediate-code generation for typical programming language constructs such as expressions, flow-of-control constructs, and procedure calls. 1.2.5 Code Optimization The machine-independent code-optimization phase attempts to improve the intermediate code so that better target code will result. Usually better means faster, but other objectives may be desired, such as shorter code, or target code that consumes less power. For example, a straightforward algorithm generates the intermediate code (1.3), using an instruction for each operator in the tree representation that comes from the semantic analyzer. A simple intermediate code generation algorithm followed by code optimization is a reasonable way to generate good target code. The optimizer can deduce that the conversion of 60 from integer to floating point can be done once and for all at compile time, so the inttofloat operation can be eliminated by replacing the integer 60 by the floating-point number 60.0. Moreover, t3 is used only once to transmit its value to id1 so the optimizer can transform (1.3) into the shorter sequence t1 = id3 * 60.0 id1 = id2 + t1 (1.4) There is a great variation in the amount of code optimization different compilers perform. In those that do the most, the so-called \"optimizing compilers,\" a significant amount of time is spent on this phase. There are simple optimizations that significantly improve the running time of the target program without slowing down compilation too much. The chapters from 8 on discuss machine-independent and machine-dependent optimizations in detail. 1.2.6 Code Generation The code generator takes as input an intermediate representation of the source program and maps it into the target language. If the target language is machine code, registers or memory locations are selected for each of the variables used by the program. Then, the intermediate instructions are translated into sequences of machine instructions that perform the same task. A crucial aspect of code generation is the judicious assignment of registers to hold variables. For example, using registers R1 and R2 , the intermediate code in (1.4) might get translated into the machine code LDF R2, id3 MULF R2, R2, #60.0 LDF R1, id2 ADDF R1, R1, R2 STF id1, R1 (1.5) 1.2.7 Symbol-Table Management An essential function of a compiler is to record the variable names used in the source program and collect information about various attributes of each name. These attributes may provide information about the storage allocated for a name, its type, its scope (where in the program its value may b e used), and in the case of procedure names, such things as the number and types of its arguments, the method of passing each argument (for example, by value or by reference), and the type returned. The symbol table is a data structure containing a record for each variable name, with fields for the attributes of the name. The data structure should be designed to allow the compiler to find the record for each name quickly and to store or retrieve data from that record quickly. Symbol tables are discussed in Chapter 2. 1.2.8 The Grouping of Phases into Passes 1.2.9 Compiler-Construction Tools The compiler writer, like any software developer, can profitably use modern software development environments containing tools such as language editors, debuggers, version managers, profilers, test harnesses, and so on. In addition to these general software-development tools, other more specialized tools have been created to help implement various phases of a compiler. These tools use specialized languages for specifying and implementing specific components, and many use quite sophisticated algorithms. The most successful tools are those that hide the details of the generation algorithm and produce components that can be easily integrated into the remainder of the compiler. Some commonly used compiler-construction tools include 1\u3001 Parser generators that automatically produce syntax analyzers from a grammatical description of a programming language. 2\u3001 Scanner generators that produce lexical analyzers from a regular-expression description of the tokens of a language. 3\u3001 Syntax-directed translation engines that produce collections of routines for walking a parse tree and generating intermediate code . 4\u3001 Code-generator generators that produce a code generator from a collection of rules for translating each operation of the intermediate language into the machine language for a target machine. 5\u3001 Data-flow analysis engines that facilitate the gathering of information about how values are transmitted from one part of a program to each other part. Data-flow analysis is a key part of code optimization. 6\u3001Compiler-construction toolkits that provide an integrated set of routines for constructing various phases of a compiler. We shall describe many of these tools throughout this book. NOTE: Wikipedia\u7684 Comparison of parser generators \u603b\u7ed3\u4e86parser generator\u548cscanner generator\u3002","title":"Introduction"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#12#the#structure#of#a#compiler","text":"Up to this point we have treated a compiler as a single box that maps a source program into a semantically equivalent target program. If we open up this box a little, we see that there are two parts to this mapping: 1\u3001analysis 2\u3001synthesis The analysis part breaks up the source program into constituent pieces and imposes a grammatical structure on them. It then uses this structure to create an intermediate representation of the source program. If the analysis part detects that the source program is either syntactically ill formed or semantically unsound, then it must provide informative messages, so the user can take corrective action. The analysis part also collects information about the source program and stores it in a data structure called a symbol table , which is passed along with the intermediate representation to the synthesis part . The synthesis part constructs the desired target program from the intermediate representation and the information in the symbol table. The analysis part is often called the front end of the compiler; the synthesis part is the back end . If we examine the compilation process in more detail, we see that it operates as a sequence of phases , each of which transforms one representation of the source program to another. A typical decomposition of a compiler into phases is shown in Fig. 1.6. In practice, several phases may be grouped together, and the intermediate representations between the grouped phases need not be constructed explicitly. The symbol table , which stores information about the entire source program, is used by all phases of the compiler. NOTE: \u7b26\u53f7\u8868\u4e5f\u662f\u4e00\u95e8\u79d1\u5b66\uff0c\u9700\u8981\u5bf9\u5b83\u4ed4\u7ec6\u7814\u7a76\uff0c\u672c\u4e66\u4e2d\u6d89\u53ca\u7b26\u53f7\u8868\u7684\u7ae0\u8282\u6709\uff1a 1.2.1 Lexical Analysis 1.2.7 Symbol-Table Management 2.7 Symbol Tables \u5173\u4e8esymbol table\u7684\u4e3b\u8981\u5185\u5bb9\u653e\u5230\u4e86 Chapter-2-A-Simple-Syntax-Directed-Translator\\2.7-Symbol-Tables \u4e2d\u3002 Some compilers have a machine-independent optimization phase between the front end and the back end . The purpose of this optimization phase is to perform transformations on the intermediate representation , so that the back end can produce a better target program than it would have otherwise produced from an unoptimized intermediate representation. Since optimization is optional, one or the other of the two optimization phases shown in Fig. 1.6 may be missing. NOTE: \u56fe1.6\u7ed9\u51fa\u4e86\u4e00\u4e2acompiler\u7684\u67b6\u6784\uff0c\u8fd9\u4e2a\u67b6\u6784\u662f\u6e05\u6670\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u601d\u8003\u4e00\u4e0b\u5982\u4f55\u6765\u5b9e\u73b0\u4e00\u4e2a\u7f16\u8bd1\u5668\uff0c\u5982\u4e0b\u662f\u4e24\u79cd\u7b56\u7565\uff1a \u5206\u5f00\u5b9e\u73b0\u5404\u4e2a\u90e8\u4ef6\u7136\u540e\u5c06\u5b83\u4eec\u7ec4\u88c5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u7f16\u8bd1\u5668\uff0c\u6bcf\u4e2a\u90e8\u4ef6\u53ef\u4ee5\u72ec\u7acb\u5de5\u4f5c\uff08\u8026\u5408\u5ea6\u4f4e\uff09 \u6240\u6709\u529f\u80fd\u96c6\u6210\u8d77\u6765\u5f62\u6210\u4e00\u4e2amonolithic compiler\uff0c\u4e0d\u80fd\u72ec\u7acb\u5730\u4f7f\u7528\u5176\u4e2d\u67d0\u4e2a\u90e8\u4ef6\uff08\u8026\u5408\u5ea6\u9ad8\uff09 \u76ee\u524dc\u7cfb\u8bed\u8a00\u4e2d\u6700\u6d41\u884c\u7684\u4e24\u6b3e\u7f16\u8bd1\u5668\uff1a gcc \u548c clang \u3002\u5728clang\u7684\u5b98\u65b9\u7f51\u7ad9\u7684\u6587\u7ae0 Clang vs Other Open Source Compilers \u4e2d\uff0c\u7ed9\u51fa\u4e86\u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u5bf9\u6bd4\uff08\u5404\u81ea\u7684\u957f\u5904\u4e0e\u77ed\u5904\uff09\uff0c\u4e0b\u9762\u662f\u4e24\u79cd\u7684\u5dee\u5f02\u4e4b\u4e00\uff1a Clang is designed as an API from its inception, allowing it to be reused by source analysis tools, refactoring, IDEs (etc) as well as for code generation. GCC is built as a monolithic static compiler, which makes it extremely difficult to use as an API and integrate into other tools. Further, its historic design and current policy makes it difficult to decouple the front-end from the rest of the compiler. \u663e\u7136\uff0cclang\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e00\u79cd\u7b56\u7565\uff0cgcc\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e8c\u79cd\u7b56\u7565\uff0c\u6b63\u5982\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4f5c\u8005\u63d0\u51fa\u7684\u89c2\u70b9 differences in goals lead to different strengths and weaknesses \u6211\u89c9\u5f97\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002\u5728clang\u7684\u5b98\u65b9\u6587\u6863\u4e2d\uff0c\u5c06clang\u7684\u8fd9\u79cd\u8bbe\u8ba1\u7b56\u7565\u79f0\u4e3a\uff1a Library Based Architecture \u3002 \u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u67b6\u6784\u6240\u91c7\u53d6\u7684\u622a\u7136\u4e0d\u540c\u7684\u4e24\u79cd\u7b56\u7565\u662f\u8f6f\u4ef6\u8bbe\u8ba1\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u5178\u578b\u7684\u95ee\u9898\u3002 \u629b\u5f00\u8f6f\u4ef6\u8bbe\u8ba1\u4e0d\u8c08\uff0c\u56de\u5f52\u672c\u4e66\u3002\u672c\u4e66\u6240\u8bb2\u8ff0\u7684\u662f\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u662f\u7eaf\u7406\u8bba\u4e0a\u7684\uff0c\u5982\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u6784\u6210\uff0c\u672c\u4e66\u4f1a\u5206\u7ae0\u8282\u6765\u4ecb\u7ecd\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u5206\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u4ece\u524d\u7aef\u5230\u540e\u7aef\u3002\u5b9e\u8df5\u662f\u6709\u52a9\u4e8e\u7406\u8bba\u7684\u7406\u89e3\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u5fc5\u987b\u8981\u4f7f\u7528\u4e00\u4e9b\u5de5\u5177\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u7406\u8bba\u3002clang\u4f5c\u4e3a\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u522b\u7684\u5b8c\u6574\u7684\u3001\u6210\u719f\u7684\u7f16\u8bd1\u5668\uff0c\u5b83\u5b9e\u73b0\u4e86\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u5404\u4e2a\u90e8\u5206\uff0c\u800c\u4e14\u5b83\u7684 Library Based Architecture \uff0c\u4f7f\u5f97\u57fa\u672c\u4e0a\u7f16\u8bd1\u5668\u7684\u6bcf\u4e2a\u90e8\u5206\u90fd\u5bf9\u5e94\u4e3a\u5b83\u7684\u4e00\u4e2a\u5e93\uff0c\u8fd9\u4e9b\u5e93\u6709\u7740\u7b80\u6d01\u7684API\uff0c\u5e76\u4e14\u6587\u6863\u8be6\u5c3d\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528clang\u6765\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u597d\u5de5\u5177\u3002 \u9664\u6b64\u4e4b\u5916\uff0c python \u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u539f\u7406\u7684\u975e\u5e38\u597d\u7684\u5de5\u5177\uff1a Design of CPython\u2019s Compiler \u00b6 \uff0c\u5176\u4e2d\u7ed9\u51facpython\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1 Python Language Services \u00b6 \uff0c\u8fd9\u662fpython\u6807\u51c6\u5e93\u63d0\u4f9b\u7684module\uff0cthese modules support tokenizing, parsing, syntax analysis, bytecode disassembly, and various other facilities. \u7ed3\u5408\u8fd9\u4e9b\u5de5\u5177\uff0c\u6211\u4eec\u80fd\u591f\u8be6\u5c3d\u5730\u89c2\u5bdf\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u5de5\u4f5c\u539f\u7406\u3002","title":"1.2 The Structure of a Compiler"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#121#lexical#analysis","text":"The first phase of a compiler is called lexical analysis or scanning . The lexical analyzer reads the stream of characters making up the source program and groups the characters into meaningful sequences called lexemes . For each lexeme , the lexical analyzer produces as output a token of the form <token-name, attribute-value> that it passes on to the subsequent phase, syntax analysis . In the token, the first component token-name is an abstract symbol that is used during syntax analysis , and the second component attribute-value points to an entry in the symbol table for this token. Information from the symbol-table entry is needed for semantic analysis and code generation . For example, suppose a source program contains the assignment statement position = initial + rate * 60 (1.1) The characters in this assignment could be grouped into the following lexemes and mapped into the following tokens passed on to the syntax analyzer : 1\u3001 position is a lexeme that would be mapped into a token <id, 1> , where id is an abstract symbol standing for identifier and 1 points to the symbol-table entry for position . The symbol-table entry for an identifier holds information about the identifier, such as its name and type . 2\u3001The assignment symbol = is a lexeme that is mapped into the token <=> . Since this token needs no attribute-value, we have omitted the second component. We could have used any abstract symbol such as assign for the token-name, but for notational convenience we have chosen to use the lexeme itself as the name of the abstract symbol. 3\u3001 initial is a lexeme that is mapped into the token <id, 2> , where 2 points to the symbol-table entry for initial . 4\u3001 + is a lexeme that is mapped into the token <+> . 5\u3001 rate is a lexeme that is mapped into the token <id, 3> , where 3 points to the symbol-table entry for rate . 6\u3001 * is a lexeme that is mapped into the token <*> . 7\u3001 60 is a lexeme that is mapped into the token <60> . Technically speaking, for the lexeme 60 we should make up a token like <number, 4> , where 4 points to the symbol table for the internal representation of integer 60 but we shall defer the discussion of tokens for numbers until Chapter 2. Chapter 3 discusses techniques for building lexical analyzers. Blanks separating the lexemes would b e discarded by the lexical analyzer. Figure 1.7 shows the representation of the assignment statement (1.1) after lexical analysis as the sequence of tokens <id, 1> <=> <id, 2> <+> <id, 3> <?> <60> (1.2) NOTE: \u4e00\u3001Lexical Analysis\u4e3b\u8981\u5728Chapter 3 Lexical Analysis\u4e2d\u8bb2\u8ff0\u3002 \u4e8c\u3001 Clang \u4e2d\u7531 liblex \u6765\u5b9e\u73b0Lexical Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Lexer and Preprocessor Library \u00b6","title":"1.2.1 Lexical Analysis"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#122#syntax#analysis","text":"The second phase of the compiler is syntax analysis or parsing . The parser uses the first components of the tokens produced by the lexical analyzer to create a tree-like intermediate representation that depicts(\u63cf\u8ff0) the grammatical structure of the token stream. A typical representation is a syntax tree in which each interior node represents an operation and the children of the node represent the arguments of the operation. A syntax tree for the token stream (1.2) is shown as the output of the syntactic analyzer in Fig. 1.7. This tree shows the order in which the operations in the assignment position = initial + rate * 60 are to be performed. The tree has an interior node labeled * with <id, 3> as its left child and the integer 60 as its right child. The node <id, 3> represents the identifier rate . The node labeled * makes it explicit that we must first multiply the value of rate by 60 . The node labeled + indicates that we must add the result of this multiplication to the value of initial. The root of the tree, labeled = , indicates that we must store the result of this addition into the location for the identifier position . This ordering of operations is consistent with the usual conventions of arithmetic which tell us that multiplication has higher precedence than addition, and hence that the multiplication is to be performed before the addition. The subsequent phases of the compiler use the grammatical structure to help analyze the source program and generate the target program. In Chapter 4 we shall use context-free grammars to specify the grammatical structure of programming languages and discuss algorithms for constructing efficient syntax analyzers automatically from certain classes of grammars. In Chapters 2 and 5 we shall see that syntax-directed definitions can help specify the translation of programming language constructs. NOTE: Clang \u4e2d\u7531 libparse \u6765\u5b9e\u73b0Syntax Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Parser Library \u00b6","title":"1.2.2 Syntax Analysis"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#123#semantic#analysis","text":"The semantic analyzer uses the syntax tree and the information in the symbol table to check the source program for semantic consistency with the language definition. It also gathers type information and saves it in either the syntax tree or the symbol table , for subsequent use during intermediate-code generation. An important part of semantic analysis is type checking , where the compiler checks that each operator has matching operands. For example, many programming language definitions require an array index to be an integer; the compiler must report an error if a floating-point number is used to index an array. The language specification may permit some type conversions called coercions . For example, a binary arithmetic operator may be applied to either a pair of integers or to a pair of floating-point numbers. If the operator is applied to a floating-point number and an integer, the compiler may convert or coerce the integer into a floating-point number. In Fig. 1.7, notice that the output of the semantic analyzer has an extra node for the operator inttofloat , which explicitly converts its integer argument into a floating-point number. Type checking and semantic analysis are discussed in Chapter 6. NOTE: Clang \u4e2d\u7531 **libsema ** \u6765\u5b9e\u73b0Semantic Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Sema Library \u00b6","title":"1.2.3 Semantic Analysis"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#124#intermediate#code#generation","text":"In the process of translating a source program into target code, a compiler may construct one or more intermediate representations , which can have a variety of forms. Syntax trees are a form of intermediate representation ; they are commonly used during syntax and semantic analysis. After syntax and semantic analysis of the source program, many compilers generate an explicit low-level or machine-like intermediate representation, which we can think of as a program for an abstract machine. This intermediate representation should have two important properties: 1\u3001it should be easy to produce 2\u3001it should be easy to translate into the target machine. In Chapter 6, we consider an intermediate form called three-address code, which consists of a sequence of assembly-like instructions with three operands per instruction. Each operand can act like a register. The output of the intermediate code generator in Fig. 1.7 consists of the three-address code sequence In Chapter 6, we cover the principal intermediate representations used in compilers. Chapter 5 introduces techniques for syntax-directed translation that are applied in Chapter 6 to type checking and intermediate-code generation for typical programming language constructs such as expressions, flow-of-control constructs, and procedure calls.","title":"1.2.4 Intermediate Code Generation"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#125#code#optimization","text":"The machine-independent code-optimization phase attempts to improve the intermediate code so that better target code will result. Usually better means faster, but other objectives may be desired, such as shorter code, or target code that consumes less power. For example, a straightforward algorithm generates the intermediate code (1.3), using an instruction for each operator in the tree representation that comes from the semantic analyzer. A simple intermediate code generation algorithm followed by code optimization is a reasonable way to generate good target code. The optimizer can deduce that the conversion of 60 from integer to floating point can be done once and for all at compile time, so the inttofloat operation can be eliminated by replacing the integer 60 by the floating-point number 60.0. Moreover, t3 is used only once to transmit its value to id1 so the optimizer can transform (1.3) into the shorter sequence t1 = id3 * 60.0 id1 = id2 + t1 (1.4) There is a great variation in the amount of code optimization different compilers perform. In those that do the most, the so-called \"optimizing compilers,\" a significant amount of time is spent on this phase. There are simple optimizations that significantly improve the running time of the target program without slowing down compilation too much. The chapters from 8 on discuss machine-independent and machine-dependent optimizations in detail.","title":"1.2.5 Code Optimization"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#126#code#generation","text":"The code generator takes as input an intermediate representation of the source program and maps it into the target language. If the target language is machine code, registers or memory locations are selected for each of the variables used by the program. Then, the intermediate instructions are translated into sequences of machine instructions that perform the same task. A crucial aspect of code generation is the judicious assignment of registers to hold variables. For example, using registers R1 and R2 , the intermediate code in (1.4) might get translated into the machine code LDF R2, id3 MULF R2, R2, #60.0 LDF R1, id2 ADDF R1, R1, R2 STF id1, R1 (1.5)","title":"1.2.6 Code Generation"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#127#symbol-table#management","text":"An essential function of a compiler is to record the variable names used in the source program and collect information about various attributes of each name. These attributes may provide information about the storage allocated for a name, its type, its scope (where in the program its value may b e used), and in the case of procedure names, such things as the number and types of its arguments, the method of passing each argument (for example, by value or by reference), and the type returned. The symbol table is a data structure containing a record for each variable name, with fields for the attributes of the name. The data structure should be designed to allow the compiler to find the record for each name quickly and to store or retrieve data from that record quickly. Symbol tables are discussed in Chapter 2.","title":"1.2.7 Symbol-Table Management"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#128#the#grouping#of#phases#into#passes","text":"","title":"1.2.8 The Grouping of Phases into Passes"},{"location":"1-Introduction/1.2-The-Structure-of-a-Compiler/#129#compiler-construction#tools","text":"The compiler writer, like any software developer, can profitably use modern software development environments containing tools such as language editors, debuggers, version managers, profilers, test harnesses, and so on. In addition to these general software-development tools, other more specialized tools have been created to help implement various phases of a compiler. These tools use specialized languages for specifying and implementing specific components, and many use quite sophisticated algorithms. The most successful tools are those that hide the details of the generation algorithm and produce components that can be easily integrated into the remainder of the compiler. Some commonly used compiler-construction tools include 1\u3001 Parser generators that automatically produce syntax analyzers from a grammatical description of a programming language. 2\u3001 Scanner generators that produce lexical analyzers from a regular-expression description of the tokens of a language. 3\u3001 Syntax-directed translation engines that produce collections of routines for walking a parse tree and generating intermediate code . 4\u3001 Code-generator generators that produce a code generator from a collection of rules for translating each operation of the intermediate language into the machine language for a target machine. 5\u3001 Data-flow analysis engines that facilitate the gathering of information about how values are transmitted from one part of a program to each other part. Data-flow analysis is a key part of code optimization. 6\u3001Compiler-construction toolkits that provide an integrated set of routines for constructing various phases of a compiler. We shall describe many of these tools throughout this book. NOTE: Wikipedia\u7684 Comparison of parser generators \u603b\u7ed3\u4e86parser generator\u548cscanner generator\u3002","title":"1.2.9 Compiler-Construction Tools"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/","text":"1.5 Applications of Compiler Technology Compiler design is not only about compilers, and many people use the technology learned by studying compilers in school, yet have never, strictly speaking, written (even part of ) a compiler for a major programming language. Compiler technology has other important uses as well. Additionally, compiler design impacts several other areas of computer science. In this section, we review the most important interactions and applications of the technology. 1.5.1 Implementation of High-Level Programming Languages A high-level programming language defines a programming abstraction: the programmer expresses an algorithm using the language, and the compiler must translate that program to the target language. 1.5.2 Optimizations for Computer Architectures The rapid evolution of computer architectures has also led to an insatiable demand for new compiler technology. Almost all high-performance systems take advantage of the same two basic techniques: parallelism and memory hierarchies . Parallelism can be found at several levels: at the instruction level, where multiple operations are executed simultaneously and at the processor level, where different threads of the same application are run on different processors. Memory hierarchies are a response to the basic limitation that we can build very fast storage or very large storage, but not storage that is both fast and large. 1.5.3 Design of New Computer Architectures In the early days of computer architecture design, compilers were developed after the machines were built. That has changed. Since programming in high-level languages is the norm, the performance of a computer system is determined not by its raw speed but also by how well compilers can exploit its features. Thus, in modern computer architecture development, compilers are developed in the processor-design stage, and compiled code, running on simulators, is used to evaluate the proposed architectural features. 1.5.4 Program Translations While we normally think of compiling as a translation from a high-level language to the machine level, the same technology can be applied to translate between different kinds of languages. The following are some of the important applications of program-translation techniques. 1.5.5 Software Productivity Tools Programs are arguably the most complicated engineering artifacts ever produced; they consist of many many details, every one of which must be correct before the program will work completely. As a result, errors are rampant in programs; errors may crash a system, produce wrong results, render a system vulnerable to security attacks, or even lead to catastrophic failures in critical systems. Testing is the primary technique for locating errors in programs. An interesting and promising complementary approach is to use data-flow analysis to locate errors statically (that is, before the program is run). Data-flow analysis can find errors along all the possible execution paths, and not just those exercised by the input data sets, as in the case of program testing. Many of the data-flow-analysis techniques, originally developed for compiler optimizations , can be used to create tools that assist programmers in their software engineering tasks. The problem of finding all program errors is undecidable. A data-flow analysis may be designed to warn the programmers of all possible statements with a particular category of errors. But if most of these warnings are false alarms, users will not use the tool. Thus, practical error detectors are often neither sound nor complete. That is, they may not find all the errors in the program, and not all errors reported are guaranteed to be real errors. Nonetheless, various static analyses have been developed and shown to be effective in finding errors, such as dereferencing null or freed pointers, in real programs. The fact that error detectors may be unsound makes them significantly different from compiler optimizations. Optimizers must be conservative and cannot alter the semantics of the program under any circumstances. In the balance of this section, we shall mention several ways in which program analysis, building up on techniques originally develop ed to optimize code in compilers, have improved software productivity. Of special importance are techniques that detect statically when a program might have a security vulnerability. Type Checking Bounds Checking Memory-Management Tools Garbage collection is another excellent example of the tradeoff between efficiency and a combination of ease of programming and software reliability. Automatic memory management obliterates all memory-management errors (e.g., \"memory leaks\"), which are a major source of problems in C and C++ programs. Various tools have been developed to help programmers find memory management errors. For example, Purify is a widely used tool that dynamically catches memory management errors as they occur. Tools that help identify some of these problems statically have also been developed. NOTE: \u73b0\u4ee3IDE\u4e00\u822c\u90fd\u4f1a\u96c6\u6210\u5f88\u591asoftware productivity tools\uff0c\u6bd4\u5982\u5728 pycharm \u4e2d\u3002 See also\uff1a List of tools for static code analysis Source Code Analysis Tools code inspection tools \u5728\u5b66\u4e60\u4e86\u672c\u4e66\u4e2d\u7684\u5185\u5bb9\u540e\uff0c\u5176\u5b9e\u8bfb\u8005\u53ef\u4ee5\u5927\u81f4\u63a8\u6d4b\u8fd9\u4e9b\u5de5\u5177\u7684\u539f\u7406\u4e86\u3002 \u663e\u7136\u8fd9\u4e9b\u90fd\u662f\u5e2e\u52a9 Software quality assurance \u7684\u6709\u6548\u5de5\u5177\u3002","title":"Introduction"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#15#applications#of#compiler#technology","text":"Compiler design is not only about compilers, and many people use the technology learned by studying compilers in school, yet have never, strictly speaking, written (even part of ) a compiler for a major programming language. Compiler technology has other important uses as well. Additionally, compiler design impacts several other areas of computer science. In this section, we review the most important interactions and applications of the technology.","title":"1.5 Applications of Compiler Technology"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#151#implementation#of#high-level#programming#languages","text":"A high-level programming language defines a programming abstraction: the programmer expresses an algorithm using the language, and the compiler must translate that program to the target language.","title":"1.5.1 Implementation of High-Level Programming Languages"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#152#optimizations#for#computer#architectures","text":"The rapid evolution of computer architectures has also led to an insatiable demand for new compiler technology. Almost all high-performance systems take advantage of the same two basic techniques: parallelism and memory hierarchies . Parallelism can be found at several levels: at the instruction level, where multiple operations are executed simultaneously and at the processor level, where different threads of the same application are run on different processors. Memory hierarchies are a response to the basic limitation that we can build very fast storage or very large storage, but not storage that is both fast and large.","title":"1.5.2 Optimizations for Computer Architectures"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#153#design#of#new#computer#architectures","text":"In the early days of computer architecture design, compilers were developed after the machines were built. That has changed. Since programming in high-level languages is the norm, the performance of a computer system is determined not by its raw speed but also by how well compilers can exploit its features. Thus, in modern computer architecture development, compilers are developed in the processor-design stage, and compiled code, running on simulators, is used to evaluate the proposed architectural features.","title":"1.5.3 Design of New Computer Architectures"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#154#program#translations","text":"While we normally think of compiling as a translation from a high-level language to the machine level, the same technology can be applied to translate between different kinds of languages. The following are some of the important applications of program-translation techniques.","title":"1.5.4 Program Translations"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#155#software#productivity#tools","text":"Programs are arguably the most complicated engineering artifacts ever produced; they consist of many many details, every one of which must be correct before the program will work completely. As a result, errors are rampant in programs; errors may crash a system, produce wrong results, render a system vulnerable to security attacks, or even lead to catastrophic failures in critical systems. Testing is the primary technique for locating errors in programs. An interesting and promising complementary approach is to use data-flow analysis to locate errors statically (that is, before the program is run). Data-flow analysis can find errors along all the possible execution paths, and not just those exercised by the input data sets, as in the case of program testing. Many of the data-flow-analysis techniques, originally developed for compiler optimizations , can be used to create tools that assist programmers in their software engineering tasks. The problem of finding all program errors is undecidable. A data-flow analysis may be designed to warn the programmers of all possible statements with a particular category of errors. But if most of these warnings are false alarms, users will not use the tool. Thus, practical error detectors are often neither sound nor complete. That is, they may not find all the errors in the program, and not all errors reported are guaranteed to be real errors. Nonetheless, various static analyses have been developed and shown to be effective in finding errors, such as dereferencing null or freed pointers, in real programs. The fact that error detectors may be unsound makes them significantly different from compiler optimizations. Optimizers must be conservative and cannot alter the semantics of the program under any circumstances. In the balance of this section, we shall mention several ways in which program analysis, building up on techniques originally develop ed to optimize code in compilers, have improved software productivity. Of special importance are techniques that detect statically when a program might have a security vulnerability.","title":"1.5.5 Software Productivity Tools"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#type#checking","text":"","title":"Type Checking"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#bounds#checking","text":"","title":"Bounds Checking"},{"location":"1-Introduction/1.5-Applications-of-Compiler-Technology/#memory-management#tools","text":"Garbage collection is another excellent example of the tradeoff between efficiency and a combination of ease of programming and software reliability. Automatic memory management obliterates all memory-management errors (e.g., \"memory leaks\"), which are a major source of problems in C and C++ programs. Various tools have been developed to help programmers find memory management errors. For example, Purify is a widely used tool that dynamically catches memory management errors as they occur. Tools that help identify some of these problems statically have also been developed. NOTE: \u73b0\u4ee3IDE\u4e00\u822c\u90fd\u4f1a\u96c6\u6210\u5f88\u591asoftware productivity tools\uff0c\u6bd4\u5982\u5728 pycharm \u4e2d\u3002 See also\uff1a List of tools for static code analysis Source Code Analysis Tools code inspection tools \u5728\u5b66\u4e60\u4e86\u672c\u4e66\u4e2d\u7684\u5185\u5bb9\u540e\uff0c\u5176\u5b9e\u8bfb\u8005\u53ef\u4ee5\u5927\u81f4\u63a8\u6d4b\u8fd9\u4e9b\u5de5\u5177\u7684\u539f\u7406\u4e86\u3002 \u663e\u7136\u8fd9\u4e9b\u90fd\u662f\u5e2e\u52a9 Software quality assurance \u7684\u6709\u6548\u5de5\u5177\u3002","title":"Memory-Management Tools"},{"location":"1-Introduction/1.6-Programming-Language-Basics/","text":"1.6 Programming Language Basics In this section, we shall cover the most important terminology and distinctions that appear in the study of programming languages. It is not our purpose to cover all concepts or all the popular programming languages. We assume that the reader is familiar with at least one of C, C++ , C#, or Java, and may have encountered other languages as well. 1.6.1 The Static/Dynamic Distinction Among the most important issues that we face when designing a compiler for a language is what decisions can the compiler make about a program. If a language uses a policy that allows the compiler to decide an issue, then we say that the language uses a static policy or that the issue can be decided at compile time . On the other hand, a policy that only allows a decision to be made when we execute the program is said to be a dynamic policy or to require a decision at run time . One issue on which we shall concentrate is the scope of declarations . The scope of a declaration of x is the region of the program in which uses of x refer to this declaration. A language uses static scope or lexical scope if it is possible to determine the scope of a declaration by looking only at the program. Otherwise, the language uses dynamic scope . With dynamic scope , as the program runs, the same use of x could refer to any of several different declarations of x . NOTE:\u4e0a\u9762\u5173\u4e8e**scope**\u7684\u5b9a\u4e49\u6240\u6307\u4ee3\u7684\u662fdeclaration\uff0c\u800c\u975evariable\u3002\u663e\u7136declaration\u80fd\u591f\u6307\u4ee3\u7684\u8303\u56f4\u8fdc\u8fdc\u5927\u4e8evariable\u3002 See also: Scope (computer science) Most languages, such as C and Java, use static scope . We shall discuss static scoping in Section 1.6.3. Example 1.3 : As another example of the static/dynamic distinction , consider the use of the term \"static\" as it applies to data in a Java class declaration. In Java, a variable is a name for a location in memory used to hold a data value . Here, \"static\" refers not to the scope of the variable, but rather to the ability of the compiler to determine the location in memory where the declared variable can be found. A declaration like public static int x ; makes x a class variable and says that there is only one copy of x , no matter how many objects of this class are created. Moreover, the compiler can determine a location in memory where this integer x will be held. In contrast, had \" static \" been omitted from this declaration, then each object of the class would have its own location where x would be held, and the compiler could not determine all these places in advance of running the program. NOTE: \u901a\u8fc7\u5173\u952e\u5b57 static \u6765\u544a\u8bc9compiler\uff1a\u4f60\u53ef\u4ee5\u5728\u8fdb\u884c\u7f16\u8bd1\u7684\u65f6\u5019\u5c31\u7ed9 x \u5206\u914dmemory\uff1b\u800c\u4e0d\u662f\u5728\u8fd0\u884c\u7a0b\u5e8f\u7684\u65f6\u5019\u4f7f\u7528 new \u6765\u521b\u5efa\u5bf9\u8c61\u65f6\u518d\u5206\u914d x \u7684memory\uff0c\u663e\u7136\u8fd9\u662f\u6240\u8c13\u7684static policy\u3002 See also: Static (keyword) 1.6.2 Environments and States Another important distinction we must make when discussing programming languages is whether changes occurring as the program runs affect the values of data elements or affect the interpretation of names for that data. For example,the execution of an assignment such as x = y + 1 changes the value denoted by the name x . More specifically, the assignment changes the value in whatever location is denoted by x . It may be less clear that the location denoted by x can change at run time . For instance, as we discussed in Example 1.3, if x is not a static (or \" class \") variable, then every object of the class has its own location for an instance of variable x. In that case, the assignment to x can change any of those \"instance\" variables, depending on the object to which a method containing that assignment is applied. The association of names with locations in memory (the store) and then with values can be described by two mappings that change as the program runs (see Fig. 1.8): 1 The environment is a mapping from names to locations in the store. Since variables refer to locations (\"l-values\" in the terminology of C), we could alternatively define an environment as a mapping from names to variables ( location ). 2 The state is a mapping from locations in store to their values . That is, the state maps l-values to their corresponding r-values , in the terminology of C. Environments change according to the scope rules of a language. NOTE: environments**\u548c**scope \u662f\u4e0d\u540c\u7684\u6982\u5ff5\u3002 Example 1.4 : Consider the C program fragment in Fig. 1.9. Integer i is declared a global variable, and also declared as a variable local to function f .When f is executing, the environment adjusts so that name i refers to the location reserved for the i that is local to f , and any use of i , such as the assignment i = 3 shown explicitly, refers to that location. Typically, the local i is given a place on the run-time stack int i ; /* global i */ void f (...) { int i ; /* local i */ ... i = 3 ; /* use of local i */ ... } ... x = i + 1 ; /* use of global i */ Whenever a function g other than f is executing, uses of i cannot refer to the i that is local to f . Uses of name i in g must be within the scope of some other declaration of i . An example is the explicitly shown statement x = i+1 ,which is inside some procedure whose definition is not shown. The i in i + 1 presumably\uff08\u5927\u6982\uff09 refers to the global i . As in most languages, declarations in C must precede their use, so a function that comes before the global i cannot refer to it. The environment and state mappings in Fig. 1.8 are dynamic , but there are a few exceptions: 1 Static versus dynamic binding of names to locations . Most binding of names to locations is dynamic , and we discuss several approaches to this binding throughout the section. Some declarations, such as the global i in Fig. 1.9, can be given a location in the store once and for all, as the compiler generates object code(\u4e5f\u5c31\u662fglobal i \u662fstatic). Technically, the C compiler will assign a location in virtual memory for the global i ,leaving it to the loader and the operating system to determine where in the physical memory of the machine i will be located. However, we shall not worry about relocation issues such as these, which have no impact on compiling. Instead, we treat the address space that the compiler uses for its output code as if it gave physical memory locations . 2 Static versus dynamic binding of locations to values . The binding of locations to values (the second stage in Fig. 1.8), is generally dynamic as well, since we cannot tell the value in a location until we run the program. Declared constants are an exception. For instance, the C definition #define ARRAYSIZE 1000 binds the name ARRAYSIZE to the value 1000 statically. We can determine this binding by looking at the statement, and we know that it is impossible for this binding to change when the program executes. 1.6.3 Static Scope and Block Structure Most languages, including C and its family, use static scope _. The scope rules for C are based on program structure ; the scope of a declaration is determined implicitly(\u9690\u5f0f\u5730) by where the declaration appears in the program. Later languages,such as C++, Java, and C#, also provide explicit control over scopes through the use of keywords like public , private , and protected . In this section we consider static-scope rules for a language with blocks , where a block is a grouping of declarations and statements . C uses braces { and } to delimit(\u5b9a\u754c) a block ; the alternative use of begin and end for the same purpose dates back to Algol. NOTE : \u5728 c++ \u4e2d\uff0cblock\u9664\u4e86\u53ef\u4ee5\u7528\u4e8e\u786e\u5b9ascope\u5916\uff0cblock\u4e5f\u53ef\u4ee5\u51b3\u5b9a*storage duration* \u3002\u4ee5\u4e0b\u662f\u6458\u81eacppreference \u7684 Storage class specifiers \u7ae0\u8282\uff1a automatic storage duration. The storage for the object is allocated at the beginning of the enclosing code block and deallocated at the end. All local objects have this storage duration, except those declared static , extern or thread_local . Names , Identifiers , and Variables Although the terms \"name\" and \"variable,\" often refer to the same thing,we use them carefully to distinguish between compile-time names and the run-time locations denoted by names. An identifier is a string of characters, typically letters or digits, that refers to (identifies) an entity , such as a data object , a procedure , a class ,or a type . All identifiers are names , but not all names are identifiers .Names can also be expressions. For example, the name x.y might denote the field y of a structure denoted by x . Here, x and y are identifiers, while x.y is a name, but not an identifier . Composite names like x.y are called qualified names . SUMMARY :\u663e\u7136\uff0cname\u662f\u6bd4identifier\u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\u3002 A variable refers to a particular location of the store. It is common for the same identifier to be declared more than once; each such declaration introduces a new variable . Even if each identifier is declared just once, an identifier local to a recursive procedure will refer to different locations of the store at different times. SUMMARY :\u663e\u7136\uff0cvariable\u66f4\u52a0\u503e\u5411\u4e8e\u8fd0\u884c\u65f6\u6982\u5ff5 Example 1.5 \uff1aC static-scope policy is as follows 1 A C program consists of a sequence of top-level declarations of variables and functions 2 Functions may have variable declarations within them, where variables include local variables and parameters . The scope of each such declaration is restricted to the function in which it appears. 3 The scope of a top-level declaration of a name x consists of the entire program that follows, with the exception of those statements that lie within a function that also has a declaration of x The additional detail regarding the C static-scope policy deals with variable declarations within statements . We examine such declarations next and in Example 1.6. 2 Procedures, Functions, and Methods To avoid saying procedures, functions, or methods, each time we want to talk about a subprogram that may be called, we shall usually refer to all of them as procedures .The exception is that when talking explicitly of programs in languages like C that have only functions, we shall refer to them as functions .Or, if we are discussing a language like Java that has only methods , we shall use that term instead. A function generally returns a value of some type (the return type), while a procedure does not return any value. C and similar languages, which have only functions , treat procedures as functions that have a special return type void , to signify no return value. Object-oriented languages like Java and C++ use the term methods .These can behave like either functions or procedures, but are associated with a particular class. In C, the syntax of blocks is given by 1 One type of statement is a block(block\u662f\u4e00\u79cdstatement). Blocks can appear anywhere that other types of statements, such as assignment statements, can appear. 2 A block is a sequence of declarations followed by a sequence of statements,all surrounded by braces Note that this syntax allows blocks to be nested inside each other. This nesting property is referred to as block structure . The C family of languages has block structure, except that a function may not be defined inside another function. We say that a declaration D belongs to a block B if B is the most closely nested block containing D ; that is, D is located within B , but not within any block that is nested within B . The static-scope rule for variable declarations in block-structured languages is as follows. If declaration D of name x belongs to block B , then the scope of D is all of B , except for any blocks B' nested to any depth within B , in which x is redeclared. Here, x is redeclared in B' if some other declaration D of the same name x belongs to B' . An equivalent way to express this rule is to focus on a use of a name x .Let B_1 B_1 , B_2 B_2 , ... B_k B_k be all the blocks that surround this use of x , with B_k B_k the smallest, nested within B_{k-1} B_{k-1} , which is nested within B_{k-2} B_{k-2} , and so on. Search for the largest i such that there is a declaration of x belonging to B_i B_i . This use of x refers to the declaration in B_i B_i . Alternatively, this use of x is within the scope of the declaration in B_i B_i . 1.6.4 Explicit Access Control Classes and structures introduce a new scope for their members. If p is an object of a class with a field (member) x , then the use of x in p.x refers to field x in the class definition. In analogy with block structure, the scope of a member declaration x in a class C extends to any subclass C' , except if C' has a local declaration of the same name x . Through the use of keywords like public , private , and protected , object-oriented languages such as C++ or Java provide explicit control over access to member names in a superclass. These keywords support encapsulation by restricting access. Thus, private names are purposely given a scope that includes only the method declarations and definitions associated with that class and any friend classes (the C++ term). Protected names are accessible to subclasses. Public names are accessible from outside the class. In C++ , a class definition may be separated from the definitions of some or all of its methods. Therefore, a name x associated with the class C may have a region of the code that is outside its scope, followed by another region (a method definition) that is within its scope. In fact, regions inside and outside the scope may alternate, until all the methods have been defined Declarations and Definitions The apparently similar terms declaration and definition for programming-language concepts are actually quite different. Declarations tell us about the types of things, while definitions tell us about their values . Thus, int i is a declaration of i , while i = 1 is a definition of i. The difference is more significant when we deal with methods or other procedures. In C++, a method is declared in a class definition , by giving the types of the arguments and result of the method (often called the signature for the method). The method is then defined, i.e., the code for executing the method is given, in another place. Similarly, it is common to define a C function in one file and declare it in other files where the function is used. 1.6.5 Dynamic Scope Technically, any scoping policy is dynamic if it is based on factor(s) that can be known only when the program executes. The term dynamic scope , however,usually refers to the following policy: a use of a name x refers to the declaration of x in the most recently called, not-yet-terminated, procedure with such a declaration. Dynamic scoping of this type appears only in special situations.We shall consider two examples of dynamic policies: macro expansion in the C preprocessor and method resolution in object-oriented programming. Example 1.7 : In the C program of Fig. 1.12, identifier a is a macro that stands for expression (x + 1) . But what is x ? We cannot resolve x statically,that is, in terms of the program text #define a (x+1) int x = 2 ; void b () { int x = 1 ; printf ( \"%d \\n \" , a ); } void c () { printf ( \"%d \\n \" , a ); } void main () { b (); c ();} In fact, in order to interpret x, we must use the usual dynamic-scope rule.We examine all the function calls that are currently active , and we take the most recently called function that has a declaration of x. It is to this declaration that the use of x refers. In the example of Fig. 1.12, the function main first calls function b . As b executes, it prints the value of the macro a . Since (x + 1) must be substituted for a , we resolve this use of x to the declaration int x=1 in function b . The reason is that b has a declaration of x , so the (x + 1) in the printf in b refers to this x . Thus, the value printed is 2. After b finishes, and c is called, we again need to print the value of macro a . However, the only x accessible to c is the global x . The printf statement in c thus refers to this declaration of x , and value 3 is printed. Dynamic scope resolution is also essential for polymorphic procedures , those that have two or more definitions for the same name , depending only on the types of the arguments(inheritance and override).In some languages, such as ML (see Section 7.3.3), it is possible to determine statically types for all uses of names, in which case the compiler can replace each use of a procedure name p by a reference to the code for the proper procedure. However, in other languages, such as Java and C++,there are times when the compiler cannot make that determination. Example 1.8 : A distinguishing feature of object-oriented programming is the ability of each object to invoke the appropriate method in response to a message.In other words, the procedure called when x.m() is executed depends on the class of the object denoted by x at that time. A typical example is as follows: There is a class C with a method named m() . D is a subclass of C , and D has its own method named m() . There is a use of m of the form x.m() , where x is an object of class C Normally, it is impossible to tell at compile time whether x will be of class C or of the subclass D . If the method application occurs several times, it is highly likely that some will be on objects denoted by x that are in class C but not D , while others will be in class D . It is not until run-time that it can be decided which definition of m is the right one. Thus, the code generated by the compiler must determine the class of the object x , and call one or the other method named m . Analogy Between Static and Dynamic Scoping While there could be any number of static or dynamic policies for scoping,there is an interesting relationship between the normal (block-structured) static scoping rule and the normal dynamic policy. In a sense, the dynamic rule is to time as the static rule is to space . While the static rule asks us to find the declaration whose unit (block) most closely surrounds the physical location of the use, the dynamic rule asks us to find the declaration whose unit (procedure invocation) most closely surrounds the time of the use. \u7ffb\u8bd1\uff1a\u52a8\u6001\u4f5c\u7528\u57df\u5904\u7406\u65f6\u95f4\u7684\u65b9\u5f0f\u7c7b\u4f3c\u4e8e\u9759\u6001\u4f5c\u7528\u57df\u5904\u7406\u7a7a\u95f4\u7684\u65b9\u5f0f\u3002 1.6.6 Parameter Passing Mechanisms All programming languages have a notion of a procedure, but they can differ in how these procedures get their arguments . In this section, we shall consider how the actual parameters (the parameters used in the call of a procedure) are associated with the formal parameters (those used in the procedure definition). Which mechanism is used determines how the calling-sequence code treats parameters. The great majority of languages use either call-by-value ,or call-by-reference , or both. We shall explain these terms, and another method known as call-by-name ,that is primarily of historical interest. Call-by-Value In call-by-value , the actual parameter is evaluated (if it is an expression ) or copied (if it is a variable). The value is placed in the location belonging to the corresponding formal parameter of the called procedure. This method is used in C and Java, and is a common option in C++ , as well as in most other languages. Call-by-value has the effect that all computation involving the formal parameters done by the called procedure is local to that procedure, and the actual parameters themselves cannot be changed. Note, however, that in C we can pass a pointer to a variable to allow that variable to be changed by the callee . Likewise, array names passed as parameters in C, C++ , or Java give the called procedure what is in effect a pointer or reference to the array itself. Thus, if a is the name of an array of the calling procedure, and it is passed by value to corresponding formal parameter x , then an assignment such as x[i] = 2 really changes the array element a[i] to 2. The reason is that, although x gets a copy of the value of a, that value is really a pointer to the beginning of the area of the store where the array named a is located. Similarly, in Java, many variables are really references, or pointers, to the things they stand for. This observation(\u7ed3\u8bba) applies to arrays, strings, and objects of all classes. Even though Java uses call-by-value exclusively(\u4ec5\u4ec5), whenever we pass the name of an object to a called procedure, the value received by that procedure is in effect a pointer to the object. Thus, the called procedure is able to affect the value of the object itself. Call-by-Reference In call-by-reference, the address of the actual parameter is passed to the callee as the value of the corresponding formal parameter. Uses of the formal parameter in the code of the callee are implemented by following this pointer to the location indicated by the caller. Changes to the formal parameter thus appear as changes to the actual parameter.If the actual parameter is an expression , however, then the expression is evaluated before the call, and its value stored in a location of its own. Changes to the formal parameter change the value in this location, but can have no effect on the data of the caller. Call-by-reference is used for \"ref \" parameters in C++ and is an option in many other languages. It is almost essential when the formal parameter is a large object, array, or structure. The reason is that strict call-by-value requires that the caller copy the entire actual parameter into the space belonging to the corresponding formal parameter. This copying gets expensive when then parameter is large. As we noted when discussing call-by-value, languages such as Java solve the problem of passing arrays, strings, or other objects by copying only a reference to those objects. The effect is that Java behaves as if it used call-by-reference for anything other than a basic type such as an integer or real. Call-by-Name A third mechanism call-by-name was used in the early programming language Algol 60. It requires that the callee execute as if the actual parameter were substituted literally for the formal parameter in the code of the callee, as if the formal parameter were a macro standing for the actual parameter (with renaming of local names in the called procedure, to keep them distinct). When the actual parameter is an expression rather than a variable, some unintuitive behaviors occur, which is one reason this mechanism is not favored to day. 1.6.7 Aliasing There is an interesting consequence of call-by-reference parameter passing or its simulation, as in Java, where references to objects are passed by value. It is possible that two formal parameters can refer to the same location; such variables are said to be aliases of one another. As a result, any two variables, which may appear to take their values from two distinct formal parameters, can become aliases of each other, as well.","title":"Introduction"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#16#programming#language#basics","text":"In this section, we shall cover the most important terminology and distinctions that appear in the study of programming languages. It is not our purpose to cover all concepts or all the popular programming languages. We assume that the reader is familiar with at least one of C, C++ , C#, or Java, and may have encountered other languages as well.","title":"1.6 Programming Language Basics"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#161#the#staticdynamic#distinction","text":"Among the most important issues that we face when designing a compiler for a language is what decisions can the compiler make about a program. If a language uses a policy that allows the compiler to decide an issue, then we say that the language uses a static policy or that the issue can be decided at compile time . On the other hand, a policy that only allows a decision to be made when we execute the program is said to be a dynamic policy or to require a decision at run time . One issue on which we shall concentrate is the scope of declarations . The scope of a declaration of x is the region of the program in which uses of x refer to this declaration. A language uses static scope or lexical scope if it is possible to determine the scope of a declaration by looking only at the program. Otherwise, the language uses dynamic scope . With dynamic scope , as the program runs, the same use of x could refer to any of several different declarations of x . NOTE:\u4e0a\u9762\u5173\u4e8e**scope**\u7684\u5b9a\u4e49\u6240\u6307\u4ee3\u7684\u662fdeclaration\uff0c\u800c\u975evariable\u3002\u663e\u7136declaration\u80fd\u591f\u6307\u4ee3\u7684\u8303\u56f4\u8fdc\u8fdc\u5927\u4e8evariable\u3002 See also: Scope (computer science) Most languages, such as C and Java, use static scope . We shall discuss static scoping in Section 1.6.3. Example 1.3 : As another example of the static/dynamic distinction , consider the use of the term \"static\" as it applies to data in a Java class declaration. In Java, a variable is a name for a location in memory used to hold a data value . Here, \"static\" refers not to the scope of the variable, but rather to the ability of the compiler to determine the location in memory where the declared variable can be found. A declaration like public static int x ; makes x a class variable and says that there is only one copy of x , no matter how many objects of this class are created. Moreover, the compiler can determine a location in memory where this integer x will be held. In contrast, had \" static \" been omitted from this declaration, then each object of the class would have its own location where x would be held, and the compiler could not determine all these places in advance of running the program. NOTE: \u901a\u8fc7\u5173\u952e\u5b57 static \u6765\u544a\u8bc9compiler\uff1a\u4f60\u53ef\u4ee5\u5728\u8fdb\u884c\u7f16\u8bd1\u7684\u65f6\u5019\u5c31\u7ed9 x \u5206\u914dmemory\uff1b\u800c\u4e0d\u662f\u5728\u8fd0\u884c\u7a0b\u5e8f\u7684\u65f6\u5019\u4f7f\u7528 new \u6765\u521b\u5efa\u5bf9\u8c61\u65f6\u518d\u5206\u914d x \u7684memory\uff0c\u663e\u7136\u8fd9\u662f\u6240\u8c13\u7684static policy\u3002 See also: Static (keyword)","title":"1.6.1 The Static/Dynamic Distinction"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#162#environments#and#states","text":"Another important distinction we must make when discussing programming languages is whether changes occurring as the program runs affect the values of data elements or affect the interpretation of names for that data. For example,the execution of an assignment such as x = y + 1 changes the value denoted by the name x . More specifically, the assignment changes the value in whatever location is denoted by x . It may be less clear that the location denoted by x can change at run time . For instance, as we discussed in Example 1.3, if x is not a static (or \" class \") variable, then every object of the class has its own location for an instance of variable x. In that case, the assignment to x can change any of those \"instance\" variables, depending on the object to which a method containing that assignment is applied. The association of names with locations in memory (the store) and then with values can be described by two mappings that change as the program runs (see Fig. 1.8): 1 The environment is a mapping from names to locations in the store. Since variables refer to locations (\"l-values\" in the terminology of C), we could alternatively define an environment as a mapping from names to variables ( location ). 2 The state is a mapping from locations in store to their values . That is, the state maps l-values to their corresponding r-values , in the terminology of C. Environments change according to the scope rules of a language. NOTE: environments**\u548c**scope \u662f\u4e0d\u540c\u7684\u6982\u5ff5\u3002 Example 1.4 : Consider the C program fragment in Fig. 1.9. Integer i is declared a global variable, and also declared as a variable local to function f .When f is executing, the environment adjusts so that name i refers to the location reserved for the i that is local to f , and any use of i , such as the assignment i = 3 shown explicitly, refers to that location. Typically, the local i is given a place on the run-time stack int i ; /* global i */ void f (...) { int i ; /* local i */ ... i = 3 ; /* use of local i */ ... } ... x = i + 1 ; /* use of global i */ Whenever a function g other than f is executing, uses of i cannot refer to the i that is local to f . Uses of name i in g must be within the scope of some other declaration of i . An example is the explicitly shown statement x = i+1 ,which is inside some procedure whose definition is not shown. The i in i + 1 presumably\uff08\u5927\u6982\uff09 refers to the global i . As in most languages, declarations in C must precede their use, so a function that comes before the global i cannot refer to it. The environment and state mappings in Fig. 1.8 are dynamic , but there are a few exceptions: 1 Static versus dynamic binding of names to locations . Most binding of names to locations is dynamic , and we discuss several approaches to this binding throughout the section. Some declarations, such as the global i in Fig. 1.9, can be given a location in the store once and for all, as the compiler generates object code(\u4e5f\u5c31\u662fglobal i \u662fstatic). Technically, the C compiler will assign a location in virtual memory for the global i ,leaving it to the loader and the operating system to determine where in the physical memory of the machine i will be located. However, we shall not worry about relocation issues such as these, which have no impact on compiling. Instead, we treat the address space that the compiler uses for its output code as if it gave physical memory locations . 2 Static versus dynamic binding of locations to values . The binding of locations to values (the second stage in Fig. 1.8), is generally dynamic as well, since we cannot tell the value in a location until we run the program. Declared constants are an exception. For instance, the C definition #define ARRAYSIZE 1000 binds the name ARRAYSIZE to the value 1000 statically. We can determine this binding by looking at the statement, and we know that it is impossible for this binding to change when the program executes.","title":"1.6.2 Environments and States"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#163#static#scope#and#block#structure","text":"Most languages, including C and its family, use static scope _. The scope rules for C are based on program structure ; the scope of a declaration is determined implicitly(\u9690\u5f0f\u5730) by where the declaration appears in the program. Later languages,such as C++, Java, and C#, also provide explicit control over scopes through the use of keywords like public , private , and protected . In this section we consider static-scope rules for a language with blocks , where a block is a grouping of declarations and statements . C uses braces { and } to delimit(\u5b9a\u754c) a block ; the alternative use of begin and end for the same purpose dates back to Algol. NOTE : \u5728 c++ \u4e2d\uff0cblock\u9664\u4e86\u53ef\u4ee5\u7528\u4e8e\u786e\u5b9ascope\u5916\uff0cblock\u4e5f\u53ef\u4ee5\u51b3\u5b9a*storage duration* \u3002\u4ee5\u4e0b\u662f\u6458\u81eacppreference \u7684 Storage class specifiers \u7ae0\u8282\uff1a automatic storage duration. The storage for the object is allocated at the beginning of the enclosing code block and deallocated at the end. All local objects have this storage duration, except those declared static , extern or thread_local . Names , Identifiers , and Variables Although the terms \"name\" and \"variable,\" often refer to the same thing,we use them carefully to distinguish between compile-time names and the run-time locations denoted by names. An identifier is a string of characters, typically letters or digits, that refers to (identifies) an entity , such as a data object , a procedure , a class ,or a type . All identifiers are names , but not all names are identifiers .Names can also be expressions. For example, the name x.y might denote the field y of a structure denoted by x . Here, x and y are identifiers, while x.y is a name, but not an identifier . Composite names like x.y are called qualified names . SUMMARY :\u663e\u7136\uff0cname\u662f\u6bd4identifier\u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\u3002 A variable refers to a particular location of the store. It is common for the same identifier to be declared more than once; each such declaration introduces a new variable . Even if each identifier is declared just once, an identifier local to a recursive procedure will refer to different locations of the store at different times. SUMMARY :\u663e\u7136\uff0cvariable\u66f4\u52a0\u503e\u5411\u4e8e\u8fd0\u884c\u65f6\u6982\u5ff5 Example 1.5 \uff1aC static-scope policy is as follows 1 A C program consists of a sequence of top-level declarations of variables and functions 2 Functions may have variable declarations within them, where variables include local variables and parameters . The scope of each such declaration is restricted to the function in which it appears. 3 The scope of a top-level declaration of a name x consists of the entire program that follows, with the exception of those statements that lie within a function that also has a declaration of x The additional detail regarding the C static-scope policy deals with variable declarations within statements . We examine such declarations next and in Example 1.6. 2 Procedures, Functions, and Methods To avoid saying procedures, functions, or methods, each time we want to talk about a subprogram that may be called, we shall usually refer to all of them as procedures .The exception is that when talking explicitly of programs in languages like C that have only functions, we shall refer to them as functions .Or, if we are discussing a language like Java that has only methods , we shall use that term instead. A function generally returns a value of some type (the return type), while a procedure does not return any value. C and similar languages, which have only functions , treat procedures as functions that have a special return type void , to signify no return value. Object-oriented languages like Java and C++ use the term methods .These can behave like either functions or procedures, but are associated with a particular class. In C, the syntax of blocks is given by 1 One type of statement is a block(block\u662f\u4e00\u79cdstatement). Blocks can appear anywhere that other types of statements, such as assignment statements, can appear. 2 A block is a sequence of declarations followed by a sequence of statements,all surrounded by braces Note that this syntax allows blocks to be nested inside each other. This nesting property is referred to as block structure . The C family of languages has block structure, except that a function may not be defined inside another function. We say that a declaration D belongs to a block B if B is the most closely nested block containing D ; that is, D is located within B , but not within any block that is nested within B . The static-scope rule for variable declarations in block-structured languages is as follows. If declaration D of name x belongs to block B , then the scope of D is all of B , except for any blocks B' nested to any depth within B , in which x is redeclared. Here, x is redeclared in B' if some other declaration D of the same name x belongs to B' . An equivalent way to express this rule is to focus on a use of a name x .Let B_1 B_1 , B_2 B_2 , ... B_k B_k be all the blocks that surround this use of x , with B_k B_k the smallest, nested within B_{k-1} B_{k-1} , which is nested within B_{k-2} B_{k-2} , and so on. Search for the largest i such that there is a declaration of x belonging to B_i B_i . This use of x refers to the declaration in B_i B_i . Alternatively, this use of x is within the scope of the declaration in B_i B_i .","title":"1.6.3 Static Scope and Block Structure"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#164#explicit#access#control","text":"Classes and structures introduce a new scope for their members. If p is an object of a class with a field (member) x , then the use of x in p.x refers to field x in the class definition. In analogy with block structure, the scope of a member declaration x in a class C extends to any subclass C' , except if C' has a local declaration of the same name x . Through the use of keywords like public , private , and protected , object-oriented languages such as C++ or Java provide explicit control over access to member names in a superclass. These keywords support encapsulation by restricting access. Thus, private names are purposely given a scope that includes only the method declarations and definitions associated with that class and any friend classes (the C++ term). Protected names are accessible to subclasses. Public names are accessible from outside the class. In C++ , a class definition may be separated from the definitions of some or all of its methods. Therefore, a name x associated with the class C may have a region of the code that is outside its scope, followed by another region (a method definition) that is within its scope. In fact, regions inside and outside the scope may alternate, until all the methods have been defined Declarations and Definitions The apparently similar terms declaration and definition for programming-language concepts are actually quite different. Declarations tell us about the types of things, while definitions tell us about their values . Thus, int i is a declaration of i , while i = 1 is a definition of i. The difference is more significant when we deal with methods or other procedures. In C++, a method is declared in a class definition , by giving the types of the arguments and result of the method (often called the signature for the method). The method is then defined, i.e., the code for executing the method is given, in another place. Similarly, it is common to define a C function in one file and declare it in other files where the function is used.","title":"1.6.4 Explicit Access Control"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#165#dynamic#scope","text":"Technically, any scoping policy is dynamic if it is based on factor(s) that can be known only when the program executes. The term dynamic scope , however,usually refers to the following policy: a use of a name x refers to the declaration of x in the most recently called, not-yet-terminated, procedure with such a declaration. Dynamic scoping of this type appears only in special situations.We shall consider two examples of dynamic policies: macro expansion in the C preprocessor and method resolution in object-oriented programming. Example 1.7 : In the C program of Fig. 1.12, identifier a is a macro that stands for expression (x + 1) . But what is x ? We cannot resolve x statically,that is, in terms of the program text #define a (x+1) int x = 2 ; void b () { int x = 1 ; printf ( \"%d \\n \" , a ); } void c () { printf ( \"%d \\n \" , a ); } void main () { b (); c ();} In fact, in order to interpret x, we must use the usual dynamic-scope rule.We examine all the function calls that are currently active , and we take the most recently called function that has a declaration of x. It is to this declaration that the use of x refers. In the example of Fig. 1.12, the function main first calls function b . As b executes, it prints the value of the macro a . Since (x + 1) must be substituted for a , we resolve this use of x to the declaration int x=1 in function b . The reason is that b has a declaration of x , so the (x + 1) in the printf in b refers to this x . Thus, the value printed is 2. After b finishes, and c is called, we again need to print the value of macro a . However, the only x accessible to c is the global x . The printf statement in c thus refers to this declaration of x , and value 3 is printed. Dynamic scope resolution is also essential for polymorphic procedures , those that have two or more definitions for the same name , depending only on the types of the arguments(inheritance and override).In some languages, such as ML (see Section 7.3.3), it is possible to determine statically types for all uses of names, in which case the compiler can replace each use of a procedure name p by a reference to the code for the proper procedure. However, in other languages, such as Java and C++,there are times when the compiler cannot make that determination. Example 1.8 : A distinguishing feature of object-oriented programming is the ability of each object to invoke the appropriate method in response to a message.In other words, the procedure called when x.m() is executed depends on the class of the object denoted by x at that time. A typical example is as follows: There is a class C with a method named m() . D is a subclass of C , and D has its own method named m() . There is a use of m of the form x.m() , where x is an object of class C Normally, it is impossible to tell at compile time whether x will be of class C or of the subclass D . If the method application occurs several times, it is highly likely that some will be on objects denoted by x that are in class C but not D , while others will be in class D . It is not until run-time that it can be decided which definition of m is the right one. Thus, the code generated by the compiler must determine the class of the object x , and call one or the other method named m . Analogy Between Static and Dynamic Scoping While there could be any number of static or dynamic policies for scoping,there is an interesting relationship between the normal (block-structured) static scoping rule and the normal dynamic policy. In a sense, the dynamic rule is to time as the static rule is to space . While the static rule asks us to find the declaration whose unit (block) most closely surrounds the physical location of the use, the dynamic rule asks us to find the declaration whose unit (procedure invocation) most closely surrounds the time of the use. \u7ffb\u8bd1\uff1a\u52a8\u6001\u4f5c\u7528\u57df\u5904\u7406\u65f6\u95f4\u7684\u65b9\u5f0f\u7c7b\u4f3c\u4e8e\u9759\u6001\u4f5c\u7528\u57df\u5904\u7406\u7a7a\u95f4\u7684\u65b9\u5f0f\u3002","title":"1.6.5 Dynamic Scope"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#166#parameter#passing#mechanisms","text":"All programming languages have a notion of a procedure, but they can differ in how these procedures get their arguments . In this section, we shall consider how the actual parameters (the parameters used in the call of a procedure) are associated with the formal parameters (those used in the procedure definition). Which mechanism is used determines how the calling-sequence code treats parameters. The great majority of languages use either call-by-value ,or call-by-reference , or both. We shall explain these terms, and another method known as call-by-name ,that is primarily of historical interest.","title":"1.6.6 Parameter Passing Mechanisms"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#call-by-value","text":"In call-by-value , the actual parameter is evaluated (if it is an expression ) or copied (if it is a variable). The value is placed in the location belonging to the corresponding formal parameter of the called procedure. This method is used in C and Java, and is a common option in C++ , as well as in most other languages. Call-by-value has the effect that all computation involving the formal parameters done by the called procedure is local to that procedure, and the actual parameters themselves cannot be changed. Note, however, that in C we can pass a pointer to a variable to allow that variable to be changed by the callee . Likewise, array names passed as parameters in C, C++ , or Java give the called procedure what is in effect a pointer or reference to the array itself. Thus, if a is the name of an array of the calling procedure, and it is passed by value to corresponding formal parameter x , then an assignment such as x[i] = 2 really changes the array element a[i] to 2. The reason is that, although x gets a copy of the value of a, that value is really a pointer to the beginning of the area of the store where the array named a is located. Similarly, in Java, many variables are really references, or pointers, to the things they stand for. This observation(\u7ed3\u8bba) applies to arrays, strings, and objects of all classes. Even though Java uses call-by-value exclusively(\u4ec5\u4ec5), whenever we pass the name of an object to a called procedure, the value received by that procedure is in effect a pointer to the object. Thus, the called procedure is able to affect the value of the object itself.","title":"Call-by-Value"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#call-by-reference","text":"In call-by-reference, the address of the actual parameter is passed to the callee as the value of the corresponding formal parameter. Uses of the formal parameter in the code of the callee are implemented by following this pointer to the location indicated by the caller. Changes to the formal parameter thus appear as changes to the actual parameter.If the actual parameter is an expression , however, then the expression is evaluated before the call, and its value stored in a location of its own. Changes to the formal parameter change the value in this location, but can have no effect on the data of the caller. Call-by-reference is used for \"ref \" parameters in C++ and is an option in many other languages. It is almost essential when the formal parameter is a large object, array, or structure. The reason is that strict call-by-value requires that the caller copy the entire actual parameter into the space belonging to the corresponding formal parameter. This copying gets expensive when then parameter is large. As we noted when discussing call-by-value, languages such as Java solve the problem of passing arrays, strings, or other objects by copying only a reference to those objects. The effect is that Java behaves as if it used call-by-reference for anything other than a basic type such as an integer or real.","title":"Call-by-Reference"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#call-by-name","text":"A third mechanism call-by-name was used in the early programming language Algol 60. It requires that the callee execute as if the actual parameter were substituted literally for the formal parameter in the code of the callee, as if the formal parameter were a macro standing for the actual parameter (with renaming of local names in the called procedure, to keep them distinct). When the actual parameter is an expression rather than a variable, some unintuitive behaviors occur, which is one reason this mechanism is not favored to day.","title":"Call-by-Name"},{"location":"1-Introduction/1.6-Programming-Language-Basics/#167#aliasing","text":"There is an interesting consequence of call-by-reference parameter passing or its simulation, as in Java, where references to objects are passed by value. It is possible that two formal parameters can refer to the same location; such variables are said to be aliases of one another. As a result, any two variables, which may appear to take their values from two distinct formal parameters, can become aliases of each other, as well.","title":"1.6.7 Aliasing"},{"location":"2-A-Simple-Syntax-Directed-Translator/","text":"Introduction This chapter is an introduction to the compiling techniques in Chapters 3 through 6 of this book. It illustrates the techniques by developing a working Java program that translates representative programming language statements into three-address code , an intermediate representation. We start small by creating a syntax-directed translator that maps infix arithmetic expressions into postfix expressions . We then extend this translator to map code fragments as shown in Fig. 2.1 into three-address code of the form in Fig. 2.2. TIPS: It is not easy to implement mapping infix arithmetic expressions into postfix expressions, there are some algorithms Shunting-yard algorithm { int i ; int j ; float [ 100 ] a ; float v ; float x ; while ( true ) { do i = i + 1 ; while ( a [ i ] < v ); do j = j - 1 ; while ( a [ j ] > v ); if ( i >= j ) break ; x = a [ i ] ; a [ i ] = a [ j ] ; a [ j ] = x ; } } Figure 2.1: A code fragment to be translated 1: i = i + 1 2: t1 = a [ i ] 3: if t1 < v goto 1 4: j = j - 1 5: t2 = a [ j ] 6: if t2 > v goto 4 7: ifFalse i >= j goto 9 8: goto 14 9: x = a [ i ] 10: t3 = a [ j ] 11: a [ i ] = t3 12: a [ j ] = x 13: goto 1 14: Figure 2.2: Simplified intermediate code for the program fragment in Fig. 2.1","title":2},{"location":"2-A-Simple-Syntax-Directed-Translator/#introduction","text":"This chapter is an introduction to the compiling techniques in Chapters 3 through 6 of this book. It illustrates the techniques by developing a working Java program that translates representative programming language statements into three-address code , an intermediate representation. We start small by creating a syntax-directed translator that maps infix arithmetic expressions into postfix expressions . We then extend this translator to map code fragments as shown in Fig. 2.1 into three-address code of the form in Fig. 2.2. TIPS: It is not easy to implement mapping infix arithmetic expressions into postfix expressions, there are some algorithms Shunting-yard algorithm { int i ; int j ; float [ 100 ] a ; float v ; float x ; while ( true ) { do i = i + 1 ; while ( a [ i ] < v ); do j = j - 1 ; while ( a [ j ] > v ); if ( i >= j ) break ; x = a [ i ] ; a [ i ] = a [ j ] ; a [ j ] = x ; } } Figure 2.1: A code fragment to be translated 1: i = i + 1 2: t1 = a [ i ] 3: if t1 < v goto 1 4: j = j - 1 5: t2 = a [ j ] 6: if t2 > v goto 4 7: ifFalse i >= j goto 9 8: goto 14 9: x = a [ i ] 10: t3 = a [ j ] 11: a [ i ] = t3 12: a [ j ] = x 13: goto 1 14: Figure 2.2: Simplified intermediate code for the program fragment in Fig. 2.1","title":"Introduction"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.1-Introduction/","text":"2.1 Introduction For compiler, what can context-free grammar do? 1\u3001Specify syntax as described in Section 2.2 2\u3001Help guide the translation of programs, a grammar-oriented compiling technique known as syntax-directed translation introduced in Section 2.4 Parsing or syntax analysis is introduced in Section 2.4.","title":"Introduction"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.1-Introduction/#21#introduction","text":"For compiler, what can context-free grammar do? 1\u3001Specify syntax as described in Section 2.2 2\u3001Help guide the translation of programs, a grammar-oriented compiling technique known as syntax-directed translation introduced in Section 2.4 Parsing or syntax analysis is introduced in Section 2.4.","title":"2.1 Introduction"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/","text":"2.2 Syntax Definition In this section, we introduce a notation \u2014 the \" context-free grammar ,\" or \"grammar\" for short \u2014 that is used to specify the syntax of a language. Grammars will be used throughout this book to organize compiler front ends . NOTE: \u4e00\u3001wikipedia has a good explanation of context-free grammar \u4e8c\u3001It is a convention that there is a section called language reference or language specification to describe grammar of the programming language in the official document of the programming language , such as 1\u3001 The Python Language Reference use extended BNF notation to describe grammar of the language. 2\u3001 The Java\u00ae Language Specification 2.2.1 Definition of Grammars A context-free grammar has four components: 1\u3001A set of terminal symbols , sometimes referred to as \"tokens.\" The terminals are the elementary symbols of the language defined by the grammar. 2\u3001A set of nonterminals , sometimes called \"syntactic variables.\" Each nonterminal represents a set of strings of terminals, in a manner we shall describe. 3\u3001A set of productions , where each production consists of a nonterminal, called the head or left side of the production, an arrow, and a sequence of terminals and/or nonterminals, called the body or right side of the production. 4\u3001A designation of one of the nonterminals as the start symbol. Example 2.1 : $$ \\begin{align} list \\to list + digit \\tag {2.1}\\ list \\to list - digit \\tag {2.2}\\ list \\to digit \\tag {2.3}\\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\tag {2.4} \\end{align} $$ NOTE: Programmer should be sensitive of recursion , it is obvious that Context-free grammar above has the property of recursion , a grammar is informally called a recursive grammar if it contains production rules that are recursive . 2.2.2 Derivations NOTE: In addition to using the word derivation /derive, word expand is also used a lot. I think word expand can visually reflect the process of expanding an inner nodes of the parse tree describing in 2.2.3. A grammar derives strings by beginning with the start symbol and repeatedly replacing a nonterminal by the body of a production for that nonterminal. The terminal strings that can be derived from the start symbol form the language defined by the grammar. NOTE: Here is the definition of language , please recall my question in the first introduction that what a programming language is. Parsing is the problem of taking a string of terminals and figuring out how to derive it from the start symbol of the grammar, and if it cannot be derived from the start symbol of the grammar, then reporting syntax errors within the string. NOTE: \u4e00\u3001The process of parsing is the inverse of the process of deriving. The process of deriving is to generate terminal strings in the direction of generation while the process of parsing is to find productions in the opposite direction of production. They are like recursion and corecursion . \u4e8c\u3001Implementation of parsing is described in chapter 2.4. 2.2.3 Parse Trees NOTE: \u4e00\u3001 Parse tree reveal us of the power of tree structure, and it is an typical application of tree . Tree data structures figure prominently in compiling. Parse tree is the software implementation of grammar. \u4e8c\u3001Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0c so do tree structure. This is one of reason why tree structure is selected as implementation of grammar. This chapter gives the algorithm of build a parse tree according to the grammar. 2.2.4 Ambiguity NOTE: Programming language should be accurate and the designer should be careful to design grammar to avoid ambiguity. Source of ambiguity includes precedence of operators associativity of operators 2.2.5 Associativity of Operators NOTE: We can define associativity by grammar, this reveals us of the power if context free grammar. By convention, 9+5+2 9+5+2 is equivalent to (9+5)+2 (9+5)+2 and 9-5-2 9-5-2 is equivalent to (9-5)-2 (9-5)-2 . When an operand like 5 has operators to its left and right, conventions are needed for deciding which operator applies to that operand. We say that the operator + associates to the left, because an operand with plus signs on both sides of it belongs to the operator to its left. In most programming languages the four arithmetic operators, addition, subtraction, multiplication, and division are left-associative . Some common operators such as exponentiation(\u5e42\u8fd0\u7b97) are right-associative . As another example, the assignment operator in C and its descendants is right-associative; that is, the expression a=b=c is treated in the same way as the expression a=(b=c) . For clarity, sample grammar for left-associative operator and sample grammar for right-associative operator are listed below along their parse tree. Strings like a=b=c with a right-associative operator are generated by the following grammar: $$ \\begin{align} right \\to letter=right|letter \\ letter \\to a|b| \\dots |z \\end{align} $$ Strings like 1+2 , 1-2 with a left-associative operator are generated by the following grammar: $$ \\begin{align} list \\to list + digit \\ list \\to list - digit \\ list \\to digit \\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\end{align} $$ The contrast between a parse tree for a left-associative operator like - and a parse tree for a right-associative operator like = is shown by Fig. 2.7. Note that the parse tree for 9-5-2 grows down towards the left, whereas the parse tree for a=b=c grows down towards the right. NOTE: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0ca grammar is informally called a recursive grammar if it contains production rules that are recursive . Here we can further subdivide, a grammar for a context-free language is left recursive if there exists a non-terminal symbol A that can be put through the production rules to produce a string with A (as the leftmost symbol), so grammars for left-associative operator are left recursive while grammars for right-associative operator are right recursive . For comparison, here are the ambiguous grammars and its parse tree. $$ list \\to list + digit | list - digit | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 $$ 2.2.6 Precedence of Operators We say that * has higher precedence than + if * takes its operands before + does. In ordinary arithmetic, multiplication and division have higher precedence than addition and subtraction. Therefore, 5 is taken by * in both 9+5*2 and 9*5+2 ; i.e., the expressions are equivalent to 9+(5*2) and (9*5)+2 , respectively. Example 2.6 : A grammar for arithmetic expressions can be constructed from a table showing the associativity and precedence of operators. We start with the four common arithmetic operators and a precedence table, showing the operators in order of increasing precedence. Operators on the same line have the same associativity and precedence: left-associative + - left-associative: * / We create two nonterminals expr and term for the two levels of precedence( expr for + - and term for * / ), and an extra nonterminal factor for generating basic units in expressions. The basic units in expressions are presently digits and parenthesized expressions. NOTE: \"factor\"\u7684\u610f\u601d\u662f\"\u56e0\u7d20\"\uff0c\u5b83\u662f\u57fa\u672c\u5355\u5143 factor \\to digit | ( expr ) $$ Now consider the binary operators, `*` and `/`, that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\\\ | term / factor \\\\ | factor $$ Similarly, `expr` generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\\\ | expr - term \\\\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\\\ term \\to term * factor | term / factor | factor \\\\ factor \\to digit | ( expr ) factor \\to digit | ( expr ) $$ Now consider the binary operators, `*` and `/`, that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\\\ | term / factor \\\\ | factor $$ Similarly, `expr` generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\\\ | expr - term \\\\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\\\ term \\to term * factor | term / factor | factor \\\\ factor \\to digit | ( expr ) NOTE: \u81ea\u5e95\u5411\u4e0a\uff0c\u4f18\u5148\u7ea7\u5728\u964d\u4f4e Generalizing the Expression Grammar of Example 2.6 We can think of a factor as an expression that cannot be \"torn apart\"(\u62c6\u5f00) by any operator. By \"torn apart,\" we mean that placing an operator next to any factor, on either side, does not cause any piece of the factor , other than the whole , to become an operand of that operator. If the factor is a parenthesized expression , the parentheses protect against such \"tearing,\" while if the factor is a single operand, it cannot be torn apart. A term (that is not also a factor) is an expression that can be torn apart by operators of the highest precedence: * and / , but not by the lower-precedence operators. An expression (that is not a term or factor) can be torn apart by any operator. We can generalize this idea to any number n of precedence levels. We need n + 1 nonterminals. The first, like factor in Example 2.6, can never be torn apart. Typically, the production bodies for this nonterminal are only single operands and parenthesized expressions . Then, for each precedence level, there is one nonterminal representing expressions that can be torn apart only by operators at that level or higher. Typically, the productions for this nonterminal have bodies representing uses of the operators at that level, plus one body that is just the nonterminal for the next higher level. NOTE: The reason one more is needed is that it is for factor NOTE: When generating a parse tree from a grammar for expression, the higher the priority of operator, the earlier it is built, and the deeper it is","title":"Introduction"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#22#syntax#definition","text":"In this section, we introduce a notation \u2014 the \" context-free grammar ,\" or \"grammar\" for short \u2014 that is used to specify the syntax of a language. Grammars will be used throughout this book to organize compiler front ends . NOTE: \u4e00\u3001wikipedia has a good explanation of context-free grammar \u4e8c\u3001It is a convention that there is a section called language reference or language specification to describe grammar of the programming language in the official document of the programming language , such as 1\u3001 The Python Language Reference use extended BNF notation to describe grammar of the language. 2\u3001 The Java\u00ae Language Specification","title":"2.2 Syntax Definition"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#221#definition#of#grammars","text":"A context-free grammar has four components: 1\u3001A set of terminal symbols , sometimes referred to as \"tokens.\" The terminals are the elementary symbols of the language defined by the grammar. 2\u3001A set of nonterminals , sometimes called \"syntactic variables.\" Each nonterminal represents a set of strings of terminals, in a manner we shall describe. 3\u3001A set of productions , where each production consists of a nonterminal, called the head or left side of the production, an arrow, and a sequence of terminals and/or nonterminals, called the body or right side of the production. 4\u3001A designation of one of the nonterminals as the start symbol. Example 2.1 : $$ \\begin{align} list \\to list + digit \\tag {2.1}\\ list \\to list - digit \\tag {2.2}\\ list \\to digit \\tag {2.3}\\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\tag {2.4} \\end{align} $$ NOTE: Programmer should be sensitive of recursion , it is obvious that Context-free grammar above has the property of recursion , a grammar is informally called a recursive grammar if it contains production rules that are recursive .","title":"2.2.1 Definition of Grammars"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#222#derivations","text":"NOTE: In addition to using the word derivation /derive, word expand is also used a lot. I think word expand can visually reflect the process of expanding an inner nodes of the parse tree describing in 2.2.3. A grammar derives strings by beginning with the start symbol and repeatedly replacing a nonterminal by the body of a production for that nonterminal. The terminal strings that can be derived from the start symbol form the language defined by the grammar. NOTE: Here is the definition of language , please recall my question in the first introduction that what a programming language is. Parsing is the problem of taking a string of terminals and figuring out how to derive it from the start symbol of the grammar, and if it cannot be derived from the start symbol of the grammar, then reporting syntax errors within the string. NOTE: \u4e00\u3001The process of parsing is the inverse of the process of deriving. The process of deriving is to generate terminal strings in the direction of generation while the process of parsing is to find productions in the opposite direction of production. They are like recursion and corecursion . \u4e8c\u3001Implementation of parsing is described in chapter 2.4.","title":"2.2.2 Derivations"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#223#parse#trees","text":"NOTE: \u4e00\u3001 Parse tree reveal us of the power of tree structure, and it is an typical application of tree . Tree data structures figure prominently in compiling. Parse tree is the software implementation of grammar. \u4e8c\u3001Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0c so do tree structure. This is one of reason why tree structure is selected as implementation of grammar. This chapter gives the algorithm of build a parse tree according to the grammar.","title":"2.2.3 Parse Trees"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#224#ambiguity","text":"NOTE: Programming language should be accurate and the designer should be careful to design grammar to avoid ambiguity. Source of ambiguity includes precedence of operators associativity of operators","title":"2.2.4 Ambiguity"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#225#associativity#of#operators","text":"NOTE: We can define associativity by grammar, this reveals us of the power if context free grammar. By convention, 9+5+2 9+5+2 is equivalent to (9+5)+2 (9+5)+2 and 9-5-2 9-5-2 is equivalent to (9-5)-2 (9-5)-2 . When an operand like 5 has operators to its left and right, conventions are needed for deciding which operator applies to that operand. We say that the operator + associates to the left, because an operand with plus signs on both sides of it belongs to the operator to its left. In most programming languages the four arithmetic operators, addition, subtraction, multiplication, and division are left-associative . Some common operators such as exponentiation(\u5e42\u8fd0\u7b97) are right-associative . As another example, the assignment operator in C and its descendants is right-associative; that is, the expression a=b=c is treated in the same way as the expression a=(b=c) . For clarity, sample grammar for left-associative operator and sample grammar for right-associative operator are listed below along their parse tree. Strings like a=b=c with a right-associative operator are generated by the following grammar: $$ \\begin{align} right \\to letter=right|letter \\ letter \\to a|b| \\dots |z \\end{align} $$ Strings like 1+2 , 1-2 with a left-associative operator are generated by the following grammar: $$ \\begin{align} list \\to list + digit \\ list \\to list - digit \\ list \\to digit \\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\end{align} $$ The contrast between a parse tree for a left-associative operator like - and a parse tree for a right-associative operator like = is shown by Fig. 2.7. Note that the parse tree for 9-5-2 grows down towards the left, whereas the parse tree for a=b=c grows down towards the right. NOTE: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0ca grammar is informally called a recursive grammar if it contains production rules that are recursive . Here we can further subdivide, a grammar for a context-free language is left recursive if there exists a non-terminal symbol A that can be put through the production rules to produce a string with A (as the leftmost symbol), so grammars for left-associative operator are left recursive while grammars for right-associative operator are right recursive . For comparison, here are the ambiguous grammars and its parse tree. $$ list \\to list + digit | list - digit | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 $$","title":"2.2.5 Associativity of Operators"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#226#precedence#of#operators","text":"We say that * has higher precedence than + if * takes its operands before + does. In ordinary arithmetic, multiplication and division have higher precedence than addition and subtraction. Therefore, 5 is taken by * in both 9+5*2 and 9*5+2 ; i.e., the expressions are equivalent to 9+(5*2) and (9*5)+2 , respectively. Example 2.6 : A grammar for arithmetic expressions can be constructed from a table showing the associativity and precedence of operators. We start with the four common arithmetic operators and a precedence table, showing the operators in order of increasing precedence. Operators on the same line have the same associativity and precedence: left-associative + - left-associative: * / We create two nonterminals expr and term for the two levels of precedence( expr for + - and term for * / ), and an extra nonterminal factor for generating basic units in expressions. The basic units in expressions are presently digits and parenthesized expressions. NOTE: \"factor\"\u7684\u610f\u601d\u662f\"\u56e0\u7d20\"\uff0c\u5b83\u662f\u57fa\u672c\u5355\u5143 factor \\to digit | ( expr ) $$ Now consider the binary operators, `*` and `/`, that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\\\ | term / factor \\\\ | factor $$ Similarly, `expr` generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\\\ | expr - term \\\\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\\\ term \\to term * factor | term / factor | factor \\\\ factor \\to digit | ( expr ) factor \\to digit | ( expr ) $$ Now consider the binary operators, `*` and `/`, that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\\\ | term / factor \\\\ | factor $$ Similarly, `expr` generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\\\ | expr - term \\\\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\\\ term \\to term * factor | term / factor | factor \\\\ factor \\to digit | ( expr ) NOTE: \u81ea\u5e95\u5411\u4e0a\uff0c\u4f18\u5148\u7ea7\u5728\u964d\u4f4e","title":"2.2.6 Precedence of Operators"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#generalizing#the#expression#grammar#of#example#26","text":"We can think of a factor as an expression that cannot be \"torn apart\"(\u62c6\u5f00) by any operator. By \"torn apart,\" we mean that placing an operator next to any factor, on either side, does not cause any piece of the factor , other than the whole , to become an operand of that operator. If the factor is a parenthesized expression , the parentheses protect against such \"tearing,\" while if the factor is a single operand, it cannot be torn apart. A term (that is not also a factor) is an expression that can be torn apart by operators of the highest precedence: * and / , but not by the lower-precedence operators. An expression (that is not a term or factor) can be torn apart by any operator. We can generalize this idea to any number n of precedence levels. We need n + 1 nonterminals. The first, like factor in Example 2.6, can never be torn apart. Typically, the production bodies for this nonterminal are only single operands and parenthesized expressions . Then, for each precedence level, there is one nonterminal representing expressions that can be torn apart only by operators at that level or higher. Typically, the productions for this nonterminal have bodies representing uses of the operators at that level, plus one body that is just the nonterminal for the next higher level. NOTE: The reason one more is needed is that it is for factor NOTE: When generating a parse tree from a grammar for expression, the higher the priority of operator, the earlier it is built, and the deeper it is","title":"Generalizing the Expression Grammar of Example 2.6"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.3-Syntax-Directed-Translation/","text":"2.3 Syntax-Directed Translation NOTE: \u8fd9\u4e2a\u6982\u5ff5\u5728\u524d\u9762\u7684\u7ae0\u8282\u4e2d\uff0c\u5df2\u7ecf\u6709\u4ecb\u7ecd\u4e86","title":"Introduction"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.3-Syntax-Directed-Translation/#23#syntax-directed#translation","text":"NOTE: \u8fd9\u4e2a\u6982\u5ff5\u5728\u524d\u9762\u7684\u7ae0\u8282\u4e2d\uff0c\u5df2\u7ecf\u6709\u4ecb\u7ecd\u4e86","title":"2.3 Syntax-Directed Translation"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/","text":"2.4 Parsing Parsing is the process of determining how a string of terminals can be generated by a grammar . In discussing this problem, it is helpful to think of a parse tree being constructed, even though a compiler may not construct one, in practice. However, a parser must be capable of constructing the tree in principle, or else the translation cannot be guaranteed correct. NOTE: The concept of parsing is introduced in chapter 2.2.2 Derivations This section introduces a parsing method called \" recursive descent ,\" which can be used both to parse and to implement syntax-directed translators. A complete Java program, implementing the translation scheme of Fig. 2.15, appears in the next section. A viable alternative is to use a software tool to generate a translator directly from a translation scheme. Section 4.9 describes such a tool Yacc; it can implement the translation scheme of Fig. 2.15 without modification. For any context-free grammar there is a parser that takes at most O (n^3) O (n^3) time to parse a string of n terminals. But cubic(\u7acb\u65b9\u7684) time is generally too expensive. Fortunately, for real programming languages, we can generally design a grammar that can be parsed quickly. Linear-time algorithms suffice to parse essentially all languages that arise in practice. Programming-language parsers almost always make a single left-to-right scan over the input, looking ahead one terminal at a time, and constructing pieces of the parse tree as they go. Most parsing methods fall into one of two classes, called the top-down and bottom-up methods. These terms refer to the order in which nodes in the parse tree are constructed. In top-down parsers, construction starts at the root and proceeds towards the leaves, while in bottom-up parsers, construction starts at the leaves and proceeds towards the root. The popularity of top-down parsers is due to the fact that efficient parsers can be constructed more easily by hand using top-down methods. Bottom-up parsing, however, can handle a larger class of grammars and translation schemes, so software tools for generating parsers directly from grammars often use bottom-up methods. NOTE: \u53c2\u89c1 wikipedia Parsing algorithms 2.4.1 Top-Down Parsing We introduce top-down parsing by considering a grammar that is well-suited for this class of methods. Later in this section, we consider the construction of top-down parsers in general. The grammar in Fig. 2.16 generates a subset of the statements of C or Java. We use the boldface terminals if and for for the keywords \" if \" and \" for \", respectively, to emphasize that these character sequences are treated as units, i.e., as single terminal symbols. Further, the terminal expr represents expressions; a more complete grammar would use a nonterminal expr and have productions for nonterminal expr . Similarly, other is a terminal representing other statement constructs. The top-down construction of a parse tree like the one in Fig. 2.17, is done by starting with the root, labeled with the starting nonterminal stmt , and repeatedly performing the following two steps. 1\u3001At node N , labeled with nonterminal A , select one of the productions for A and construct children at N for the symbols in the production body. 2\u3001Find the next node at which a subtree is to be constructed, typically the leftmost unexpanded nonterminal of the tree. For some grammars, the above steps can be implemented during a single left-to-right scan of the input string. The current terminal being scanned in the input is frequently referred to as the lookahead symbol . Initially, the lookahead symbol is the first, i.e., leftmost, terminal of the input string. Figure 2.18 illustrates the construction of the parse tree in Fig. 2.17 for the input string for ( ; expr ; expr ) other Initially, the terminal for is the lookahead symbol , and the known part of the parse tree consists of the root, labeled with the starting nonterminal stmt in Fig. 2.18(a). The objective is to construct the remainder of the parse tree in such a way that the string generated by the parse tree matches the input string. For a match to occur, the nonterminal stmt in Fig. 2.18(a) must derive(\u6d3e\u751f\u3001expand) a string that starts with the lookahead symbol for . In the grammar of Fig. 2.16, there is just one production for stmt that can derive such a string, so we select it, and construct the children of the root labeled with the symbols in the production body. This expansion of the parse tree is shown in Fig. 2.18(b). Each of the three snapshots in Fig. 2.18 has arrows marking the lookahead symbol in the input and the node in the parse tree that is being considered. Once children are constructed at a node, we next consider the leftmost child. In Fig. 2.18(b), children have just been constructed at the root, and the leftmost child labeled with for is being considered. At the nonterminal node labeled optexpr , we repeat the process of selecting a production for a nonterminal. Productions with \\epsilon \\epsilon as the body (\" \\epsilon \\epsilon -productions\") require special treatment. For the moment, we use them as a default when no other production can be used; we return to them in Section 2.4.3. With nonterminal optexpr and lookahead ; , the \\epsilon \\epsilon -productions is used, since ; does not match the only other production for optexpr , which has terminal expr as its body. In general, the selection of a production for a nonterminal may involve trial-and-error ; that is, we may have to try a production and backtrack to try another production if the first is found to be unsuitable. A production is unsuitable if, after using the production, we cannot complete the tree to match the input string. Backtracking is not needed, however, in an important special case called predictive parsing, which we discuss next. NOTE: There is a good illustration of Top-Down Parser in this blog . 2.4.2 Predictive Parsing Recursive-descent parsing is a top-down method of syntax analysis in which a set of recursive procedures is used to process the input. One procedure is associated with each nonterminal of a grammar. Here, we consider a simple form of recursive-descent parsing , called predictive parsing , in which the lookahead symbol unambiguously determines the flow of control through the procedure body for each nonterminal . The sequence of procedure calls during the analysis of an input string implicitly defines a parse tree for the input, and can be used to build an explicit parse tree , if desired. The predictive parser in Fig. 2.19 consists of procedures for the nonterminals stmt and optexpr of the grammar in Fig. 2.16 and an additional procedure match, used to simplify the code for stmt and optexpr . Procedure match(t) compares its argument t with the lookahead symbol and advances to the next input terminal if they match. Thus match changes the value of variable lookahead, a global variable that holds the currently scanned input terminal. void stmt () { switch ( lookahead ) { case expr: match (expr); match (';'); break; case 'if': match ('if'); match ('('); match (expr); match (')'); stmt (); break; case 'for': match ('for'); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); break; case other; match (other); break; default: report (\"syntax error\"); } void optexpr () { if ( lookahead == expr ) match (expr); } void match (terminal t) { if ( lookahead == t ) lookahead = nextTerminal; else report (\"syntax error\"); } Figure 2.19: Pseudo code for a predictive parser NOTE: A predictive parsing approach is one that hardcodes the grammar into the program Parsing begins with a call of the procedure for the starting nonterminal stmt . With the same input as in Fig. 2.18, lookahead is initially the first terminal for . Procedure stmt executes code corresponding to the production $$ stmt \\to for ( optexpr ; optexpr ; optexpr ) stmt $$ In the code for the production body -- that is, the for case of procedure stmt --each terminal is matched with the lookahead symbol, and each nonterminal leads to a call of its procedure, in the following sequence of calls: match (for); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); Predictive parsing relies on information about the first symbols that can be generated by a production body. More precisely, let \\alpha \\alpha be a string of grammar symbols (terminals and/or nonterminals). We define FIRST (\\alpha) FIRST (\\alpha) to be the set of terminals that appear as the first symbols of one or more strings of terminals generated from \\alpha \\alpha . If \\alpha \\alpha is \\epsilon \\epsilon or can generate \\epsilon \\epsilon , then \\epsilon \\epsilon is also in FIRST (\\alpha) FIRST (\\alpha) . NOTE: Only when FIRST (\\alpha) FIRST (\\alpha) is known, can flow of control determine. The details of how one computes FIRST (\\alpha) FIRST (\\alpha) are in Section 4.4.2. Here, we shall just use ad hoc reasoning to deduce the symbols in FIRST (\\alpha) FIRST (\\alpha) ; typically, \\alpha \\alpha will either begin with a terminal, which is therefore the only symbol in FIRST (\\alpha) FIRST (\\alpha) , or \\alpha \\alpha will begin with a nonterminal whose production bodies begin with terminals, in which case these terminals are the only members of FIRST (\\alpha) FIRST (\\alpha) . For example, with respect to the grammar of Fig. 2.16, the following are correct calculations of FIRST FIRST . FIRST (stmt) = {expr; if; for; other} FIRST(expr ;) = {expr} NOTE: FIRST (stmt) corresponds to the case of void stmt () . The FIRST sets must be considered if there are two pro ductions A \\to \\alpha A \\to \\alpha and A \\to \\beta A \\to \\beta . Ignoring \\epsilon \\epsilon - productions for the moment, predictive parsing requires FIRST (\\alpha) FIRST (\\alpha) and FIRST(\\beta ) FIRST(\\beta ) to b e disjoint. The lookahead symbol can then be used to decide which production to use; if the lookahead symbol is in FIRST (\\alpha) FIRST (\\alpha) , then \\alpha \\alpha is used. Otherwise, if the lookahead symbol is in FIRST (\\beta) FIRST (\\beta) , then \\beta \\beta is used. 2.4.3 When to Use \\epsilon \\epsilon -Productions","title":"Introduction"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#24#parsing","text":"Parsing is the process of determining how a string of terminals can be generated by a grammar . In discussing this problem, it is helpful to think of a parse tree being constructed, even though a compiler may not construct one, in practice. However, a parser must be capable of constructing the tree in principle, or else the translation cannot be guaranteed correct. NOTE: The concept of parsing is introduced in chapter 2.2.2 Derivations This section introduces a parsing method called \" recursive descent ,\" which can be used both to parse and to implement syntax-directed translators. A complete Java program, implementing the translation scheme of Fig. 2.15, appears in the next section. A viable alternative is to use a software tool to generate a translator directly from a translation scheme. Section 4.9 describes such a tool Yacc; it can implement the translation scheme of Fig. 2.15 without modification. For any context-free grammar there is a parser that takes at most O (n^3) O (n^3) time to parse a string of n terminals. But cubic(\u7acb\u65b9\u7684) time is generally too expensive. Fortunately, for real programming languages, we can generally design a grammar that can be parsed quickly. Linear-time algorithms suffice to parse essentially all languages that arise in practice. Programming-language parsers almost always make a single left-to-right scan over the input, looking ahead one terminal at a time, and constructing pieces of the parse tree as they go. Most parsing methods fall into one of two classes, called the top-down and bottom-up methods. These terms refer to the order in which nodes in the parse tree are constructed. In top-down parsers, construction starts at the root and proceeds towards the leaves, while in bottom-up parsers, construction starts at the leaves and proceeds towards the root. The popularity of top-down parsers is due to the fact that efficient parsers can be constructed more easily by hand using top-down methods. Bottom-up parsing, however, can handle a larger class of grammars and translation schemes, so software tools for generating parsers directly from grammars often use bottom-up methods. NOTE: \u53c2\u89c1 wikipedia Parsing algorithms","title":"2.4 Parsing"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#241#top-down#parsing","text":"We introduce top-down parsing by considering a grammar that is well-suited for this class of methods. Later in this section, we consider the construction of top-down parsers in general. The grammar in Fig. 2.16 generates a subset of the statements of C or Java. We use the boldface terminals if and for for the keywords \" if \" and \" for \", respectively, to emphasize that these character sequences are treated as units, i.e., as single terminal symbols. Further, the terminal expr represents expressions; a more complete grammar would use a nonterminal expr and have productions for nonterminal expr . Similarly, other is a terminal representing other statement constructs. The top-down construction of a parse tree like the one in Fig. 2.17, is done by starting with the root, labeled with the starting nonterminal stmt , and repeatedly performing the following two steps. 1\u3001At node N , labeled with nonterminal A , select one of the productions for A and construct children at N for the symbols in the production body. 2\u3001Find the next node at which a subtree is to be constructed, typically the leftmost unexpanded nonterminal of the tree. For some grammars, the above steps can be implemented during a single left-to-right scan of the input string. The current terminal being scanned in the input is frequently referred to as the lookahead symbol . Initially, the lookahead symbol is the first, i.e., leftmost, terminal of the input string. Figure 2.18 illustrates the construction of the parse tree in Fig. 2.17 for the input string for ( ; expr ; expr ) other Initially, the terminal for is the lookahead symbol , and the known part of the parse tree consists of the root, labeled with the starting nonterminal stmt in Fig. 2.18(a). The objective is to construct the remainder of the parse tree in such a way that the string generated by the parse tree matches the input string. For a match to occur, the nonterminal stmt in Fig. 2.18(a) must derive(\u6d3e\u751f\u3001expand) a string that starts with the lookahead symbol for . In the grammar of Fig. 2.16, there is just one production for stmt that can derive such a string, so we select it, and construct the children of the root labeled with the symbols in the production body. This expansion of the parse tree is shown in Fig. 2.18(b). Each of the three snapshots in Fig. 2.18 has arrows marking the lookahead symbol in the input and the node in the parse tree that is being considered. Once children are constructed at a node, we next consider the leftmost child. In Fig. 2.18(b), children have just been constructed at the root, and the leftmost child labeled with for is being considered. At the nonterminal node labeled optexpr , we repeat the process of selecting a production for a nonterminal. Productions with \\epsilon \\epsilon as the body (\" \\epsilon \\epsilon -productions\") require special treatment. For the moment, we use them as a default when no other production can be used; we return to them in Section 2.4.3. With nonterminal optexpr and lookahead ; , the \\epsilon \\epsilon -productions is used, since ; does not match the only other production for optexpr , which has terminal expr as its body. In general, the selection of a production for a nonterminal may involve trial-and-error ; that is, we may have to try a production and backtrack to try another production if the first is found to be unsuitable. A production is unsuitable if, after using the production, we cannot complete the tree to match the input string. Backtracking is not needed, however, in an important special case called predictive parsing, which we discuss next. NOTE: There is a good illustration of Top-Down Parser in this blog .","title":"2.4.1 Top-Down Parsing"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#242#predictive#parsing","text":"Recursive-descent parsing is a top-down method of syntax analysis in which a set of recursive procedures is used to process the input. One procedure is associated with each nonterminal of a grammar. Here, we consider a simple form of recursive-descent parsing , called predictive parsing , in which the lookahead symbol unambiguously determines the flow of control through the procedure body for each nonterminal . The sequence of procedure calls during the analysis of an input string implicitly defines a parse tree for the input, and can be used to build an explicit parse tree , if desired. The predictive parser in Fig. 2.19 consists of procedures for the nonterminals stmt and optexpr of the grammar in Fig. 2.16 and an additional procedure match, used to simplify the code for stmt and optexpr . Procedure match(t) compares its argument t with the lookahead symbol and advances to the next input terminal if they match. Thus match changes the value of variable lookahead, a global variable that holds the currently scanned input terminal. void stmt () { switch ( lookahead ) { case expr: match (expr); match (';'); break; case 'if': match ('if'); match ('('); match (expr); match (')'); stmt (); break; case 'for': match ('for'); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); break; case other; match (other); break; default: report (\"syntax error\"); } void optexpr () { if ( lookahead == expr ) match (expr); } void match (terminal t) { if ( lookahead == t ) lookahead = nextTerminal; else report (\"syntax error\"); } Figure 2.19: Pseudo code for a predictive parser NOTE: A predictive parsing approach is one that hardcodes the grammar into the program Parsing begins with a call of the procedure for the starting nonterminal stmt . With the same input as in Fig. 2.18, lookahead is initially the first terminal for . Procedure stmt executes code corresponding to the production $$ stmt \\to for ( optexpr ; optexpr ; optexpr ) stmt $$ In the code for the production body -- that is, the for case of procedure stmt --each terminal is matched with the lookahead symbol, and each nonterminal leads to a call of its procedure, in the following sequence of calls: match (for); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); Predictive parsing relies on information about the first symbols that can be generated by a production body. More precisely, let \\alpha \\alpha be a string of grammar symbols (terminals and/or nonterminals). We define FIRST (\\alpha) FIRST (\\alpha) to be the set of terminals that appear as the first symbols of one or more strings of terminals generated from \\alpha \\alpha . If \\alpha \\alpha is \\epsilon \\epsilon or can generate \\epsilon \\epsilon , then \\epsilon \\epsilon is also in FIRST (\\alpha) FIRST (\\alpha) . NOTE: Only when FIRST (\\alpha) FIRST (\\alpha) is known, can flow of control determine. The details of how one computes FIRST (\\alpha) FIRST (\\alpha) are in Section 4.4.2. Here, we shall just use ad hoc reasoning to deduce the symbols in FIRST (\\alpha) FIRST (\\alpha) ; typically, \\alpha \\alpha will either begin with a terminal, which is therefore the only symbol in FIRST (\\alpha) FIRST (\\alpha) , or \\alpha \\alpha will begin with a nonterminal whose production bodies begin with terminals, in which case these terminals are the only members of FIRST (\\alpha) FIRST (\\alpha) . For example, with respect to the grammar of Fig. 2.16, the following are correct calculations of FIRST FIRST . FIRST (stmt) = {expr; if; for; other} FIRST(expr ;) = {expr} NOTE: FIRST (stmt) corresponds to the case of void stmt () . The FIRST sets must be considered if there are two pro ductions A \\to \\alpha A \\to \\alpha and A \\to \\beta A \\to \\beta . Ignoring \\epsilon \\epsilon - productions for the moment, predictive parsing requires FIRST (\\alpha) FIRST (\\alpha) and FIRST(\\beta ) FIRST(\\beta ) to b e disjoint. The lookahead symbol can then be used to decide which production to use; if the lookahead symbol is in FIRST (\\alpha) FIRST (\\alpha) , then \\alpha \\alpha is used. Otherwise, if the lookahead symbol is in FIRST (\\beta) FIRST (\\beta) , then \\beta \\beta is used.","title":"2.4.2 Predictive Parsing"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#243#when#to#use#epsilonepsilon-productions","text":"","title":"2.4.3 When to Use \\epsilon\\epsilon-Productions"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/","text":"2.7 Symbol Tables Symbol tables are data structures that are used by compilers to hold information about source-program constructs. The information is collected incrementally by the analysis phases of a compiler and used by the synthesis phases to generate the target code . Entries in the symbol table contain information about an identifier such as its character string (or lexeme), its type, its position in storage,and any other relevant information. Symbol tables typically need to support multiple declarations of the same identifier within a program. From Section 1.6.1, the scope of a declaration is the portion of a program to which the declaration applies. We shall implement scopes by setting up a separate symbol table for each scope . A program block with declarations will have its own symbol table with an entry for each declaration in the block. This approach also works for other constructs that set up scopes; for example, a class would have its own table, with an entry for each field and method . This section contains a symbol-table module suitable for use with the Java translator fragments in this chapter. The module will be used as is when we put together the translator in Appendix A. Meanwhile, for simplicity, the main example of this section is a stripped-down(\u88ab\u7b80\u5316\u7684) language with just the key constructs that touch symbol tables; namely, blocks, declarations, and factors. All of the other statement and expression constructs are omitted so we can focus on the symbol-table operations. A program consists of blocks with optional declarations and statements consisting of single identifiers. Each such statement represents a use of the identifier. Here is a sample program in this language Who Creates Symbol-Table Entries? Symbol-table entries are created and used during the analysis phase by the lexical analyzer, the parser, and the semantic analyzer. In this chapter,we have the parser create entries. With its knowledge of the syntactic structure of a program, a parser is often in a better position than the lexical analyzer to distinguish among different declarations of an identifier.In some cases, a lexical analyzer can create a symbol-table entry as so on as it sees the characters that make up a lexeme. More often, the lexical analyzer can only return to the parser a token, say id, along with a pointer to the lexeme. Only the parser, however, can decide whether to use a previously created symbol-table entry or create a new one for the identifier.","title":"2.7-Symbol-Tables"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/#27#symbol#tables","text":"Symbol tables are data structures that are used by compilers to hold information about source-program constructs. The information is collected incrementally by the analysis phases of a compiler and used by the synthesis phases to generate the target code . Entries in the symbol table contain information about an identifier such as its character string (or lexeme), its type, its position in storage,and any other relevant information. Symbol tables typically need to support multiple declarations of the same identifier within a program. From Section 1.6.1, the scope of a declaration is the portion of a program to which the declaration applies. We shall implement scopes by setting up a separate symbol table for each scope . A program block with declarations will have its own symbol table with an entry for each declaration in the block. This approach also works for other constructs that set up scopes; for example, a class would have its own table, with an entry for each field and method . This section contains a symbol-table module suitable for use with the Java translator fragments in this chapter. The module will be used as is when we put together the translator in Appendix A. Meanwhile, for simplicity, the main example of this section is a stripped-down(\u88ab\u7b80\u5316\u7684) language with just the key constructs that touch symbol tables; namely, blocks, declarations, and factors. All of the other statement and expression constructs are omitted so we can focus on the symbol-table operations. A program consists of blocks with optional declarations and statements consisting of single identifiers. Each such statement represents a use of the identifier. Here is a sample program in this language Who Creates Symbol-Table Entries? Symbol-table entries are created and used during the analysis phase by the lexical analyzer, the parser, and the semantic analyzer. In this chapter,we have the parser create entries. With its knowledge of the syntactic structure of a program, a parser is often in a better position than the lexical analyzer to distinguish among different declarations of an identifier.In some cases, a lexical analyzer can create a symbol-table entry as so on as it sees the characters that make up a lexeme. More often, the lexical analyzer can only return to the parser a token, say id, along with a pointer to the lexeme. Only the parser, however, can decide whether to use a previously created symbol-table entry or create a new one for the identifier.","title":"2.7 Symbol Tables"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/","text":"Symbol table Symbol table\u5373\u7b26\u53f7\u8868\uff0c\u5728 Chapter-1-Introduction\\1.2-The-Structure-of-a-Compiler.md \u4e2d\u63d0\u53ca\u4e86\u5b83\uff0c\u672c\u7ae0\u5bf9\u5b83\u8fdb\u884c\u4e13\u95e8\u603b\u7ed3\u3002 \u9f99\u4e66\u4e2d\u6d89\u53casymbol table\u7684\u7ae0\u8282 \u7b26\u53f7\u8868\u4e5f\u662f\u4e00\u95e8\u79d1\u5b66\uff0c\u9700\u8981\u5bf9\u5b83\u4ed4\u7ec6\u7814\u7a76\uff0c\u672c\u4e66\u4e2d\u6d89\u53ca\u7b26\u53f7\u8868\u7684\u7ae0\u8282\u6709\uff1a 1.2.1 Lexical Analysis 1.2.7 Symbol-Table Management 2.7 Symbol Tables What is symbol\uff1f \u7ed3\u5408\u5177\u4f53\u8bed\u8a00\u6765\u8bf4\uff1a C++ std Symbol Index symbol table\u7684\u91cd\u8981\u4ef7\u503c symbol table\u4ee5\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u5b58\u50a8\u7740\u5173\u4e8esource code\u7684\u4fe1\u606f\u3002 compile\u9636\u6bb5\uff1a\u5728compile\u7684\u5404\u4e2a\u9636\u6bb5\u90fd\u9700\u8981\u5b83\u3002 debug\u9636\u6bb5\uff1a\u5728debug\u7684\u65f6\u5019\uff0c\u53ea\u6709\u8bfb\u53d6\u4e86symbol table\u4e86\uff0c\u624d\u80fd\u591f\u7075\u6d3b\u5730\u8bbe\u7f6e\u3002 wikipedia Symbol table In computer science , a symbol table is a data structure used by a language translator such as a compiler or interpreter , where each identifier (a.k.a. symbol ) in a program's source code is associated with information relating to its declaration or appearance in the source. In other words, the entries of a symbol table store the information related to the entry's corresponding symbol. Implementation \u53ef\u4ee5\u53c2\u8003\u4e0b\u9762\u4e24\u79cd\u5b9e\u73b0\uff1a CPython symbol table \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Python\\Language\\Developer's-guide\\25-Design-of-CPython's-Compiler\\04-cpython-symbol-table \u7ae0\u8282\u3002 Clang symbol table \u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Compile\\Implementation\\LLVM\\Clang \u3002","title":"Symbol-table"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#symbol#table","text":"Symbol table\u5373\u7b26\u53f7\u8868\uff0c\u5728 Chapter-1-Introduction\\1.2-The-Structure-of-a-Compiler.md \u4e2d\u63d0\u53ca\u4e86\u5b83\uff0c\u672c\u7ae0\u5bf9\u5b83\u8fdb\u884c\u4e13\u95e8\u603b\u7ed3\u3002","title":"Symbol table"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#symbol#table_1","text":"\u7b26\u53f7\u8868\u4e5f\u662f\u4e00\u95e8\u79d1\u5b66\uff0c\u9700\u8981\u5bf9\u5b83\u4ed4\u7ec6\u7814\u7a76\uff0c\u672c\u4e66\u4e2d\u6d89\u53ca\u7b26\u53f7\u8868\u7684\u7ae0\u8282\u6709\uff1a 1.2.1 Lexical Analysis 1.2.7 Symbol-Table Management 2.7 Symbol Tables","title":"\u9f99\u4e66\u4e2d\u6d89\u53casymbol table\u7684\u7ae0\u8282"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#what#is#symbol","text":"\u7ed3\u5408\u5177\u4f53\u8bed\u8a00\u6765\u8bf4\uff1a C++ std Symbol Index","title":"What is symbol\uff1f"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#symbol#table_2","text":"symbol table\u4ee5\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u5b58\u50a8\u7740\u5173\u4e8esource code\u7684\u4fe1\u606f\u3002 compile\u9636\u6bb5\uff1a\u5728compile\u7684\u5404\u4e2a\u9636\u6bb5\u90fd\u9700\u8981\u5b83\u3002 debug\u9636\u6bb5\uff1a\u5728debug\u7684\u65f6\u5019\uff0c\u53ea\u6709\u8bfb\u53d6\u4e86symbol table\u4e86\uff0c\u624d\u80fd\u591f\u7075\u6d3b\u5730\u8bbe\u7f6e\u3002","title":"symbol table\u7684\u91cd\u8981\u4ef7\u503c"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#wikipedia#symbol#table","text":"In computer science , a symbol table is a data structure used by a language translator such as a compiler or interpreter , where each identifier (a.k.a. symbol ) in a program's source code is associated with information relating to its declaration or appearance in the source. In other words, the entries of a symbol table store the information related to the entry's corresponding symbol.","title":"wikipedia Symbol table"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#implementation","text":"\u53ef\u4ee5\u53c2\u8003\u4e0b\u9762\u4e24\u79cd\u5b9e\u73b0\uff1a","title":"Implementation"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#cpython#symbol#table","text":"\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 Python\\Language\\Developer's-guide\\25-Design-of-CPython's-Compiler\\04-cpython-symbol-table \u7ae0\u8282\u3002","title":"CPython symbol table"},{"location":"2-A-Simple-Syntax-Directed-Translator/2.7-Symbol-Tables/Symbol-table/#clang#symbol#table","text":"\u53c2\u89c1\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Compile\\Implementation\\LLVM\\Clang \u3002","title":"Clang symbol table"},{"location":"3-Lexical-Analysis/","text":"Chapter 3 Lexical Analysis In this chapter we show how to construct a lexical analyzer . To implement a lexical analyzer by hand, it helps to start with a diagram or other description for the lexemes of each token. We can then write code to identify each occurrence of each lexeme on the input and to return information about the token identified. NOTE: Manual implementation just to make it easier for the readers to understand how the lexical analyzer works, what should be focused on is how does it work automatically and how to implement it. We can also produce a lexical analyzer automatically by specifying the lexeme patterns to a lexical-analyzer generator and compiling those patterns into code that functions as a lexical analyzer . This approach makes it easier to modify a lexical analyzer , since we have only to rewrite the affected patterns, not the entire program. It also speeds up the process of implementing the lexical analyzer, since the programmer specifies the software at the very high level of patterns and relies on the generator to produce the detailed code. We shall introduce in Section 3.5 a lexical-analyzer generator called Lex (or Flex in a more recent embodiment). NOTE: In fact, the essence lies in how lex works. We begin the study of lexical-analyzer generators by introducing regular expressions, a convenient notation for specifying lexeme patterns. We show how this notation can be transformed, first into nondeterministic automata and then into deterministic automata. The latter two notations can be used as input to a \"driver,\" that is, code which simulates these automata and uses them as a guide to determining the next token. This driver and the specification of the automaton form the nucleus of the lexical analyzer. NOTE: Nucleus of the lexical analyzer is what we should focus on. NOTE: This chapter is mainly about algorithms of lexical analysis, I extend to include the algorithms relevant to lexical analysis in wikipedia which is listed in section wikipedia String","title":3},{"location":"3-Lexical-Analysis/#chapter#3#lexical#analysis","text":"In this chapter we show how to construct a lexical analyzer . To implement a lexical analyzer by hand, it helps to start with a diagram or other description for the lexemes of each token. We can then write code to identify each occurrence of each lexeme on the input and to return information about the token identified. NOTE: Manual implementation just to make it easier for the readers to understand how the lexical analyzer works, what should be focused on is how does it work automatically and how to implement it. We can also produce a lexical analyzer automatically by specifying the lexeme patterns to a lexical-analyzer generator and compiling those patterns into code that functions as a lexical analyzer . This approach makes it easier to modify a lexical analyzer , since we have only to rewrite the affected patterns, not the entire program. It also speeds up the process of implementing the lexical analyzer, since the programmer specifies the software at the very high level of patterns and relies on the generator to produce the detailed code. We shall introduce in Section 3.5 a lexical-analyzer generator called Lex (or Flex in a more recent embodiment). NOTE: In fact, the essence lies in how lex works. We begin the study of lexical-analyzer generators by introducing regular expressions, a convenient notation for specifying lexeme patterns. We show how this notation can be transformed, first into nondeterministic automata and then into deterministic automata. The latter two notations can be used as input to a \"driver,\" that is, code which simulates these automata and uses them as a guide to determining the next token. This driver and the specification of the automaton form the nucleus of the lexical analyzer. NOTE: Nucleus of the lexical analyzer is what we should focus on. NOTE: This chapter is mainly about algorithms of lexical analysis, I extend to include the algorithms relevant to lexical analysis in wikipedia which is listed in section wikipedia String","title":"Chapter 3 Lexical Analysis"},{"location":"3-Lexical-Analysis/3.1-The-Role-of-the-Lexical-Analyzer/","text":"3.1 The Role of the Lexical Analyzer As the first phase of a compiler, the main task of the lexical analyzer is to read the input characters of the source program, group them into lexemes , and produce as output a sequence of tokens for each lexeme in the source program. The stream of tokens is sent to the parser for syntax analysis . It is common for the lexical analyzer to interact with the symbol table as well. When the lexical analyzer discovers a lexeme constituting an identifier, it needs to enter that lexeme into the symbol table. In some cases, information regarding the kind of identifier may be read from the symbol table by the lexical analyzer to assist it in determining the proper token it must pass to the parser.","title":"Introduction"},{"location":"3-Lexical-Analysis/3.1-The-Role-of-the-Lexical-Analyzer/#31#the#role#of#the#lexical#analyzer","text":"As the first phase of a compiler, the main task of the lexical analyzer is to read the input characters of the source program, group them into lexemes , and produce as output a sequence of tokens for each lexeme in the source program. The stream of tokens is sent to the parser for syntax analysis . It is common for the lexical analyzer to interact with the symbol table as well. When the lexical analyzer discovers a lexeme constituting an identifier, it needs to enter that lexeme into the symbol table. In some cases, information regarding the kind of identifier may be read from the symbol table by the lexical analyzer to assist it in determining the proper token it must pass to the parser.","title":"3.1 The Role of the Lexical Analyzer"},{"location":"3-Lexical-Analysis/3.4-Recognition-of-Tokens/","text":"3.4 Recognition of Tokens NOTE: This chapter describes conversion from regular-expression patterns to transition diagrams by hand, which is intended to help reader to understand the finite automaton (finite state machine) widely used in engineering described in section 3.5. In essence, a transition diagram is a visual representation of a finite state machine. In the previous section we learned how to express patterns using regular expressions. Now, we must study how to take the patterns for all the needed tokens and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns. Our discussion will make use of the following running example. $$ \\begin{align} stmt \\to if \\; expr \\; then \\; stmt \\ | if \\; expr \\; then \\; stmt \\; else \\; stmt \\ | ? \\ expr \\to term \\; relop \\; term \\ | term \\ term \\to id \\ | number \\end{align} $$ Figure 3.10: A grammar for branching statements Aho-Corasick algorithm NOTE: The main content of this chapter is passed, but in the exercise, the author introduce Aho-Corasick algorithm and KMP algorithm which are interesting. The following exercises, up to Exercise 3.4.12, introduce the Aho-Corasick algorithm for recognizing a collection of keywords in a text string in time proportional to the length of the text and the sum of the length of the keywords. This algorithm uses a special form of transition diagram called a trie . A trie is a tree-structured transition diagram with distinct labels on the edges leading from a node to its children. Leaves of the trie represent recognized keywords. NOTE: A trie is a tree-structured transition diagram or finite automaton , do you see the power of tree structure here as I do Knuth, Morris, and Pratt presented an algorithm for recognizing a single keyword b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n (length of the word is n) in a text string. Here the trie is a transition diagram with n+ 1 states, 0 through n. State 0 is the initial state, and state n represents acceptance, that is, discovery of the keyword. From each state s from 0 through n - 1, there is a transition to state s + 1, labeled by symbol b_{s+1} b_{s+1} . For example, the trie for the keyword ababaa is: In order to process text strings rapidly and search those strings for a keyword, it is useful to define, for keyword b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n and position s in that keyword (corresponding to state s of its trie), a failure function , f(s) , computed as in Fig. 3.19. The objective is that b_1 b_2 \\dots b_{f(s)} b_1 b_2 \\dots b_{f(s)} is the longest proper prefix of b_1 b_2 \\dots b_s b_1 b_2 \\dots b_s that is also a suffix of b_1 b_2 \\dots b_s b_1 b_2 \\dots b_s . The reason f (s) f (s) is important is that if we are trying to match a text string for b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n , and we have matched the first s positions, but we then fail (i.e., the next position of the text string does not hold b_{s+1} b_{s+1} ), then f (s) f (s) is the longest prefix of b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n that could possibly match the text string up to the point we are at. Of course, the next character of the text string must be b_{f (s)+1} b_{f (s)+1} , or else we still have problems and must consider a yet shorter prefix, which will be b_{f (f (s))} b_{f (f (s))} . t = 0; f (1) = 0; for (s = 1; s < n; s + +) f while (t > 0 && b[s+1] ! = b[t+1]) t = f (t); if (b[s+1] == b[t+1]) { t = t + 1; f (s + 1) = t; } else f (s + 1) = 0; } Figure 3.19: Algorithm to compute the failure function for keyword b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n As an example, the failure function for the trie constructed above for ababaa is: s 1 2 3 4 5 6 f(s) 0 0 1 2 3 1 For instance, states 3 and 1 represent prefixes aba and a , respectively. f (3) = 1 because a is the longest proper prefix of aba that is also a suffix of aba . Also, f (2) = 0 , because the longest proper prefix of ab that is also a suffix is the empty string. Aho and Corasick generalized the KMP algorithm to recognize any of a set of keywords in a text string. In this case, the trie is a true tree, with branching from the root. There is one state for every string that is a prefix (not necessarily proper) of any keyword. The parent of a state corresponding to string b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n is the state that corresponds to b_1 b_2 \\dots b_k b_1 b_2 \\dots b_k . A state is accepting if it corresponds to a complete keyword. For example, Fig. 3.21 shows the trie for the keywords he , she , his , and hers .","title":"Introduction"},{"location":"3-Lexical-Analysis/3.4-Recognition-of-Tokens/#34#recognition#of#tokens","text":"NOTE: This chapter describes conversion from regular-expression patterns to transition diagrams by hand, which is intended to help reader to understand the finite automaton (finite state machine) widely used in engineering described in section 3.5. In essence, a transition diagram is a visual representation of a finite state machine. In the previous section we learned how to express patterns using regular expressions. Now, we must study how to take the patterns for all the needed tokens and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns. Our discussion will make use of the following running example. $$ \\begin{align} stmt \\to if \\; expr \\; then \\; stmt \\ | if \\; expr \\; then \\; stmt \\; else \\; stmt \\ | ? \\ expr \\to term \\; relop \\; term \\ | term \\ term \\to id \\ | number \\end{align} $$ Figure 3.10: A grammar for branching statements","title":"3.4 Recognition of Tokens"},{"location":"3-Lexical-Analysis/3.4-Recognition-of-Tokens/#aho-corasick#algorithm","text":"NOTE: The main content of this chapter is passed, but in the exercise, the author introduce Aho-Corasick algorithm and KMP algorithm which are interesting. The following exercises, up to Exercise 3.4.12, introduce the Aho-Corasick algorithm for recognizing a collection of keywords in a text string in time proportional to the length of the text and the sum of the length of the keywords. This algorithm uses a special form of transition diagram called a trie . A trie is a tree-structured transition diagram with distinct labels on the edges leading from a node to its children. Leaves of the trie represent recognized keywords. NOTE: A trie is a tree-structured transition diagram or finite automaton , do you see the power of tree structure here as I do Knuth, Morris, and Pratt presented an algorithm for recognizing a single keyword b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n (length of the word is n) in a text string. Here the trie is a transition diagram with n+ 1 states, 0 through n. State 0 is the initial state, and state n represents acceptance, that is, discovery of the keyword. From each state s from 0 through n - 1, there is a transition to state s + 1, labeled by symbol b_{s+1} b_{s+1} . For example, the trie for the keyword ababaa is: In order to process text strings rapidly and search those strings for a keyword, it is useful to define, for keyword b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n and position s in that keyword (corresponding to state s of its trie), a failure function , f(s) , computed as in Fig. 3.19. The objective is that b_1 b_2 \\dots b_{f(s)} b_1 b_2 \\dots b_{f(s)} is the longest proper prefix of b_1 b_2 \\dots b_s b_1 b_2 \\dots b_s that is also a suffix of b_1 b_2 \\dots b_s b_1 b_2 \\dots b_s . The reason f (s) f (s) is important is that if we are trying to match a text string for b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n , and we have matched the first s positions, but we then fail (i.e., the next position of the text string does not hold b_{s+1} b_{s+1} ), then f (s) f (s) is the longest prefix of b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n that could possibly match the text string up to the point we are at. Of course, the next character of the text string must be b_{f (s)+1} b_{f (s)+1} , or else we still have problems and must consider a yet shorter prefix, which will be b_{f (f (s))} b_{f (f (s))} . t = 0; f (1) = 0; for (s = 1; s < n; s + +) f while (t > 0 && b[s+1] ! = b[t+1]) t = f (t); if (b[s+1] == b[t+1]) { t = t + 1; f (s + 1) = t; } else f (s + 1) = 0; } Figure 3.19: Algorithm to compute the failure function for keyword b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n As an example, the failure function for the trie constructed above for ababaa is: s 1 2 3 4 5 6 f(s) 0 0 1 2 3 1 For instance, states 3 and 1 represent prefixes aba and a , respectively. f (3) = 1 because a is the longest proper prefix of aba that is also a suffix of aba . Also, f (2) = 0 , because the longest proper prefix of ab that is also a suffix is the empty string. Aho and Corasick generalized the KMP algorithm to recognize any of a set of keywords in a text string. In this case, the trie is a true tree, with branching from the root. There is one state for every string that is a prefix (not necessarily proper) of any keyword. The parent of a state corresponding to string b_1 b_2 \\dots b_n b_1 b_2 \\dots b_n is the state that corresponds to b_1 b_2 \\dots b_k b_1 b_2 \\dots b_k . A state is accepting if it corresponds to a complete keyword. For example, Fig. 3.21 shows the trie for the keywords he , she , his , and hers .","title":"Aho-Corasick algorithm"},{"location":"3-Lexical-Analysis/3.6-Finite-Automata/","text":"3.6 Finite Automata We shall now discover how Lex turns its input program into a lexical analyzer . At the heart of the transition is the formalism known as finite automata . These are essentially graphs, like transition diagrams, with a few differences: 1\u3001 Finite automata are recognizers ; they simply say \"yes\" or \"no\" about each possible input string. 2\u3001 Finite automata come in two favors: (a) Nondeterministic finite automata (NFA) have no restrictions on the labels of their edges. A symbol can label several edges out of the same state, and \\epsilon \\epsilon , the empty string, is a possible label. (b) Deterministic finite automata (DFA) have, for each state, and for each symbol of its input alphabet exactly one edge with that symbol leaving that state. Both deterministic and nondeterministic finite automata are capable of recognizing the same languages. In fact these languages are exactly the same languages, called the regular languages , that regular expressions can describe. There is a small lacuna: as we defined them, regular expressions cannot describe the empty language, since we would never want to use this pattern in practice. However, finite automata can define the empty language. In the theory, \\phi \\phi is treated as an additional regular expression for the sole purpose of defining the empty language. NOTE: In chapter 3.4 Recognition of Tokens, there are description as below A trie is a tree-structured transition diagram or finite automaton , It is known to us all that tree is graph. In first paragraph, the author also regard finite automata as graph. 3.6.1 Nondeterministic Finite Automata A nondeterministic finite automaton (NFA) consists of: 1\u3001A finite set of states S . 2\u3001A set of input symbols $ \\Sigma $, the input alphabet . We assume that \\epsilon \\epsilon , which stands for the empty string, is never a member of $ \\Sigma $. 3\u3001A transition function that gives, for each state, and for each symbol in $ \\Sigma \\cup {\\epsilon}$ a set of next states . 4\u3001A state s_0 s_0 from S S that is distinguished as the start state (or initial state). 5\u3001A set of states F F , a subset of S S , that is distinguished as the accepting states (or final states ). We can represent either an NFA or DFA by a transition graph , where the nodes are states and the labeled edges represent the transition function. There is an edge labeled a from state s s to state t t if and only if t t is one of the next states for state s s and input a a . This graph(transition graph) is very much like a transition diagram , except: a) The same symbol can label edges from one state to several different states, and b) An edge may be labeled by \\epsilon \\epsilon , the empty string, instead of, or in addition to, symbols from the input alphabet. NOTE: Transition diagram is formally described in chapter 3.4.1 Transition Diagrams Example 3.14 : The transition graph for an NFA recognizing the language of regular expression (a|b)*abb (a|b)*abb is shown in Fig. 3.24. This abstract example, describing all strings of a 's and b 's ending in the particular string abb , will be used throughout this section. It is similar to regular expressions that describe languages of real interest, however. For instance, an expression describing all files whose name ends in .o is any *.o , where any stands for any printable character. NOTE: There are two edges labeled with a from state 0 while this is not allowed in DFA. Below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. Following our convention for transition diagrams, the double circle around state 3 indicates that this state is accepting. Notice that the only ways to get from the start state 0 to the accepting state is to follow some path that stays in state 0 for a while, then go es to states 1, 2, and 3 by reading abb from the input. Thus, the only strings getting to the accepting state are those that end in abb . 3.6.2 Transition Tables We can also represent an NFA by a transition table , whose rows correspond to states, and whose columns correspond to the input symbols and \\epsilon \\epsilon . The entry for a given state and input is the value of the transition function applied to those arguments. If the transition function has no information about that state-input pair, we put \\phi \\phi in the table for the pair. Example 3.15 : The transition table for the NFA of Fig. 3.24 is shown in Fig. 3.25. The transition table has the advantage that we can easily find the transitions on a given state and input. Its disadvantage is that it takes a lot of space, when the input alphabet is large, yet most states do not have any moves on most of the input symbols. NOTE: Transition diagrams and transition table correspond to two kind of data structure describing finite automata 3.6.4 Deterministic Finite Automata A deterministic finite automaton (DFA) is a special case of an NFA where: 1\u3001There are no moves on input \\epsilon \\epsilon , and 2\u3001For each state s and input symbol a , there is exactly one edge out of s labeled a . If we are using a transition table to represent a DFA, then each entry is a single state. We may therefore represent this state without the curly braces that we use to form sets. While the NFA is an abstract representation of an algorithm to recognize the strings of a certain language, the DFA is a simple, concrete algorithm for recognizing strings. It is fortunate indeed that every regular expression and every NFA can be converted to a DFA accepting the same language, because it is the DFA that we really implement or simulate when building lexical analyzers. The following algorithm shows how to apply a DFA to a string. Algorithm 3.18 : Simulating a DFA. INPUT : An input string x terminated by an end-of-file character eof . A DFA D with start state s_0 s_0 , accepting states F , and transition function move . OUTPUT : Answer \"yes\" if D accepts x ;\"no\" otherwise. METHOD : Apply the algorithm in Fig. 3.27 to the input string x . The function move(s, c) gives the state to which there is an edge from state s on input c . The function nextChar returns the next character of the input string x . Example 3.19 : In Fig. 3.28 we see the transition graph of a DFA accepting the language (a|b)*abb (a|b)*abb , the same as that accepted by the NFA of Fig. 3.24. Given the input string ababb , this DFA enters the sequence of states 0, 1, 2, 1, 2, 3 and returns \" yes.\"","title":"Introduction"},{"location":"3-Lexical-Analysis/3.6-Finite-Automata/#36#finite#automata","text":"We shall now discover how Lex turns its input program into a lexical analyzer . At the heart of the transition is the formalism known as finite automata . These are essentially graphs, like transition diagrams, with a few differences: 1\u3001 Finite automata are recognizers ; they simply say \"yes\" or \"no\" about each possible input string. 2\u3001 Finite automata come in two favors: (a) Nondeterministic finite automata (NFA) have no restrictions on the labels of their edges. A symbol can label several edges out of the same state, and \\epsilon \\epsilon , the empty string, is a possible label. (b) Deterministic finite automata (DFA) have, for each state, and for each symbol of its input alphabet exactly one edge with that symbol leaving that state. Both deterministic and nondeterministic finite automata are capable of recognizing the same languages. In fact these languages are exactly the same languages, called the regular languages , that regular expressions can describe. There is a small lacuna: as we defined them, regular expressions cannot describe the empty language, since we would never want to use this pattern in practice. However, finite automata can define the empty language. In the theory, \\phi \\phi is treated as an additional regular expression for the sole purpose of defining the empty language. NOTE: In chapter 3.4 Recognition of Tokens, there are description as below A trie is a tree-structured transition diagram or finite automaton , It is known to us all that tree is graph. In first paragraph, the author also regard finite automata as graph.","title":"3.6 Finite Automata"},{"location":"3-Lexical-Analysis/3.6-Finite-Automata/#361#nondeterministic#finite#automata","text":"A nondeterministic finite automaton (NFA) consists of: 1\u3001A finite set of states S . 2\u3001A set of input symbols $ \\Sigma $, the input alphabet . We assume that \\epsilon \\epsilon , which stands for the empty string, is never a member of $ \\Sigma $. 3\u3001A transition function that gives, for each state, and for each symbol in $ \\Sigma \\cup {\\epsilon}$ a set of next states . 4\u3001A state s_0 s_0 from S S that is distinguished as the start state (or initial state). 5\u3001A set of states F F , a subset of S S , that is distinguished as the accepting states (or final states ). We can represent either an NFA or DFA by a transition graph , where the nodes are states and the labeled edges represent the transition function. There is an edge labeled a from state s s to state t t if and only if t t is one of the next states for state s s and input a a . This graph(transition graph) is very much like a transition diagram , except: a) The same symbol can label edges from one state to several different states, and b) An edge may be labeled by \\epsilon \\epsilon , the empty string, instead of, or in addition to, symbols from the input alphabet. NOTE: Transition diagram is formally described in chapter 3.4.1 Transition Diagrams Example 3.14 : The transition graph for an NFA recognizing the language of regular expression (a|b)*abb (a|b)*abb is shown in Fig. 3.24. This abstract example, describing all strings of a 's and b 's ending in the particular string abb , will be used throughout this section. It is similar to regular expressions that describe languages of real interest, however. For instance, an expression describing all files whose name ends in .o is any *.o , where any stands for any printable character. NOTE: There are two edges labeled with a from state 0 while this is not allowed in DFA. Below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. Following our convention for transition diagrams, the double circle around state 3 indicates that this state is accepting. Notice that the only ways to get from the start state 0 to the accepting state is to follow some path that stays in state 0 for a while, then go es to states 1, 2, and 3 by reading abb from the input. Thus, the only strings getting to the accepting state are those that end in abb .","title":"3.6.1 Nondeterministic Finite Automata"},{"location":"3-Lexical-Analysis/3.6-Finite-Automata/#362#transition#tables","text":"We can also represent an NFA by a transition table , whose rows correspond to states, and whose columns correspond to the input symbols and \\epsilon \\epsilon . The entry for a given state and input is the value of the transition function applied to those arguments. If the transition function has no information about that state-input pair, we put \\phi \\phi in the table for the pair. Example 3.15 : The transition table for the NFA of Fig. 3.24 is shown in Fig. 3.25. The transition table has the advantage that we can easily find the transitions on a given state and input. Its disadvantage is that it takes a lot of space, when the input alphabet is large, yet most states do not have any moves on most of the input symbols. NOTE: Transition diagrams and transition table correspond to two kind of data structure describing finite automata","title":"3.6.2 Transition Tables"},{"location":"3-Lexical-Analysis/3.6-Finite-Automata/#364#deterministic#finite#automata","text":"A deterministic finite automaton (DFA) is a special case of an NFA where: 1\u3001There are no moves on input \\epsilon \\epsilon , and 2\u3001For each state s and input symbol a , there is exactly one edge out of s labeled a . If we are using a transition table to represent a DFA, then each entry is a single state. We may therefore represent this state without the curly braces that we use to form sets. While the NFA is an abstract representation of an algorithm to recognize the strings of a certain language, the DFA is a simple, concrete algorithm for recognizing strings. It is fortunate indeed that every regular expression and every NFA can be converted to a DFA accepting the same language, because it is the DFA that we really implement or simulate when building lexical analyzers. The following algorithm shows how to apply a DFA to a string. Algorithm 3.18 : Simulating a DFA. INPUT : An input string x terminated by an end-of-file character eof . A DFA D with start state s_0 s_0 , accepting states F , and transition function move . OUTPUT : Answer \"yes\" if D accepts x ;\"no\" otherwise. METHOD : Apply the algorithm in Fig. 3.27 to the input string x . The function move(s, c) gives the state to which there is an edge from state s on input c . The function nextChar returns the next character of the input string x . Example 3.19 : In Fig. 3.28 we see the transition graph of a DFA accepting the language (a|b)*abb (a|b)*abb , the same as that accepted by the NFA of Fig. 3.24. Given the input string ababb , this DFA enters the sequence of states 0, 1, 2, 1, 2, 3 and returns \" yes.\"","title":"3.6.4 Deterministic Finite Automata"},{"location":"3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/","text":"3.7 From Regular Expressions to Automata NOTE: What this chapter describe is mainly three algorithms Name Function chapter Thompson's construction algorithm transforming a regular expression into an equivalent nondeterministic finite automaton (NFA) 3.7.4 Construction of an NFA from a Regular Expression subset construction algorithm converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language 3.7.1 Conversion of an NFA to a DFA DFA minimization transforming a given deterministic finite automaton (DFA) into an equivalent DFA that has a minimum number of states. Using the algorithms above, a regular expression can be converted to the corresponding best DFA. The regular expression is the notation of choice for describing lexical analyzers and other pattern-processing software, as was reflected in Section 3.5. However, implementation of that software requires the simulation of a DFA, as in Algorithm 3.18, or perhaps simulation of an NFA. Because an NFA often has a choice of move on an input symbol (as Fig. 3.24 does on input a from state 0) or on \\epsilon \\epsilon (as Fig. 3.26 does from state 0), or even a choice of making a transition on \\epsilon \\epsilon or on a real input symbol, its simulation is less straightforward than for a DFA. Thus often it is important to convert an NFA to a DFA that accepts the same language. 3.7.1 Conversion of an NFA to a DFA NOTE: The subset construction explained in this book is too abstract to grasp, wikipedia's explanation about this algorithm is more intuitionistic. The general idea behind the subset construction is that each state of the constructed DFA corresponds to a set of NFA states. After reading input a_1 a_2 \\dots a_n a_1 a_2 \\dots a_n , the DFA is in that state which corresponds to the set of states that the NFA can reach, from its start state, following paths labeled a_1 a_2 \\dots a_n a_1 a_2 \\dots a_n . It is p ossible that the number of DFA states is exponential in the number of NFA states, which could lead to difficulties when we try to implement this DFA. However, part of the p ower of the automaton-based approach to lexical analysis is that for real languages, the NFA and DFA have approximately the same number of states, and the exponential behavior is not seen. Algorithm 3.20 : The subset construction of a DFA from an NFA. INPUT : An NFA N . OUTPUT : A DFA D accepting the same language as N . METHOD : Our algorithm constructs a transition table D_{tran} D_{tran} for D . Each state of D is a set of NFA states, and we construct D_{tran} D_{tran} so D will simulate \"in parallel\" all possible moves N can make on a given input string. Our first problem is to deal with \\epsilon \\epsilon -transitions of N properly. In Fig. 3.31 we see the definitions of several functions that describe basic computations on the states of N that are needed in the algorithm. Note that s is a single state of N , while T is a set of states of N . OPERATION DESCRIPTION \\epsilon-closure(s) \\epsilon-closure(s) Set of NFA states reachable from NFA state s on \\epsilon \\epsilon -transitions alone. \\epsilon-closure(T) \\epsilon-closure(T) Set of NFA states reachable from some NFA state s in set T on \\epsilon \\epsilon -transitions alone; = \\cup _{s \\in T} {\\epsilon-closure(s)} = \\cup _{s \\in T} {\\epsilon-closure(s)} move(T, a) Set of NFA states to which there is a transition on input symbol a from some state s in T . Figure 3.31: Operations on NFA states We must explore those sets of states that N can be in after seeing some input string. As a basis, before reading the first input symbol, N can be in any of the states of \\epsilon-closure(s_0) \\epsilon-closure(s_0) , where s_0 s_0 is its start state . For the induction, suppose that N can be in set of states T after reading input string x . If it next reads input a , then N can immediately go to any of the states in move (T , a) move (T , a) . However, after reading a , it may also make several \\epsilon-transitions \\epsilon-transitions ; thus N could be in any state of \\epsilon-closure(move(T, a)) \\epsilon-closure(move(T, a)) after reading input xa . Following these ideas, the construction of the set of D 's states, D_{states} D_{states} , and its transition function D_{tran} D_{tran} , is shown in Fig. 3.32. The start state of D is \\epsilon-closure(s_0) \\epsilon-closure(s_0) , and the accepting states of D are all those sets of N 's states that include at least one accepting state of N . To complete our description of the subset construction, we need only to show how \\epsilon-closure(T) \\epsilon-closure(T) is computed for any set of NFA states T . This process, shown in Fig. 3.33, is a straightforward search in a graph from a set of states. In this case, imagine that only the \\epsilon \\epsilon -labeled edges are available in the graph. 3.7.4 Construction of an NFA from a Regular Expression We now give an algorithm for converting any regular expression to an NFA that defines the same language. The algorithm is syntax-directed , in the sense that it works recursively up the parse tree for the regular expression. For each sub expression the algorithm constructs an NFA with a single accepting state. NOTE: bottom-up NOTE: How to build a parse tree for a regular expression? Efficiently building a parse tree from a regular expression Algorithm 3.23 : The McNaughton-Yamada-Thompson algorithm to convert a regular expression to an NFA. INPUT : A regular expression r over alphabet \\Sigma \\Sigma . OUTPUT : An NFA N accepting L(r ) . METHOD: Begin by parsing r into its constituent subexpressions. The rules for constructing an NFA consist of basis rules for handling subexpressions with no operators, and inductive rules for constructing larger NFA's from the NFA's for the immediate subexpressions of a given expression. BASIS : For expression \\epsilon \\epsilon construct the NFA Here, i is a new state, the start state of this NFA, and f is another new state, the accepting state for the NFA. For any subexpression a in \\Sigma \\Sigma , construct the NFA where again i and f are new states, the start and accepting states, respectively. Note that in both of the basis constructions, we construct a distinct NFA, with new states, for every occurrence of \\epsilon \\epsilon or some a as a subexpression of r . INDUCTION : Suppose N (s) and N (t) are NFA's for regular expressions s and t , respectively. a) Suppose r = s|t . Then N (r ) , the NFA for r , is constructed as in Fig. 3.40. Here, i and f are new states, the start and accepting states of N (r ) , respectively. There are \\epsilon \\epsilon -transitions from i to the start states of N (s) and N (t) , and each of their accepting states have \\epsilon \\epsilon -transitions to the accepting state f . Note that the accepting states of N (s) and N (t) are not accepting in N (r) . Since any path from i to f must pass through either N (s) or N (t) exclusively, and since the label of that path is not changed by the \\epsilon \\epsilon 's leaving i or entering f , we conclude that N (r ) accepts L(s) \\cup L(t) L(s) \\cup L(t) , which is the same as L(r ) . That is, Fig. 3.40 is a correct construction for the union operator. b) Suppose r = st . Then construct N (r ) as in Fig. 3.41. The start state of N (s) becomes the start state of N (r ) , and the accepting state of N (t) is the only accepting state of N (r ) . The accepting state of N(s) and the start state of N (t) are merged into a single state, with all the transitions in or out of either state. A path from i to f in Fig. 3.41 must go first through N (s) , and therefore its label will begin with some string in L(s) . The path then continues through N (t) , so the path's label finishes with a string in L(t) . As we shall so on argue, accepting states never have edges out and start states never have edges in, so it is not possible for a path to re-enter N (s) after leaving it. Thus, N (r ) accepts exactly L(s)L(t) , and is a correct NFA for r = st . c) Suppose r = s^* r = s^* . Then for r we construct the NFA N (r ) shown in Fig. 3.42. Here, i and f are new states, the start state and lone accepting state of N (r ) . To get from i to f , we can either follow the introduced path labeled \\epsilon \\epsilon , which takes care of the one string in L(s)^0 L(s)^0 , or we can go to the start state of N(s) , through that NFA, then from its accepting state back to its start state zero or more times. These options allow N (r ) to accept all the strings in L(s)^1 L(s)^1 , L(s)^2 L(s)^2 , and so on, so the entire set of strings accepted by N (r ) is L(s^*) L(s^*) . d) Finally, suppose r = (s) . Then L(r ) = L(s) , and we can use the NFA N (s) as N (r ) . NOTE: Implementing a regular expression engine is far more complex than the algorithm described here. The method description in Algorithm 3.23 contains hints as to why the inductive construction works as it should. We shall not give a formal correctness proof, but we shall list several properties of the constructed NFA's, in addition to the all-important fact that N (r ) accepts language L(r ) . These properties are interesting in their own right, and helpful in making a formal proof. N (r ) has at most twice as many states as there are operators and operands in r . This bound follows from the fact that each step of the algorithm creates at most two new states. N (r ) has one start state and one accepting state . The accepting state has no outgoing transitions, and the start state has no incoming transitions. Each state of N (r ) other than the accepting state has either one outgoing transition on a symbol in \\Sigma \\Sigma or two outgoing transitions, both on \\epsilon \\epsilon . Example 3.24 : Let us use Algorithm 3.23 to construct an NFA for r = (a | b) ^*abb r = (a | b) ^*abb . Figure 3.43 shows a parse tree for r that is analogous to the parse trees constructed for arithmetic expressions in Section 2.2.3. For subexpression r_1 r_1 , the first a , we construct the NFA: State numbers have been chosen for consistency with what follows. For r_2 r_2 we construct: We can now combine N (r_1) N (r_1) and N (r_2) N (r_2) , using the construction of Fig. 3.40 to obtain the NFA for r_3 =r_1|r_2 r_3 =r_1|r_2 ; this NFA is shown in Fig. 3.44. The NFA for r_4= (r_3) r_4= (r_3) is the same as that for r_3 r_3 . The NFA for r_5= (r_3) r_5= (r_3) is then as shown in Fig. 3.45. We have used the construction in Fig. 3.42 to build this NFA from the NFA in Fig. 3.44. Now, consider subexpression r_6 r_6 , which is another a . We use the basis construction for a again, but we must use new states. It is not permissible to reuse the NFA we constructed for r_1 r_1 , even though r_1 r_1 and r_6 r_6 are the same expression. The NFA for r_6 r_6 is: To obtain the NFA for r_7= r_5r_6 r_7= r_5r_6 , we apply the construction of Fig. 3.41. We merge states 7 and 7^{'} 7^{'} , yielding the NFA of Fig. 3.46. Continuing in this fashion with new NFA's for the two subexpressions b called r_8 r_8 and r_{10} r_{10} , we eventually construct the NFA for tha r = (a | b) ^*abb r = (a | b) ^*abb that we first met in Fig. 3.34.","title":"Introduction"},{"location":"3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/#37#from#regular#expressions#to#automata","text":"NOTE: What this chapter describe is mainly three algorithms Name Function chapter Thompson's construction algorithm transforming a regular expression into an equivalent nondeterministic finite automaton (NFA) 3.7.4 Construction of an NFA from a Regular Expression subset construction algorithm converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language 3.7.1 Conversion of an NFA to a DFA DFA minimization transforming a given deterministic finite automaton (DFA) into an equivalent DFA that has a minimum number of states. Using the algorithms above, a regular expression can be converted to the corresponding best DFA. The regular expression is the notation of choice for describing lexical analyzers and other pattern-processing software, as was reflected in Section 3.5. However, implementation of that software requires the simulation of a DFA, as in Algorithm 3.18, or perhaps simulation of an NFA. Because an NFA often has a choice of move on an input symbol (as Fig. 3.24 does on input a from state 0) or on \\epsilon \\epsilon (as Fig. 3.26 does from state 0), or even a choice of making a transition on \\epsilon \\epsilon or on a real input symbol, its simulation is less straightforward than for a DFA. Thus often it is important to convert an NFA to a DFA that accepts the same language.","title":"3.7 From Regular Expressions to Automata"},{"location":"3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/#371#conversion#of#an#nfa#to#a#dfa","text":"NOTE: The subset construction explained in this book is too abstract to grasp, wikipedia's explanation about this algorithm is more intuitionistic. The general idea behind the subset construction is that each state of the constructed DFA corresponds to a set of NFA states. After reading input a_1 a_2 \\dots a_n a_1 a_2 \\dots a_n , the DFA is in that state which corresponds to the set of states that the NFA can reach, from its start state, following paths labeled a_1 a_2 \\dots a_n a_1 a_2 \\dots a_n . It is p ossible that the number of DFA states is exponential in the number of NFA states, which could lead to difficulties when we try to implement this DFA. However, part of the p ower of the automaton-based approach to lexical analysis is that for real languages, the NFA and DFA have approximately the same number of states, and the exponential behavior is not seen. Algorithm 3.20 : The subset construction of a DFA from an NFA. INPUT : An NFA N . OUTPUT : A DFA D accepting the same language as N . METHOD : Our algorithm constructs a transition table D_{tran} D_{tran} for D . Each state of D is a set of NFA states, and we construct D_{tran} D_{tran} so D will simulate \"in parallel\" all possible moves N can make on a given input string. Our first problem is to deal with \\epsilon \\epsilon -transitions of N properly. In Fig. 3.31 we see the definitions of several functions that describe basic computations on the states of N that are needed in the algorithm. Note that s is a single state of N , while T is a set of states of N . OPERATION DESCRIPTION \\epsilon-closure(s) \\epsilon-closure(s) Set of NFA states reachable from NFA state s on \\epsilon \\epsilon -transitions alone. \\epsilon-closure(T) \\epsilon-closure(T) Set of NFA states reachable from some NFA state s in set T on \\epsilon \\epsilon -transitions alone; = \\cup _{s \\in T} {\\epsilon-closure(s)} = \\cup _{s \\in T} {\\epsilon-closure(s)} move(T, a) Set of NFA states to which there is a transition on input symbol a from some state s in T . Figure 3.31: Operations on NFA states We must explore those sets of states that N can be in after seeing some input string. As a basis, before reading the first input symbol, N can be in any of the states of \\epsilon-closure(s_0) \\epsilon-closure(s_0) , where s_0 s_0 is its start state . For the induction, suppose that N can be in set of states T after reading input string x . If it next reads input a , then N can immediately go to any of the states in move (T , a) move (T , a) . However, after reading a , it may also make several \\epsilon-transitions \\epsilon-transitions ; thus N could be in any state of \\epsilon-closure(move(T, a)) \\epsilon-closure(move(T, a)) after reading input xa . Following these ideas, the construction of the set of D 's states, D_{states} D_{states} , and its transition function D_{tran} D_{tran} , is shown in Fig. 3.32. The start state of D is \\epsilon-closure(s_0) \\epsilon-closure(s_0) , and the accepting states of D are all those sets of N 's states that include at least one accepting state of N . To complete our description of the subset construction, we need only to show how \\epsilon-closure(T) \\epsilon-closure(T) is computed for any set of NFA states T . This process, shown in Fig. 3.33, is a straightforward search in a graph from a set of states. In this case, imagine that only the \\epsilon \\epsilon -labeled edges are available in the graph.","title":"3.7.1 Conversion of an NFA to a DFA"},{"location":"3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/#374#construction#of#an#nfa#from#a#regular#expression","text":"We now give an algorithm for converting any regular expression to an NFA that defines the same language. The algorithm is syntax-directed , in the sense that it works recursively up the parse tree for the regular expression. For each sub expression the algorithm constructs an NFA with a single accepting state. NOTE: bottom-up NOTE: How to build a parse tree for a regular expression? Efficiently building a parse tree from a regular expression Algorithm 3.23 : The McNaughton-Yamada-Thompson algorithm to convert a regular expression to an NFA. INPUT : A regular expression r over alphabet \\Sigma \\Sigma . OUTPUT : An NFA N accepting L(r ) . METHOD: Begin by parsing r into its constituent subexpressions. The rules for constructing an NFA consist of basis rules for handling subexpressions with no operators, and inductive rules for constructing larger NFA's from the NFA's for the immediate subexpressions of a given expression. BASIS : For expression \\epsilon \\epsilon construct the NFA Here, i is a new state, the start state of this NFA, and f is another new state, the accepting state for the NFA. For any subexpression a in \\Sigma \\Sigma , construct the NFA where again i and f are new states, the start and accepting states, respectively. Note that in both of the basis constructions, we construct a distinct NFA, with new states, for every occurrence of \\epsilon \\epsilon or some a as a subexpression of r . INDUCTION : Suppose N (s) and N (t) are NFA's for regular expressions s and t , respectively. a) Suppose r = s|t . Then N (r ) , the NFA for r , is constructed as in Fig. 3.40. Here, i and f are new states, the start and accepting states of N (r ) , respectively. There are \\epsilon \\epsilon -transitions from i to the start states of N (s) and N (t) , and each of their accepting states have \\epsilon \\epsilon -transitions to the accepting state f . Note that the accepting states of N (s) and N (t) are not accepting in N (r) . Since any path from i to f must pass through either N (s) or N (t) exclusively, and since the label of that path is not changed by the \\epsilon \\epsilon 's leaving i or entering f , we conclude that N (r ) accepts L(s) \\cup L(t) L(s) \\cup L(t) , which is the same as L(r ) . That is, Fig. 3.40 is a correct construction for the union operator. b) Suppose r = st . Then construct N (r ) as in Fig. 3.41. The start state of N (s) becomes the start state of N (r ) , and the accepting state of N (t) is the only accepting state of N (r ) . The accepting state of N(s) and the start state of N (t) are merged into a single state, with all the transitions in or out of either state. A path from i to f in Fig. 3.41 must go first through N (s) , and therefore its label will begin with some string in L(s) . The path then continues through N (t) , so the path's label finishes with a string in L(t) . As we shall so on argue, accepting states never have edges out and start states never have edges in, so it is not possible for a path to re-enter N (s) after leaving it. Thus, N (r ) accepts exactly L(s)L(t) , and is a correct NFA for r = st . c) Suppose r = s^* r = s^* . Then for r we construct the NFA N (r ) shown in Fig. 3.42. Here, i and f are new states, the start state and lone accepting state of N (r ) . To get from i to f , we can either follow the introduced path labeled \\epsilon \\epsilon , which takes care of the one string in L(s)^0 L(s)^0 , or we can go to the start state of N(s) , through that NFA, then from its accepting state back to its start state zero or more times. These options allow N (r ) to accept all the strings in L(s)^1 L(s)^1 , L(s)^2 L(s)^2 , and so on, so the entire set of strings accepted by N (r ) is L(s^*) L(s^*) . d) Finally, suppose r = (s) . Then L(r ) = L(s) , and we can use the NFA N (s) as N (r ) . NOTE: Implementing a regular expression engine is far more complex than the algorithm described here. The method description in Algorithm 3.23 contains hints as to why the inductive construction works as it should. We shall not give a formal correctness proof, but we shall list several properties of the constructed NFA's, in addition to the all-important fact that N (r ) accepts language L(r ) . These properties are interesting in their own right, and helpful in making a formal proof. N (r ) has at most twice as many states as there are operators and operands in r . This bound follows from the fact that each step of the algorithm creates at most two new states. N (r ) has one start state and one accepting state . The accepting state has no outgoing transitions, and the start state has no incoming transitions. Each state of N (r ) other than the accepting state has either one outgoing transition on a symbol in \\Sigma \\Sigma or two outgoing transitions, both on \\epsilon \\epsilon . Example 3.24 : Let us use Algorithm 3.23 to construct an NFA for r = (a | b) ^*abb r = (a | b) ^*abb . Figure 3.43 shows a parse tree for r that is analogous to the parse trees constructed for arithmetic expressions in Section 2.2.3. For subexpression r_1 r_1 , the first a , we construct the NFA: State numbers have been chosen for consistency with what follows. For r_2 r_2 we construct: We can now combine N (r_1) N (r_1) and N (r_2) N (r_2) , using the construction of Fig. 3.40 to obtain the NFA for r_3 =r_1|r_2 r_3 =r_1|r_2 ; this NFA is shown in Fig. 3.44. The NFA for r_4= (r_3) r_4= (r_3) is the same as that for r_3 r_3 . The NFA for r_5= (r_3) r_5= (r_3) is then as shown in Fig. 3.45. We have used the construction in Fig. 3.42 to build this NFA from the NFA in Fig. 3.44. Now, consider subexpression r_6 r_6 , which is another a . We use the basis construction for a again, but we must use new states. It is not permissible to reuse the NFA we constructed for r_1 r_1 , even though r_1 r_1 and r_6 r_6 are the same expression. The NFA for r_6 r_6 is: To obtain the NFA for r_7= r_5r_6 r_7= r_5r_6 , we apply the construction of Fig. 3.41. We merge states 7 and 7^{'} 7^{'} , yielding the NFA of Fig. 3.46. Continuing in this fashion with new NFA's for the two subexpressions b called r_8 r_8 and r_{10} r_{10} , we eventually construct the NFA for tha r = (a | b) ^*abb r = (a | b) ^*abb that we first met in Fig. 3.34.","title":"3.7.4 Construction of an NFA from a Regular Expression"},{"location":"4-Syntax-Analysis/","text":"Chapter 4 Syntax Analysis NOTE: This chapter is mainly about algorithms of parsing or syntax analysis, I extend it to include algorithms relevant to parsing in wikipedia which is listed in section Wikipedia Parsing algorithms","title":4},{"location":"4-Syntax-Analysis/#chapter#4#syntax#analysis","text":"NOTE: This chapter is mainly about algorithms of parsing or syntax analysis, I extend it to include algorithms relevant to parsing in wikipedia which is listed in section Wikipedia Parsing algorithms","title":"Chapter 4 Syntax Analysis"},{"location":"4-Syntax-Analysis/4.2-Context-Free-Grammars/","text":"4.2 Context-Free Grammars 4.2.3 Derivations NOTE: Derivation means \u63a8\u5bfc in Chinese. The construction of a parse tree can be made precise by taking a derivational view, in which productions are treated as rewriting rules. Beginning with the start symbol, each rewriting step replaces a nonterminal by the body of one of its productions. This derivational view corresponds to the top-down construction of a parse tree , but the precision afforded by derivations will be especially helpful when bottom-up parsing is discussed. As we shall see, bottom-up parsing is related to a class of derivations known as \" rightmost \" derivations, in which the rightmost nonterminal is rewritten at each step. NOTE: I still do not understand why bottom-up parsing corresponds \" rightmost \" derivations. \\Rightarrow \\Rightarrow If S \\xrightarrow []{ \\ast} \\alpha S \\xrightarrow []{ \\ast} \\alpha where S S is the start symbol of a grammar G G , we say that \\alpha \\alpha is a sentential form of G. Note that a sentential form may contain both terminals and nonterminals , and may be empty. A sentence of G is a sentential form with no nonterminals . The language generated by a grammar is its set of sentences . Thus, a string of terminals w w is in L(G) L(G) , the language generated by G, if and only if w w is a sentence of G (or $S \\xrightarrow []{ \\ast} w $. A language that can be generated by a grammar is said to be a context-free language . If two grammars generate the same language, the grammars are said to be equivalent . NOTE: Natural language is not context-free language . To understand how parsers work, we shall consider derivations in which the nonterminal to be replaced at each step is chosen as follows: 1\u3001In leftmost derivations , the leftmost nonterminal in each sentential is always chosen. If $\\alpha \\to \\beta $ is a step in which the leftmost nonterminal in \\alpha \\alpha is replaced, we write \\alpha \\xrightarrow [ \\text{lm} ]{} \\beta \\alpha \\xrightarrow [ \\text{lm} ]{} \\beta . 2\u3001In rightmost derivations , the rightmost nonterminal is always chosen; we write \\alpha \\xrightarrow [ \\text{rm} ]{} \\beta \\alpha \\xrightarrow [ \\text{rm} ]{} \\beta in this case. If , then we say that is a left-sentential form of the grammar at hand. Analogous definitions hold for rightmost derivations . Rightmost derivations are sometimes called canonical derivations . 4.2.4 Parse Trees and Derivations A parse tree is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal A in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation. NOTE: Left-most derivation and right-most derivation yield the same tree in the end but the order in which productions are applied to replace nonterminals is different. 4.2.7 Context-Free Grammars Versus Regular Expressions NOTE: \u4e0b\u9762\u8fd9\u4e9b\u5185\u5bb9\u662f\u5728\u9605\u8bfb wikipedia Comparison of parser generators \u4e2d Regular languages \u7ae0\u8282\u65f6\uff0c\u53d1\u73b0\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u6b63\u597d\u548c\u672c\u8282\u5185\u5bb9\u76f8\u5173\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u8865\u5145\uff0c\u5e76\u4e14\u5b83\u7684\u8bb2\u89e3\u4e5f\u6bd4\u8f83\u6613\u61c2\uff1a Regular languages are a category of languages (sometimes termed Chomsky Type 3 ) which can be matched by a state machine (more specifically, by a deterministic finite automaton or a nondeterministic finite automaton ) constructed from a regular expression . In particular, a regular language can match constructs like \"A follows B\", \"Either A or B\", \"A, followed by zero or more instances of B\", but cannot match constructs which require consistency between non-adjacent elements, such as \"some instances of A followed by the same number of instances of B\", and also cannot express the concept of recursive \"nesting\" (\"every A is eventually followed by a matching B\"). A classic example of a problem which a regular grammar cannot handle is the question of whether a given string contains correctly-nested parentheses. (This is typically handled by a Chomsky Type 2 grammar, also termed a context-free grammar .) regular expression\u5c5e\u4e8e Regular languages \uff0c\u4f7f\u7528 regular grammar \uff0c\u6309\u7167 Chomsky hierarchy : regular grammar \u5c5e\u4e8e Type-3 grammars context-free grammar \u5c5e\u4e8e Type-2 grammars Every regular language is context-free , every context-free language is context-sensitive , every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.","title":"Introduction"},{"location":"4-Syntax-Analysis/4.2-Context-Free-Grammars/#42#context-free#grammars","text":"","title":"4.2 Context-Free Grammars"},{"location":"4-Syntax-Analysis/4.2-Context-Free-Grammars/#423#derivations","text":"NOTE: Derivation means \u63a8\u5bfc in Chinese. The construction of a parse tree can be made precise by taking a derivational view, in which productions are treated as rewriting rules. Beginning with the start symbol, each rewriting step replaces a nonterminal by the body of one of its productions. This derivational view corresponds to the top-down construction of a parse tree , but the precision afforded by derivations will be especially helpful when bottom-up parsing is discussed. As we shall see, bottom-up parsing is related to a class of derivations known as \" rightmost \" derivations, in which the rightmost nonterminal is rewritten at each step. NOTE: I still do not understand why bottom-up parsing corresponds \" rightmost \" derivations. \\Rightarrow \\Rightarrow If S \\xrightarrow []{ \\ast} \\alpha S \\xrightarrow []{ \\ast} \\alpha where S S is the start symbol of a grammar G G , we say that \\alpha \\alpha is a sentential form of G. Note that a sentential form may contain both terminals and nonterminals , and may be empty. A sentence of G is a sentential form with no nonterminals . The language generated by a grammar is its set of sentences . Thus, a string of terminals w w is in L(G) L(G) , the language generated by G, if and only if w w is a sentence of G (or $S \\xrightarrow []{ \\ast} w $. A language that can be generated by a grammar is said to be a context-free language . If two grammars generate the same language, the grammars are said to be equivalent . NOTE: Natural language is not context-free language . To understand how parsers work, we shall consider derivations in which the nonterminal to be replaced at each step is chosen as follows: 1\u3001In leftmost derivations , the leftmost nonterminal in each sentential is always chosen. If $\\alpha \\to \\beta $ is a step in which the leftmost nonterminal in \\alpha \\alpha is replaced, we write \\alpha \\xrightarrow [ \\text{lm} ]{} \\beta \\alpha \\xrightarrow [ \\text{lm} ]{} \\beta . 2\u3001In rightmost derivations , the rightmost nonterminal is always chosen; we write \\alpha \\xrightarrow [ \\text{rm} ]{} \\beta \\alpha \\xrightarrow [ \\text{rm} ]{} \\beta in this case. If , then we say that is a left-sentential form of the grammar at hand. Analogous definitions hold for rightmost derivations . Rightmost derivations are sometimes called canonical derivations .","title":"4.2.3 Derivations"},{"location":"4-Syntax-Analysis/4.2-Context-Free-Grammars/#424#parse#trees#and#derivations","text":"A parse tree is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal A in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation. NOTE: Left-most derivation and right-most derivation yield the same tree in the end but the order in which productions are applied to replace nonterminals is different.","title":"4.2.4 Parse Trees and Derivations"},{"location":"4-Syntax-Analysis/4.2-Context-Free-Grammars/#427#context-free#grammars#versus#regular#expressions","text":"NOTE: \u4e0b\u9762\u8fd9\u4e9b\u5185\u5bb9\u662f\u5728\u9605\u8bfb wikipedia Comparison of parser generators \u4e2d Regular languages \u7ae0\u8282\u65f6\uff0c\u53d1\u73b0\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u6b63\u597d\u548c\u672c\u8282\u5185\u5bb9\u76f8\u5173\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u8865\u5145\uff0c\u5e76\u4e14\u5b83\u7684\u8bb2\u89e3\u4e5f\u6bd4\u8f83\u6613\u61c2\uff1a Regular languages are a category of languages (sometimes termed Chomsky Type 3 ) which can be matched by a state machine (more specifically, by a deterministic finite automaton or a nondeterministic finite automaton ) constructed from a regular expression . In particular, a regular language can match constructs like \"A follows B\", \"Either A or B\", \"A, followed by zero or more instances of B\", but cannot match constructs which require consistency between non-adjacent elements, such as \"some instances of A followed by the same number of instances of B\", and also cannot express the concept of recursive \"nesting\" (\"every A is eventually followed by a matching B\"). A classic example of a problem which a regular grammar cannot handle is the question of whether a given string contains correctly-nested parentheses. (This is typically handled by a Chomsky Type 2 grammar, also termed a context-free grammar .) regular expression\u5c5e\u4e8e Regular languages \uff0c\u4f7f\u7528 regular grammar \uff0c\u6309\u7167 Chomsky hierarchy : regular grammar \u5c5e\u4e8e Type-3 grammars context-free grammar \u5c5e\u4e8e Type-2 grammars Every regular language is context-free , every context-free language is context-sensitive , every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.","title":"4.2.7 Context-Free Grammars Versus Regular Expressions"},{"location":"4-Syntax-Analysis/4.3-Writing-a-Grammar/","text":"","title":"Introduction"},{"location":"4-Syntax-Analysis/4.5-Bottom-Up-Parsing/","text":"4.5 Bottom-Up Parsing NOTE: \u8fd9\u4e00\u7ae0\u8282\u91cd\u8981\u63cf\u8ff0\u7684\u5982\u4f55\u624b\u5de5\u5730\u5b9e\u73b0shift-reduce parsing A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). It is convenient to describe parsing as the process of building parse trees, although a front end may in fact carry out a translation directly without building an explicit tree . The sequence of tree snapshots in Fig. 4.25 illustrates a bottom-up parse of the token stream id \\ast id id \\ast id , with respect to the expression grammar (4.1). NOTE: Thinking of parsing as the process of building parse trees can help us grasp the content more easily. This section introduces a general style of bottom-up parsing known as shift-reduce parsing . The largest class of grammars for which shift-reduce parsers can be built, the LR grammars , will be discussed in Sections 4.6 and 4.7. Although it is too much work to build an LR parser by hand, tools called automatic parser generators make it easy to construct efficient LR parsers from suitable grammars. The concepts in this section are helpful for writing suitable grammars to make effective use of an LR parser generator. Algorithms for implementing parser generators appear in Section 4.7. NOTE: LR parser uses shift-reduce parsing . 4.5.1 Reductions NOTE: Reduction is the reverse of derivation. We can think of bottom-up parsing as the process of \"reducing\" a string w w to the start symbol of the grammar. At each reduction step, a specific substring matching the body of a production is replaced by the nonterminal at the head of that production. The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds. NOTE: 1\u3001\u4f55\u65f6reduce 2\u3001\u4f7f\u7528\u54ea\u4e2aproduction\u8fdb\u884creduce Example 4.37 : The snapshots in Fig. 4.25 illustrate a sequence of reductions; the grammar is the expression grammar (4.1). The reductions will be discussed in terms of the sequence of strings $$ id \\ast id, F \\ast id, T \\ast id, T \\ast F, T, E $$ The strings in this sequence are formed from the roots of all the subtrees in the snapshots. The sequence starts with the input string id \\ast id id \\ast id . The first reduction produces F \\ast id F \\ast id by reducing the leftmost id id to F F , using the production F \\to id F \\to id . The second reduction produces T \\ast id T \\ast id by reducing F F to T T . By definition, a reduction is the reverse of a step in a derivation (recall that in a derivation, a nonterminal in a sentential form is replaced by the body of one of its productions). The goal of bottom-up parsing is therefore to construct a derivation in reverse. The following corresponds to the parse in Fig. 4.25: $$ E \\Rightarrow T \\Rightarrow T \\ast F \\Rightarrow T \\ast id \\Rightarrow F \\ast id \\Rightarrow id \\ast id $$ This derivation is in fact a rightmost derivation . NOTE: We know that Reduction is the reverse of derivation, so what is done first in reduction is done later in production. The last paragraph concludes with a very important conclusion but the reasoning is a bit murky. I didn't get it right the first time I read it. Here is my analysis of the conclusion. Given grammar, we can choose leftmost derivation or rightmost derivation to derivate the parse tree. No matter order of derivation, the resulting parse tree and the resulting sentence is the same, but the derivation process is different. This is equivalent to that the shape of the parse tree is the same but the process of construction is different. In leftmost derivation , the left subtree is always constructed first and then the right subtree, so the right of the sentence is always derivated first while in rightmost derivation the order is reverse. Now let's turn our attention to reduction. A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). Fig. 4.25 illustrate the process of bottom-up construction of the parse tree. Given a sentence, bottom-up parser choose to reduce from left symbol to right symbol, so it always construct the subtree from lower left corner to upper right corner until the complete parse tree. It is obvious that eventually it will be able to construct a complete parse tree. The order of productions used in its construction is the exact opposite of the rightmost derivation. parsing\u662f\u6839\u636einput symbol\u6765\u9006\u63a8\u5176derivation\u7684\u8fc7\u7a0b\uff0cparser\u5bf9input symbol\u7684\u5904\u7406\u662f\u56fa\u5b9a\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u4e00\u6b21\u5904\u7406\u4e00\u4e2a\u5b57\u7b26\uff0c\u56e0\u6b64\u5b83\u53ea\u80fd\u591f\u5c06input symbol\u4e2d\u5de6\u4fa7\u7684terminal\u9010\u4e2a\u89c4\u7ea6\u4e3anon-terminal\uff0c\u7136\u540e\u518d\u8ddfinput symbol\u4e2d\u4f59\u4e0b\u7684terminal\u4e00\u8d77\u8fdb\u884c\u89c4\u7ea6\u3002\u663e\u7136\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u89c4\u7ea6\u662fderivation\u7684\u9006\u8fc7\u7a0b\uff0c\u4ece\u5de6\u81f3\u53f3\u7684reduction\u5bf9\u5e94\u7684\u662f\u4ece\u53f3\u81f3\u5de6\u7684derivation\uff0c\u6240\u4ee5right-most derivation\u88ab\u79f0\u4e3acanonical derivation\u3002 \u4e3a\u4ec0\u4e48right-most derivation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u800cleft-most derivation\u7684\u8fc7\u7a0b\u65e0\u6cd5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u8fd8\u662f\u8981\u628a\u63e1\u4e00\u70b9\uff1aparser\u5bf9input symbol\u662f\u4ece\u5de6\u81f3\u53f3\u8fdb\u884c\u5904\u7406\u7684\uff0c\u6240\u4ee5\u5b83\u53ea\u80fd\u591f\u4ec5\u4ec5\u6839\u636e\u5df2\u7ecf\u8bfb\u5230\u7684terminal symbol\u6765\u8fdb\u884creduction\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4e8e\u540e\u9762\u7684\u672a\u8bfb\u5165\u7684\u5b57\u7b26\u3002\u5728left-most derivation\u4e2d\uff0c\u4f18\u5148derivate \u7684\u662fleft-most\u7684non-terminal\u3002 4.5.2 Handle Pruning Bottom-up parsing during a left-to-right scan of the input constructs a right-most derivation in reverse. Formally, if $S \\xrightarrow [rm]{\\ast} \\alpha A w \\xrightarrow [rm]{\\ast} \\alpha \\beta w $ , as in Fig. 4.27, then production $A \\to \\beta $ in the position following \\alpha \\alpha is a handle of \\alpha \\beta w \\alpha \\beta w . Alternatively, a handle of a right-sentential form $\\gamma $ is a production A \\to \\beta A \\to \\beta and a position of \\gamma \\gamma where the string \\beta \\beta may be found, such that replacing \\beta \\beta at that position by A A produces the previous right-sentential form in a rightmost derivation of \\gamma \\gamma . Notice that the string w w to the right of the handle must contain only terminal symbols. NOTE: This is because rightmost derivation always choose the rightmost nonterminal to expand; A rightmost derivation in reverse can be obtained by \"handle pruning.\" That is, we start with a string of terminals w w to be parsed. If w w is a sentence of the grammar at hand, then let w = \\gamma _n w = \\gamma _n , where \\gamma _n \\gamma _n is the n n th right-sentential form of some as yet unknown rightmost derivation $$ S = \\gamma 0 \\xrightarrow [rm] {} \\gamma 1 \\xrightarrow [rm] {} \\gamma 2 \\xrightarrow [rm] {} \\ldots \\xrightarrow [rm] {}\\gamma _{n-1} \\xrightarrow [rm] {} \\gamma _n = w $$ To reconstruct this derivation in reverse order, we locate the handle \\beta_n \\beta_n in \\gamma_n \\gamma_n and replace \\beta_n \\beta_n by the head of the relevant production $A_n \\to \\beta_n $ to obtain the previous right-sentential form \\gamma_{n-1} \\gamma_{n-1} . Note that we do not yet know how handles are to be found, but we shall see methods of doing so shortly. We then repeat this process. That is, we locate the handle \\beta_{n-1} \\beta_{n-1} in \\gamma_{n-1} \\gamma_{n-1} and reduce this handle to obtain the right-sentential form \\gamma_{n-2} \\gamma_{n-2} . If by continuing this process we produce a right-sentential form consisting only of the start symbol S S , then we halt and announce successful completion of parsing. The reverse of the sequence of productions used in the reductions is a rightmost derivation for the input string. 4.5.3 Shift-Reduce Parsing Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. As we shall see, the handle always appears at the top of the stack just before it is identified as the handle . Up on entering this configuration , the parser halts and announces successful completion of parsing. Figure 4.28 steps through the actions a shift-reduce parser might take in parsing the input string id \\ast id id \\ast id according to the expression grammar (4.1). While the primary operations are shift and reduce, there are actually four possible actions a shift-reduce parser can make: (1) shift, (2) reduce, (3) accept, and (4) error. Shift . Shift the next input symbol onto the top of the stack. Reduce . The right end of the string to be reduced must be at the top of the stack. Locate the left end of the string within the stack and decide with what nonterminal to replace the string. Accept . Announce successful completion of parsing. Error . Discover a syntax error and call an error recovery routine. The use of a stack in shift-reduce parsing is justified by an important fact: the handle will always eventually appear on top of the stack, never inside. This fact can be shown by considering the possible forms of two successive steps in any rightmost derivation . Figure 4.29 illustrates the two possible cases. In case (1), A A is replaced by \\beta B y \\beta B y , and then the rightmost nonterminal B B in the body \\beta B y \\beta B y is replaced by gamma gamma . In case (2), A A is again expanded first, but this time the body is a string y y of terminals only. The next rightmost nonterminal B B will be somewhere to the left of y y . NOTE: The proof is omitted because I think it's a natural conclusion. If you read the note in 4.5.1 Reductions, you will find the conclusion is easy to understand. Here the author enumerates one example to support this. 4.5.4 Conflicts During Shift-Reduce Parsing There are context-free grammars for which shift-reduce parsing cannot be used. Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack and also the next k input symbols, cannot decide whether to shift or to reduce (a shift/reduce conflict ), or cannot decide which of several reductions to make (a reduce/reduce conflict ). We now give some examples of syntactic constructs that give rise to such grammars. Technically, these grammars are not in the LR(k) class of grammars defined in Section 4.7; we refer to them as non-LR grammars . The k in LR(k) refers to the number of symbols of lookahead on the input. Grammars used in compiling usually fall in the LR(1) class, with one symbol of lookahead at most.","title":"Introduction"},{"location":"4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#45#bottom-up#parsing","text":"NOTE: \u8fd9\u4e00\u7ae0\u8282\u91cd\u8981\u63cf\u8ff0\u7684\u5982\u4f55\u624b\u5de5\u5730\u5b9e\u73b0shift-reduce parsing A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). It is convenient to describe parsing as the process of building parse trees, although a front end may in fact carry out a translation directly without building an explicit tree . The sequence of tree snapshots in Fig. 4.25 illustrates a bottom-up parse of the token stream id \\ast id id \\ast id , with respect to the expression grammar (4.1). NOTE: Thinking of parsing as the process of building parse trees can help us grasp the content more easily. This section introduces a general style of bottom-up parsing known as shift-reduce parsing . The largest class of grammars for which shift-reduce parsers can be built, the LR grammars , will be discussed in Sections 4.6 and 4.7. Although it is too much work to build an LR parser by hand, tools called automatic parser generators make it easy to construct efficient LR parsers from suitable grammars. The concepts in this section are helpful for writing suitable grammars to make effective use of an LR parser generator. Algorithms for implementing parser generators appear in Section 4.7. NOTE: LR parser uses shift-reduce parsing .","title":"4.5 Bottom-Up Parsing"},{"location":"4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#451#reductions","text":"NOTE: Reduction is the reverse of derivation. We can think of bottom-up parsing as the process of \"reducing\" a string w w to the start symbol of the grammar. At each reduction step, a specific substring matching the body of a production is replaced by the nonterminal at the head of that production. The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds. NOTE: 1\u3001\u4f55\u65f6reduce 2\u3001\u4f7f\u7528\u54ea\u4e2aproduction\u8fdb\u884creduce Example 4.37 : The snapshots in Fig. 4.25 illustrate a sequence of reductions; the grammar is the expression grammar (4.1). The reductions will be discussed in terms of the sequence of strings $$ id \\ast id, F \\ast id, T \\ast id, T \\ast F, T, E $$ The strings in this sequence are formed from the roots of all the subtrees in the snapshots. The sequence starts with the input string id \\ast id id \\ast id . The first reduction produces F \\ast id F \\ast id by reducing the leftmost id id to F F , using the production F \\to id F \\to id . The second reduction produces T \\ast id T \\ast id by reducing F F to T T . By definition, a reduction is the reverse of a step in a derivation (recall that in a derivation, a nonterminal in a sentential form is replaced by the body of one of its productions). The goal of bottom-up parsing is therefore to construct a derivation in reverse. The following corresponds to the parse in Fig. 4.25: $$ E \\Rightarrow T \\Rightarrow T \\ast F \\Rightarrow T \\ast id \\Rightarrow F \\ast id \\Rightarrow id \\ast id $$ This derivation is in fact a rightmost derivation . NOTE: We know that Reduction is the reverse of derivation, so what is done first in reduction is done later in production. The last paragraph concludes with a very important conclusion but the reasoning is a bit murky. I didn't get it right the first time I read it. Here is my analysis of the conclusion. Given grammar, we can choose leftmost derivation or rightmost derivation to derivate the parse tree. No matter order of derivation, the resulting parse tree and the resulting sentence is the same, but the derivation process is different. This is equivalent to that the shape of the parse tree is the same but the process of construction is different. In leftmost derivation , the left subtree is always constructed first and then the right subtree, so the right of the sentence is always derivated first while in rightmost derivation the order is reverse. Now let's turn our attention to reduction. A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). Fig. 4.25 illustrate the process of bottom-up construction of the parse tree. Given a sentence, bottom-up parser choose to reduce from left symbol to right symbol, so it always construct the subtree from lower left corner to upper right corner until the complete parse tree. It is obvious that eventually it will be able to construct a complete parse tree. The order of productions used in its construction is the exact opposite of the rightmost derivation. parsing\u662f\u6839\u636einput symbol\u6765\u9006\u63a8\u5176derivation\u7684\u8fc7\u7a0b\uff0cparser\u5bf9input symbol\u7684\u5904\u7406\u662f\u56fa\u5b9a\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u4e00\u6b21\u5904\u7406\u4e00\u4e2a\u5b57\u7b26\uff0c\u56e0\u6b64\u5b83\u53ea\u80fd\u591f\u5c06input symbol\u4e2d\u5de6\u4fa7\u7684terminal\u9010\u4e2a\u89c4\u7ea6\u4e3anon-terminal\uff0c\u7136\u540e\u518d\u8ddfinput symbol\u4e2d\u4f59\u4e0b\u7684terminal\u4e00\u8d77\u8fdb\u884c\u89c4\u7ea6\u3002\u663e\u7136\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u89c4\u7ea6\u662fderivation\u7684\u9006\u8fc7\u7a0b\uff0c\u4ece\u5de6\u81f3\u53f3\u7684reduction\u5bf9\u5e94\u7684\u662f\u4ece\u53f3\u81f3\u5de6\u7684derivation\uff0c\u6240\u4ee5right-most derivation\u88ab\u79f0\u4e3acanonical derivation\u3002 \u4e3a\u4ec0\u4e48right-most derivation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u800cleft-most derivation\u7684\u8fc7\u7a0b\u65e0\u6cd5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u8fd8\u662f\u8981\u628a\u63e1\u4e00\u70b9\uff1aparser\u5bf9input symbol\u662f\u4ece\u5de6\u81f3\u53f3\u8fdb\u884c\u5904\u7406\u7684\uff0c\u6240\u4ee5\u5b83\u53ea\u80fd\u591f\u4ec5\u4ec5\u6839\u636e\u5df2\u7ecf\u8bfb\u5230\u7684terminal symbol\u6765\u8fdb\u884creduction\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4e8e\u540e\u9762\u7684\u672a\u8bfb\u5165\u7684\u5b57\u7b26\u3002\u5728left-most derivation\u4e2d\uff0c\u4f18\u5148derivate \u7684\u662fleft-most\u7684non-terminal\u3002","title":"4.5.1 Reductions"},{"location":"4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#452#handle#pruning","text":"Bottom-up parsing during a left-to-right scan of the input constructs a right-most derivation in reverse. Formally, if $S \\xrightarrow [rm]{\\ast} \\alpha A w \\xrightarrow [rm]{\\ast} \\alpha \\beta w $ , as in Fig. 4.27, then production $A \\to \\beta $ in the position following \\alpha \\alpha is a handle of \\alpha \\beta w \\alpha \\beta w . Alternatively, a handle of a right-sentential form $\\gamma $ is a production A \\to \\beta A \\to \\beta and a position of \\gamma \\gamma where the string \\beta \\beta may be found, such that replacing \\beta \\beta at that position by A A produces the previous right-sentential form in a rightmost derivation of \\gamma \\gamma . Notice that the string w w to the right of the handle must contain only terminal symbols. NOTE: This is because rightmost derivation always choose the rightmost nonterminal to expand; A rightmost derivation in reverse can be obtained by \"handle pruning.\" That is, we start with a string of terminals w w to be parsed. If w w is a sentence of the grammar at hand, then let w = \\gamma _n w = \\gamma _n , where \\gamma _n \\gamma _n is the n n th right-sentential form of some as yet unknown rightmost derivation $$ S = \\gamma 0 \\xrightarrow [rm] {} \\gamma 1 \\xrightarrow [rm] {} \\gamma 2 \\xrightarrow [rm] {} \\ldots \\xrightarrow [rm] {}\\gamma _{n-1} \\xrightarrow [rm] {} \\gamma _n = w $$ To reconstruct this derivation in reverse order, we locate the handle \\beta_n \\beta_n in \\gamma_n \\gamma_n and replace \\beta_n \\beta_n by the head of the relevant production $A_n \\to \\beta_n $ to obtain the previous right-sentential form \\gamma_{n-1} \\gamma_{n-1} . Note that we do not yet know how handles are to be found, but we shall see methods of doing so shortly. We then repeat this process. That is, we locate the handle \\beta_{n-1} \\beta_{n-1} in \\gamma_{n-1} \\gamma_{n-1} and reduce this handle to obtain the right-sentential form \\gamma_{n-2} \\gamma_{n-2} . If by continuing this process we produce a right-sentential form consisting only of the start symbol S S , then we halt and announce successful completion of parsing. The reverse of the sequence of productions used in the reductions is a rightmost derivation for the input string.","title":"4.5.2 Handle Pruning"},{"location":"4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#453#shift-reduce#parsing","text":"Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. As we shall see, the handle always appears at the top of the stack just before it is identified as the handle . Up on entering this configuration , the parser halts and announces successful completion of parsing. Figure 4.28 steps through the actions a shift-reduce parser might take in parsing the input string id \\ast id id \\ast id according to the expression grammar (4.1). While the primary operations are shift and reduce, there are actually four possible actions a shift-reduce parser can make: (1) shift, (2) reduce, (3) accept, and (4) error. Shift . Shift the next input symbol onto the top of the stack. Reduce . The right end of the string to be reduced must be at the top of the stack. Locate the left end of the string within the stack and decide with what nonterminal to replace the string. Accept . Announce successful completion of parsing. Error . Discover a syntax error and call an error recovery routine. The use of a stack in shift-reduce parsing is justified by an important fact: the handle will always eventually appear on top of the stack, never inside. This fact can be shown by considering the possible forms of two successive steps in any rightmost derivation . Figure 4.29 illustrates the two possible cases. In case (1), A A is replaced by \\beta B y \\beta B y , and then the rightmost nonterminal B B in the body \\beta B y \\beta B y is replaced by gamma gamma . In case (2), A A is again expanded first, but this time the body is a string y y of terminals only. The next rightmost nonterminal B B will be somewhere to the left of y y . NOTE: The proof is omitted because I think it's a natural conclusion. If you read the note in 4.5.1 Reductions, you will find the conclusion is easy to understand. Here the author enumerates one example to support this.","title":"4.5.3 Shift-Reduce Parsing"},{"location":"4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#454#conflicts#during#shift-reduce#parsing","text":"There are context-free grammars for which shift-reduce parsing cannot be used. Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack and also the next k input symbols, cannot decide whether to shift or to reduce (a shift/reduce conflict ), or cannot decide which of several reductions to make (a reduce/reduce conflict ). We now give some examples of syntactic constructs that give rise to such grammars. Technically, these grammars are not in the LR(k) class of grammars defined in Section 4.7; we refer to them as non-LR grammars . The k in LR(k) refers to the number of symbols of lookahead on the input. Grammars used in compiling usually fall in the LR(1) class, with one symbol of lookahead at most.","title":"4.5.4 Conflicts During Shift-Reduce Parsing"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/","text":"4.6 Introduction to LR Parsing: Simple LR The most prevalent type of bottom-up parser to day is based on a concept called LR(k ) parsing; the \"L\" is for left-to-right scanning of the input, the \"R\" for constructing a rightmost derivation in reverse, and the k for the number of input symbols of lookahead that are used in making parsing decisions. The cases k = 0 or k = 1 are of practical interest, and we shall only consider LR parsers with k \\le 1 k \\le 1 here. When (k ) is omitted, k is assumed to be 1. This section introduces the basic concepts of LR parsing and the easiest method for constructing shift-reduce parsers, called \"simple LR\" (or SLR, for short). Some familiarity with the basic concepts is helpful even if the LR parser itself is constructed using an automatic parser generator. We begin with \"items\" and \"parser states;\" the diagnostic output from an LR parser generator typically includes parser states , which can be used to isolate the sources of parsing conflicts. Section 4.7 introduces two, more complex methods canonical-LR and LALR that are used in the majority of LR parsers. 4.6.1 Why LR Parsers? LR parsers are table-driven, much like the nonrecursive LL parsers of Section 4.4.4. A grammar for which we can construct a parsing table using one of the methods in this section and the next is said to be an LR grammar . Intuitively, for a grammar to be LR it is sufficient that a left-to-right shift-reduce parser be able to recognize handles of right-sentential forms when they appear on top of the stack. 4.6.2 Items and the LR(0) Automaton How do es a shift-reduce parser know when to shift and when to reduce? For example, with stack contents $T and next input symbol * in Fig. 4.28, how does the parser know that T on the top of the stack is not a handle, so the appropriate action is to shift and not to reduce T to E ? An LR parser makes shift-reduce decisions by maintaining states to keep track of where we are in a parse. States represent sets of \"items.\" An LR(0) item ( item for short) of a grammar G is a production of G with a dot at some position of the body. Thus, production A \\to XYZ A \\to XYZ yields the four items The production A \\to \\epsilon A \\to \\epsilon generates only one item, A \\to \\bullet A \\to \\bullet . Intuitively, an item indicates how much of a production we have seen at a given point in the parsing process. For example, the item A \\to X Y Z A \\to X Y Z indicates that we hope to see a string derivable from X Y Z X Y Z next on the input. Item A \\to X \\cdot Y Z A \\to X \\cdot Y Z indicates that we have just seen on the input a string derivable from X X and that we hope next to see a string derivable from Y Z Y Z . Item A \\to X Y Z A \\to X Y Z indicates that we have seen the body X Y Z X Y Z and that it may b e time to reduce X Y Z X Y Z to A A . One collection of sets of LR(0) items, called the canonical LR(0) collection, provides the basis for constructing a deterministic finite automaton that is used to make parsing decisions. Such an automaton is called an LR(0) automaton . In particular, each state of the LR(0) automaton represents a set of items in the canonical LR(0) collection . The automaton for the expression grammar (4.1), shown in Fig. 4.31, will serve as the running example for discussing the canonical LR(0) collection for a grammar. To construct the canonical LR(0) collection for a grammar, we define an augmented grammar and two functions, CLOSURE and GOTO. If G G is a grammar with start symbol S S , then \\acute{G} \\acute{G} , the augmented grammar for G, is G G with a new start symbol \\acute{S} \\acute{S} and production $\\acute{S} \\to S $. The purpose of this new starting production is to indicate to the parser when it should stop parsing and announce acceptance of the input. That is, acceptance occurs when and only when the parser is about to reduce by $\\acute{S} \\to S $. Closure of Item Sets The Function GOTO Use of the LR(0) Automaton The central idea behind \u201cSimple LR,\" or SLR, parsing is the construction from the grammar of the LR(0) automaton. The states of this automaton are the sets of items from the canonical LR(0) collection, and the transitions are given by the GOTO function. The LR(0) automaton for the expression grammar (4.1) appeared earlier in Fig. 4.31. The start state of the LR(0) automaton is CLOSURE({[ \\acute{S} \\to S]}) CLOSURE({[ \\acute{S} \\to S]}) , where \\acute{S} \\acute{S} is the start symbol of the augmented grammar. All states are accepting states. We say \"state j \" to refer to the state corresponding to the set of items I_j I_j . \u6587\u6cd5\u548cLR(0)\u81ea\u52a8\u673a \u4e00\u4e2a\u4ea7\u751f\u5f0f\u53ef\u80fd\u6709\u591a\u4e2a\u72b6\u6001\uff0c\u5b83\u4eec\u88ab\u6210\u4e3a\u9879\u3002\u6839\u636e\u9879\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u770b\u51fa\u5b83\u662f\u4e3areduction\u800c\u751f\u7684\uff0c\u5b83\u8868\u793a\u4e86\u5206\u6790\u5668\u5728\u5206\u6790\u7684\u8fc7\u7a0b\u4e2d\u5df2\u7ecf\u770b\u5230\u4e86**\u4ea7\u751f\u5f0f\u4f53**\u7684\u54ea\u4e9b\u90e8\u5206\uff0c\u4ee5\u53ca\u5b83\u6240\u671f\u671b\u770b\u5230\u7684\u5408\u4e4e\u5b83\u7684\u6587\u6cd5\u7684\u7b26\u53f7\u4e32\u3002\u5df2\u7ecf\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u5de6\u8fb9\uff0c\u671f\u671b\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u53f3\u8fb9\u3002\u663e\u7136\uff0c\u5f53\u5df2\u7ecf\u770b\u5230\u4e86\u4e00\u4e2a\u4ea7\u751f\u5f0f\u4f53\u7684\u5168\u90e8\u7b26\u53f7\u540e\uff0c\u5c31\u53ef\u4ee5\u8fdb\u884c\u89c4\u7ea6\u4e86\u3002 \u65e2\u7136\u5b9a\u4e49\u4e86\u72b6\u6001\uff0c\u90a3\u4e48\u80af\u5b9a\u5c31\u4f1a\u6d89\u53ca\u5230\u72b6\u6001\u7684\u8f6c\u6362\uff1a\u72b6\u6001\u7684\u8f6c\u6362\u6216\u8005\u8bf4\u9879\u7684\u8f6c\u6362\u662f\u7531\u5206\u6790\u5668\u5728\u5206\u6790\u8fc7\u7a0b\u4e2d\u770b\u5230\u7684\u6587\u6cd5\u7b26\u53f7\u800c\u89e6\u53d1\u7684\uff0c\u6bcf\u6b21\u770b\u5230\u4e00\u4e2a\u6587\u6cd5\u7b26\u53f7\uff0c\u5b83\u5c31\u53ef\u4ee5\u5c06\u4ea7\u751f\u5f0f\u4f53\u4e2d\u7684 \\bullet \\bullet \u5411\u53f3\u79fb\u52a8\u4e00\u6b21\uff0c\u4ece\u800c\u8fdb\u5165\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u72b6\u6001\u3002 \u90a3\u7ed9\u5b9a\u6587\u6cd5\uff0c\u6211\u4eec\u80fd\u5426\u63d0\u524d\u5c31\u5206\u6790\u51fa\u5b83\u4f1a\u6709\u54ea\u4e9b\u72b6\u6001\uff0c\u54ea\u4e9b\u8f6c\u6362\u5462\uff1f\u7b54\u6848\u662f\u80af\u5b9a\u7684\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u6784\u9020\u51fa\u7684LR(0)\u81ea\u52a8\u673a\u3002\u663e\u7136\uff0c\u4e00\u4e2aLR(0)\u81ea\u52a8\u673a\u7ed9\u51fa\u4e86\u7ed9\u5b9a\u6587\u6cd5\u4e2d\u7684\u6240\u6709\u7684\u53ef\u80fd\u7684\u6709\u6548\u7684\u8f6c\u6362\u3002 \u4eceLR(0)\u81ea\u52a8\u673a\u6765\u770b\u5f85LR\u8bed\u6cd5\u5206\u6790\u7b97\u6cd5 LR(0)\u81ea\u52a8\u673a\u662f**\u786e\u5b9a\u6709\u7a77\u72b6\u6001\u673a**\uff0c\u5b83\u4ece**\u72b6\u60010**\u5f00\u59cb\u5728\u6bcf\u4e2a\u7b26\u53f7\u4e0a\u90fd\u6709\u8f6c\u6362\u3002\u5982\u679c\u4e00\u4e2a\u72b6\u6001\u8868\u793a\u7684\u4ea7\u751f\u5f0f\u7684\u4f53\u5df2\u7ecf\u5168\u90e8\u90fd\u770b\u5230\u4e86\uff0c\u90a3\u4e48\u663e\u7136\u8fd9\u4e2a\u72b6\u6001\u5c31\u4e0d\u4f1a\u518d\u6709\u8f6c\u6362\u4e86\uff0c\u56e0\u6b64\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9009\u62e9\u5bf9\u5b83\u8fdb\u884c**\u89c4\u7ea6**\u3002Example 4.43 \u4e2d\u7ed9\u51fa\u7684LR parser\u53ef\u4ee5\u8ba4\u4e3a\u6709\u4e24\u4e2a\u6808: \u5b57\u7b26\u6808 \u72b6\u6001\u6808 **\u79fb\u5165\u64cd\u4f5c**\u5bf9\u5e94\u7684\u662f\u5728LR(0)\u72b6\u6001\u673a\u8fdb\u884c\u72b6\u6001\u8f6c\u6362\uff0c\u5373\u4ece\u4e00\u4e2a\u72b6\u6001\u8f6c\u79fb\u5230\u53e6\u5916\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u5c06INPUT\u4e2d\u7684symbol\u79fb\u5165\u5230\u5b57\u7b26\u6808\u4e2d\uff0c\u76f8\u5e94\u7684\uff0c\u4e5f\u4f1a\u5c06\u65b0\u5230\u8fbe\u7684\u72b6\u6001\u538b\u5165\u5230\u72b6\u6001\u6808\u4e2d\u3002\u663e\u7136\uff0c\u72b6\u6001\u6808\u8bb0\u5f55\u4e86\u72b6\u6001\u8f6c\u6362\u7684\u8def\u5f84\uff0c\u5373\u6808\u4e2d\u6bcf\u6761\u8bb0\u5f55\u662f\u4ece\u5b83\u540e\u9762\u7684\u4e00\u6761\u8bb0\u5f55\u8f6c\u6362\u800c\u6765\u7684\u3002 \u90a3\u4e48\u89c4\u7ea6\u610f\u5473\u7740\u5bf9\u4e0a\u8ff0\u4e24\u4e2a\u6808\u6267\u884c\u4ec0\u4e48\u64cd\u4f5c\u5462\uff1f\u89c4\u7ea6\u610f\u5473\u7740\u5c06\u5b57\u7b26\u6808\u4e2d**\u4ea7\u751f\u5f0f\u7684\u4f53**\u5f39\u51fa\u6808\uff0c\u76f8\u5e94\u7684\u4e5f\u8981\u4ece\u72b6\u6001\u6808\u4e2d\u5f39\u51fa\u76f8\u5e94\u6570\u91cf\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u5c06\u4ea7\u751f\u5f0f\u7684\u5934\u538b\u5165\u6808\u4e2d\uff0c\u90a3\u4e48\u6b64\u65f6\u8981\u538b\u5165\u4ec0\u4e48\u72b6\u6001\u5462\uff1f \u79fb\u5165\u662f\u6cbf\u7740\u81ea\u52a8\u673a\u7684\u67d0\u6761\u8def\u5f84\u8fdb\u884c\u8f6c\u6362\uff0c\u89c4\u7ea6\u5219\u662f\u56de\u5230\u8fd9\u6761\u8def\u5f84\u7684\u8d77\u70b9\uff0c\u663e\u7136\u89c4\u7ea6\u4f1a\u5f97\u5230\u4e00\u4e2anon-terminal\uff08\u5176\u5b9e\u8fd9\u5c31\u76f8\u5f53\u4e8e\u5df2\u7ecf\u6784\u9020\u597d\u4e86\u5b50\u6811\u4e86\uff09\u3002\u7136\u540e\u5728\u5f53\u524d\u6808\u9876\u7684\u72b6\u6001\u57fa\u4e8e\u524d\u9762\u89c4\u7ea6\u7684\u7b26\u53f7\u8fdb\u884c\u8f6c\u6362\uff0c\u7136\u540e\u5c06\u8f6c\u6362\u5f97\u5230\u7684\u65b0\u72b6\u6001\u538b\u5165\u6808\u4e2d\u3002\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u56e0\u4e3aparser\u5df2\u7ecf\u770b\u5230\u4e86\u8fd9\u4e2anon-terminal\u4e86\uff0c\u6240\u4ee5\u5fc5\u7136\u8981\u8fdb\u884c\u72b6\u6001\u7684\u8f6c\u6362\uff1b \u6811\u4e0e\u6808 \u4eceFigure 4.31: LR(0) automaton for the expression grammar (4.1)\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cLR(0) automaton\u4e5f\u662f\u6811\u5f62\u7684\uff0cLR\u8bed\u6cd5\u5206\u6790\u5668\u5728\u8fd9\u68f5\u6811\u4e0a\u7684\u64cd\u4f5c\u4e5f\u662f\u57fa\u4e8e**\u6808**\u7684\u3002\u5176\u5b9e\u5728\u601d\u8003LR(0) parser\u7684\u65f6\u5019\uff0c\u6211\u60f3\u5230\u4e86\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u5982\u679c\u5c06\u7a0b\u5e8f\u6267\u884c\u8fc7\u7a0b\u7cbe\u7b80\u6210\u51fd\u6570\u8c03\u7528\u7684\u8bdd\uff0c\u5176\u5b9e\u6574\u4e2a\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u753b\u6210\u6811\u5f62\u7684\uff0c\u5373\u7a0b\u5e8f\u7684**\u51fd\u6570\u8c03\u7528\u6811**\uff1a\u8fd9\u68f5\u6811\u7684 root\u8282\u70b9\u5c31\u662fmain\u51fd\u6570\uff0cmain\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u8fd9\u68f5\u6811\u7684\u7b2c\u4e00\u5c42\uff0c\u7b2c\u4e00\u5c42\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u7b2c\u4e8c\u5c42\uff0c\u4f9d\u6b21\u9012\u63a8\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u68f5\u5b8c\u6574\u7684\u6811\u4e86\uff1b\u5e76\u4e14\u548cLR(0) parser\u4e00\u6837\uff0c\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u662f\u4f7f\u7528\u7684stack\u3002\u663e\u7136\uff0c\u4e24\u79cd\u60c5\u51b5\u90fd\u6d89\u53ca\u4e86tree\u548cstack\uff1a \u5728LR parser\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u7b26\u53f7\u538b\u6808\u76f4\u81f3\u770b\u5230\u4e86\u5b8c\u6574\u7684\u4ea7\u751f\u5f0f\u4f53\u624d\u5c06\u4ea7\u751f\u5f0f\u4f53\u5f39\u51fa\u6808\u3001\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5f0f\u7684\u5934\u90e8->\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5934\u4f9d\u8d56\u4e8e\u5f97\u5230\u6240\u6709\u7684\u4ea7\u751f\u4f53->\u4e0d\u65ad\u5730\u5c06\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u538b\u6808\uff0c\u76f4\u5230\u89c1\u5230\u5168\u90e8\u7684\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u5c31\u51fa\u6808 \u5728\u51fd\u6570\u8c03\u7528\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u5165call stack\u4e2d\uff0c\u76f4\u81f3\u6240\u6709\u7684\u5b50\u51fd\u6570\u90fd\u8fd4\u56de\u4e3b\u51fd\u6570\u624d\u5f97\u4ee5\u6267\u884c\u5b8c\u6210 ->\u4e3b\u51fd\u6570\u7684\u503c\u4f9d\u8d56\u4e8e\u6240\u6709\u7684\u5b50\u51fd\u6570->\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u6808\uff0c\u77e5\u9053\u6700\u5e95\u5c42\u7684\u5b50\u51fd\u6570\u6c42\u89e3\uff0c\u624d\u4f9d\u6b21\u51fa\u6808 \u770b\uff0c\u4e24\u8005\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u591a\u4e48\u5730\u7c7b\u4f3c\uff1b \u4ece\u6811\u7684\u6784\u9020\u7684\u89d2\u5ea6\u6765\u770b\u5f85LR parser\u4e2d\u7684tree\u548c\u51fd\u6570\u8c03\u7528\u4e2d\u7684\u6811\uff1aLR parser\u4e2d\uff0ctree\u7684\u6784\u9020\u662f\u81ea\u5e95\u5411\u4e0a\u7684\uff0c\u800c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u81ea\u9876\u5411\u4e0b\u7684\uff0c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u6bd4\u8f83\u7c7b\u4f3c\u4e8eLR(0) parser\u4e2d\u6811\u7684\u6784\u9020\u3002 \u65e0\u8bba\u662f**LR(0)\u81ea\u52a8\u673a**\u4ee5\u53ca**\u51fd\u6570\u8c03\u7528\u6811**\uff0c\u5b83\u4eec\u90fd\u662f\u662f\u6211\u4eec\u4ece\u5168\u5c40\u7684\u89d2\u5ea6\uff08\u6574\u4f53\u7684\u89d2\u5ea6\uff0c\u5206\u6790\u7684\u89d2\u5ea6\uff09\u6765\u5206\u6790\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b83\u4eec\u662f\u7406\u8bba\u5c42\u9762\u7684\u5206\u6790\uff0c\u800c\u4e0d\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\uff0c\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u538b\u6839\u5c31\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u6784\u9020\u51fa\u8fd9\u6837\u7684\u4e00\u68f5\u6811\uff0c\u5e76\u4e14\u538b\u6839\u5c31\u65e0\u9700\u77e5\u9053\u6574\u4e2a\u6811\u662f\u600e\u6837\u7684\u3002\u6bd4\u5982\u5728LR parser\u4e2d\uff0cparser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u51fd\u6570\u7684\u6267\u884c\u662f\u987a\u5e8f\u6267\u884c\u7684\uff0c\u4e00\u6b21\u53ea\u4f1a\u6267\u884c\u4e00\u4e2a\u51fd\u6570\uff1b\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u6211\u89c9\u5f97\u8fd9\u662f\u7531\u8ba1\u7b97\u673a\u7684\u4f53\u7cfb\u7ed3\u6784\u6240\u51b3\u5b9a\u7684\uff0c\u6b63\u5982\u5404\u79cdautomaton\u6a21\u578b\u6240\u5c55\u793a\u7684\u90a3\u6837\uff0c\u8ba1\u7b97\u673a\u5c31\u662f\u8fd9\u6837\u7684\u89c4\u5219\uff0c\u5c31\u662f\u8fd9\u6837\u7684\u987a\u5e8f\uff0c\u6240\u4ee5\u6211\u4eec\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e5f\u662f\u9700\u8981\u5bfb\u627e\u89c4\u5219\uff0c\u987a\u5e8f\uff0c\u8fd9\u662f\u4e00\u79cd\u8ba1\u7b97\u601d\u7ef4\uff1b \u6240\u4ee5\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\uff08\u6709\u8d77\u70b9\uff0c\u6709\u7ec8\u70b9\uff09\uff0c\u663e\u7136\u8fd9\u6761\u8def\u5f84\u662f**\u7ebf\u6027\u7684**\uff0c\u662f**\u8fde\u7eed\u7684**\uff08\u80fd\u591f\u4ece\u7ec8\u70b9\u518d\u8fd4\u56de\u5230\u8d77\u70b9\uff09\u3002\u5982\u679c\u6211\u4eec\u5c06\u6267\u884c\u7684\u8def\u5f84\u8fde\u63a5\u8d77\u6765\uff08\u56e0\u4e3a\u8fd9\u4e9b\u8def\u5f84\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5c06\u5b83\u4eec\u8fde\u63a5\u8d77\u6765\uff09\uff0c\u4ee5\u9002\u5f53\u7684\u65b9\u5f0f\u753b\u51fa\u4e86\uff08\u6b63\u5982Fig. 4.31\uff09\uff0c\u90a3\u4e48\u5b83\u5c31\u80fd\u591f\u5c55\u73b0\u51fa\u6211\u4eec\u7684\u5728\u7406\u8bba\u5c42\u9762\u5206\u6790\u7684\u5f62\u6001\u3002 \u5982\u679c\u4ece\u6811\u7684\u6784\u9020\u6765\u770b\u7684\u8bdd\uff0c\u5728parser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u4ece\u5b50\u6811\u5f00\u59cb\u6784\u9020\uff0c\u7136\u540e\u5c06\u4e00\u68f5\u4e00\u68f5\u7684\u5b50\u6811\u7ed3\u5408\u8d77\u6765\u6784\u9020\u66f4\u5927\u7684\u6811\u3002\u5728data structure\u4e2d\uff0c\u6811\u662f\u6709\u4e00\u4e2a\u4e00\u4e2a\u7684node\u8fde\u63a5\u800c\u6210\u7684\uff0c\u6240\u4ee5\u8bbf\u95ee\u4e00\u68f5\u6811\u53ea\u9700\u8981\u5f97\u5230\u8fd9\u68f5\u6811\u7684\u6839\u7ed3\u70b9\u5373\u53ef\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528node\u6765\u4ee3\u66ff\u4e00\u68f5\u6811\u3002\u6240\u4ee5\u5728\u6811\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\uff0c\u6240\u64cd\u4f5c\u7684\u662f\u4e00\u4e2a\u4e00\u4e2a\u7684node\uff0c\u6240\u4ee5\u4f7f\u7528\u4f7f\u7528stack\u5c31\u53ef\u4ee5\u5b8c\u6210\u4e00\u68f5\u6811\u7684\u6784\u9020\u3002 \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5bf9\u7406\u8bba\u6a21\u578b\u7684\u5b9e\u73b0\u65f6\u5f80\u5f80\u9009\u62e9\u7684\u662f\u901a\u7528\u7684\uff0c\u7b80\u5355\u7684\u65b9\u5f0f\uff08\u8282\u7ea6\u5185\u5b58\u7b49\uff09\uff0c\"\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\"\uff0c\u6240\u4ee5\u6211\u4eec\u4ec5\u4ec5\u9700\u8981\u7684\u662f\u80fd\u591f\u6ee1\u8db3\u8fd9\u6761\u8def\u5f84\u7684\u7ed3\u6784\u5373\u53ef\u3002\u800c\u6808\u8fd9\u79cd\u7ed3\u6784\u5c31\u6b63\u597d\u7b26\u5408\u8fd9\u4e9b\u8981\u6c42\uff1a \u6808\u662f\u7ebf\u6027\u7684 \u6808\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5b9e\u73b0\u4ece\u7ec8\u70b9\u56de\u5230\u8d77\u70b9 \u518d\u56de\u5230\u7406\u8bba\u5206\u6790\u5c42\u9762\uff0c\u5b9e\u9645\u6267\u884c\u8fc7\u7a0b\u548c\u7406\u8bba\u5c42\u9762\u7684\u6a21\u578b\u4e4b\u95f4\u662f\u600e\u6837\u7684\u5173\u8054\u5462\uff1f\u5b9e\u9645\u6267\u884c\u6d41\u7a0b\u5bf9\u5e94\u7684\u662f\u5bf9\u6811\u6267\u884c\u6df1\u5ea6\u4f18\u5148\u540e\u5e8f\u904d\u5386\uff1b 4.6.5 Viable Prefixes Why can LR(0) automata be used to make shift-reduce decisions? The LR(0) automaton for a grammar characterizes the strings of grammar symbols that can appear on the stack of a shift-reduce parser for the grammar. The stack contents must be a prefix of a right-sentential form . If the stack holds \\alpha \\alpha and the rest of the input is x x , then a sequence of reductions will take \\alpha x \\alpha x to S S . In terms of derivations, S \\xrightarrow[rm]{*} \\alpha x S \\xrightarrow[rm]{*} \\alpha x . Not all prefixes of right-sentential forms can appear on the stack, however, since the parser must not shift past the handle . For example, suppose E \\xrightarrow[rm]{ \\ast } F \\ast id \\xrightarrow[rm]{} (E) \\ast id E \\xrightarrow[rm]{ \\ast } F \\ast id \\xrightarrow[rm]{} (E) \\ast id Then, at various times during the parse, the stack will hold ( ( ; (E (E , and (E ) (E ) , but it must not hold (E ) \\ast (E ) \\ast , since (E ) (E ) is a handle, which the parser must reduce to F F before shifting \\ast \\ast . The prefixes of right sentential forms that can appear on the stack of a shift-reduce parser are called viable prefixes . They are defined as follows: a viable prefix is a prefix of a right-sentential form that does not continue past the right end of the rightmost handle of that sentential form . By this definition, it is always possible to add terminal symbols to the end of a viable prefix to obtain a right-sentential form. NOTE: In simple terms, a prefix cannot contain a handle. Once it does, it should be reduced. SLR parsing is based on the fact that LR(0) automata recognize viable prefixes . We say item A \\to \\beta _1 \\beta _2 A \\to \\beta _1 \\beta _2 is valid for a viable prefix \\alpha \\beta _1 \\alpha \\beta _1 if there is a derivation S \\xrightarrow [rm] {*} \\alpha Aw \\xrightarrow [rm]{*} \\alpha \\beta _1 \\beta _2 w S \\xrightarrow [rm] {*} \\alpha Aw \\xrightarrow [rm]{*} \\alpha \\beta _1 \\beta _2 w . In general, an item will be valid for many viable prefixes. NOTE: viable prefix\u548c","title":"Introduction"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#46#introduction#to#lr#parsing#simple#lr","text":"The most prevalent type of bottom-up parser to day is based on a concept called LR(k ) parsing; the \"L\" is for left-to-right scanning of the input, the \"R\" for constructing a rightmost derivation in reverse, and the k for the number of input symbols of lookahead that are used in making parsing decisions. The cases k = 0 or k = 1 are of practical interest, and we shall only consider LR parsers with k \\le 1 k \\le 1 here. When (k ) is omitted, k is assumed to be 1. This section introduces the basic concepts of LR parsing and the easiest method for constructing shift-reduce parsers, called \"simple LR\" (or SLR, for short). Some familiarity with the basic concepts is helpful even if the LR parser itself is constructed using an automatic parser generator. We begin with \"items\" and \"parser states;\" the diagnostic output from an LR parser generator typically includes parser states , which can be used to isolate the sources of parsing conflicts. Section 4.7 introduces two, more complex methods canonical-LR and LALR that are used in the majority of LR parsers.","title":"4.6 Introduction to LR Parsing: Simple LR"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#461#why#lr#parsers","text":"LR parsers are table-driven, much like the nonrecursive LL parsers of Section 4.4.4. A grammar for which we can construct a parsing table using one of the methods in this section and the next is said to be an LR grammar . Intuitively, for a grammar to be LR it is sufficient that a left-to-right shift-reduce parser be able to recognize handles of right-sentential forms when they appear on top of the stack.","title":"4.6.1 Why LR Parsers?"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#462#items#and#the#lr0#automaton","text":"How do es a shift-reduce parser know when to shift and when to reduce? For example, with stack contents $T and next input symbol * in Fig. 4.28, how does the parser know that T on the top of the stack is not a handle, so the appropriate action is to shift and not to reduce T to E ? An LR parser makes shift-reduce decisions by maintaining states to keep track of where we are in a parse. States represent sets of \"items.\" An LR(0) item ( item for short) of a grammar G is a production of G with a dot at some position of the body. Thus, production A \\to XYZ A \\to XYZ yields the four items The production A \\to \\epsilon A \\to \\epsilon generates only one item, A \\to \\bullet A \\to \\bullet . Intuitively, an item indicates how much of a production we have seen at a given point in the parsing process. For example, the item A \\to X Y Z A \\to X Y Z indicates that we hope to see a string derivable from X Y Z X Y Z next on the input. Item A \\to X \\cdot Y Z A \\to X \\cdot Y Z indicates that we have just seen on the input a string derivable from X X and that we hope next to see a string derivable from Y Z Y Z . Item A \\to X Y Z A \\to X Y Z indicates that we have seen the body X Y Z X Y Z and that it may b e time to reduce X Y Z X Y Z to A A . One collection of sets of LR(0) items, called the canonical LR(0) collection, provides the basis for constructing a deterministic finite automaton that is used to make parsing decisions. Such an automaton is called an LR(0) automaton . In particular, each state of the LR(0) automaton represents a set of items in the canonical LR(0) collection . The automaton for the expression grammar (4.1), shown in Fig. 4.31, will serve as the running example for discussing the canonical LR(0) collection for a grammar. To construct the canonical LR(0) collection for a grammar, we define an augmented grammar and two functions, CLOSURE and GOTO. If G G is a grammar with start symbol S S , then \\acute{G} \\acute{G} , the augmented grammar for G, is G G with a new start symbol \\acute{S} \\acute{S} and production $\\acute{S} \\to S $. The purpose of this new starting production is to indicate to the parser when it should stop parsing and announce acceptance of the input. That is, acceptance occurs when and only when the parser is about to reduce by $\\acute{S} \\to S $.","title":"4.6.2 Items and the LR(0) Automaton"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#closure#of#item#sets","text":"","title":"Closure of Item Sets"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#the#function#goto","text":"","title":"The Function GOTO"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#use#of#the#lr0#automaton","text":"The central idea behind \u201cSimple LR,\" or SLR, parsing is the construction from the grammar of the LR(0) automaton. The states of this automaton are the sets of items from the canonical LR(0) collection, and the transitions are given by the GOTO function. The LR(0) automaton for the expression grammar (4.1) appeared earlier in Fig. 4.31. The start state of the LR(0) automaton is CLOSURE({[ \\acute{S} \\to S]}) CLOSURE({[ \\acute{S} \\to S]}) , where \\acute{S} \\acute{S} is the start symbol of the augmented grammar. All states are accepting states. We say \"state j \" to refer to the state corresponding to the set of items I_j I_j .","title":"Use of the LR(0) Automaton"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#lr0","text":"\u4e00\u4e2a\u4ea7\u751f\u5f0f\u53ef\u80fd\u6709\u591a\u4e2a\u72b6\u6001\uff0c\u5b83\u4eec\u88ab\u6210\u4e3a\u9879\u3002\u6839\u636e\u9879\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u770b\u51fa\u5b83\u662f\u4e3areduction\u800c\u751f\u7684\uff0c\u5b83\u8868\u793a\u4e86\u5206\u6790\u5668\u5728\u5206\u6790\u7684\u8fc7\u7a0b\u4e2d\u5df2\u7ecf\u770b\u5230\u4e86**\u4ea7\u751f\u5f0f\u4f53**\u7684\u54ea\u4e9b\u90e8\u5206\uff0c\u4ee5\u53ca\u5b83\u6240\u671f\u671b\u770b\u5230\u7684\u5408\u4e4e\u5b83\u7684\u6587\u6cd5\u7684\u7b26\u53f7\u4e32\u3002\u5df2\u7ecf\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u5de6\u8fb9\uff0c\u671f\u671b\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u53f3\u8fb9\u3002\u663e\u7136\uff0c\u5f53\u5df2\u7ecf\u770b\u5230\u4e86\u4e00\u4e2a\u4ea7\u751f\u5f0f\u4f53\u7684\u5168\u90e8\u7b26\u53f7\u540e\uff0c\u5c31\u53ef\u4ee5\u8fdb\u884c\u89c4\u7ea6\u4e86\u3002 \u65e2\u7136\u5b9a\u4e49\u4e86\u72b6\u6001\uff0c\u90a3\u4e48\u80af\u5b9a\u5c31\u4f1a\u6d89\u53ca\u5230\u72b6\u6001\u7684\u8f6c\u6362\uff1a\u72b6\u6001\u7684\u8f6c\u6362\u6216\u8005\u8bf4\u9879\u7684\u8f6c\u6362\u662f\u7531\u5206\u6790\u5668\u5728\u5206\u6790\u8fc7\u7a0b\u4e2d\u770b\u5230\u7684\u6587\u6cd5\u7b26\u53f7\u800c\u89e6\u53d1\u7684\uff0c\u6bcf\u6b21\u770b\u5230\u4e00\u4e2a\u6587\u6cd5\u7b26\u53f7\uff0c\u5b83\u5c31\u53ef\u4ee5\u5c06\u4ea7\u751f\u5f0f\u4f53\u4e2d\u7684 \\bullet \\bullet \u5411\u53f3\u79fb\u52a8\u4e00\u6b21\uff0c\u4ece\u800c\u8fdb\u5165\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u72b6\u6001\u3002 \u90a3\u7ed9\u5b9a\u6587\u6cd5\uff0c\u6211\u4eec\u80fd\u5426\u63d0\u524d\u5c31\u5206\u6790\u51fa\u5b83\u4f1a\u6709\u54ea\u4e9b\u72b6\u6001\uff0c\u54ea\u4e9b\u8f6c\u6362\u5462\uff1f\u7b54\u6848\u662f\u80af\u5b9a\u7684\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u6784\u9020\u51fa\u7684LR(0)\u81ea\u52a8\u673a\u3002\u663e\u7136\uff0c\u4e00\u4e2aLR(0)\u81ea\u52a8\u673a\u7ed9\u51fa\u4e86\u7ed9\u5b9a\u6587\u6cd5\u4e2d\u7684\u6240\u6709\u7684\u53ef\u80fd\u7684\u6709\u6548\u7684\u8f6c\u6362\u3002","title":"\u6587\u6cd5\u548cLR(0)\u81ea\u52a8\u673a"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#lr0lr","text":"LR(0)\u81ea\u52a8\u673a\u662f**\u786e\u5b9a\u6709\u7a77\u72b6\u6001\u673a**\uff0c\u5b83\u4ece**\u72b6\u60010**\u5f00\u59cb\u5728\u6bcf\u4e2a\u7b26\u53f7\u4e0a\u90fd\u6709\u8f6c\u6362\u3002\u5982\u679c\u4e00\u4e2a\u72b6\u6001\u8868\u793a\u7684\u4ea7\u751f\u5f0f\u7684\u4f53\u5df2\u7ecf\u5168\u90e8\u90fd\u770b\u5230\u4e86\uff0c\u90a3\u4e48\u663e\u7136\u8fd9\u4e2a\u72b6\u6001\u5c31\u4e0d\u4f1a\u518d\u6709\u8f6c\u6362\u4e86\uff0c\u56e0\u6b64\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9009\u62e9\u5bf9\u5b83\u8fdb\u884c**\u89c4\u7ea6**\u3002Example 4.43 \u4e2d\u7ed9\u51fa\u7684LR parser\u53ef\u4ee5\u8ba4\u4e3a\u6709\u4e24\u4e2a\u6808: \u5b57\u7b26\u6808 \u72b6\u6001\u6808 **\u79fb\u5165\u64cd\u4f5c**\u5bf9\u5e94\u7684\u662f\u5728LR(0)\u72b6\u6001\u673a\u8fdb\u884c\u72b6\u6001\u8f6c\u6362\uff0c\u5373\u4ece\u4e00\u4e2a\u72b6\u6001\u8f6c\u79fb\u5230\u53e6\u5916\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u5c06INPUT\u4e2d\u7684symbol\u79fb\u5165\u5230\u5b57\u7b26\u6808\u4e2d\uff0c\u76f8\u5e94\u7684\uff0c\u4e5f\u4f1a\u5c06\u65b0\u5230\u8fbe\u7684\u72b6\u6001\u538b\u5165\u5230\u72b6\u6001\u6808\u4e2d\u3002\u663e\u7136\uff0c\u72b6\u6001\u6808\u8bb0\u5f55\u4e86\u72b6\u6001\u8f6c\u6362\u7684\u8def\u5f84\uff0c\u5373\u6808\u4e2d\u6bcf\u6761\u8bb0\u5f55\u662f\u4ece\u5b83\u540e\u9762\u7684\u4e00\u6761\u8bb0\u5f55\u8f6c\u6362\u800c\u6765\u7684\u3002 \u90a3\u4e48\u89c4\u7ea6\u610f\u5473\u7740\u5bf9\u4e0a\u8ff0\u4e24\u4e2a\u6808\u6267\u884c\u4ec0\u4e48\u64cd\u4f5c\u5462\uff1f\u89c4\u7ea6\u610f\u5473\u7740\u5c06\u5b57\u7b26\u6808\u4e2d**\u4ea7\u751f\u5f0f\u7684\u4f53**\u5f39\u51fa\u6808\uff0c\u76f8\u5e94\u7684\u4e5f\u8981\u4ece\u72b6\u6001\u6808\u4e2d\u5f39\u51fa\u76f8\u5e94\u6570\u91cf\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u5c06\u4ea7\u751f\u5f0f\u7684\u5934\u538b\u5165\u6808\u4e2d\uff0c\u90a3\u4e48\u6b64\u65f6\u8981\u538b\u5165\u4ec0\u4e48\u72b6\u6001\u5462\uff1f \u79fb\u5165\u662f\u6cbf\u7740\u81ea\u52a8\u673a\u7684\u67d0\u6761\u8def\u5f84\u8fdb\u884c\u8f6c\u6362\uff0c\u89c4\u7ea6\u5219\u662f\u56de\u5230\u8fd9\u6761\u8def\u5f84\u7684\u8d77\u70b9\uff0c\u663e\u7136\u89c4\u7ea6\u4f1a\u5f97\u5230\u4e00\u4e2anon-terminal\uff08\u5176\u5b9e\u8fd9\u5c31\u76f8\u5f53\u4e8e\u5df2\u7ecf\u6784\u9020\u597d\u4e86\u5b50\u6811\u4e86\uff09\u3002\u7136\u540e\u5728\u5f53\u524d\u6808\u9876\u7684\u72b6\u6001\u57fa\u4e8e\u524d\u9762\u89c4\u7ea6\u7684\u7b26\u53f7\u8fdb\u884c\u8f6c\u6362\uff0c\u7136\u540e\u5c06\u8f6c\u6362\u5f97\u5230\u7684\u65b0\u72b6\u6001\u538b\u5165\u6808\u4e2d\u3002\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u56e0\u4e3aparser\u5df2\u7ecf\u770b\u5230\u4e86\u8fd9\u4e2anon-terminal\u4e86\uff0c\u6240\u4ee5\u5fc5\u7136\u8981\u8fdb\u884c\u72b6\u6001\u7684\u8f6c\u6362\uff1b","title":"\u4eceLR(0)\u81ea\u52a8\u673a\u6765\u770b\u5f85LR\u8bed\u6cd5\u5206\u6790\u7b97\u6cd5"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#_1","text":"\u4eceFigure 4.31: LR(0) automaton for the expression grammar (4.1)\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cLR(0) automaton\u4e5f\u662f\u6811\u5f62\u7684\uff0cLR\u8bed\u6cd5\u5206\u6790\u5668\u5728\u8fd9\u68f5\u6811\u4e0a\u7684\u64cd\u4f5c\u4e5f\u662f\u57fa\u4e8e**\u6808**\u7684\u3002\u5176\u5b9e\u5728\u601d\u8003LR(0) parser\u7684\u65f6\u5019\uff0c\u6211\u60f3\u5230\u4e86\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u5982\u679c\u5c06\u7a0b\u5e8f\u6267\u884c\u8fc7\u7a0b\u7cbe\u7b80\u6210\u51fd\u6570\u8c03\u7528\u7684\u8bdd\uff0c\u5176\u5b9e\u6574\u4e2a\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u753b\u6210\u6811\u5f62\u7684\uff0c\u5373\u7a0b\u5e8f\u7684**\u51fd\u6570\u8c03\u7528\u6811**\uff1a\u8fd9\u68f5\u6811\u7684 root\u8282\u70b9\u5c31\u662fmain\u51fd\u6570\uff0cmain\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u8fd9\u68f5\u6811\u7684\u7b2c\u4e00\u5c42\uff0c\u7b2c\u4e00\u5c42\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u7b2c\u4e8c\u5c42\uff0c\u4f9d\u6b21\u9012\u63a8\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u68f5\u5b8c\u6574\u7684\u6811\u4e86\uff1b\u5e76\u4e14\u548cLR(0) parser\u4e00\u6837\uff0c\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u662f\u4f7f\u7528\u7684stack\u3002\u663e\u7136\uff0c\u4e24\u79cd\u60c5\u51b5\u90fd\u6d89\u53ca\u4e86tree\u548cstack\uff1a \u5728LR parser\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u7b26\u53f7\u538b\u6808\u76f4\u81f3\u770b\u5230\u4e86\u5b8c\u6574\u7684\u4ea7\u751f\u5f0f\u4f53\u624d\u5c06\u4ea7\u751f\u5f0f\u4f53\u5f39\u51fa\u6808\u3001\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5f0f\u7684\u5934\u90e8->\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5934\u4f9d\u8d56\u4e8e\u5f97\u5230\u6240\u6709\u7684\u4ea7\u751f\u4f53->\u4e0d\u65ad\u5730\u5c06\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u538b\u6808\uff0c\u76f4\u5230\u89c1\u5230\u5168\u90e8\u7684\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u5c31\u51fa\u6808 \u5728\u51fd\u6570\u8c03\u7528\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u5165call stack\u4e2d\uff0c\u76f4\u81f3\u6240\u6709\u7684\u5b50\u51fd\u6570\u90fd\u8fd4\u56de\u4e3b\u51fd\u6570\u624d\u5f97\u4ee5\u6267\u884c\u5b8c\u6210 ->\u4e3b\u51fd\u6570\u7684\u503c\u4f9d\u8d56\u4e8e\u6240\u6709\u7684\u5b50\u51fd\u6570->\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u6808\uff0c\u77e5\u9053\u6700\u5e95\u5c42\u7684\u5b50\u51fd\u6570\u6c42\u89e3\uff0c\u624d\u4f9d\u6b21\u51fa\u6808 \u770b\uff0c\u4e24\u8005\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u591a\u4e48\u5730\u7c7b\u4f3c\uff1b \u4ece\u6811\u7684\u6784\u9020\u7684\u89d2\u5ea6\u6765\u770b\u5f85LR parser\u4e2d\u7684tree\u548c\u51fd\u6570\u8c03\u7528\u4e2d\u7684\u6811\uff1aLR parser\u4e2d\uff0ctree\u7684\u6784\u9020\u662f\u81ea\u5e95\u5411\u4e0a\u7684\uff0c\u800c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u81ea\u9876\u5411\u4e0b\u7684\uff0c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u6bd4\u8f83\u7c7b\u4f3c\u4e8eLR(0) parser\u4e2d\u6811\u7684\u6784\u9020\u3002 \u65e0\u8bba\u662f**LR(0)\u81ea\u52a8\u673a**\u4ee5\u53ca**\u51fd\u6570\u8c03\u7528\u6811**\uff0c\u5b83\u4eec\u90fd\u662f\u662f\u6211\u4eec\u4ece\u5168\u5c40\u7684\u89d2\u5ea6\uff08\u6574\u4f53\u7684\u89d2\u5ea6\uff0c\u5206\u6790\u7684\u89d2\u5ea6\uff09\u6765\u5206\u6790\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b83\u4eec\u662f\u7406\u8bba\u5c42\u9762\u7684\u5206\u6790\uff0c\u800c\u4e0d\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\uff0c\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u538b\u6839\u5c31\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u6784\u9020\u51fa\u8fd9\u6837\u7684\u4e00\u68f5\u6811\uff0c\u5e76\u4e14\u538b\u6839\u5c31\u65e0\u9700\u77e5\u9053\u6574\u4e2a\u6811\u662f\u600e\u6837\u7684\u3002\u6bd4\u5982\u5728LR parser\u4e2d\uff0cparser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u51fd\u6570\u7684\u6267\u884c\u662f\u987a\u5e8f\u6267\u884c\u7684\uff0c\u4e00\u6b21\u53ea\u4f1a\u6267\u884c\u4e00\u4e2a\u51fd\u6570\uff1b\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u6211\u89c9\u5f97\u8fd9\u662f\u7531\u8ba1\u7b97\u673a\u7684\u4f53\u7cfb\u7ed3\u6784\u6240\u51b3\u5b9a\u7684\uff0c\u6b63\u5982\u5404\u79cdautomaton\u6a21\u578b\u6240\u5c55\u793a\u7684\u90a3\u6837\uff0c\u8ba1\u7b97\u673a\u5c31\u662f\u8fd9\u6837\u7684\u89c4\u5219\uff0c\u5c31\u662f\u8fd9\u6837\u7684\u987a\u5e8f\uff0c\u6240\u4ee5\u6211\u4eec\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e5f\u662f\u9700\u8981\u5bfb\u627e\u89c4\u5219\uff0c\u987a\u5e8f\uff0c\u8fd9\u662f\u4e00\u79cd\u8ba1\u7b97\u601d\u7ef4\uff1b \u6240\u4ee5\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\uff08\u6709\u8d77\u70b9\uff0c\u6709\u7ec8\u70b9\uff09\uff0c\u663e\u7136\u8fd9\u6761\u8def\u5f84\u662f**\u7ebf\u6027\u7684**\uff0c\u662f**\u8fde\u7eed\u7684**\uff08\u80fd\u591f\u4ece\u7ec8\u70b9\u518d\u8fd4\u56de\u5230\u8d77\u70b9\uff09\u3002\u5982\u679c\u6211\u4eec\u5c06\u6267\u884c\u7684\u8def\u5f84\u8fde\u63a5\u8d77\u6765\uff08\u56e0\u4e3a\u8fd9\u4e9b\u8def\u5f84\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5c06\u5b83\u4eec\u8fde\u63a5\u8d77\u6765\uff09\uff0c\u4ee5\u9002\u5f53\u7684\u65b9\u5f0f\u753b\u51fa\u4e86\uff08\u6b63\u5982Fig. 4.31\uff09\uff0c\u90a3\u4e48\u5b83\u5c31\u80fd\u591f\u5c55\u73b0\u51fa\u6211\u4eec\u7684\u5728\u7406\u8bba\u5c42\u9762\u5206\u6790\u7684\u5f62\u6001\u3002 \u5982\u679c\u4ece\u6811\u7684\u6784\u9020\u6765\u770b\u7684\u8bdd\uff0c\u5728parser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u4ece\u5b50\u6811\u5f00\u59cb\u6784\u9020\uff0c\u7136\u540e\u5c06\u4e00\u68f5\u4e00\u68f5\u7684\u5b50\u6811\u7ed3\u5408\u8d77\u6765\u6784\u9020\u66f4\u5927\u7684\u6811\u3002\u5728data structure\u4e2d\uff0c\u6811\u662f\u6709\u4e00\u4e2a\u4e00\u4e2a\u7684node\u8fde\u63a5\u800c\u6210\u7684\uff0c\u6240\u4ee5\u8bbf\u95ee\u4e00\u68f5\u6811\u53ea\u9700\u8981\u5f97\u5230\u8fd9\u68f5\u6811\u7684\u6839\u7ed3\u70b9\u5373\u53ef\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528node\u6765\u4ee3\u66ff\u4e00\u68f5\u6811\u3002\u6240\u4ee5\u5728\u6811\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\uff0c\u6240\u64cd\u4f5c\u7684\u662f\u4e00\u4e2a\u4e00\u4e2a\u7684node\uff0c\u6240\u4ee5\u4f7f\u7528\u4f7f\u7528stack\u5c31\u53ef\u4ee5\u5b8c\u6210\u4e00\u68f5\u6811\u7684\u6784\u9020\u3002 \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5bf9\u7406\u8bba\u6a21\u578b\u7684\u5b9e\u73b0\u65f6\u5f80\u5f80\u9009\u62e9\u7684\u662f\u901a\u7528\u7684\uff0c\u7b80\u5355\u7684\u65b9\u5f0f\uff08\u8282\u7ea6\u5185\u5b58\u7b49\uff09\uff0c\"\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\"\uff0c\u6240\u4ee5\u6211\u4eec\u4ec5\u4ec5\u9700\u8981\u7684\u662f\u80fd\u591f\u6ee1\u8db3\u8fd9\u6761\u8def\u5f84\u7684\u7ed3\u6784\u5373\u53ef\u3002\u800c\u6808\u8fd9\u79cd\u7ed3\u6784\u5c31\u6b63\u597d\u7b26\u5408\u8fd9\u4e9b\u8981\u6c42\uff1a \u6808\u662f\u7ebf\u6027\u7684 \u6808\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5b9e\u73b0\u4ece\u7ec8\u70b9\u56de\u5230\u8d77\u70b9 \u518d\u56de\u5230\u7406\u8bba\u5206\u6790\u5c42\u9762\uff0c\u5b9e\u9645\u6267\u884c\u8fc7\u7a0b\u548c\u7406\u8bba\u5c42\u9762\u7684\u6a21\u578b\u4e4b\u95f4\u662f\u600e\u6837\u7684\u5173\u8054\u5462\uff1f\u5b9e\u9645\u6267\u884c\u6d41\u7a0b\u5bf9\u5e94\u7684\u662f\u5bf9\u6811\u6267\u884c\u6df1\u5ea6\u4f18\u5148\u540e\u5e8f\u904d\u5386\uff1b","title":"\u6811\u4e0e\u6808"},{"location":"4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#465#viable#prefixes","text":"Why can LR(0) automata be used to make shift-reduce decisions? The LR(0) automaton for a grammar characterizes the strings of grammar symbols that can appear on the stack of a shift-reduce parser for the grammar. The stack contents must be a prefix of a right-sentential form . If the stack holds \\alpha \\alpha and the rest of the input is x x , then a sequence of reductions will take \\alpha x \\alpha x to S S . In terms of derivations, S \\xrightarrow[rm]{*} \\alpha x S \\xrightarrow[rm]{*} \\alpha x . Not all prefixes of right-sentential forms can appear on the stack, however, since the parser must not shift past the handle . For example, suppose E \\xrightarrow[rm]{ \\ast } F \\ast id \\xrightarrow[rm]{} (E) \\ast id E \\xrightarrow[rm]{ \\ast } F \\ast id \\xrightarrow[rm]{} (E) \\ast id Then, at various times during the parse, the stack will hold ( ( ; (E (E , and (E ) (E ) , but it must not hold (E ) \\ast (E ) \\ast , since (E ) (E ) is a handle, which the parser must reduce to F F before shifting \\ast \\ast . The prefixes of right sentential forms that can appear on the stack of a shift-reduce parser are called viable prefixes . They are defined as follows: a viable prefix is a prefix of a right-sentential form that does not continue past the right end of the rightmost handle of that sentential form . By this definition, it is always possible to add terminal symbols to the end of a viable prefix to obtain a right-sentential form. NOTE: In simple terms, a prefix cannot contain a handle. Once it does, it should be reduced. SLR parsing is based on the fact that LR(0) automata recognize viable prefixes . We say item A \\to \\beta _1 \\beta _2 A \\to \\beta _1 \\beta _2 is valid for a viable prefix \\alpha \\beta _1 \\alpha \\beta _1 if there is a derivation S \\xrightarrow [rm] {*} \\alpha Aw \\xrightarrow [rm]{*} \\alpha \\beta _1 \\beta _2 w S \\xrightarrow [rm] {*} \\alpha Aw \\xrightarrow [rm]{*} \\alpha \\beta _1 \\beta _2 w . In general, an item will be valid for many viable prefixes. NOTE: viable prefix\u548c","title":"4.6.5 Viable Prefixes"},{"location":"4-Syntax-Analysis/4.7-More-Powerful-LR-Parsers/","text":"4.7 More Powerful LR Parsers In this section, we shall extend the previous LR parsing techniques to use one symbol of lookahead on the input. There are two different methods: The \"canonical-LR\" or just \"LR\" method The \"lookahead-LR\" or \"LALR\" method After introducing both these methods, we conclude with a discussion of how to compact LR parsing tables for environments with limited memory. 4.7.4 Constructing LALR Parsing Tables We now introduce our last parser construction method, the LALR (lookahead-LR) technique. This method is often used in practice, because the tables obtained by it are considerably smaller than the canonical LR tables, yet most common syntactic constructs of programming languages can be expressed conveniently by an LALR grammar. The same is almost true for SLR grammars, but there are a few constructs that cannot be conveniently handled by SLR techniques (see Example 4.48, for example). For a comparison of parser size, the SLR and LALR tables for a grammar always have the same number of states, and this number is typically several hundred states for a language like C. The canonical LR table would typically have several thousand states for the same-size language. Thus, it is much easier and more economical to construct SLR and LALR tables than the canonical LR tables.","title":"Introduction"},{"location":"4-Syntax-Analysis/4.7-More-Powerful-LR-Parsers/#47#more#powerful#lr#parsers","text":"In this section, we shall extend the previous LR parsing techniques to use one symbol of lookahead on the input. There are two different methods: The \"canonical-LR\" or just \"LR\" method The \"lookahead-LR\" or \"LALR\" method After introducing both these methods, we conclude with a discussion of how to compact LR parsing tables for environments with limited memory.","title":"4.7 More Powerful LR Parsers"},{"location":"4-Syntax-Analysis/4.7-More-Powerful-LR-Parsers/#474#constructing#lalr#parsing#tables","text":"We now introduce our last parser construction method, the LALR (lookahead-LR) technique. This method is often used in practice, because the tables obtained by it are considerably smaller than the canonical LR tables, yet most common syntactic constructs of programming languages can be expressed conveniently by an LALR grammar. The same is almost true for SLR grammars, but there are a few constructs that cannot be conveniently handled by SLR techniques (see Example 4.48, for example). For a comparison of parser size, the SLR and LALR tables for a grammar always have the same number of states, and this number is typically several hundred states for a language like C. The canonical LR table would typically have several thousand states for the same-size language. Thus, it is much easier and more economical to construct SLR and LALR tables than the canonical LR tables.","title":"4.7.4 Constructing LALR Parsing Tables"},{"location":"5-Syntax-Directed-Translation/","text":"Chapter 5 Syntax-Directed Translation This chapter develops the theme of Section 2.3: the translation of languages guided by context-free grammars . The translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation . The techniques are also useful for implementing little languages for specialized tasks; this chapter includes an example from typesetting. We associate information with a language construct by attaching attributes to the grammar symbol(s) representing the construct, as discussed in Section 2.3.2. A syntax-directed definition specifies the values of attributes by associating semantic rules with the grammar productions. For example, an infix-to-postfix translator might have a production and rule From Section 2.3.5, a syntax-directed translation scheme embeds program fragments called semantic actions within production bodies, as in By convention, semantic actions are enclosed within curly braces. Between the two notations, syntax-directed definitions can be more readable, and hence more useful for specifications. However, translation schemes can be more efficient, and hence more useful for implementations. syntax-directed definition SDD semantic rule syntax-directed translation scheme SDT semantic action The most general approach to syntax-directed translation is to construct a parse tree or a syntax tree , and then to compute the values of attributes at the nodes of the tree by visiting the nodes of the tree. In many cases, translation can be done during parsing, without building an explicit tree. We shall therefore study a class of syntax-directed translations called \"L-attributed translations\" (L for left-to-right), which encompass virtually all translations that can be performed during parsing. We also study a smaller class, called \"S-attributed translations\" (S for synthesized), which can be performed easily in connection with a bottom-up parse. NOTE: \u672c\u4e66\u5bf9Syntax-directed translation\u7684\u529f\u80fd\u6ca1\u6709\u8fdb\u884c\u76f4\u63a5\u7684\u63cf\u8ff0\uff0c\u662f\u975e\u5e38\u4efb\u610f\u8ba9\u4eba\u7cca\u6d82\u7684\uff0cwikipedia\u7684 Syntax-directed translation \u975e\u5e38\u76f4\u63a5\u7b80\u660e\u7684\u63cf\u8ff0\u4e86Syntax-directed translation\u7684\u529f\u80fd: Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar . Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax . \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5df2\u7ecf\u5c06SDT\u548c semantics \u5173\u8054\u5230\u4e00\u8d77\u4e86\uff0c\u5176\u5b9eSDT\u662f Semantic analysis \u7684\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u6b63\u5982wikipedia\u7684 compiler \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a A compiler is likely to perform many or all of the following operations: preprocessing , lexical analysis , parsing , semantic analysis ( syntax-directed translation ), conversion of input programs to an intermediate representation , code optimization and code generation .","title":5},{"location":"5-Syntax-Directed-Translation/#chapter#5#syntax-directed#translation","text":"This chapter develops the theme of Section 2.3: the translation of languages guided by context-free grammars . The translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation . The techniques are also useful for implementing little languages for specialized tasks; this chapter includes an example from typesetting. We associate information with a language construct by attaching attributes to the grammar symbol(s) representing the construct, as discussed in Section 2.3.2. A syntax-directed definition specifies the values of attributes by associating semantic rules with the grammar productions. For example, an infix-to-postfix translator might have a production and rule From Section 2.3.5, a syntax-directed translation scheme embeds program fragments called semantic actions within production bodies, as in By convention, semantic actions are enclosed within curly braces. Between the two notations, syntax-directed definitions can be more readable, and hence more useful for specifications. However, translation schemes can be more efficient, and hence more useful for implementations. syntax-directed definition SDD semantic rule syntax-directed translation scheme SDT semantic action The most general approach to syntax-directed translation is to construct a parse tree or a syntax tree , and then to compute the values of attributes at the nodes of the tree by visiting the nodes of the tree. In many cases, translation can be done during parsing, without building an explicit tree. We shall therefore study a class of syntax-directed translations called \"L-attributed translations\" (L for left-to-right), which encompass virtually all translations that can be performed during parsing. We also study a smaller class, called \"S-attributed translations\" (S for synthesized), which can be performed easily in connection with a bottom-up parse. NOTE: \u672c\u4e66\u5bf9Syntax-directed translation\u7684\u529f\u80fd\u6ca1\u6709\u8fdb\u884c\u76f4\u63a5\u7684\u63cf\u8ff0\uff0c\u662f\u975e\u5e38\u4efb\u610f\u8ba9\u4eba\u7cca\u6d82\u7684\uff0cwikipedia\u7684 Syntax-directed translation \u975e\u5e38\u76f4\u63a5\u7b80\u660e\u7684\u63cf\u8ff0\u4e86Syntax-directed translation\u7684\u529f\u80fd: Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar . Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax . \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5df2\u7ecf\u5c06SDT\u548c semantics \u5173\u8054\u5230\u4e00\u8d77\u4e86\uff0c\u5176\u5b9eSDT\u662f Semantic analysis \u7684\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u6b63\u5982wikipedia\u7684 compiler \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a A compiler is likely to perform many or all of the following operations: preprocessing , lexical analysis , parsing , semantic analysis ( syntax-directed translation ), conversion of input programs to an intermediate representation , code optimization and code generation .","title":"Chapter 5 Syntax-Directed Translation"},{"location":"5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/","text":"5.1 Syntax-Directed Definitions A syntax-directed definition (SDD) is a context-free grammar together with attributes and rules . Attributes are associated with grammar symbols and rules are associated with productions. If X is a symbol and a is one of its attributes, then we write X.a to denote the value of a at a particular parse-tree node labeled X . If we implement the nodes of the parse tree by records or objects, then the attributes of X can b e implemented by data fields in the records that represent the nodes for X . Attributes may be of any kind: numbers, types, table references, or strings, for instance. The strings may even be long sequences of code, say code in the intermediate language used by a compiler. NOTE: SDD consist of two element: attribute and rule, So SDD is used in the future, you need to be clear it means attribute and rule. 5.1.1 Inherited and Synthesized Attributes We shall deal with two kinds of attributes for nonterminals : A synthesized attribute for a nonterminal A at a parse-tree node N is defined by a semantic rule associated with the production at N . Note that the production must have A as its head . A synthesized attribute at node N is defined only in terms of attribute values at the children of N and at N itself. An inherited attribute for a nonterminal B at a parse-tree node N is defined by a semantic rule associated with the production at the parent of N . Note that the production must have B as a symbol in its body. An inherited attribute at node N is defined only in terms of attribute values at N 's parent, N itself, and N 's siblings. NOTE: The above classification method is based on how to calculate the attribute value. It is obvious that the direction of computation of synthesized attribute is contrast to inherited attribute 's. More precisely, synthesized attribute is suitable to bottom-up parsing while inherited attribute is suitable to top-down parsing . Example 5.2 show how synthesized attribute is calculated while example 5.3 show how inherited attribute is calculated. The computation of attribute will be discussed in later chapter. NOTE: A SDD can has inherited attribute and inherited attribute at the same time, which is introduced in chapter 5.1.2. While we do not allow an inherited attribute at node N to be defined in terms of attribute values at the children of node N , we do allow a synthesized attribute at node N to be defined in terms of inherited attribute values at node N itself. NOTE: Inherited attribute , the name has implied that the attribute is inherited from parent, so it is natural that inherited attribute at node N can not be defined in terms of attribute values at the children of node N or it will be self-contradictory. Terminals can have synthesized attributes , but not inherited attributes . Attributes for terminals have lexical values that are supplied by the lexical analyzer; there are no semantic rules in the SDD itself for computing the value of an attribute for a terminal. NOTE: How about a start symbol? It is obvious that a start symbol can not has inherited attribute because it is the ancestor and it has no parent. Example 5.1 : skipped An SDD that involves only synthesized attributes is called S-attributed ; the SDD in Fig. 5.1 has this property. In an S-attributed SDD , each rule computes an attribute for the nonterminal at the head of a production from attributes taken from the body of the production. For simplicity, the examples in this section have semantic rules without side effects. In practice, it is convenient to allow SDD's to have limited side effects, such as printing the result computed by a desk calculator or interacting with a symbol table. Once the order of evaluation of attributes is discussed in Section 5.2, we shall allow semantic rules to compute arbitrary functions, possibly involving side effects. An S-attributed SDD can be implemented naturally in conjunction with an LR parser . An SDD without side effects is sometimes called an attribute grammar . The rules in an attribute grammar define the value of an attribute purely in terms of the values of other attributes and constants. 5.1.2 Evaluating an SDD at the Nodes of a Parse Tree To visualize the translation specified by an SDD, it helps to work with parse trees, even though a translator need not actually build a parse tree. Imagine therefore that the rules of an SDD are applied by first constructing a parse tree and then using the rules to evaluate all of the attributes at each of the nodes of the parse tree . A parse tree , showing the value(s) of its attribute(s) is called an annotated parse tree . How do we construct an annotated parse tree ? In what order do we evaluate attributes? Before we can evaluate an attribute at a node of a parse tree, we must evaluate all the attributes up on which its value depends. For example, if all attributes are synthesized , as in Example 5.1, then we must evaluate the val attributes at all of the children of a node before we can evaluate the val attribute at the node itself. With synthesized attributes , we can evaluate attributes in any bottom-up order, such as that of a postorder traversal of the parse tree; the evaluation of S-attributed definitions is discussed in Section 5.2.3. For SDD's with both inherited and synthesized attributes, there is no guarantee that there is even one order in which to evaluate attributes at nodes. For instance, consider nonterminals A and B , with synthesized and inherited attributes A.s and B.i , respectively, along with the production and rules PRODUCTION SEMANTIC RULES A \\to B A \\to B A.s = B.i; B.i = A.s + 1 These rules are circular; it is impossible to evaluate either A.s at a node N or B.i at the child of N without first evaluating the other. The circular dependency of A.s and B.i at some pair of nodes in a parse tree is suggested by Fig. 5.2. It is computationally difficult to determine whether or not there exist any circularities in any of the parse trees that a given SDD could have to translate. Fortunately, there are useful sub classes of SDD's that are sufficient to guarantee that an order of evaluation exists, as we shall see in Section 5.2. NOTE: Below is the explanation if why determining whether or not there exist any circularities in any of the parse trees of a given SDD is computationally difficult: Without going into details, while the problem is decidable, it cannot be solved by a polynomial-time algorithm, even if P = N P , since it has exponential time complexity. In fact, this is an algorithm problem to find cycle in graph . Example 5.2 : skipped Example 5.3 : The SDD in Fig. 5.4 computes terms like 3 * 5 and 3 * 5 * 7 . The top-down parse of input 3 * 5 begins with the production T \\to F T' T \\to F T' . Here, F generates the digit 3, but the operator * is generated by T' . Thus, the left operand 3 appears in a different subtree of the parse tree from * . An inherited attribute will therefore be used to pass the operand to the operator. The grammar in this example is an excerpt from a non-left-recursive version of the familiar expression grammar; we used such a grammar as a running example to illustrate top-down parsing in Section 4.4. PRODUCTION SEMANTIC RULES T \\to F T' T \\to F T' T'.inh = F.val \\\\ T.val = T'.syn T'.inh = F.val \\\\ T.val = T'.syn T' \\to * F T_1' T' \\to * F T_1' T_1'.inh = T'.inh * F.val \\\\ T'.syn= T_1'.syn T_1'.inh = T'.inh * F.val \\\\ T'.syn= T_1'.syn T' \\to \\epsilon T' \\to \\epsilon T'.syn = T'.inh T'.syn = T'.inh F \\to digit F \\to digit F.val = digit.lexval F.val = digit.lexval Figure 5.4: An SDD based on a grammar suitable for top-down parsing Each of the nonterminals T and F has a synthesized attribute val ; the terminal digit has a synthesized attribute lexval . The nonterminal T' has two attributes: an inherited attribute inh and a synthesized attribute syn . The semantic rules are based on the idea that the left operand of the operator * is inherited. More precisely, the head T' of the production T' \\to * F T_1' T' \\to * F T_1' inherits the left operand of * in the production body. Given a term x * y * z , the root of the subtree for * y * z * y * z inherits x . Then, the root of the subtree for * z inherits the value of * x * y , and so on, if there are more factors in the term. Once all the factors have been accumulated, the result is passed back up the tree using synthesized attributes . To see how the semantic rules are used, consider the annotated parse tree for 3 * 5 in Fig. 5.5. The leftmost leaf in the parse tree, labeled digit , has attribute value lexval = 3 , where the 3 is supplied by the lexical analyzer . Its parent is for production 4, F \\to digit F \\to digit . The only semantic rule associated with this production defines F.val = digit.lexval F.val = digit.lexval , which equals 3. At the second child of the root, the inherited attribute T'.inh is defined by the semantic rule T'.inh = F.val associated with production 1. Thus, the left operand, 3, for the * operator is passed from left to right across the children of the root. The production at the node for T' is T' \\to * F T_1' T' \\to * F T_1' . (We retain the subscript 1 in the annotated parse tree to distinguish between the two nodes for T' .) The inherited attribute $T_1'.inh $ is defined by the semantic rule T_1'.inh = T'.inh * F.val T_1'.inh = T'.inh * F.val associated with production 2. With T'.inh = 3 T'.inh = 3 and F.val = 5 F.val = 5 , we get T_1'.inh = 15 T_1'.inh = 15 . At the lower node for T_1' T_1' , the production is T' \\to \\epsilon T' \\to \\epsilon . The semantic rule T'.syn = T'.inh T'.syn = T'.inh defines T_1'.syn = 15 T_1'.syn = 15 . The syn attributes at the nodes for T' T' pass the value 15 up the tree to the node for T , where T.val = 15 .","title":"5.1-Syntax-Directed-Definitions"},{"location":"5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/#51#syntax-directed#definitions","text":"A syntax-directed definition (SDD) is a context-free grammar together with attributes and rules . Attributes are associated with grammar symbols and rules are associated with productions. If X is a symbol and a is one of its attributes, then we write X.a to denote the value of a at a particular parse-tree node labeled X . If we implement the nodes of the parse tree by records or objects, then the attributes of X can b e implemented by data fields in the records that represent the nodes for X . Attributes may be of any kind: numbers, types, table references, or strings, for instance. The strings may even be long sequences of code, say code in the intermediate language used by a compiler. NOTE: SDD consist of two element: attribute and rule, So SDD is used in the future, you need to be clear it means attribute and rule.","title":"5.1 Syntax-Directed Definitions"},{"location":"5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/#511#inherited#and#synthesized#attributes","text":"We shall deal with two kinds of attributes for nonterminals : A synthesized attribute for a nonterminal A at a parse-tree node N is defined by a semantic rule associated with the production at N . Note that the production must have A as its head . A synthesized attribute at node N is defined only in terms of attribute values at the children of N and at N itself. An inherited attribute for a nonterminal B at a parse-tree node N is defined by a semantic rule associated with the production at the parent of N . Note that the production must have B as a symbol in its body. An inherited attribute at node N is defined only in terms of attribute values at N 's parent, N itself, and N 's siblings. NOTE: The above classification method is based on how to calculate the attribute value. It is obvious that the direction of computation of synthesized attribute is contrast to inherited attribute 's. More precisely, synthesized attribute is suitable to bottom-up parsing while inherited attribute is suitable to top-down parsing . Example 5.2 show how synthesized attribute is calculated while example 5.3 show how inherited attribute is calculated. The computation of attribute will be discussed in later chapter. NOTE: A SDD can has inherited attribute and inherited attribute at the same time, which is introduced in chapter 5.1.2. While we do not allow an inherited attribute at node N to be defined in terms of attribute values at the children of node N , we do allow a synthesized attribute at node N to be defined in terms of inherited attribute values at node N itself. NOTE: Inherited attribute , the name has implied that the attribute is inherited from parent, so it is natural that inherited attribute at node N can not be defined in terms of attribute values at the children of node N or it will be self-contradictory. Terminals can have synthesized attributes , but not inherited attributes . Attributes for terminals have lexical values that are supplied by the lexical analyzer; there are no semantic rules in the SDD itself for computing the value of an attribute for a terminal. NOTE: How about a start symbol? It is obvious that a start symbol can not has inherited attribute because it is the ancestor and it has no parent. Example 5.1 : skipped An SDD that involves only synthesized attributes is called S-attributed ; the SDD in Fig. 5.1 has this property. In an S-attributed SDD , each rule computes an attribute for the nonterminal at the head of a production from attributes taken from the body of the production. For simplicity, the examples in this section have semantic rules without side effects. In practice, it is convenient to allow SDD's to have limited side effects, such as printing the result computed by a desk calculator or interacting with a symbol table. Once the order of evaluation of attributes is discussed in Section 5.2, we shall allow semantic rules to compute arbitrary functions, possibly involving side effects. An S-attributed SDD can be implemented naturally in conjunction with an LR parser . An SDD without side effects is sometimes called an attribute grammar . The rules in an attribute grammar define the value of an attribute purely in terms of the values of other attributes and constants.","title":"5.1.1 Inherited and Synthesized Attributes"},{"location":"5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/#512#evaluating#an#sdd#at#the#nodes#of#a#parse#tree","text":"To visualize the translation specified by an SDD, it helps to work with parse trees, even though a translator need not actually build a parse tree. Imagine therefore that the rules of an SDD are applied by first constructing a parse tree and then using the rules to evaluate all of the attributes at each of the nodes of the parse tree . A parse tree , showing the value(s) of its attribute(s) is called an annotated parse tree . How do we construct an annotated parse tree ? In what order do we evaluate attributes? Before we can evaluate an attribute at a node of a parse tree, we must evaluate all the attributes up on which its value depends. For example, if all attributes are synthesized , as in Example 5.1, then we must evaluate the val attributes at all of the children of a node before we can evaluate the val attribute at the node itself. With synthesized attributes , we can evaluate attributes in any bottom-up order, such as that of a postorder traversal of the parse tree; the evaluation of S-attributed definitions is discussed in Section 5.2.3. For SDD's with both inherited and synthesized attributes, there is no guarantee that there is even one order in which to evaluate attributes at nodes. For instance, consider nonterminals A and B , with synthesized and inherited attributes A.s and B.i , respectively, along with the production and rules PRODUCTION SEMANTIC RULES A \\to B A \\to B A.s = B.i; B.i = A.s + 1 These rules are circular; it is impossible to evaluate either A.s at a node N or B.i at the child of N without first evaluating the other. The circular dependency of A.s and B.i at some pair of nodes in a parse tree is suggested by Fig. 5.2. It is computationally difficult to determine whether or not there exist any circularities in any of the parse trees that a given SDD could have to translate. Fortunately, there are useful sub classes of SDD's that are sufficient to guarantee that an order of evaluation exists, as we shall see in Section 5.2. NOTE: Below is the explanation if why determining whether or not there exist any circularities in any of the parse trees of a given SDD is computationally difficult: Without going into details, while the problem is decidable, it cannot be solved by a polynomial-time algorithm, even if P = N P , since it has exponential time complexity. In fact, this is an algorithm problem to find cycle in graph . Example 5.2 : skipped Example 5.3 : The SDD in Fig. 5.4 computes terms like 3 * 5 and 3 * 5 * 7 . The top-down parse of input 3 * 5 begins with the production T \\to F T' T \\to F T' . Here, F generates the digit 3, but the operator * is generated by T' . Thus, the left operand 3 appears in a different subtree of the parse tree from * . An inherited attribute will therefore be used to pass the operand to the operator. The grammar in this example is an excerpt from a non-left-recursive version of the familiar expression grammar; we used such a grammar as a running example to illustrate top-down parsing in Section 4.4. PRODUCTION SEMANTIC RULES T \\to F T' T \\to F T' T'.inh = F.val \\\\ T.val = T'.syn T'.inh = F.val \\\\ T.val = T'.syn T' \\to * F T_1' T' \\to * F T_1' T_1'.inh = T'.inh * F.val \\\\ T'.syn= T_1'.syn T_1'.inh = T'.inh * F.val \\\\ T'.syn= T_1'.syn T' \\to \\epsilon T' \\to \\epsilon T'.syn = T'.inh T'.syn = T'.inh F \\to digit F \\to digit F.val = digit.lexval F.val = digit.lexval Figure 5.4: An SDD based on a grammar suitable for top-down parsing Each of the nonterminals T and F has a synthesized attribute val ; the terminal digit has a synthesized attribute lexval . The nonterminal T' has two attributes: an inherited attribute inh and a synthesized attribute syn . The semantic rules are based on the idea that the left operand of the operator * is inherited. More precisely, the head T' of the production T' \\to * F T_1' T' \\to * F T_1' inherits the left operand of * in the production body. Given a term x * y * z , the root of the subtree for * y * z * y * z inherits x . Then, the root of the subtree for * z inherits the value of * x * y , and so on, if there are more factors in the term. Once all the factors have been accumulated, the result is passed back up the tree using synthesized attributes . To see how the semantic rules are used, consider the annotated parse tree for 3 * 5 in Fig. 5.5. The leftmost leaf in the parse tree, labeled digit , has attribute value lexval = 3 , where the 3 is supplied by the lexical analyzer . Its parent is for production 4, F \\to digit F \\to digit . The only semantic rule associated with this production defines F.val = digit.lexval F.val = digit.lexval , which equals 3. At the second child of the root, the inherited attribute T'.inh is defined by the semantic rule T'.inh = F.val associated with production 1. Thus, the left operand, 3, for the * operator is passed from left to right across the children of the root. The production at the node for T' is T' \\to * F T_1' T' \\to * F T_1' . (We retain the subscript 1 in the annotated parse tree to distinguish between the two nodes for T' .) The inherited attribute $T_1'.inh $ is defined by the semantic rule T_1'.inh = T'.inh * F.val T_1'.inh = T'.inh * F.val associated with production 2. With T'.inh = 3 T'.inh = 3 and F.val = 5 F.val = 5 , we get T_1'.inh = 15 T_1'.inh = 15 . At the lower node for T_1' T_1' , the production is T' \\to \\epsilon T' \\to \\epsilon . The semantic rule T'.syn = T'.inh T'.syn = T'.inh defines T_1'.syn = 15 T_1'.syn = 15 . The syn attributes at the nodes for T' T' pass the value 15 up the tree to the node for T , where T.val = 15 .","title":"5.1.2 Evaluating an SDD at the Nodes of a Parse Tree"},{"location":"5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD%27s/","text":"5.2 Evaluation Orders for SDD's Dependency graphs\" are a useful tool for determining an evaluation order for the attribute instances in a given parse tree . While an annotated parse tree shows the values of attributes, a dependency graph helps us determine how those values can be computed. In this section, in addition to dependency graphs, we define two important classes of SDD's: the \"S-attributed\" and the more general \"L-attributed\" SDD's. The translations specified by these two classes fit well with the parsing methods we have studied, and most translations encountered in practice can be written to conform to the requirements of at least one of these classes. 5.2.1 Dependency Graphs A dependency graph depicts the flow of information among the attribute instances in a particular parse tree ; an edge from one attribute instance to another means that the value of the first is needed to compute the second. Edges express constraints implied by the semantic rules . In more detail: For each parse-tree node, say a node labeled by grammar symbol X , the dependency graph has a node for each attribute associated with X . Suppose that a semantic rule associated with a production p defines the value of synthesized attribute A.b in terms of the value of X.c (the rule may define A.b in terms of other attributes in addition to X.c ). Then, the dependency graph has an edge from X.c to A.b . More precisely, at every node N labeled A where production p is applied, create an edge to attribute b at N , from the attribute c at the child of N corresponding to this instance of the symbol X in the body of the production. Since a node N can have several children labeled X , we again assume that subscripts distinguish among uses of the same symbol at different places in the production. Suppose that a semantic rule associated with a production p defines the value of inherited attribute B.c in terms of the value of X.a . Then, the dependency graph has an edge from X.a to B.c . For each node N labeled B that corresponds to an occurrence of this B in the body of production p , create an edge to attribute c at N from the attribute a at the node M that corresponds to this occurrence of X . Note that M could be either the parent or a sibling of N . NOTE: What is described above is an algorithm for constructing a dependency graph. 5.2.2 Ordering the Evaluation of Attributes The dependency graph characterizes the possible orders in which we can evaluate the attributes at the various nodes of a parse tree . If the dependency graph has an edge from node M to node N , then the attribute corresponding to M must be evaluated before the attribute of N . Thus, the only allowable orders of evaluation are those sequences of nodes N_1, N_2,\\dots , N_k N_1, N_2,\\dots , N_k such that if there is an edge of the dependency graph from N_i N_i to N_j N_j , then i < j . Such an ordering embeds a directed graph into a linear order, and is called a topological sort of the graph. If there is any cycle in the graph, then there are no topological sorts ; that is, there is no way to evaluate the SDD on this parse tree . If there are no cycles , however, then there is always at least one topological sort. To see why, since there are no cycles, we can surely find a node with no edge entering. For if there were no such node, we could proceed from predecessor to predecessor until we came back to some node we had already seen, yielding a cycle. Make this node the first in the topological order, remove it from the dependency graph, and repeat the process on the remaining nodes. 5.2.3 S-Attributed Definitions As mentioned earlier, given an SDD, it is very hard to tell whether there exist any parse trees whose dependency graphs have cycles. In practice, translations can be implemented using classes of SDD's that guarantee an evaluation order, since they do not permit dependency graphs with cycles. Moreover, the two classes introduced in this section can be implemented efficiently in connection with top-down or bottom-up parsing. The first class is defined as follows: An SDD is S-attributed if every attribute is synthesized. When an SDD is S-attributed , we can evaluate its attributes in any bottom-up order of the nodes of the parse tree . It is often especially simple to evaluate the attributes by performing a postorder traversal of the parse tree and evaluating the attributes at a node N when the traversal leaves N for the last time. That is, we apply the function postorder , defined below, to the root of the parse tree (see also the box \"Preorder and Postorder Traversals\" in Section 2.3.4): S-attributed definitions can be implemented during bottom-up parsing, since a bottom-up parse corresponds to a postorder traversal . Specifically, postorder corresponds exactly to the order in which an LR parser reduces a production body to its head. This fact will b e used in Section 5.4.2 to evaluate synthesized attributes and store them on the stack during LR parsing , without creating the tree nodes explicitly. 5.2.4 L-Attributed Definitions The second class of SDD's is called L-attributed definitions . The idea behind this class is that, between the attributes associated with a production body, dependency-graph edges can go from left to right , but not from right to left (hence \"L-attributed\"). More precisely, each attribute must be either Synthesized, or Inherited, but with the rules limited as follows. Suppose that there is a production A \\to X_1, X_2, \\dots, X_n A \\to X_1, X_2, \\dots, X_n , and that there is an inherited attribute X_i.a X_i.a computed by a rule associated with this production. Then the rule may use only: Inherited attributes associated with the head A . Either inherited or synthesized attributes associated with the occurrences of symbols X_1, X_2, \\dots, X_{i-1} X_1, X_2, \\dots, X_{i-1} located to the left of X_i X_i . Inherited or synthesized attributes associated with this occurrence of X_i X_i itself, but only in such a way that there are no cycles in a dependency graph formed by the attributes of this X_i X_i . Example 5.8 : The SDD in Fig. 5.4 is L-attributed . To see why, consider the semantic rules for inherited attributes, which are repeated here for convenience: The first of these rules defines the inherited attribute T'.inh using only F.val , and F appears to the left of T' in the production body, as required. The second rule defines T_1'.inh T_1'.inh using the inherited attribute T'inh associated with the head , and F.val , where F appears to the left of T_1' T_1' in the production body. In each of these cases, the rules use information \"from above or from the left,\" as required by the class. The remaining attributes are synthesized. Hence, the SDD is L-attributed. Example 5.9 : Any SDD containing the following production and rules cannot be L-attributed: The second rule defines an inherited attribute B.i , so the entire SDD cannot be S-attributed. Further, although the rule is legal, the SDD cannot be L-attributed, because the attribute C.c is used to help define B.i , and C is to the right of B in the production body. 5.2.5 Semantic Rules with Controlled Side Effects In practice, translations involve side effects: a desk calculator might print a result; a code generator might enter the type of an identifier into a symbol table. With SDD's, we strike a balance between attribute grammars and translation schemes . Attribute grammars have no side effects and allow any evaluation order consistent with the dependency graph. Translation schemes impose left-to-right evaluation and allow semantic actions to contain any program fragment; translation schemes are discussed in Section 5.4. We shall control side effcts in SDD's in one of the following ways: Permit incidental side effects that do not constrain attribute evaluation. In other words, permit side effects when attribute evaluation based on any topological sort of the dependency graph produces a \"correct\" translation, where \"correct\" depends on the application. Constrain the allowable evaluation orders, so that the same translation is produced for any allowable order. The constraints can be thought of as implicit edges added to the dependency graph.","title":"5.2-Evaluation-Orders-for-SDD's"},{"location":"5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD%27s/#52#evaluation#orders#for#sdds","text":"Dependency graphs\" are a useful tool for determining an evaluation order for the attribute instances in a given parse tree . While an annotated parse tree shows the values of attributes, a dependency graph helps us determine how those values can be computed. In this section, in addition to dependency graphs, we define two important classes of SDD's: the \"S-attributed\" and the more general \"L-attributed\" SDD's. The translations specified by these two classes fit well with the parsing methods we have studied, and most translations encountered in practice can be written to conform to the requirements of at least one of these classes.","title":"5.2 Evaluation Orders for SDD's"},{"location":"5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD%27s/#521#dependency#graphs","text":"A dependency graph depicts the flow of information among the attribute instances in a particular parse tree ; an edge from one attribute instance to another means that the value of the first is needed to compute the second. Edges express constraints implied by the semantic rules . In more detail: For each parse-tree node, say a node labeled by grammar symbol X , the dependency graph has a node for each attribute associated with X . Suppose that a semantic rule associated with a production p defines the value of synthesized attribute A.b in terms of the value of X.c (the rule may define A.b in terms of other attributes in addition to X.c ). Then, the dependency graph has an edge from X.c to A.b . More precisely, at every node N labeled A where production p is applied, create an edge to attribute b at N , from the attribute c at the child of N corresponding to this instance of the symbol X in the body of the production. Since a node N can have several children labeled X , we again assume that subscripts distinguish among uses of the same symbol at different places in the production. Suppose that a semantic rule associated with a production p defines the value of inherited attribute B.c in terms of the value of X.a . Then, the dependency graph has an edge from X.a to B.c . For each node N labeled B that corresponds to an occurrence of this B in the body of production p , create an edge to attribute c at N from the attribute a at the node M that corresponds to this occurrence of X . Note that M could be either the parent or a sibling of N . NOTE: What is described above is an algorithm for constructing a dependency graph.","title":"5.2.1 Dependency Graphs"},{"location":"5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD%27s/#522#ordering#the#evaluation#of#attributes","text":"The dependency graph characterizes the possible orders in which we can evaluate the attributes at the various nodes of a parse tree . If the dependency graph has an edge from node M to node N , then the attribute corresponding to M must be evaluated before the attribute of N . Thus, the only allowable orders of evaluation are those sequences of nodes N_1, N_2,\\dots , N_k N_1, N_2,\\dots , N_k such that if there is an edge of the dependency graph from N_i N_i to N_j N_j , then i < j . Such an ordering embeds a directed graph into a linear order, and is called a topological sort of the graph. If there is any cycle in the graph, then there are no topological sorts ; that is, there is no way to evaluate the SDD on this parse tree . If there are no cycles , however, then there is always at least one topological sort. To see why, since there are no cycles, we can surely find a node with no edge entering. For if there were no such node, we could proceed from predecessor to predecessor until we came back to some node we had already seen, yielding a cycle. Make this node the first in the topological order, remove it from the dependency graph, and repeat the process on the remaining nodes.","title":"5.2.2 Ordering the Evaluation of Attributes"},{"location":"5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD%27s/#523#s-attributed#definitions","text":"As mentioned earlier, given an SDD, it is very hard to tell whether there exist any parse trees whose dependency graphs have cycles. In practice, translations can be implemented using classes of SDD's that guarantee an evaluation order, since they do not permit dependency graphs with cycles. Moreover, the two classes introduced in this section can be implemented efficiently in connection with top-down or bottom-up parsing. The first class is defined as follows: An SDD is S-attributed if every attribute is synthesized. When an SDD is S-attributed , we can evaluate its attributes in any bottom-up order of the nodes of the parse tree . It is often especially simple to evaluate the attributes by performing a postorder traversal of the parse tree and evaluating the attributes at a node N when the traversal leaves N for the last time. That is, we apply the function postorder , defined below, to the root of the parse tree (see also the box \"Preorder and Postorder Traversals\" in Section 2.3.4): S-attributed definitions can be implemented during bottom-up parsing, since a bottom-up parse corresponds to a postorder traversal . Specifically, postorder corresponds exactly to the order in which an LR parser reduces a production body to its head. This fact will b e used in Section 5.4.2 to evaluate synthesized attributes and store them on the stack during LR parsing , without creating the tree nodes explicitly.","title":"5.2.3 S-Attributed Definitions"},{"location":"5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD%27s/#524#l-attributed#definitions","text":"The second class of SDD's is called L-attributed definitions . The idea behind this class is that, between the attributes associated with a production body, dependency-graph edges can go from left to right , but not from right to left (hence \"L-attributed\"). More precisely, each attribute must be either Synthesized, or Inherited, but with the rules limited as follows. Suppose that there is a production A \\to X_1, X_2, \\dots, X_n A \\to X_1, X_2, \\dots, X_n , and that there is an inherited attribute X_i.a X_i.a computed by a rule associated with this production. Then the rule may use only: Inherited attributes associated with the head A . Either inherited or synthesized attributes associated with the occurrences of symbols X_1, X_2, \\dots, X_{i-1} X_1, X_2, \\dots, X_{i-1} located to the left of X_i X_i . Inherited or synthesized attributes associated with this occurrence of X_i X_i itself, but only in such a way that there are no cycles in a dependency graph formed by the attributes of this X_i X_i . Example 5.8 : The SDD in Fig. 5.4 is L-attributed . To see why, consider the semantic rules for inherited attributes, which are repeated here for convenience: The first of these rules defines the inherited attribute T'.inh using only F.val , and F appears to the left of T' in the production body, as required. The second rule defines T_1'.inh T_1'.inh using the inherited attribute T'inh associated with the head , and F.val , where F appears to the left of T_1' T_1' in the production body. In each of these cases, the rules use information \"from above or from the left,\" as required by the class. The remaining attributes are synthesized. Hence, the SDD is L-attributed. Example 5.9 : Any SDD containing the following production and rules cannot be L-attributed: The second rule defines an inherited attribute B.i , so the entire SDD cannot be S-attributed. Further, although the rule is legal, the SDD cannot be L-attributed, because the attribute C.c is used to help define B.i , and C is to the right of B in the production body.","title":"5.2.4 L-Attributed Definitions"},{"location":"5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD%27s/#525#semantic#rules#with#controlled#side#effects","text":"In practice, translations involve side effects: a desk calculator might print a result; a code generator might enter the type of an identifier into a symbol table. With SDD's, we strike a balance between attribute grammars and translation schemes . Attribute grammars have no side effects and allow any evaluation order consistent with the dependency graph. Translation schemes impose left-to-right evaluation and allow semantic actions to contain any program fragment; translation schemes are discussed in Section 5.4. We shall control side effcts in SDD's in one of the following ways: Permit incidental side effects that do not constrain attribute evaluation. In other words, permit side effects when attribute evaluation based on any topological sort of the dependency graph produces a \"correct\" translation, where \"correct\" depends on the application. Constrain the allowable evaluation orders, so that the same translation is produced for any allowable order. The constraints can be thought of as implicit edges added to the dependency graph.","title":"5.2.5 Semantic Rules with Controlled Side Effects"},{"location":"5-Syntax-Directed-Translation/5.3-Applications-of-Syntax-Directed-Translation/","text":"5.3 Applications of Syntax-Directed Translation The syntax-directed translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation. Here, we consider selected examples to illustrate some representative SDD's. The main application in this section is the construction of syntax trees . Since some compilers use syntax trees as an intermediate representation , a common form of SDD turns its input string into a tree. To complete the translation to intermediate code , the compiler may then walk the syntax tree , using another set of rules that are in effect an SDD on the syntax tree rather than the parse tree. (Chapter 6 also discusses approaches to intermediate-code generation that apply an SDD without ever constructing a tree explicitly.) We consider two SDD's for constructing syntax trees for expressions. The first, an S-attributed definition, is suitable for use during bottom-up parsing. The second, L-attributed, is suitable for use during top-down parsing. The final example of this section is an L-attributed definition that deals with basic and array types. 5.3.1 Construction of Syntax Trees As discussed in Section 2.8.2, each node in a syntax tree represents a construct; the children of the node represent the meaningful components of the construct. A syntax-tree node representing an expression E_1 + E_2 E_1 + E_2 has label + and two children representing the sub expressions E_1 E_1 and E_2 E_2 . We shall implement the nodes of a syntax tree by objects with a suitable number of fields. Each object will have an op field that is the label of the node. The objects will have additional fields as follows: If the node is a leaf, an additional field holds the lexical value for the leaf. A constructor function Leaf (op, val ) creates a leaf object. Alternatively, if nodes are viewed as records, then Leaf returns a pointer to a new record for a leaf. If the node is an interior node , there are as many additional fields as the node has children in the syntax tree . A constructor function Node takes two or more arguments: Node(op, c_1, c_2, c_3, \\dots, c_k) Node(op, c_1, c_2, c_3, \\dots, c_k) creates an object with first field op and k additional fields for the k children c_1, c_2, c_3, \\dots, c_k c_1, c_2, c_3, \\dots, c_k . Example 5.11 : The S-attributed definition in Fig. 5.10 constructs syntax trees for a simple expression grammar involving only the binary operators + and - . As usual, these operators are at the same precedence level and are jointly left associative. All nonterminals have one synthesized attribute node , which represents a node of the syntax tree. Every time the first production E \\to E_1 + T E \\to E_1 + T is used, its rule creates a node with + for op and two children, E_1.node E_1.node and T.node T.node , for the sub expressions. The second production has a similar rule. For production 3, E \\to T E \\to T , no node is created, since E.node E.node is the same as T.node T.node . Similarly, no node is created for production 4, T \\to (E) T \\to (E) . The value of T.node is the same as E.node , since parentheses are used only for grouping; they influence the structure of the parse tree and the syntax tree , but once their job is done, there is no further need to retain them in the syntax tree .","title":"5.3-Applications-of-Syntax-Directed-Translation"},{"location":"5-Syntax-Directed-Translation/5.3-Applications-of-Syntax-Directed-Translation/#53#applications#of#syntax-directed#translation","text":"The syntax-directed translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation. Here, we consider selected examples to illustrate some representative SDD's. The main application in this section is the construction of syntax trees . Since some compilers use syntax trees as an intermediate representation , a common form of SDD turns its input string into a tree. To complete the translation to intermediate code , the compiler may then walk the syntax tree , using another set of rules that are in effect an SDD on the syntax tree rather than the parse tree. (Chapter 6 also discusses approaches to intermediate-code generation that apply an SDD without ever constructing a tree explicitly.) We consider two SDD's for constructing syntax trees for expressions. The first, an S-attributed definition, is suitable for use during bottom-up parsing. The second, L-attributed, is suitable for use during top-down parsing. The final example of this section is an L-attributed definition that deals with basic and array types.","title":"5.3 Applications of Syntax-Directed Translation"},{"location":"5-Syntax-Directed-Translation/5.3-Applications-of-Syntax-Directed-Translation/#531#construction#of#syntax#trees","text":"As discussed in Section 2.8.2, each node in a syntax tree represents a construct; the children of the node represent the meaningful components of the construct. A syntax-tree node representing an expression E_1 + E_2 E_1 + E_2 has label + and two children representing the sub expressions E_1 E_1 and E_2 E_2 . We shall implement the nodes of a syntax tree by objects with a suitable number of fields. Each object will have an op field that is the label of the node. The objects will have additional fields as follows: If the node is a leaf, an additional field holds the lexical value for the leaf. A constructor function Leaf (op, val ) creates a leaf object. Alternatively, if nodes are viewed as records, then Leaf returns a pointer to a new record for a leaf. If the node is an interior node , there are as many additional fields as the node has children in the syntax tree . A constructor function Node takes two or more arguments: Node(op, c_1, c_2, c_3, \\dots, c_k) Node(op, c_1, c_2, c_3, \\dots, c_k) creates an object with first field op and k additional fields for the k children c_1, c_2, c_3, \\dots, c_k c_1, c_2, c_3, \\dots, c_k . Example 5.11 : The S-attributed definition in Fig. 5.10 constructs syntax trees for a simple expression grammar involving only the binary operators + and - . As usual, these operators are at the same precedence level and are jointly left associative. All nonterminals have one synthesized attribute node , which represents a node of the syntax tree. Every time the first production E \\to E_1 + T E \\to E_1 + T is used, its rule creates a node with + for op and two children, E_1.node E_1.node and T.node T.node , for the sub expressions. The second production has a similar rule. For production 3, E \\to T E \\to T , no node is created, since E.node E.node is the same as T.node T.node . Similarly, no node is created for production 4, T \\to (E) T \\to (E) . The value of T.node is the same as E.node , since parentheses are used only for grouping; they influence the structure of the parse tree and the syntax tree , but once their job is done, there is no further need to retain them in the syntax tree .","title":"5.3.1 Construction of Syntax Trees"},{"location":"5-Syntax-Directed-Translation/wikipedia-Syntax-directed-translation/","text":"Syntax-directed translation Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar .[ 1] Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax . Overview Syntax-directed translation fundamentally works by adding actions to the productions in a context-free grammar , resulting in a Syntax-Directed Definition (SDD).[ 2] Actions are steps or procedures that will be carried out when that production is used in a derivation. A grammar specification embedded with actions to be performed is called a syntax-directed translation scheme [ 1] (sometimes simply called a 'translation scheme'.) Each symbol in the grammar can have an attribute , which is a value that is to be associated with the symbol. Common attributes could include a variable type, the value of an expression, etc. Given a symbol X , with an attribute t , that attribute is referred to as X . t Thus, given actions and attributes, the grammar can be used for translating strings from its language by applying the actions and carrying information through each symbol's attribute.","title":"wikipedia-Syntax-directed-translation"},{"location":"5-Syntax-Directed-Translation/wikipedia-Syntax-directed-translation/#syntax-directed#translation","text":"Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar .[ 1] Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax .","title":"Syntax-directed translation"},{"location":"5-Syntax-Directed-Translation/wikipedia-Syntax-directed-translation/#overview","text":"Syntax-directed translation fundamentally works by adding actions to the productions in a context-free grammar , resulting in a Syntax-Directed Definition (SDD).[ 2] Actions are steps or procedures that will be carried out when that production is used in a derivation. A grammar specification embedded with actions to be performed is called a syntax-directed translation scheme [ 1] (sometimes simply called a 'translation scheme'.) Each symbol in the grammar can have an attribute , which is a value that is to be associated with the symbol. Common attributes could include a variable type, the value of an expression, etc. Given a symbol X , with an attribute t , that attribute is referred to as X . t Thus, given actions and attributes, the grammar can be used for translating strings from its language by applying the actions and carrying information through each symbol's attribute.","title":"Overview"},{"location":"6-Intermediate-Code-Generation/","text":"Chapter 6 Intermediate-Code Generation","title":6},{"location":"6-Intermediate-Code-Generation/#chapter#6#intermediate-code#generation","text":"","title":"Chapter 6 Intermediate-Code Generation"},{"location":"6-Intermediate-Code-Generation/6.3-Types-and-Declarations/","text":"6.3 Types and Declarations The applications of types can be grouped under checking and translation : 1\u3001 Type checking uses logical rules to reason(\u63a8\u7406) about the behavior of a program at run time. Specifically(\u66f4\u52a0\u786e\u5207\u5730\u8bf4), it ensures that the types of the operands match the type expected by an operator. For example, the && operator in Java expects its two operands to be booleans; the result is also of type boolean. 2\u3001 Translation Applications . From the type of a name, a compiler can determine the storage that will be needed for that name at run time. Type information is also needed to calculate the address denoted by an array reference, to insert explicit type conversions, and to choose the right version of an arithmetic operator, among other things. In this section, we examine types and storage layout for names declared within a procedure or a class . The actual storage for a procedure call or an object is allocated at run time , when the procedure is called or the object is created. As we examine(\u68c0\u67e5) local declarations at compile time , we can, however,layout relative addresses , where the relative address of a name or a component of a data structure is an offset from the start of a data area. 6.3.1 Type Expressions Types have structure, which we shall represent using type expressions : a type expression is either a basic type or is formed by applying an operator called a type constructor (\u7c7b\u578b\u6784\u9020\u7b97\u5b50) to a type expression. The sets of basic types and constructors depend on the language to be checked. Example 6.8 : The array type int[2][3] can be read as array of 2 arrays of 3 integers each and written as a type expression array (2; array (3; integer)) .This type is represented by the tree in Fig. 6.14. The operator array takes two parameters, a number and a type(\u5982\u4e0b\u6240\u793a\uff0carray\u8282\u70b9\u6709\u4e24\u4e2a\u5b50\u8282\u70b9). We shall use the following definition of type expressions : A basic type is a type expression . Typical basic types for a language include boolean, char, integer, float, and void ; the latter denotes the absence of a value. A type name is a type expression . A type expression can be formed by applying the array type constructor to a number and a type expression(\u5b9a\u4e49\u6570\u7ec4\u7c7b\u578b). A record is a data structure with named fields. A type expression can be formed by applying the record type constructor to the field names and their types. Record types will be implemented in Section 6.3.6 by applying the constructor record to a symbol table containing entries for the fields. A type expression can be formed by using the type constructor-> for function types . We write s->t for function from type s to type t . Function types will be useful when type checking is discussed in Section 6.5. If s and t are type expressions, then their Cartesian product s * t is a type expression. Products are introduced for completeness(\u5b8c\u6574\u6027); they can be used to represent a list or tuple of types (e.g., for function parameters).We assume that associates to the left and that it has higher precedence than -> . Type expressions may contain variables whose values are type expressions .Compiler-generated type variables will be used in Section 6.5.4. A convenient way to represent a type expression is to use a graph. The value-number method of Section 6.1.2, can b e adapted to construct a DAG for a type expression, with interior nodes for type constructors and leaves for basic types, type names, and type variables; for example, see the tree in Fig. 6.14. Type Names and Recursive Types Once a class is defined, its name can be used as a type name in C++ or Java; for example, consider Node in the program fragment public class Node { ... } ... public Node n ; Names can be used to define recursive types, which are needed for data structures such as linked lists. The pseudo code for a list element class Cell { int info ; Cell next ; ... } defines the recursive type Cell as a class that contains a field info and a field next of type Cell . Similar recursive types can be defined in C using records and pointers. The techniques in this chapter carry over to recursive types. 6.3.4 Storage Layout for Local Names From the type of a name, we can determine the amount of storage that will be needed for the name at run time. At compile time, we can use these amounts to assign each name a relative address . The type and relative address are saved in the symbol-table entry for the name. Data of varying length, such as strings, or data whose size cannot be determined until run time, such as dynamic arrays, is handled by reserving a known fixed amount of storage for a pointer to the data. Run-time storage management is discussed in Chapter 7. Address Alignment The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. For example, instructions to add integers may expect integers to be aligned, that is, placed at certain positions in memory such as an address divisible by 4. Although an array of ten characters needs only enough bytes to hold ten characters, a compiler may therefore allocate 12 bytes,the next multiple of 4 ,leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium(\u5b9d\u8d35), a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. Suppose that storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory. Typically, a byte is eight bits, and some number of bytes form a machine word. Multibyte objects are stored in consecutive bytes and given the address of the first byte. The width of a type is the number of storage units needed for objects of that type. A basic type, such as a character, integer, or float, requires an integral number of bytes. For easy access, storage for aggregates such as arrays and classes is allocated in one contiguous block of bytes. The translation scheme (SDT) (\u7ffb\u8bd1\u65b9\u6848) in Fig. 6.15 computes types and their widths for basic and array types; record types will be discussed in Section 6.3.6. The SDT uses synthesized attributes type and width for each nonterminal and two variables t and w to pass type and width information from a B node in a parse tree to the node for the production C . In a syntax-directed definition, t and w would be inherited attributes for C . T -> B f t = B :type; w = B :width ; g C f T :type = C :type ; T :width = C :width ; g B -> int f B :type = integer; B :width = 4; g B -> ?oat f B :type = ?oat; B :width = 8; g C -> ? f C :type = t; C :width = w ; g C -> [ num ] C 1 f C :type = array (num:value ; C 1 :type); C :width = num:value ? C 1 :width ; g","title":"Introduction"},{"location":"6-Intermediate-Code-Generation/6.3-Types-and-Declarations/#63#types#and#declarations","text":"The applications of types can be grouped under checking and translation : 1\u3001 Type checking uses logical rules to reason(\u63a8\u7406) about the behavior of a program at run time. Specifically(\u66f4\u52a0\u786e\u5207\u5730\u8bf4), it ensures that the types of the operands match the type expected by an operator. For example, the && operator in Java expects its two operands to be booleans; the result is also of type boolean. 2\u3001 Translation Applications . From the type of a name, a compiler can determine the storage that will be needed for that name at run time. Type information is also needed to calculate the address denoted by an array reference, to insert explicit type conversions, and to choose the right version of an arithmetic operator, among other things. In this section, we examine types and storage layout for names declared within a procedure or a class . The actual storage for a procedure call or an object is allocated at run time , when the procedure is called or the object is created. As we examine(\u68c0\u67e5) local declarations at compile time , we can, however,layout relative addresses , where the relative address of a name or a component of a data structure is an offset from the start of a data area.","title":"6.3 Types and Declarations"},{"location":"6-Intermediate-Code-Generation/6.3-Types-and-Declarations/#631#type#expressions","text":"Types have structure, which we shall represent using type expressions : a type expression is either a basic type or is formed by applying an operator called a type constructor (\u7c7b\u578b\u6784\u9020\u7b97\u5b50) to a type expression. The sets of basic types and constructors depend on the language to be checked. Example 6.8 : The array type int[2][3] can be read as array of 2 arrays of 3 integers each and written as a type expression array (2; array (3; integer)) .This type is represented by the tree in Fig. 6.14. The operator array takes two parameters, a number and a type(\u5982\u4e0b\u6240\u793a\uff0carray\u8282\u70b9\u6709\u4e24\u4e2a\u5b50\u8282\u70b9). We shall use the following definition of type expressions : A basic type is a type expression . Typical basic types for a language include boolean, char, integer, float, and void ; the latter denotes the absence of a value. A type name is a type expression . A type expression can be formed by applying the array type constructor to a number and a type expression(\u5b9a\u4e49\u6570\u7ec4\u7c7b\u578b). A record is a data structure with named fields. A type expression can be formed by applying the record type constructor to the field names and their types. Record types will be implemented in Section 6.3.6 by applying the constructor record to a symbol table containing entries for the fields. A type expression can be formed by using the type constructor-> for function types . We write s->t for function from type s to type t . Function types will be useful when type checking is discussed in Section 6.5. If s and t are type expressions, then their Cartesian product s * t is a type expression. Products are introduced for completeness(\u5b8c\u6574\u6027); they can be used to represent a list or tuple of types (e.g., for function parameters).We assume that associates to the left and that it has higher precedence than -> . Type expressions may contain variables whose values are type expressions .Compiler-generated type variables will be used in Section 6.5.4. A convenient way to represent a type expression is to use a graph. The value-number method of Section 6.1.2, can b e adapted to construct a DAG for a type expression, with interior nodes for type constructors and leaves for basic types, type names, and type variables; for example, see the tree in Fig. 6.14. Type Names and Recursive Types Once a class is defined, its name can be used as a type name in C++ or Java; for example, consider Node in the program fragment public class Node { ... } ... public Node n ; Names can be used to define recursive types, which are needed for data structures such as linked lists. The pseudo code for a list element class Cell { int info ; Cell next ; ... } defines the recursive type Cell as a class that contains a field info and a field next of type Cell . Similar recursive types can be defined in C using records and pointers. The techniques in this chapter carry over to recursive types.","title":"6.3.1 Type Expressions"},{"location":"6-Intermediate-Code-Generation/6.3-Types-and-Declarations/#634#storage#layout#for#local#names","text":"From the type of a name, we can determine the amount of storage that will be needed for the name at run time. At compile time, we can use these amounts to assign each name a relative address . The type and relative address are saved in the symbol-table entry for the name. Data of varying length, such as strings, or data whose size cannot be determined until run time, such as dynamic arrays, is handled by reserving a known fixed amount of storage for a pointer to the data. Run-time storage management is discussed in Chapter 7. Address Alignment The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. For example, instructions to add integers may expect integers to be aligned, that is, placed at certain positions in memory such as an address divisible by 4. Although an array of ten characters needs only enough bytes to hold ten characters, a compiler may therefore allocate 12 bytes,the next multiple of 4 ,leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium(\u5b9d\u8d35), a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. Suppose that storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory. Typically, a byte is eight bits, and some number of bytes form a machine word. Multibyte objects are stored in consecutive bytes and given the address of the first byte. The width of a type is the number of storage units needed for objects of that type. A basic type, such as a character, integer, or float, requires an integral number of bytes. For easy access, storage for aggregates such as arrays and classes is allocated in one contiguous block of bytes. The translation scheme (SDT) (\u7ffb\u8bd1\u65b9\u6848) in Fig. 6.15 computes types and their widths for basic and array types; record types will be discussed in Section 6.3.6. The SDT uses synthesized attributes type and width for each nonterminal and two variables t and w to pass type and width information from a B node in a parse tree to the node for the production C . In a syntax-directed definition, t and w would be inherited attributes for C . T -> B f t = B :type; w = B :width ; g C f T :type = C :type ; T :width = C :width ; g B -> int f B :type = integer; B :width = 4; g B -> ?oat f B :type = ?oat; B :width = 8; g C -> ? f C :type = t; C :width = w ; g C -> [ num ] C 1 f C :type = array (num:value ; C 1 :type); C :width = num:value ? C 1 :width ; g","title":"6.3.4 Storage Layout for Local Names"},{"location":"7-Run-Time-Environments/","text":"Chapter 7 Run-Time Environments A compiler must accurately implement the abstractions embodied in the source-language definition. These abstractions typically include the concepts we discussed in Section 1.6 such as names, scopes, bindings, data types, operators, procedures , parameters, and flow-of-control constructs. The compiler must cooperate with the operating system and other systems software to support these abstractions on the target machine. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684**abstractions**\u662f\u6709\u7279\u6b8a\u542b\u4e49\u7684\uff0c\u53c2\u89c1\u6587\u7ae0 Abstraction \u3002 Compiler\u9700\u8981\u4f7f\u7528 machine language \u6765\u5b9e\u73b0programming language\u4e2d\u7684\u5404\u79cdabstraction\u3002 To do so, the compiler creates and manages a run-time environment in which it assumes its target programs are being executed. This environment deals with a variety of issues such as the layout and allocation of storage locations for the objects named in the source program, the mechanisms used by the target program to access variables , the linkages between procedures, the mechanisms for passing parameters, and the interfaces to the operating system, input/output devices, and other programs. NOTE: \u663e\u7136\uff0c**run-time environment**\u662f\u4e00\u4e2a \u6982\u5ff5\u6a21\u578b \uff08\u662f\u4e00\u4e2a\u7b80\u5316\u7684\u6a21\u578b\uff0c\u6ca1\u6709\u8003\u8651multi-thread\u7b49\uff09\uff0c\u5173\u4e8e\u5bf9\u8fd9\u4e2a\u6982\u5ff5\u6a21\u578b\u7684\u5b9e\u73b0\uff0c\u53c2\u89c1\u5de5\u7a0b Linux-OS \u7684 Process mode: run time environment \u3002 \u672c\u6bb5\u63cf\u8ff0\u7684\u8fd9\u4e9b\u5185\u5bb9\u662f\u7531 Application binary interface \u6240\u7ea6\u5b9a\u3002 The two themes in this chapter are the allocation of storage locations and access to variables and data . We shall discuss memory management in some detail, including stack allocation , heap management , and garbage collection . In the next chapter, we present techniques for generating target code for many common language constructs. NOTE: \u672c\u7ae0\u7684\u5185\u5bb9\u662f\u6bd4\u8f83\u91cd\u8981\u7684\uff0c\u56e0\u4e3aprocess\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u8981\u6d3b\u52a8\u90fd\u662f\u53d1\u751f\u5728stack\u548cheap\u3002","title":7},{"location":"7-Run-Time-Environments/#chapter#7#run-time#environments","text":"A compiler must accurately implement the abstractions embodied in the source-language definition. These abstractions typically include the concepts we discussed in Section 1.6 such as names, scopes, bindings, data types, operators, procedures , parameters, and flow-of-control constructs. The compiler must cooperate with the operating system and other systems software to support these abstractions on the target machine. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684**abstractions**\u662f\u6709\u7279\u6b8a\u542b\u4e49\u7684\uff0c\u53c2\u89c1\u6587\u7ae0 Abstraction \u3002 Compiler\u9700\u8981\u4f7f\u7528 machine language \u6765\u5b9e\u73b0programming language\u4e2d\u7684\u5404\u79cdabstraction\u3002 To do so, the compiler creates and manages a run-time environment in which it assumes its target programs are being executed. This environment deals with a variety of issues such as the layout and allocation of storage locations for the objects named in the source program, the mechanisms used by the target program to access variables , the linkages between procedures, the mechanisms for passing parameters, and the interfaces to the operating system, input/output devices, and other programs. NOTE: \u663e\u7136\uff0c**run-time environment**\u662f\u4e00\u4e2a \u6982\u5ff5\u6a21\u578b \uff08\u662f\u4e00\u4e2a\u7b80\u5316\u7684\u6a21\u578b\uff0c\u6ca1\u6709\u8003\u8651multi-thread\u7b49\uff09\uff0c\u5173\u4e8e\u5bf9\u8fd9\u4e2a\u6982\u5ff5\u6a21\u578b\u7684\u5b9e\u73b0\uff0c\u53c2\u89c1\u5de5\u7a0b Linux-OS \u7684 Process mode: run time environment \u3002 \u672c\u6bb5\u63cf\u8ff0\u7684\u8fd9\u4e9b\u5185\u5bb9\u662f\u7531 Application binary interface \u6240\u7ea6\u5b9a\u3002 The two themes in this chapter are the allocation of storage locations and access to variables and data . We shall discuss memory management in some detail, including stack allocation , heap management , and garbage collection . In the next chapter, we present techniques for generating target code for many common language constructs. NOTE: \u672c\u7ae0\u7684\u5185\u5bb9\u662f\u6bd4\u8f83\u91cd\u8981\u7684\uff0c\u56e0\u4e3aprocess\u5728\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684\u4e3b\u8981\u6d3b\u52a8\u90fd\u662f\u53d1\u751f\u5728stack\u548cheap\u3002","title":"Chapter 7 Run-Time Environments"},{"location":"7-Run-Time-Environments/7.1-Storage-Organization/","text":"7.1 Storage Organization From the perspective of the compiler writer, the executing target program runs in its own logical address space in which each program value has a location . The management and organization of this logical address space is shared between the compiler, operating system, and target machine. The operating system maps the logical addresses into physical addresses , which are usually spread throughout memory. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u7684\u610f\u601d\u662fcompiler\u3001operating system\u548ctarget machine\u9075\u5faa\u4e2d\u76f8\u540c\u7684management and organization of this logical address space \uff0c\u90a3\u5b83\u4eec\u4e4b\u95f4\u7684\u7ea6\u5b9a\u662f\u4ec0\u4e48\u5462\uff1f\u662f ELF \u5417\uff1f \u80af\u5b9a\u4e0d\u662fELF\uff0cELF\u6240\u63cf\u8ff0\u7684\u662f\u53ef\u6267\u884c\u7a0b\u5e8f\u7684\u6587\u4ef6\u683c\u5f0f\u3002\u672c\u6587\u4e2d\u7684**logical address space**\u5176\u5b9e\u662fprocess\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684address space\u7684 \u62bd\u8c61\u6a21\u578b \uff0c\u5b83\u4e0d\u6d89\u53ca\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002 The run-time representation of an object program in the logical address space consists of data and program areas as shown in Fig. 7.1. A compiler for a language like C++ on an operating system like Linux might sub divide memory in this way. Throughout this book, we assume the run-time storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory(\u5b57\u8282\u662f\u5185\u5b58\u7684\u6700\u5c0f\u7f16\u5740\u5355\u5143). A byte is eight bits and four bytes form a machine word . Multibyte objects are stored in consecutive bytes and given the address of the first byte(\u591a\u5b57\u8282\u6570\u636e\u5bf9\u8c61\u603b\u6570\u5b58\u50a8\u5728\u4e00\u6bb5\u8fde\u7eed\u7684\u5b57\u8282\u4e2d\uff0c\u5e76\u4e14\u5c06\u7b2c\u4e00\u4e2a\u5b57\u8282\u4f5c\u4e3a\u5730\u5740). As discussed in Chapter 6, the amount of storage needed for a name is determined from its type . An elementary data type, such as a character, integer,or float, can be stored in an integral number of bytes. Storage for an aggregate type, such as an array or structure, must be large enough to hold all its components. The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. On many machines, instructions to add integers may expect integers to be aligned , that is, placed at an address divisible by 4. Although a character array (as in C) of length 10 needs only enough bytes to hold ten characters, a compiler may allocate 12 bytes to get the proper alignment, leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium, a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u4e2d\u6240\u8ba8\u8bba\u7684\u95ee\u9898\uff0c\u5728Hardware\u9879\u76ee\u4e2d\u6709\u5bf9\u5b83\u7684\u4e13\u95e8\u8ba8\u8bba\u3002 The size of the generated target code is fixed at compile time, so the compiler can place the executable target code in a statically determined area Code ,usually in the low end of memory. Similarly, the size of some program data objects , such as global constants , and data generated by the compiler , such as information to support garbage collection, may be known at compile time , and these data objects can be placed in another statically determined area called Static . One reason for statically allocating as many data objects as possible is that the addresses of these objects can be compiled into the target code . In early versions of Fortran, all data objects could be allocated statically. NOTE: \u7f16\u8bd1\u5668\u662f\u77e5\u9053statically allocating data objects\u7684\u5730\u5740\u7684\uff0c\u6240\u4ee5\u5b83\u80fd\u591f\u4f7f\u7528\u5b83\u4eec\u7684\u5730\u5740\u6765\u66ff\u4ee3\u5b83\u4eec\u3002 To maximize the utilization of space at run time , the other two areas, Stack and Heap , are at the opposite ends of the remainder of the address space . These areas are dynamic ; their size can change as the program executes. These areas grow towards each other as needed. The stack is used to store data structures called activation records that get generated during procedure calls. NOTE: \u5173\u4e8e***activation records***\uff0c\u53c2\u89c1 Call stack \u3002 In practice, the stack grows towards lower addresses, the heap towards higher. However, throughout this chapter and the next we shall assume that the stack grows towards higher addresses so that we can use positive offsets for notational convenience in all our examples. As we shall see in the next section, an activation record is used to store information about the status of the machine, such as the value of the program counter and machine registers , when a procedure call occurs. When control returns from the call, the activation of the calling procedure can be restarted after restoring the values of relevant registers and setting the program counter to the point immediately after the call. Data objects whose lifetimes are contained in that of an activation can be allocated on the stack along with other information associated with the activation. NOTE: \u672c\u5730\u6240\u63cf\u8ff0\u7684\u662f\u5bf9memory area Stack\u4f7f\u7528\uff0c\u663e\u7136\uff0c\u5b83\u4e3b\u8981\u7528\u4e8e\u51fd\u6570\u8c03\u7528\u3002 **calling procedure**\u7684\u610f\u601d\u662f\u4e3b\u8c03\u51fd\u6570\u3002 \u5173\u4e8eprogram counter\uff0c\u53c2\u89c1 Program counter \u3002 Many programming languages allow the programmer to allocate and deallocate data under program control. For example, C has the functions malloc and free that can be used to obtain and give back arbitrary chunks of storage. The heap is used to manage this kind of long-lived data. Section 7.4 will discuss various memory-management algorithms that can be used to maintain the heap. NOTE: \u672c\u6bb5\u6240\u63cf\u8ff0\u7684\u5bf9memory area Heap\u7684\u4f7f\u7528\u3002 7.1.1 Static Versus Dynamic Storage Allocation The layout and allocation of data to memory locations in the run-time environment are key issues in storage management . These issues are tricky because the same name in a program text can refer to multiple locations at run time. The two adjectives static and dynamic distinguish between compile time and run time , respectively. We say that a storage-allocation decision is static , if it can be made by the compiler looking only at the text of the program, not at what the program does when it executes. Conversely, a decision is dynamic if it can be decided only while the program is running. Many compilers use some combination of the following two strategies for dynamic storage allocation: Stack storage . Names local to a procedure are allocated space on a stack . We discuss the run-time stack starting in Section 7.2. The stack supports the normal call/return policy for procedures Heap storage . Data that may outlive the call to the procedure that created it is usually allocated on a heap of reusable storage. We discuss heap management starting in Section 7.4. The heap is an area of virtual memory that allows objects or other data elements to obtain storage when they are created and to return that storage when they are invalidated To support heap management , garbage collection enables the run-time system to detect useless data elements and reuse their storage, even if the programmer does not return their space explicitly. Automatic garbage collection is an essential feature of many modern languages, despite it being a difficult operation to do efficiently; it may not even be possible for some languages.","title":"7.1-Storage-Organization"},{"location":"7-Run-Time-Environments/7.1-Storage-Organization/#71#storage#organization","text":"From the perspective of the compiler writer, the executing target program runs in its own logical address space in which each program value has a location . The management and organization of this logical address space is shared between the compiler, operating system, and target machine. The operating system maps the logical addresses into physical addresses , which are usually spread throughout memory. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u7684\u610f\u601d\u662fcompiler\u3001operating system\u548ctarget machine\u9075\u5faa\u4e2d\u76f8\u540c\u7684management and organization of this logical address space \uff0c\u90a3\u5b83\u4eec\u4e4b\u95f4\u7684\u7ea6\u5b9a\u662f\u4ec0\u4e48\u5462\uff1f\u662f ELF \u5417\uff1f \u80af\u5b9a\u4e0d\u662fELF\uff0cELF\u6240\u63cf\u8ff0\u7684\u662f\u53ef\u6267\u884c\u7a0b\u5e8f\u7684\u6587\u4ef6\u683c\u5f0f\u3002\u672c\u6587\u4e2d\u7684**logical address space**\u5176\u5b9e\u662fprocess\u8fd0\u884c\u8fc7\u7a0b\u4e2d\u7684address space\u7684 \u62bd\u8c61\u6a21\u578b \uff0c\u5b83\u4e0d\u6d89\u53ca\u5177\u4f53\u5b9e\u73b0\u7ec6\u8282\u3002 The run-time representation of an object program in the logical address space consists of data and program areas as shown in Fig. 7.1. A compiler for a language like C++ on an operating system like Linux might sub divide memory in this way. Throughout this book, we assume the run-time storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory(\u5b57\u8282\u662f\u5185\u5b58\u7684\u6700\u5c0f\u7f16\u5740\u5355\u5143). A byte is eight bits and four bytes form a machine word . Multibyte objects are stored in consecutive bytes and given the address of the first byte(\u591a\u5b57\u8282\u6570\u636e\u5bf9\u8c61\u603b\u6570\u5b58\u50a8\u5728\u4e00\u6bb5\u8fde\u7eed\u7684\u5b57\u8282\u4e2d\uff0c\u5e76\u4e14\u5c06\u7b2c\u4e00\u4e2a\u5b57\u8282\u4f5c\u4e3a\u5730\u5740). As discussed in Chapter 6, the amount of storage needed for a name is determined from its type . An elementary data type, such as a character, integer,or float, can be stored in an integral number of bytes. Storage for an aggregate type, such as an array or structure, must be large enough to hold all its components. The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. On many machines, instructions to add integers may expect integers to be aligned , that is, placed at an address divisible by 4. Although a character array (as in C) of length 10 needs only enough bytes to hold ten characters, a compiler may allocate 12 bytes to get the proper alignment, leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium, a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u4e2d\u6240\u8ba8\u8bba\u7684\u95ee\u9898\uff0c\u5728Hardware\u9879\u76ee\u4e2d\u6709\u5bf9\u5b83\u7684\u4e13\u95e8\u8ba8\u8bba\u3002 The size of the generated target code is fixed at compile time, so the compiler can place the executable target code in a statically determined area Code ,usually in the low end of memory. Similarly, the size of some program data objects , such as global constants , and data generated by the compiler , such as information to support garbage collection, may be known at compile time , and these data objects can be placed in another statically determined area called Static . One reason for statically allocating as many data objects as possible is that the addresses of these objects can be compiled into the target code . In early versions of Fortran, all data objects could be allocated statically. NOTE: \u7f16\u8bd1\u5668\u662f\u77e5\u9053statically allocating data objects\u7684\u5730\u5740\u7684\uff0c\u6240\u4ee5\u5b83\u80fd\u591f\u4f7f\u7528\u5b83\u4eec\u7684\u5730\u5740\u6765\u66ff\u4ee3\u5b83\u4eec\u3002 To maximize the utilization of space at run time , the other two areas, Stack and Heap , are at the opposite ends of the remainder of the address space . These areas are dynamic ; their size can change as the program executes. These areas grow towards each other as needed. The stack is used to store data structures called activation records that get generated during procedure calls. NOTE: \u5173\u4e8e***activation records***\uff0c\u53c2\u89c1 Call stack \u3002 In practice, the stack grows towards lower addresses, the heap towards higher. However, throughout this chapter and the next we shall assume that the stack grows towards higher addresses so that we can use positive offsets for notational convenience in all our examples. As we shall see in the next section, an activation record is used to store information about the status of the machine, such as the value of the program counter and machine registers , when a procedure call occurs. When control returns from the call, the activation of the calling procedure can be restarted after restoring the values of relevant registers and setting the program counter to the point immediately after the call. Data objects whose lifetimes are contained in that of an activation can be allocated on the stack along with other information associated with the activation. NOTE: \u672c\u5730\u6240\u63cf\u8ff0\u7684\u662f\u5bf9memory area Stack\u4f7f\u7528\uff0c\u663e\u7136\uff0c\u5b83\u4e3b\u8981\u7528\u4e8e\u51fd\u6570\u8c03\u7528\u3002 **calling procedure**\u7684\u610f\u601d\u662f\u4e3b\u8c03\u51fd\u6570\u3002 \u5173\u4e8eprogram counter\uff0c\u53c2\u89c1 Program counter \u3002 Many programming languages allow the programmer to allocate and deallocate data under program control. For example, C has the functions malloc and free that can be used to obtain and give back arbitrary chunks of storage. The heap is used to manage this kind of long-lived data. Section 7.4 will discuss various memory-management algorithms that can be used to maintain the heap. NOTE: \u672c\u6bb5\u6240\u63cf\u8ff0\u7684\u5bf9memory area Heap\u7684\u4f7f\u7528\u3002","title":"7.1 Storage Organization"},{"location":"7-Run-Time-Environments/7.1-Storage-Organization/#711#static#versus#dynamic#storage#allocation","text":"The layout and allocation of data to memory locations in the run-time environment are key issues in storage management . These issues are tricky because the same name in a program text can refer to multiple locations at run time. The two adjectives static and dynamic distinguish between compile time and run time , respectively. We say that a storage-allocation decision is static , if it can be made by the compiler looking only at the text of the program, not at what the program does when it executes. Conversely, a decision is dynamic if it can be decided only while the program is running. Many compilers use some combination of the following two strategies for dynamic storage allocation: Stack storage . Names local to a procedure are allocated space on a stack . We discuss the run-time stack starting in Section 7.2. The stack supports the normal call/return policy for procedures Heap storage . Data that may outlive the call to the procedure that created it is usually allocated on a heap of reusable storage. We discuss heap management starting in Section 7.4. The heap is an area of virtual memory that allows objects or other data elements to obtain storage when they are created and to return that storage when they are invalidated To support heap management , garbage collection enables the run-time system to detect useless data elements and reuse their storage, even if the programmer does not return their space explicitly. Automatic garbage collection is an essential feature of many modern languages, despite it being a difficult operation to do efficiently; it may not even be possible for some languages.","title":"7.1.1 Static Versus Dynamic Storage Allocation"},{"location":"7-Run-Time-Environments/7.2-Stack-Allocation-of-Space/","text":"7.2 Stack Allocation of Space Almost all compilers for languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack . Each time a procedure is called, space for its local variables is pushed onto a stack , and when the procedure terminates, that space is popped off the stack. As we shall see, this arrangement not only allows space to be shared by procedure calls whose durations do not overlap in time, but it allows us to compile code for a procedure in such a way that the relative addresses of its nonlocal variables are always the same, regardless of the sequence of procedure calls. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684**nonlocal variables**\uff0c\u53c2\u89c1 Non-local variable \u3002 7.2.1 Activation Trees Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time . The following example illustrates nesting of procedure calls. NOTE: **nest in time**\u4f1a\u5728\u540e\u9762\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002 Example 7.1 : Figure 7.2 contains a sketch of a program that reads nine integers into an array a and sorts them using the recursive quicksort algorithm . int a [ 11 ]; void readArray () { /* Reads 9 integers into a[1]; :::; a[9]. */ int i ; ... } int partition ( int m , int n ) { /* Picks a separator value v , and partitions a[m .. n] so that a[m , p - 1] are less than v , a[p] = v , and a[p + 1 , n] are equal to or greater than v . Returns p. */ ... } void quicksort ( int m , int n ) { int i ; if ( n > m ) { i = partition ( m , n ); quicksort ( m , i -1 ); quicksort ( i + 1 , n ); } } main () { readArray (); a [ 0 ] = -9999 ; a [ 10 ] = 9999 ; quicksort ( 1 , 9 ); } Figure 7.2: Sketch of a quicksort program The main function has three tasks. It calls readArray , sets the sentinels, and then calls quicksort on the entire data array. Figure 7.3 suggests a sequence of calls that might result from an execution of the program. In this execution, the call to partition (1, 9) returns 4, so a[1] through a[3] hold elements less than its chosen separator value v , while the larger elements are in a[5] through a[9] . In this example, as is true in general, procedure activations are nested in time . If an activation of procedure p calls procedure q , then that activation of q must end before the activation of p can end. There are three common cases: The activation of q terminates normally. Then in essentially any language, control resumes just after the point of p at which the call to q was made. The activation of q , or some procedure q called, either directly or indirectly, aborts; i.e., it becomes impossible for execution to continue. In that case, p ends simultaneously with q . The activation of q terminates because of an exception that q cannot handle. Procedure p may handle the exception, in which case the activation of q has terminated while the activation of p continues, although not necessarily from the point at which the call to q was made. If p cannot handle the exception, then this activation of p terminates at the same time as the activation of q , and presumably the exception will be handled by some other open activation of a procedure. We therefore can represent the activations of procedures during the running of an entire program by a tree, called an activation tree . Each node corresponds to one activation, and the root is the activation of the \"main\" procedure that initiates execution of the program. At a node for an activation of procedure p , the children correspond to activations of the procedures called by this activation of p . We show these activations in the order that they are called, from left to right. Notice that one child must finish before the activation to its right can begin. NOTE: \u901a\u8fc7\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c\u6211\u4eec\u5e94\u8be5\u80fd\u591f\u7406\u89e3\u5728\u672c\u8282\u7b2c\u4e00\u6bb5\u4e2d\u63d0\u51fa\u7684**nest in time**\u7684\u542b\u4e49\u4e86\uff0cnest\u7684\u4e2d\u6587\u610f\u601d\u662f\u5d4c\u5957\uff0c\u5176\u5b9e\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8fdb\u884c\u7c7b\u6bd4nest in time\uff0c\u8fdb\u5165\u51fd\u6570\u5c31\u76f8\u5f53\u4e8e\u5f00\u62ec\u53f7\uff0c\u9000\u51fa\u51fd\u6570\u5c31\u76f8\u5f53\u4e8e\u95ed\u62ec\u53f7\uff0c\u8fd9\u6837\u51fd\u6570\u8c03\u7528\u7684\u8fc7\u7a0b\u5c31\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8fdb\u884c\u8868\u793a\u4e86\uff0cnest in time\u76f8\u5f53\u4e8e\u8bf4\u662f\u62ec\u53f7\u662f\u5b8c\u5168\u5339\u914d\u7684\u3002 Example 7.2 : One possible activation tree that completes the sequence of calls and returns suggested in Fig. 7.3 is shown in Fig. 7.4. Functions are represented by the first letters of their names. Remember that this tree is only one possibility, since the arguments of subsequent calls, and also the number of calls along any branch is influenced by the values returned by partition. The use of a run-time stack is enabled by several useful relationships between the activation tree and the behavior of the program: The sequence of procedure calls corresponds to a preorder traversal of the activation tree. The sequence of returns corresponds to a postorder traversal of the activation tree. Suppose that control lies within a particular activation of some procedure, corresponding to a node N of the activation tree. Then the activations that are currently open (live ) are those that correspond to node N and its ancestors. The order in which these activations were called is the order in which they appear along the path to N , starting at the root, and they will return in the reverse of that order. NOTE: \u867d\u7136\u53ef\u4ee5\u4f7f\u7528activation tree\u6765\u523b\u753b\u51fd\u6570\u6267\u884c\u8fc7\u7a0b\uff0c\u4f46\u662f\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u5bf9\u5e94\u7684\u662factivation tree\u4e2d\u7684\u4e00\u6761\u8def\u5f84\uff0c\u6b63\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bf4\u7684\u201c Then the activations that are currently open (live ) are those that correspond to node N and its ancestors.\u201d 7.2.2 Activation Records Procedure calls and returns are usually managed by a run-time stack called the control stack . Each live activation has an activation record (sometimes called a frame ) on the control stack , with the root of the activation tree at the bottom, and the entire sequence of activation records on the stack corresponding to the path in the activation tree to the activation where control currently resides. The latter activation has its record at the top of the stack. Example 7.3 : If control is currently in the activation q (2; 3) of the tree of Fig. 7.4, then the activation record for q (2; 3) is at the top of the control stack . Just below is the activation record for q (1; 3) , the parent of q (2; 3) in the tree. Below that is the activation record q (1; 9) , and at the bottom is the activation record for m , the main function and root of the activation tree. We shall conventionally draw control stacks with the bottom of the stack higher than the top, so the elements in an activation record that appear lowest on the page are actually closest to the top of the stack. The contents of activation records vary with the language being implemented. Here is a list of the kinds of data that might appear in an activation record (see Fig. 7.5 for a summary and possible order for these elements): Temporary values , such as those arising from the evaluation of expressions, in cases where those temporaries cannot be held in registers. Local data belonging to the procedure whose activation record this is. A saved machine status , with information about the state of the machine just before the call to the procedure. This information typically includes the return address (value of the program counter , to which the called procedure must return) and the contents of registers that were used by the calling procedure and that must be restored when the return occurs. An \" access link \" may be needed to locate data needed by the called procedure but found elsewhere, e.g., in another activation record. Access links are discussed in Section 7.3.5. A control link , pointing to the activation record of the caller. Space for the return value of the called function , if any. Again, not all called procedures return a value, and if one does, we may prefer to place that value in a register for efficiency. The actual parameters used by the calling procedure . Commonly, these values are not placed in the activation record but rather in registers, when possible, for greater efficiency. However, we show a space for them to be completely general.","title":"7.2-Stack-Allocation-of-Space"},{"location":"7-Run-Time-Environments/7.2-Stack-Allocation-of-Space/#72#stack#allocation#of#space","text":"Almost all compilers for languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack . Each time a procedure is called, space for its local variables is pushed onto a stack , and when the procedure terminates, that space is popped off the stack. As we shall see, this arrangement not only allows space to be shared by procedure calls whose durations do not overlap in time, but it allows us to compile code for a procedure in such a way that the relative addresses of its nonlocal variables are always the same, regardless of the sequence of procedure calls. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4e2d\u7684**nonlocal variables**\uff0c\u53c2\u89c1 Non-local variable \u3002","title":"7.2 Stack Allocation of Space"},{"location":"7-Run-Time-Environments/7.2-Stack-Allocation-of-Space/#721#activation#trees","text":"Stack allocation would not be feasible if procedure calls, or activations of procedures, did not nest in time . The following example illustrates nesting of procedure calls. NOTE: **nest in time**\u4f1a\u5728\u540e\u9762\u8fdb\u884c\u8be6\u7ec6\u8bf4\u660e\u3002 Example 7.1 : Figure 7.2 contains a sketch of a program that reads nine integers into an array a and sorts them using the recursive quicksort algorithm . int a [ 11 ]; void readArray () { /* Reads 9 integers into a[1]; :::; a[9]. */ int i ; ... } int partition ( int m , int n ) { /* Picks a separator value v , and partitions a[m .. n] so that a[m , p - 1] are less than v , a[p] = v , and a[p + 1 , n] are equal to or greater than v . Returns p. */ ... } void quicksort ( int m , int n ) { int i ; if ( n > m ) { i = partition ( m , n ); quicksort ( m , i -1 ); quicksort ( i + 1 , n ); } } main () { readArray (); a [ 0 ] = -9999 ; a [ 10 ] = 9999 ; quicksort ( 1 , 9 ); } Figure 7.2: Sketch of a quicksort program The main function has three tasks. It calls readArray , sets the sentinels, and then calls quicksort on the entire data array. Figure 7.3 suggests a sequence of calls that might result from an execution of the program. In this execution, the call to partition (1, 9) returns 4, so a[1] through a[3] hold elements less than its chosen separator value v , while the larger elements are in a[5] through a[9] . In this example, as is true in general, procedure activations are nested in time . If an activation of procedure p calls procedure q , then that activation of q must end before the activation of p can end. There are three common cases: The activation of q terminates normally. Then in essentially any language, control resumes just after the point of p at which the call to q was made. The activation of q , or some procedure q called, either directly or indirectly, aborts; i.e., it becomes impossible for execution to continue. In that case, p ends simultaneously with q . The activation of q terminates because of an exception that q cannot handle. Procedure p may handle the exception, in which case the activation of q has terminated while the activation of p continues, although not necessarily from the point at which the call to q was made. If p cannot handle the exception, then this activation of p terminates at the same time as the activation of q , and presumably the exception will be handled by some other open activation of a procedure. We therefore can represent the activations of procedures during the running of an entire program by a tree, called an activation tree . Each node corresponds to one activation, and the root is the activation of the \"main\" procedure that initiates execution of the program. At a node for an activation of procedure p , the children correspond to activations of the procedures called by this activation of p . We show these activations in the order that they are called, from left to right. Notice that one child must finish before the activation to its right can begin. NOTE: \u901a\u8fc7\u4e0a\u9762\u7684\u63cf\u8ff0\uff0c\u6211\u4eec\u5e94\u8be5\u80fd\u591f\u7406\u89e3\u5728\u672c\u8282\u7b2c\u4e00\u6bb5\u4e2d\u63d0\u51fa\u7684**nest in time**\u7684\u542b\u4e49\u4e86\uff0cnest\u7684\u4e2d\u6587\u610f\u601d\u662f\u5d4c\u5957\uff0c\u5176\u5b9e\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8fdb\u884c\u7c7b\u6bd4nest in time\uff0c\u8fdb\u5165\u51fd\u6570\u5c31\u76f8\u5f53\u4e8e\u5f00\u62ec\u53f7\uff0c\u9000\u51fa\u51fd\u6570\u5c31\u76f8\u5f53\u4e8e\u95ed\u62ec\u53f7\uff0c\u8fd9\u6837\u51fd\u6570\u8c03\u7528\u7684\u8fc7\u7a0b\u5c31\u53ef\u4ee5\u4f7f\u7528\u62ec\u53f7\u6765\u8fdb\u884c\u8868\u793a\u4e86\uff0cnest in time\u76f8\u5f53\u4e8e\u8bf4\u662f\u62ec\u53f7\u662f\u5b8c\u5168\u5339\u914d\u7684\u3002 Example 7.2 : One possible activation tree that completes the sequence of calls and returns suggested in Fig. 7.3 is shown in Fig. 7.4. Functions are represented by the first letters of their names. Remember that this tree is only one possibility, since the arguments of subsequent calls, and also the number of calls along any branch is influenced by the values returned by partition. The use of a run-time stack is enabled by several useful relationships between the activation tree and the behavior of the program: The sequence of procedure calls corresponds to a preorder traversal of the activation tree. The sequence of returns corresponds to a postorder traversal of the activation tree. Suppose that control lies within a particular activation of some procedure, corresponding to a node N of the activation tree. Then the activations that are currently open (live ) are those that correspond to node N and its ancestors. The order in which these activations were called is the order in which they appear along the path to N , starting at the root, and they will return in the reverse of that order. NOTE: \u867d\u7136\u53ef\u4ee5\u4f7f\u7528activation tree\u6765\u523b\u753b\u51fd\u6570\u6267\u884c\u8fc7\u7a0b\uff0c\u4f46\u662f\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u5bf9\u5e94\u7684\u662factivation tree\u4e2d\u7684\u4e00\u6761\u8def\u5f84\uff0c\u6b63\u5982\u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6240\u8bf4\u7684\u201c Then the activations that are currently open (live ) are those that correspond to node N and its ancestors.\u201d","title":"7.2.1 Activation Trees"},{"location":"7-Run-Time-Environments/7.2-Stack-Allocation-of-Space/#722#activation#records","text":"Procedure calls and returns are usually managed by a run-time stack called the control stack . Each live activation has an activation record (sometimes called a frame ) on the control stack , with the root of the activation tree at the bottom, and the entire sequence of activation records on the stack corresponding to the path in the activation tree to the activation where control currently resides. The latter activation has its record at the top of the stack. Example 7.3 : If control is currently in the activation q (2; 3) of the tree of Fig. 7.4, then the activation record for q (2; 3) is at the top of the control stack . Just below is the activation record for q (1; 3) , the parent of q (2; 3) in the tree. Below that is the activation record q (1; 9) , and at the bottom is the activation record for m , the main function and root of the activation tree. We shall conventionally draw control stacks with the bottom of the stack higher than the top, so the elements in an activation record that appear lowest on the page are actually closest to the top of the stack. The contents of activation records vary with the language being implemented. Here is a list of the kinds of data that might appear in an activation record (see Fig. 7.5 for a summary and possible order for these elements): Temporary values , such as those arising from the evaluation of expressions, in cases where those temporaries cannot be held in registers. Local data belonging to the procedure whose activation record this is. A saved machine status , with information about the state of the machine just before the call to the procedure. This information typically includes the return address (value of the program counter , to which the called procedure must return) and the contents of registers that were used by the calling procedure and that must be restored when the return occurs. An \" access link \" may be needed to locate data needed by the called procedure but found elsewhere, e.g., in another activation record. Access links are discussed in Section 7.3.5. A control link , pointing to the activation record of the caller. Space for the return value of the called function , if any. Again, not all called procedures return a value, and if one does, we may prefer to place that value in a register for efficiency. The actual parameters used by the calling procedure . Commonly, these values are not placed in the activation record but rather in registers, when possible, for greater efficiency. However, we show a space for them to be completely general.","title":"7.2.2 Activation Records"},{"location":"7-Run-Time-Environments/7.3-Access-to-Non-local-Data-on-the-Stack/","text":"","title":"7.3-Access-to-Non-local-Data-on-the-Stack"},{"location":"7-Run-Time-Environments/7.4-Heap-Management/","text":"","title":"7.4-Heap-Management"},{"location":"9-Machine-Independent-Optimizations/","text":"Chapter 9 Machine-Independent Optimizations","title":9},{"location":"9-Machine-Independent-Optimizations/#chapter#9#machine-independent#optimizations","text":"","title":"Chapter 9 Machine-Independent Optimizations"},{"location":"Guide/","text":"\u5173\u4e8e\u672c\u7ae0 \u672c\u7ae0\u7684\u5185\u5bb9\u662f\u6211\u57fa\u4e8e\u9605\u8bfb\u672c\u4e66\uff08\u90e8\u5206\u7ae0\u8282\uff09\u540e\u6240\u603b\u7ed3\u7684\uff0c\u5bf9\u4e00\u4e9b\u4e13\u9898\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u68b3\u7406\u4e86\u8109\u7edc\u3002","title":"Introduction"},{"location":"Guide/#_1","text":"\u672c\u7ae0\u7684\u5185\u5bb9\u662f\u6211\u57fa\u4e8e\u9605\u8bfb\u672c\u4e66\uff08\u90e8\u5206\u7ae0\u8282\uff09\u540e\u6240\u603b\u7ed3\u7684\uff0c\u5bf9\u4e00\u4e9b\u4e13\u9898\u8fdb\u884c\u4e86\u603b\u7ed3\uff0c\u68b3\u7406\u4e86\u8109\u7edc\u3002","title":"\u5173\u4e8e\u672c\u7ae0"},{"location":"Guide/Expression-tree/","text":"Binary expression tree \u5173\u4e8e expression tree\uff0c\u9700\u8981\u601d\u8003\u5982\u4e0b\u95ee\u9898: 1\u3001how to evaluate expression tree\uff1f 2\u3001how to construct an expression tree of an expression\uff1f 3\u3001how to construct an expression of an expression tree\uff1f 4\u3001operator precedence and construction of expression tree 5\u3001 Operator associativity and construction of expression tree Expression tree \u662f AST \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cexpression tree \u662f AST\u3002 wikipedia Binary expression tree A binary expression tree is a specific kind of a binary tree used to represent expressions. Two common types of expressions that a binary expression tree can represent are algebraic [ 1] and boolean . These trees can represent expressions that contain both unary and binary operators.[ 1] Each node of a binary tree , and hence of a binary expression tree , has zero, one, or two children. This restricted structure simplifies the processing of expression trees . NOTE: : \u4e8c\u53c9\u6811\u80fd\u591f\u6ee1\u8db3 unary and binary operators\u6709\u4e00\u4e2a\u6216\u4e24\u4e2aoperands\u7684\u9700\u6c42\uff1b Overview The leaves of a binary expression tree are operands , such as constants or variable names, and the other nodes contain operators . These particular trees happen to be binary, because all of the operations are binary, and although this is the simplest case, it is possible for nodes to have more than two children. It is also possible for a node to have only one child, as is the case with the unary minus operator. An expression tree, T , can be evaluated by applying the operator at the root to the values obtained by recursively evaluating the left and right subtrees. NOTE: \u5173\u4e8eEvaluation of Expression Tree\u53c2\u89c1 Evaluation \u7ae0\u8282 Traversal NOTE: : \u5bf9binary expression tree\u8fdb\u884ctraverse\u4ece\u800c\u83b7\u5f97\u5bf9\u5e94\u7684algebraic expression\u3002 Infix traversal NOTE: \u5176\u5b9e\u5c31\u662finorder traversal When an infix expression is printed, an opening and closing parenthesis must be added at the beginning and ending of each expression. As every subtree represents a subexpression , an opening parenthesis is printed at its start and the closing parenthesis is printed after processing all of its children. Pseudocode: Algorithm infix (tree) /*Print the infix expression for an expression tree. Pre : tree is a pointer to an expression tree Post: the infix expression has been printed*/ if (tree not empty) if (tree token is operator) print (open parenthesis) end if infix (tree left subtree) print (tree token) infix (tree right subtree) if (tree token is operator) print (close parenthesis) end if end if end infix Postfix traversal The postfix expression is formed by the basic postorder traversal of any binary tree. It does not require parentheses. Pseudocode: Algorithm postfix (tree) /*Print the postfix expression for an expression tree. Pre : tree is a pointer to an expression tree Post: the postfix expression has been printed*/ if (tree not empty) postfix (tree left subtree) postfix (tree right subtree) print (tree token) end if end postfix Prefix traversal The prefix expression formed by prefix traversal uses the standard pre-order tree traversal. No parentheses are necessary. Pseudocode: Algorithm prefix (tree) /*Print the prefix expression for an expression tree. Pre : tree is a pointer to an expression tree Post: the prefix expression has been printed*/ if (tree not empty) print (tree token) prefix (tree left subtree) prefix (tree right subtree) end if end prefix Construction of an expression tree The evaluation of the tree takes place by reading the postfix expression one symbol at a time. If the symbol is an operand, a one-node tree is created and its pointer is pushed onto a stack . If the symbol is an operator, the pointers to two trees T1 and T2 are popped from the stack and a new tree whose root is the operator and whose left and right children point to T2 and T1 respectively is formed . A pointer to this new tree is then pushed to the Stack.[ 4]","title":"Introduction"},{"location":"Guide/Expression-tree/#binary#expression#tree","text":"\u5173\u4e8e expression tree\uff0c\u9700\u8981\u601d\u8003\u5982\u4e0b\u95ee\u9898: 1\u3001how to evaluate expression tree\uff1f 2\u3001how to construct an expression tree of an expression\uff1f 3\u3001how to construct an expression of an expression tree\uff1f 4\u3001operator precedence and construction of expression tree 5\u3001 Operator associativity and construction of expression tree","title":"Binary expression tree"},{"location":"Guide/Expression-tree/#expression#tree#ast","text":"\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cexpression tree \u662f AST\u3002","title":"Expression tree \u662f AST"},{"location":"Guide/Expression-tree/#wikipedia#binary#expression#tree","text":"A binary expression tree is a specific kind of a binary tree used to represent expressions. Two common types of expressions that a binary expression tree can represent are algebraic [ 1] and boolean . These trees can represent expressions that contain both unary and binary operators.[ 1] Each node of a binary tree , and hence of a binary expression tree , has zero, one, or two children. This restricted structure simplifies the processing of expression trees . NOTE: : \u4e8c\u53c9\u6811\u80fd\u591f\u6ee1\u8db3 unary and binary operators\u6709\u4e00\u4e2a\u6216\u4e24\u4e2aoperands\u7684\u9700\u6c42\uff1b","title":"wikipedia Binary expression tree"},{"location":"Guide/Expression-tree/#overview","text":"The leaves of a binary expression tree are operands , such as constants or variable names, and the other nodes contain operators . These particular trees happen to be binary, because all of the operations are binary, and although this is the simplest case, it is possible for nodes to have more than two children. It is also possible for a node to have only one child, as is the case with the unary minus operator. An expression tree, T , can be evaluated by applying the operator at the root to the values obtained by recursively evaluating the left and right subtrees. NOTE: \u5173\u4e8eEvaluation of Expression Tree\u53c2\u89c1 Evaluation \u7ae0\u8282","title":"Overview"},{"location":"Guide/Expression-tree/#traversal","text":"NOTE: : \u5bf9binary expression tree\u8fdb\u884ctraverse\u4ece\u800c\u83b7\u5f97\u5bf9\u5e94\u7684algebraic expression\u3002","title":"Traversal"},{"location":"Guide/Expression-tree/#infix#traversal","text":"NOTE: \u5176\u5b9e\u5c31\u662finorder traversal When an infix expression is printed, an opening and closing parenthesis must be added at the beginning and ending of each expression. As every subtree represents a subexpression , an opening parenthesis is printed at its start and the closing parenthesis is printed after processing all of its children. Pseudocode: Algorithm infix (tree) /*Print the infix expression for an expression tree. Pre : tree is a pointer to an expression tree Post: the infix expression has been printed*/ if (tree not empty) if (tree token is operator) print (open parenthesis) end if infix (tree left subtree) print (tree token) infix (tree right subtree) if (tree token is operator) print (close parenthesis) end if end if end infix","title":"Infix traversal"},{"location":"Guide/Expression-tree/#postfix#traversal","text":"The postfix expression is formed by the basic postorder traversal of any binary tree. It does not require parentheses. Pseudocode: Algorithm postfix (tree) /*Print the postfix expression for an expression tree. Pre : tree is a pointer to an expression tree Post: the postfix expression has been printed*/ if (tree not empty) postfix (tree left subtree) postfix (tree right subtree) print (tree token) end if end postfix","title":"Postfix traversal"},{"location":"Guide/Expression-tree/#prefix#traversal","text":"The prefix expression formed by prefix traversal uses the standard pre-order tree traversal. No parentheses are necessary. Pseudocode: Algorithm prefix (tree) /*Print the prefix expression for an expression tree. Pre : tree is a pointer to an expression tree Post: the prefix expression has been printed*/ if (tree not empty) print (tree token) prefix (tree left subtree) prefix (tree right subtree) end if end prefix","title":"Prefix traversal"},{"location":"Guide/Expression-tree/#construction#of#an#expression#tree","text":"The evaluation of the tree takes place by reading the postfix expression one symbol at a time. If the symbol is an operand, a one-node tree is created and its pointer is pushed onto a stack . If the symbol is an operator, the pointers to two trees T1 and T2 are popped from the stack and a new tree whose root is the operator and whose left and right children point to T2 and T1 respectively is formed . A pointer to this new tree is then pushed to the Stack.[ 4]","title":"Construction of an expression tree"},{"location":"Guide/Expression-tree/binary-expression-tree-and-operator-associativity/","text":"\u5bf9\u4e00\u4e2a\u6211\u4eec\u5e73\u5e38\u624b\u5199\u7684math expression\uff08infix format\uff09\uff0c operator associativity \u5bf9expression\u7684\u542b\u4e49\u7684\u89e3\u91ca\u662f\u975e\u5e38\u91cd\u8981\u7684\uff0c\u6bd4\u5982\uff1a \u6211\u4eec\u77e5\u9053 - \u662fleft associative\u7684\uff0c\u6240\u4ee5 1-2-3 \u5c31\u9700\u8981\u5148\u8ba1\u7b97 1-2 \uff0c\u800c\u4e0d\u662f\u5148\u8ba1\u7b97 2-3 \uff0c\u663e\u7136\uff0cexpression\uff08infix format\uff09 1-2-3 \u53ea\u5bf9\u5e94\u4e86\u4e00\u68f5binary expression tree\u3002 \u6211\u4eec\u77e5\u9053 = \u662fright associative\u7684\uff0c\u6240\u4ee5 a=b=c \u5c31\u9700\u8981\u5148\u8ba1\u7b97 b=c \uff0c\u800c\u4e0d\u662f\u5148\u8ba1\u7b97 a=b \uff0c\u663e\u7136\uff0cexpression\uff08infix format\uff09 a=b=c \u53ea\u5bf9\u5e94\u4e86\u4e00\u68f5binary expression tree\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6709\u4e9b\u8fd0\u7b97\u7b26\u4e0d\u6b62\u4e00\u79cdassociativity\uff0c\u6bd4\u5982\uff1a\u4e58\u6cd5\u8fd0\u7b97 * \u548c\u52a0\u6cd5\u8fd0\u7b97 + are both left and right associative\uff0c\u6240\u4ee5\u5305\u542b\u5b83\u4eec\u7684expression\uff08infix format\uff09\u53ef\u80fd\u5bf9\u5e94\u4e0d\u6b62\u4e00\u68f5binary expression tree\uff0c\u6bcf\u4e00\u68f5binary expression tree\u5bf9\u5e94\u4e86\u4e00\u79cd\u8ba1\u7b97\u6b21\u5e8f\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\u4e0d\u540c\u7684\u8ba1\u7b97\u6b21\u5e8f\uff0c\u5373\u4e0d\u540c\u7684binary expression tree\uff0c\u6240\u5bf9\u5e94\u7684\u8ba1\u7b97\u6210\u672c\u662f\u4e0d\u540c\u7684\uff0c\u5173\u4e8e\u8fd9\u7684\u4e00\u4e2a\u5178\u578b\u95ee\u9898\u5c31\u662f matrix chain multiplication \u95ee\u9898\uff1b \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5728\u4e0a\u8ff0\u8868\u8ff0\u4e2d\uff0c\u6211\u4f7f\u7528\u4e86\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a \u6bcf\u4e00\u68f5**binary expression tree**\u5bf9\u5e94\u4e86\u4e00\u79cd**\u8ba1\u7b97\u6b21\u5e8f** \u663e\u7136\uff0c\u5bf9\u4e8e\u4e00\u4e2aexpression\uff08infix format\uff09\uff0c\u5b83\u7684\u8ba1\u7b97\u6b21\u5e8f\u53ef\u80fd\u662f\u4e0d\u56fa\u5b9a\u7684\uff0c\u4f46\u662f\u4e00\u65e6\u5c06\u5b83\u8f6c\u6362\u4e3a\u4e00\u4e2abinary expression tree\uff0c\u5219\u5b83\u7684\u8ba1\u7b97\u6b21\u5e8f\u5c31\u56fa\u5b9a\u4e86\uff0c\u8fd9\u662f\u56e0\u4e3abinary expression tree\u7684\u7ed3\u6784\u51b3\u5b9a\u7684\uff1a\u6bcf\u4e2a\u5185\u8282\u70b9\u5bf9\u5e94\u4e86\u4e00\u4e2aoperator\uff0c\u6bcf\u4e2a\u53f6\u5b50\u8282\u70b9\u5bf9\u5e94\u4e86\u4e00\u4e2aoperand\uff0c\u901a\u8fc7\u5185\u8282\u70b9\u5c06\u5404\u4e2a\u53f6\u5b50\u8282\u70b9\u7ed9\u8fde\u63a5\u8d77\u6765\uff0c\u5176\u5b9e\u5c31\u51b3\u5b9a\u4e86expression\u7684\u89e3\u91ca\u4e86\uff0c\u8fd9\u5c31\u662fbinary expression tree\u7684\u6784\u9020\u8fc7\u7a0b\u3002 \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0cexpression\uff08prefix format\uff09\u548cexpression\uff08postfix format\uff09\u4e0d\u5b58\u5728expression\uff08prefix format\uff09\u7684\u95ee\u9898\uff0c\u5b83\u4eec\u7684\u8ba1\u7b97\u6b21\u5e8f\u662f\u56fa\u5b9a\u7684\uff1b \u90a3\u5982\u4f55\u5b9e\u73b0binary expression tree\u7684\u6784\u9020\u5462\uff1f\u5176\u5b9e\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u590d\u6742\u7684\uff1b Precedence and Associativity http://www.cs.ecu.edu/karl/5220/spr16/Notes/CFG/precedence.html","title":"Binary expression tree and operator associativity"},{"location":"Guide/Expression-tree/binary-expression-tree-and-operator-associativity/#precedence#and#associativity","text":"http://www.cs.ecu.edu/karl/5220/spr16/Notes/CFG/precedence.html","title":"Precedence and Associativity"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/","text":"Build && evaluate build \u548c evaluate\u672c\u8d28\u4e0a\u662f\u975e\u5e38\u7c7b\u4f3c\u7684: \u4e00\u3001\u90fd\u4e0eAST\u76f8\u5173 \"evaluate\"\u7684\u8fc7\u7a0b\u5176\u5b9e\u662f\u5bf9AST\u7684traversal \u4e8c\u3001\u4e24\u4e2a\u65b9\u5411: 1\u3001\u81ea\u5e95\u5411\u4e0a 2\u3001\u81ea\u9876\u5411\u4e0b build\u3001construct \u5982\u4f55\u6839\u636eexpression\u6784\u5efaexpression tree\uff1f\u53c2\u8003\u6587\u7ae0: \u4e00\u3001wikipedia Binary expression tree \u4e8c\u3001geeksforgeeks Expression Tree reverse polish postfix notation expression\u3001\u81ea\u5e95\u5411\u4e0a(\u4f7f\u7528stack)\u6784\u5efaexpression tree\u7684\u793a\u4f8b\u4ee3\u7801\u3002 \u4e09\u3001geeksforgeeks Building Expression tree from Prefix Expression polish prefix notation expression\u3001\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b evaluate evaluation of expression tree \u4e00\u3001geeksforgeeks Expression Tree \u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value \u4e8c\u3001geeksforgeeks Evaluation of Expression Tree \u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value evaluation of expression \u4e00\u3001infogalactic Polish notation \u81ea\u53f3\u5411\u5de6\u3001\u81ea\u5e95\u5411\u4e0a(\u4f7f\u7528stack) \u4e8c\u3001infogalactic Reverse Polish notation \u81ea\u53f3\u5411\u5de6\u3001\u81ea\u5e95\u5411\u4e0a(\u4f7f\u7528stack) \u4e09\u3001leetcode 150. \u9006\u6ce2\u5170\u8868\u8fbe\u5f0f\u6c42\u503c Post-order traversal\u3001dependency\u3001stack \u8868\u8fbe\u5f0f\u6811\u4e5f\u662f\u6811\uff0c\u65e2\u7136\u662f\u6811\uff0c\u90a3\u4e48\u5b83\u7684\u7ed3\u6784\u5c31\u5177\u5907\u9012\u5f52\u6027 \u8868\u8fbe\u5f0f\u6811**\u7684evaluation\u4e00\u5b9a\u662f**\u6df1\u5ea6\u4f18\u5148**\u7684\uff0c\u4e0d\u53ef\u4ee5\u662f**\u5e7f\u5ea6\u4f18\u5148 \uff1b\u56e0\u4e3a\u53ea\u6709\u5b50\u6811\u7684\u7ed3\u679c\u90fd\u8ba1\u7b97\u51fa\u6765\u4e86\uff0c\u624d\u80fd\u591f\u8ba1\u7b97\u5f97\u5230\u8fd9\u4e9b\u5b50\u6811\u7684\u7236\u8282\u70b9\u7684\u503c\uff1b\u5982\u679c\u4ece\u904d\u5386\u7684\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u5b83\u6240\u5bf9\u5e94\u7684\u662f\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u7684\u540e\u5e8f\u904d\u5386\u3002 Evaluation TODO geeksforgeeks Evaluation of Expression Tree stackoverflow Evaluating expression trees geeksforgeeks Expression Evaluation geeksforgeeks Stack | Set 4 (Evaluation of Postfix Expression) codechef Expression Tree Construction and Evaluation codeproject Binary Tree Expression Solver","title":"Introduction"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/#build#evaluate","text":"build \u548c evaluate\u672c\u8d28\u4e0a\u662f\u975e\u5e38\u7c7b\u4f3c\u7684: \u4e00\u3001\u90fd\u4e0eAST\u76f8\u5173 \"evaluate\"\u7684\u8fc7\u7a0b\u5176\u5b9e\u662f\u5bf9AST\u7684traversal \u4e8c\u3001\u4e24\u4e2a\u65b9\u5411: 1\u3001\u81ea\u5e95\u5411\u4e0a 2\u3001\u81ea\u9876\u5411\u4e0b","title":"Build &amp;&amp; evaluate"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/#buildconstruct","text":"\u5982\u4f55\u6839\u636eexpression\u6784\u5efaexpression tree\uff1f\u53c2\u8003\u6587\u7ae0: \u4e00\u3001wikipedia Binary expression tree \u4e8c\u3001geeksforgeeks Expression Tree reverse polish postfix notation expression\u3001\u81ea\u5e95\u5411\u4e0a(\u4f7f\u7528stack)\u6784\u5efaexpression tree\u7684\u793a\u4f8b\u4ee3\u7801\u3002 \u4e09\u3001geeksforgeeks Building Expression tree from Prefix Expression polish prefix notation expression\u3001\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b","title":"build\u3001construct"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/#evaluate","text":"","title":"evaluate"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/#evaluation#of#expression#tree","text":"\u4e00\u3001geeksforgeeks Expression Tree \u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value \u4e8c\u3001geeksforgeeks Evaluation of Expression Tree \u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value","title":"evaluation of expression tree"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/#evaluation#of#expression","text":"\u4e00\u3001infogalactic Polish notation \u81ea\u53f3\u5411\u5de6\u3001\u81ea\u5e95\u5411\u4e0a(\u4f7f\u7528stack) \u4e8c\u3001infogalactic Reverse Polish notation \u81ea\u53f3\u5411\u5de6\u3001\u81ea\u5e95\u5411\u4e0a(\u4f7f\u7528stack) \u4e09\u3001leetcode 150. \u9006\u6ce2\u5170\u8868\u8fbe\u5f0f\u6c42\u503c","title":"evaluation of expression"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/#post-order#traversaldependencystack","text":"\u8868\u8fbe\u5f0f\u6811\u4e5f\u662f\u6811\uff0c\u65e2\u7136\u662f\u6811\uff0c\u90a3\u4e48\u5b83\u7684\u7ed3\u6784\u5c31\u5177\u5907\u9012\u5f52\u6027 \u8868\u8fbe\u5f0f\u6811**\u7684evaluation\u4e00\u5b9a\u662f**\u6df1\u5ea6\u4f18\u5148**\u7684\uff0c\u4e0d\u53ef\u4ee5\u662f**\u5e7f\u5ea6\u4f18\u5148 \uff1b\u56e0\u4e3a\u53ea\u6709\u5b50\u6811\u7684\u7ed3\u679c\u90fd\u8ba1\u7b97\u51fa\u6765\u4e86\uff0c\u624d\u80fd\u591f\u8ba1\u7b97\u5f97\u5230\u8fd9\u4e9b\u5b50\u6811\u7684\u7236\u8282\u70b9\u7684\u503c\uff1b\u5982\u679c\u4ece\u904d\u5386\u7684\u89d2\u5ea6\u6765\u770b\u7684\u8bdd\uff0c\u5b83\u6240\u5bf9\u5e94\u7684\u662f\u6df1\u5ea6\u4f18\u5148\u904d\u5386\u7684\u540e\u5e8f\u904d\u5386\u3002","title":"Post-order traversal\u3001dependency\u3001stack"},{"location":"Guide/Expression-tree/Build-evaluate-expression-tree/#evaluation#todo","text":"geeksforgeeks Evaluation of Expression Tree stackoverflow Evaluating expression trees geeksforgeeks Expression Evaluation geeksforgeeks Stack | Set 4 (Evaluation of Postfix Expression) codechef Expression Tree Construction and Evaluation codeproject Binary Tree Expression Solver","title":"Evaluation TODO"},{"location":"Guide/Expression-tree/LeetCode/","text":"","title":"Introduction"},{"location":"Guide/Expression-tree/LeetCode/LeetCode-150-%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B1%82%E5%80%BC/","text":"leetcode 150. \u9006\u6ce2\u5170\u8868\u8fbe\u5f0f\u6c42\u503c \u6211\u7684\u89e3\u9898 class Solution { public : int evalRPN ( vector < string > & tokens ) { stack < int > st ; for ( auto && token : tokens ) { if ( isOperator ( token )) { int operand2 = st . top (); st . pop (); int operand1 = st . top (); st . pop (); st . push ( cal ( operand1 , operand2 , token )); } else { st . push ( stoi ( token )); } } return st . top (); } bool isOperator ( string & s ) { return s == \"+\" || s == \"-\" || s == \"*\" || s == \"/\" ; } int cal ( int operand1 , int operand2 , string & Operator ) { if ( Operator == \"+\" ) { return operand1 + operand2 ; } else if ( Operator == \"-\" ) { return operand1 - operand2 ; } else if ( Operator == \"*\" ) { return operand1 * operand2 ; } else if ( Operator == \"/\" ) { return operand1 / operand2 ; } else { return 0 ; } } };","title":"Introduction"},{"location":"Guide/Expression-tree/LeetCode/LeetCode-150-%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B1%82%E5%80%BC/#leetcode#150","text":"","title":"leetcode 150. \u9006\u6ce2\u5170\u8868\u8fbe\u5f0f\u6c42\u503c"},{"location":"Guide/Expression-tree/LeetCode/LeetCode-150-%E9%80%86%E6%B3%A2%E5%85%B0%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B1%82%E5%80%BC/#_1","text":"class Solution { public : int evalRPN ( vector < string > & tokens ) { stack < int > st ; for ( auto && token : tokens ) { if ( isOperator ( token )) { int operand2 = st . top (); st . pop (); int operand1 = st . top (); st . pop (); st . push ( cal ( operand1 , operand2 , token )); } else { st . push ( stoi ( token )); } } return st . top (); } bool isOperator ( string & s ) { return s == \"+\" || s == \"-\" || s == \"*\" || s == \"/\" ; } int cal ( int operand1 , int operand2 , string & Operator ) { if ( Operator == \"+\" ) { return operand1 + operand2 ; } else if ( Operator == \"-\" ) { return operand1 - operand2 ; } else if ( Operator == \"*\" ) { return operand1 * operand2 ; } else if ( Operator == \"/\" ) { return operand1 / operand2 ; } else { return 0 ; } } };","title":"\u6211\u7684\u89e3\u9898"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/","text":"Mathematical Expression Parsing stackoverflow Equation (expression) parser with precedence? codeproject Simple Guide to Mathematical Expression Parsing","title":"Introduction"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/#mathematical#expression#parsing","text":"","title":"Mathematical Expression Parsing"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/#stackoverflow#equation#expression#parser#with#precedence","text":"","title":"stackoverflow Equation (expression) parser with precedence?"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/#codeproject#simple#guide#to#mathematical#expression#parsing","text":"","title":"codeproject Simple Guide to Mathematical Expression Parsing"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/Operator-precedence-parser/","text":"Operator-precedence parser wikipedia Operator-precedence parser In computer science , an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar . For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations to a format that is optimized for evaluation such as Reverse Polish notation (RPN). Edsger Dijkstra 's shunting yard algorithm is commonly used to implement operator precedence parsers. Other algorithms include the precedence climbing method and the top down operator precedence method . Shunting yard algorithm \u53c2\u89c1 Guide\\Expression-tree\\Shunting-yard-algorithm \u3002","title":"Introduction"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/Operator-precedence-parser/#operator-precedence#parser","text":"","title":"Operator-precedence parser"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/Operator-precedence-parser/#wikipedia#operator-precedence#parser","text":"In computer science , an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar . For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations to a format that is optimized for evaluation such as Reverse Polish notation (RPN). Edsger Dijkstra 's shunting yard algorithm is commonly used to implement operator precedence parsers. Other algorithms include the precedence climbing method and the top down operator precedence method .","title":"wikipedia Operator-precedence parser"},{"location":"Guide/Expression-tree/Mathematical-Expression-Parsing/Operator-precedence-parser/#shunting#yard#algorithm","text":"\u53c2\u89c1 Guide\\Expression-tree\\Shunting-yard-algorithm \u3002","title":"Shunting yard algorithm"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/","text":"prefix\u3001infix\u3001postfix notation \u94fe\u63a5 prefix Polish notation Prefix notation (\"Polish\") infix Infix notation postfix Reverse Polish notation Postfix notation (\"Reverse Polish\") AST traversal notation AST traversal prefix pre-order infix in-order postfix post-order Conversion Infix-to-Postfix Postfix-to-Prefix Prefix-to-Infix Build AST \u53c2\u89c1 Build-AST \u7ae0\u8282\u3002","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/#prefixinfixpostfix","text":"notation \u94fe\u63a5 prefix Polish notation Prefix notation (\"Polish\") infix Infix notation postfix Reverse Polish notation Postfix notation (\"Reverse Polish\")","title":"prefix\u3001infix\u3001postfix"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/#ast#traversal","text":"notation AST traversal prefix pre-order infix in-order postfix post-order","title":"AST traversal"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/#conversion","text":"Infix-to-Postfix Postfix-to-Prefix Prefix-to-Infix","title":"Conversion"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/#build#ast","text":"\u53c2\u89c1 Build-AST \u7ae0\u8282\u3002","title":"Build AST"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-notation/","text":"Infix notation \"infix notation\" \u5373 \"\u4e2d\u7f00\u8868\u8fbe\u5f0f\"\uff0c\u8fd9\u662f\u6211\u4eec\u5e73\u65f6\u4e66\u5199\u7684\u65f6\u5019\u4f7f\u7528\u7684\u65b9\u5f0f\u3002 infogalactic Infix notation Infix notation is the notation commonly used in arithmetical and logical formulae and statements. It is characterized by the placement of operators between operands \u2013 \" infixed operators\" \u2013 such as the plus sign in \"2 + 2\". Usage Infix notation is more difficult to parse by computers than prefix notation ( e.g. + 2 2 ) or postfix notation ( e.g. 2 2 + ). However many programming languages use it due to its familiarity. It is more used in arithmetic, e.g. 2+2, 5\u00d76. Shunting yard algorithm Shunting yard algorithm , used to convert infix notation to postfix notation or to a tree","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-notation/#infix#notation","text":"\"infix notation\" \u5373 \"\u4e2d\u7f00\u8868\u8fbe\u5f0f\"\uff0c\u8fd9\u662f\u6211\u4eec\u5e73\u65f6\u4e66\u5199\u7684\u65f6\u5019\u4f7f\u7528\u7684\u65b9\u5f0f\u3002","title":"Infix notation"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-notation/#infogalactic#infix#notation","text":"Infix notation is the notation commonly used in arithmetical and logical formulae and statements. It is characterized by the placement of operators between operands \u2013 \" infixed operators\" \u2013 such as the plus sign in \"2 + 2\".","title":"infogalactic Infix notation"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-notation/#usage","text":"Infix notation is more difficult to parse by computers than prefix notation ( e.g. + 2 2 ) or postfix notation ( e.g. 2 2 + ). However many programming languages use it due to its familiarity. It is more used in arithmetic, e.g. 2+2, 5\u00d76.","title":"Usage"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-notation/#shunting#yard#algorithm","text":"Shunting yard algorithm , used to convert infix notation to postfix notation or to a tree","title":"Shunting yard algorithm"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/","text":"","title":"Index"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/Shunting-yard-algorithm/","text":"Shunting-yard algorithm wikipedia Shunting-yard algorithm NOTE: \u8c03\u5ea6\u573a\u7b97\u6cd5 In computer science , the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation . It can produce either a postfix notation string, also known as Reverse Polish notation (RPN), or an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the \"shunting yard\"\uff08\u8c03\u8f66\u573a\uff09 algorithm because its operation resembles that of a railroad shunting yard . Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61 . Like the evaluation of RPN\uff08 Reverse Polish notation \uff09, the shunting yard algorithm is stack -based. Infix expressions are the form of mathematical notation most people are used to, for instance \"3 + 4\" or \"3 + 4 \u00d7 (2 \u2212 1)\". For the conversion there are two text variables ( strings ), the input and the output. There is also a stack that holds operators not yet added to the output queue . To convert, the program reads each symbol in order and does something based on that symbol. The result for the above examples would be (in Reverse Polish notation ) \"3 4 +\" and \"3 4 2 1 \u2212 \u00d7 +\", respectively. The shunting-yard algorithm was later generalized\uff08\u6cdb\u5316\uff09 into operator-precedence parsing . A simple conversion Input: 3 + 4 Push 3 to the output queue (whenever a number is read it is pushed to the output) Push + (or its ID) onto the operator stack Push 4 to the output queue After reading the expression, pop the operators off the stack and add them to the output . In this case there is only one, \"+\". Output: 3 4 + This already shows a couple of rules: All numbers are pushed to the output when they are read. At the end of reading the expression, pop all operators off the stack and onto the output. NOTE: \u65e0\u8bba\u54ea\u79cd\u8868\u8fbe\u5f0f\uff0c\u5b83\u4eec\u7684operand\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u5404\u79cd\u8868\u8fbe\u5f0f\u7684\u533a\u522b\u5c31\u5728\u4e8e\u5b83\u4eec\u7684operator\u7684\u4f4d\u7f6e\u4e0d\u540c\uff0c\u5176\u5b9e\u8be5\u7b97\u6cd5\u6240\u505a\u7684\u662f\u51b3\u5b9a\u4f55\u65f6\u5c06operator\u6dfb\u52a0\u5230output\u4e2d\uff0c\u5b83\u6240\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u57fa\u4e8eoperator\u7684precedence\u8fdb\u884c\u6bd4\u8f83\uff0coperator stack\u6709precedence\u7684\u6bd4\u8f83\uff0c\u540c\u65f6\u4e5f\u8003\u8651\u4e86associative\uff1b\u7531\u4e8e\u5b83\u9700\u8981\u8f6c\u6362\u4e3apostfix\uff0c\u6240\u4ee5operator\u770b\u5230\u662f\u653e\u5230operand\u7684\u540e\u9762\u7684\uff0c\u5f53\u4f18\u5148\u7ea7\u66f4\u9ad8\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u51fa\u6808\uff0c\u6dfb\u52a0\u5230output\u4e2d\uff1b\u8fd8\u9700\u8981\u8003\u8651\u62ec\u53f7\u7684\u60c5\u51b5\uff0c\u5176\u5b9e\u53ef\u4ee5\u8fd9\u6837\u6765\u770b\u5f85\u62ec\u53f7\uff0c\u62ec\u53f7\u5176\u5b9e\u662f\u4e00\u79cd\u9694\u79bb\uff0c\u5c06\u62ec\u53f7\u5185\u7684operator\u7684stack\u548c\u62ec\u53f7\u5916\u7684operator\u7684stack\u9694\u79bb\u5f00\u6765\u4e86\uff1b Graphical illustration Graphical illustration of algorithm, using a three-way railroad junction \uff08\u4e09\u65b9\u94c1\u8def\u67a2\u7ebd\uff09. The input is processed one symbol at a time: if a variable or number is found, it is copied directly to the output a), c), e), h). If the symbol is an operator , it is pushed onto the operator stack b), d), f). If the operator's precedence is less than that of the operators at the top of the stack or the precedences are equal and the operator is left associative , then that operator is popped off the stack and added to the output g). Finally, any remaining operators are popped off the stack and added to the output i). NOTE: \u5982\u679c\u662fleft associative\uff08\u5982\u9664\u6cd5\uff0c\u51cf\u6cd5\uff09\uff0c\u5219\u4f1a The algorithm in detail Important terms: Token , Function , Operator associativity , Precedence /* This implementation does not implement composite functions,functions with variable number of arguments, and unary operators. */ while there are tokens to be read do: read a token. if the token is a number, then: push it to the output queue. if the token is a function then: push it onto the operator stack if the token is an operator, then: while ((there is a function at the top of the operator stack) or (there is an operator at the top of the operator stack with greater precedence) or (the operator at the top of the operator stack has equal precedence and is left associative)) and (the operator at the top of the operator stack is not a left parenthesis): pop operators from the operator stack onto the output queue. push it onto the operator stack. if the token is a left paren (i.e. \"(\"), then: push it onto the operator stack. if the token is a right paren (i.e. \")\"), then: while the operator at the top of the operator stack is not a left paren: pop the operator from the operator stack onto the output queue. /* if the stack runs out without finding a left paren, then there are mismatched parentheses. */ if there is a left paren at the top of the operator stack, then: pop the operator from the operator stack and discard it after while loop, if operator stack not null, pop everything to output queue if there are no more tokens to read then: while there are still operator tokens on the stack: /* if the operator token on the top of the stack is a paren, then there are mismatched parentheses. */ pop the operator from the operator stack onto the output queue. exit. To analyze the running time complexity of this algorithm, one has only to note that each token will be read once, each number, function, or operator will be printed once, and each function, operator, or parenthesis will be pushed onto the stack and popped off the stack once\u2014therefore, there are at most a constant number of operations executed per token, and the running time is thus O( n )\u2014linear in the size of the input. The shunting yard algorithm can also be applied to produce prefix notation (also known as Polish notation ). To do this one would simply start from the end of a string of tokens to be parsed and work backwards, reverse the output queue (therefore making the output queue an output stack), and flip the left and right parenthesis behavior (remembering that the now-left parenthesis behavior should pop until it finds a now-right parenthesis). And changing the associativity condition to right. TODO stackoverflow Algorithm for converting expression to binary tree [closed] codeproject Binary Tree Expression Solver cnblogs shunting-yard \u8c03\u5ea6\u573a\u7b97\u6cd5\u3001\u4e2d\u7f00\u8868\u8fbe\u5f0f\u8f6c\u9006\u6ce2\u5170\u8868\u8fbe\u5f0f geeksforgeeks Program to convert Infix notation to Expression Tree","title":"Shunting-yard algorithm"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/Shunting-yard-algorithm/#shunting-yard#algorithm","text":"","title":"Shunting-yard algorithm"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/Shunting-yard-algorithm/#wikipedia#shunting-yard#algorithm","text":"NOTE: \u8c03\u5ea6\u573a\u7b97\u6cd5 In computer science , the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation . It can produce either a postfix notation string, also known as Reverse Polish notation (RPN), or an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the \"shunting yard\"\uff08\u8c03\u8f66\u573a\uff09 algorithm because its operation resembles that of a railroad shunting yard . Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61 . Like the evaluation of RPN\uff08 Reverse Polish notation \uff09, the shunting yard algorithm is stack -based. Infix expressions are the form of mathematical notation most people are used to, for instance \"3 + 4\" or \"3 + 4 \u00d7 (2 \u2212 1)\". For the conversion there are two text variables ( strings ), the input and the output. There is also a stack that holds operators not yet added to the output queue . To convert, the program reads each symbol in order and does something based on that symbol. The result for the above examples would be (in Reverse Polish notation ) \"3 4 +\" and \"3 4 2 1 \u2212 \u00d7 +\", respectively. The shunting-yard algorithm was later generalized\uff08\u6cdb\u5316\uff09 into operator-precedence parsing .","title":"wikipedia Shunting-yard algorithm"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/Shunting-yard-algorithm/#a#simple#conversion","text":"Input: 3 + 4 Push 3 to the output queue (whenever a number is read it is pushed to the output) Push + (or its ID) onto the operator stack Push 4 to the output queue After reading the expression, pop the operators off the stack and add them to the output . In this case there is only one, \"+\". Output: 3 4 + This already shows a couple of rules: All numbers are pushed to the output when they are read. At the end of reading the expression, pop all operators off the stack and onto the output. NOTE: \u65e0\u8bba\u54ea\u79cd\u8868\u8fbe\u5f0f\uff0c\u5b83\u4eec\u7684operand\u7684\u987a\u5e8f\u662f\u76f8\u540c\u7684\uff0c\u5404\u79cd\u8868\u8fbe\u5f0f\u7684\u533a\u522b\u5c31\u5728\u4e8e\u5b83\u4eec\u7684operator\u7684\u4f4d\u7f6e\u4e0d\u540c\uff0c\u5176\u5b9e\u8be5\u7b97\u6cd5\u6240\u505a\u7684\u662f\u51b3\u5b9a\u4f55\u65f6\u5c06operator\u6dfb\u52a0\u5230output\u4e2d\uff0c\u5b83\u6240\u91c7\u7528\u7684\u65b9\u5f0f\u662f\u57fa\u4e8eoperator\u7684precedence\u8fdb\u884c\u6bd4\u8f83\uff0coperator stack\u6709precedence\u7684\u6bd4\u8f83\uff0c\u540c\u65f6\u4e5f\u8003\u8651\u4e86associative\uff1b\u7531\u4e8e\u5b83\u9700\u8981\u8f6c\u6362\u4e3apostfix\uff0c\u6240\u4ee5operator\u770b\u5230\u662f\u653e\u5230operand\u7684\u540e\u9762\u7684\uff0c\u5f53\u4f18\u5148\u7ea7\u66f4\u9ad8\u7684\u65f6\u5019\uff0c\u5c31\u9700\u8981\u51fa\u6808\uff0c\u6dfb\u52a0\u5230output\u4e2d\uff1b\u8fd8\u9700\u8981\u8003\u8651\u62ec\u53f7\u7684\u60c5\u51b5\uff0c\u5176\u5b9e\u53ef\u4ee5\u8fd9\u6837\u6765\u770b\u5f85\u62ec\u53f7\uff0c\u62ec\u53f7\u5176\u5b9e\u662f\u4e00\u79cd\u9694\u79bb\uff0c\u5c06\u62ec\u53f7\u5185\u7684operator\u7684stack\u548c\u62ec\u53f7\u5916\u7684operator\u7684stack\u9694\u79bb\u5f00\u6765\u4e86\uff1b","title":"A simple conversion"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/Shunting-yard-algorithm/#graphical#illustration","text":"Graphical illustration of algorithm, using a three-way railroad junction \uff08\u4e09\u65b9\u94c1\u8def\u67a2\u7ebd\uff09. The input is processed one symbol at a time: if a variable or number is found, it is copied directly to the output a), c), e), h). If the symbol is an operator , it is pushed onto the operator stack b), d), f). If the operator's precedence is less than that of the operators at the top of the stack or the precedences are equal and the operator is left associative , then that operator is popped off the stack and added to the output g). Finally, any remaining operators are popped off the stack and added to the output i). NOTE: \u5982\u679c\u662fleft associative\uff08\u5982\u9664\u6cd5\uff0c\u51cf\u6cd5\uff09\uff0c\u5219\u4f1a","title":"Graphical illustration"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/Shunting-yard-algorithm/#the#algorithm#in#detail","text":"Important terms: Token , Function , Operator associativity , Precedence /* This implementation does not implement composite functions,functions with variable number of arguments, and unary operators. */ while there are tokens to be read do: read a token. if the token is a number, then: push it to the output queue. if the token is a function then: push it onto the operator stack if the token is an operator, then: while ((there is a function at the top of the operator stack) or (there is an operator at the top of the operator stack with greater precedence) or (the operator at the top of the operator stack has equal precedence and is left associative)) and (the operator at the top of the operator stack is not a left parenthesis): pop operators from the operator stack onto the output queue. push it onto the operator stack. if the token is a left paren (i.e. \"(\"), then: push it onto the operator stack. if the token is a right paren (i.e. \")\"), then: while the operator at the top of the operator stack is not a left paren: pop the operator from the operator stack onto the output queue. /* if the stack runs out without finding a left paren, then there are mismatched parentheses. */ if there is a left paren at the top of the operator stack, then: pop the operator from the operator stack and discard it after while loop, if operator stack not null, pop everything to output queue if there are no more tokens to read then: while there are still operator tokens on the stack: /* if the operator token on the top of the stack is a paren, then there are mismatched parentheses. */ pop the operator from the operator stack onto the output queue. exit. To analyze the running time complexity of this algorithm, one has only to note that each token will be read once, each number, function, or operator will be printed once, and each function, operator, or parenthesis will be pushed onto the stack and popped off the stack once\u2014therefore, there are at most a constant number of operations executed per token, and the running time is thus O( n )\u2014linear in the size of the input. The shunting yard algorithm can also be applied to produce prefix notation (also known as Polish notation ). To do this one would simply start from the end of a string of tokens to be parsed and work backwards, reverse the output queue (therefore making the output queue an output stack), and flip the left and right parenthesis behavior (remembering that the now-left parenthesis behavior should pop until it finds a now-right parenthesis). And changing the associativity condition to right.","title":"The algorithm in detail"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Infix-to-Postfix/Shunting-yard-algorithm/#todo","text":"stackoverflow Algorithm for converting expression to binary tree [closed] codeproject Binary Tree Expression Solver cnblogs shunting-yard \u8c03\u5ea6\u573a\u7b97\u6cd5\u3001\u4e2d\u7f00\u8868\u8fbe\u5f0f\u8f6c\u9006\u6ce2\u5170\u8868\u8fbe\u5f0f geeksforgeeks Program to convert Infix notation to Expression Tree","title":"TODO"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Polish-prefix-notation/","text":"Polish prefix notation \"infix notation\" \u5373 \"\u4e2d\u7f00\u8868\u8fbe\u5f0f\"\uff0c\"polish notation\" \u5373 \"\u6ce2\u5170\u5f0f\"\u3002 build && evaluate \u4e00\u3001evaluate \u5728 infogalactic Polish notation \u4e2d\uff0c\u7ed9\u51fa\u7684\u662f\u81ea\u5e95\u5411\u4e0a\u5730evaluate\u7684\u65b9\u5f0f \u4e8c\u3001build \u5728 geeksforgeeks Building Expression tree from Prefix Expression \u7ed9\u51fa\u4e86\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u5730\u6784\u5efaexpression tree \u8fc7\u7a0b infogalactic Polish notation Polish notation ( PN ), also known as normal Polish notation ( NPN ),[ 1] \u0141ukasiewicz notation , Warsaw notation , Polish prefix notation or simply prefix notation , is a form of notation for logic , arithmetic , and algebra . Its distinguishing feature is that it places operators to the left of their operands . If the arity of the operators is fixed, the result is a syntax lacking parentheses or other brackets that can still be parsed without ambiguity. The Polish logician Jan \u0141ukasiewicz invented this notation in 1924 in order to simplify sentential logic . NOTE: \u662f\u7531\u6ce2\u5170\u4eba Jan \u0141ukasiewicz \u53d1\u660e\u7684 When Polish notation is used as a syntax for mathematical expressions by programming language interpreters , it is readily parsed into abstract syntax trees and can, in fact, define a one-to-one representation for the same. Because of this, Lisp ( see below ) and related programming languages define their entire syntax in terms of prefix notation (and others use postfix notation). Order of operations An example shows the ease with which a complex statement in prefix notation can be deciphered through order of operations: \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 + 1 1 = \u2212 \u00d7 \u00f7 15 \u2212 7 2 3 + 2 + 1 1 = \u2212 \u00d7 \u00f7 15 5 3 + 2 + 1 1 = \u2212 \u00d7 3 3 + 2 + 1 1 = \u2212 9 + 2 + 1 1 = \u2212 9 + 2 2 = \u2212 9 4 = 5 NOTE: \u81ea\u540e\u5411\u524d\u8fdb\u884cevaluation Here is an implementation (in pseudocode) of prefix evaluation using a stack. Note that under this implementation the input string is scanned from right to left. This differs from the algorithm described above in which the string is processed from left to right. Both algorithms compute the same value for all valid strings. Scan the given prefix expression from right to left for each symbol { if operand then push onto stack if operator then { operand1=pop stack operand2=pop stack compute operand1 operator operand2 push result onto stack } } return top of stack as result Applying this algorithm to the example above yields the following: \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 + 1 1 = \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 2 = \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 4 = \u2212 \u00d7 \u00f7 15 \u2212 7 2 3 4 = \u2212 \u00d7 \u00f7 15 5 3 4 = \u2212 \u00d7 3 3 4 = \u2212 9 4 = 5 Example This uses the same expression as before and the algorithm above. \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 + 1 1 Token Action Stack Notes 1 Operand 1 Push onto stack. 1 Operand 1 1 Push onto stack. + Operator 2 Pop the two operands (1, 1), calculate (1 + 1 = 2) and push onto stack. 2 Operand 2 2 Push onto stack. + Operator 4 Pop the two operands (2, 2), calculate (2 + 2 = 4) and push onto stack. 3 Operand 3 4 Push onto stack. 1 Operand 1 3 4 Push onto stack. 1 Operand 1 1 3 4 Push onto stack. + Operator 2 3 4 Pop the two operands (1, 1), calculate (1 + 1 = 2) and push onto stack. 7 Operand 7 2 3 4 Push onto stack. \u2212 Operator 5 3 4 Pop the two operands (7, 2), calculate (7 \u2212 2 = 5) and push onto stack. 15 Operand 15 5 3 4 Push onto stack. \u00f7 Operator 3 3 4 Pop the two operands (15, 5), calculate (15 \u00f7 5 = 3) and push onto stack. \u00d7 Operator 9 4 Pop the two operands (3, 3), calculate (3 \u00d7 3 = 9) and push onto stack. \u2212 Operator 5 Pop the two operands (9, 4), calculate (9 \u2212 4 = 5) and push onto stack. The result is at the top of the stack.","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Polish-prefix-notation/#polish#prefix#notation","text":"\"infix notation\" \u5373 \"\u4e2d\u7f00\u8868\u8fbe\u5f0f\"\uff0c\"polish notation\" \u5373 \"\u6ce2\u5170\u5f0f\"\u3002","title":"Polish prefix notation"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Polish-prefix-notation/#build#evaluate","text":"\u4e00\u3001evaluate \u5728 infogalactic Polish notation \u4e2d\uff0c\u7ed9\u51fa\u7684\u662f\u81ea\u5e95\u5411\u4e0a\u5730evaluate\u7684\u65b9\u5f0f \u4e8c\u3001build \u5728 geeksforgeeks Building Expression tree from Prefix Expression \u7ed9\u51fa\u4e86\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u5730\u6784\u5efaexpression tree \u8fc7\u7a0b","title":"build &amp;&amp; evaluate"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Polish-prefix-notation/#infogalactic#polish#notation","text":"Polish notation ( PN ), also known as normal Polish notation ( NPN ),[ 1] \u0141ukasiewicz notation , Warsaw notation , Polish prefix notation or simply prefix notation , is a form of notation for logic , arithmetic , and algebra . Its distinguishing feature is that it places operators to the left of their operands . If the arity of the operators is fixed, the result is a syntax lacking parentheses or other brackets that can still be parsed without ambiguity. The Polish logician Jan \u0141ukasiewicz invented this notation in 1924 in order to simplify sentential logic . NOTE: \u662f\u7531\u6ce2\u5170\u4eba Jan \u0141ukasiewicz \u53d1\u660e\u7684 When Polish notation is used as a syntax for mathematical expressions by programming language interpreters , it is readily parsed into abstract syntax trees and can, in fact, define a one-to-one representation for the same. Because of this, Lisp ( see below ) and related programming languages define their entire syntax in terms of prefix notation (and others use postfix notation).","title":"infogalactic Polish notation"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Polish-prefix-notation/#order#of#operations","text":"An example shows the ease with which a complex statement in prefix notation can be deciphered through order of operations: \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 + 1 1 = \u2212 \u00d7 \u00f7 15 \u2212 7 2 3 + 2 + 1 1 = \u2212 \u00d7 \u00f7 15 5 3 + 2 + 1 1 = \u2212 \u00d7 3 3 + 2 + 1 1 = \u2212 9 + 2 + 1 1 = \u2212 9 + 2 2 = \u2212 9 4 = 5 NOTE: \u81ea\u540e\u5411\u524d\u8fdb\u884cevaluation Here is an implementation (in pseudocode) of prefix evaluation using a stack. Note that under this implementation the input string is scanned from right to left. This differs from the algorithm described above in which the string is processed from left to right. Both algorithms compute the same value for all valid strings. Scan the given prefix expression from right to left for each symbol { if operand then push onto stack if operator then { operand1=pop stack operand2=pop stack compute operand1 operator operand2 push result onto stack } } return top of stack as result Applying this algorithm to the example above yields the following: \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 + 1 1 = \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 2 = \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 4 = \u2212 \u00d7 \u00f7 15 \u2212 7 2 3 4 = \u2212 \u00d7 \u00f7 15 5 3 4 = \u2212 \u00d7 3 3 4 = \u2212 9 4 = 5","title":"Order of operations"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Polish-prefix-notation/#example","text":"This uses the same expression as before and the algorithm above. \u2212 \u00d7 \u00f7 15 \u2212 7 + 1 1 3 + 2 + 1 1 Token Action Stack Notes 1 Operand 1 Push onto stack. 1 Operand 1 1 Push onto stack. + Operator 2 Pop the two operands (1, 1), calculate (1 + 1 = 2) and push onto stack. 2 Operand 2 2 Push onto stack. + Operator 4 Pop the two operands (2, 2), calculate (2 + 2 = 4) and push onto stack. 3 Operand 3 4 Push onto stack. 1 Operand 1 3 4 Push onto stack. 1 Operand 1 1 3 4 Push onto stack. + Operator 2 3 4 Pop the two operands (1, 1), calculate (1 + 1 = 2) and push onto stack. 7 Operand 7 2 3 4 Push onto stack. \u2212 Operator 5 3 4 Pop the two operands (7, 2), calculate (7 \u2212 2 = 5) and push onto stack. 15 Operand 15 5 3 4 Push onto stack. \u00f7 Operator 3 3 4 Pop the two operands (15, 5), calculate (15 \u00f7 5 = 3) and push onto stack. \u00d7 Operator 9 4 Pop the two operands (3, 3), calculate (3 \u00d7 3 = 9) and push onto stack. \u2212 Operator 5 Pop the two operands (9, 4), calculate (9 \u2212 4 = 5) and push onto stack. The result is at the top of the stack.","title":"Example"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Postfix-to-Prefix/","text":"","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Postfix-to-Prefix/geeksforgeeks-Postfix-to-Prefix-Conversion/","text":"","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Prefix-to-Infix/","text":"","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Prefix-to-Infix/geeksforgeeks-Prefix-to-Infix-Conversion/","text":"geeksforgeeks Prefix to Infix Conversion","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Prefix-to-Infix/geeksforgeeks-Prefix-to-Infix-Conversion/#geeksforgeeks#prefix#to#infix#conversion","text":"","title":"geeksforgeeks Prefix to Infix Conversion"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Reverse-Polish-postfix-notation/","text":"Reverse Polish notation infogalactic Reverse Polish notation Reverse Polish notation ( RPN ) is a mathematical notation in which every operator follows all of its operands , in contrast to Polish notation (PN), which puts the operator in the prefix position. It is also known as postfix notation and is parenthesis-free as long as operator arities are fixed. The description \"Polish\" refers to the nationality of logician Jan \u0141ukasiewicz ,[ 1] who invented (prefix) Polish notation in the 1920s. Postfix algorithm Converting from infix notation Main article: Shunting-yard algorithm Edsger Dijkstra invented the shunting-yard algorithm to convert infix expressions to postfix (RPN), so named because its operation resembles that of a railroad shunting yard . There are other ways of producing postfix expressions from infix notation. Most operator-precedence parsers can be modified to produce postfix expressions; in particular, once an abstract syntax tree has been constructed, the corresponding postfix expression is given by a simple post-order traversal of that tree.","title":"Introduction"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Reverse-Polish-postfix-notation/#reverse#polish#notation","text":"","title":"Reverse Polish notation"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Reverse-Polish-postfix-notation/#infogalactic#reverse#polish#notation","text":"Reverse Polish notation ( RPN ) is a mathematical notation in which every operator follows all of its operands , in contrast to Polish notation (PN), which puts the operator in the prefix position. It is also known as postfix notation and is parenthesis-free as long as operator arities are fixed. The description \"Polish\" refers to the nationality of logician Jan \u0141ukasiewicz ,[ 1] who invented (prefix) Polish notation in the 1920s.","title":"infogalactic Reverse Polish notation"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Reverse-Polish-postfix-notation/#postfix#algorithm","text":"","title":"Postfix algorithm"},{"location":"Guide/Expression-tree/Prefix-Infix-Postfix/Reverse-Polish-postfix-notation/#converting#from#infix#notation","text":"Main article: Shunting-yard algorithm Edsger Dijkstra invented the shunting-yard algorithm to convert infix expressions to postfix (RPN), so named because its operation resembles that of a railroad shunting yard . There are other ways of producing postfix expressions from infix notation. Most operator-precedence parsers can be modified to produce postfix expressions; in particular, once an abstract syntax tree has been constructed, the corresponding postfix expression is given by a simple post-order traversal of that tree.","title":"Converting from infix notation"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/","text":"geeksforgeeks expression tree geeksforgeeks Expression Tree \u5176\u4e2d\u63cf\u8ff0\u4e86 1\u3001evaluation 2\u3001construction of expression tree from postfix notation expression geeksforgeeks Building Expression tree from Prefix Expression \u5185\u5bb9\u76f8\u5bf9\u7b80\u5355: \u6839\u636eprefix expression\u6784\u9020expression tree geeksforgeeks Program to convert Infix notation to Expression Tree \u4e3b\u8981\u662f\u8bb2 shunting-yard-algorithm geeksforgeeks Convert ternary expression to Binary Tree using Stack \u4e3b\u8981\u8bb2\u5982\u4f55\u5c06\u4e00\u4e2a\u4e09\u5143\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3abinary tree\uff0c\u5b83\u4f7f\u7528\u7684\u662fstack\uff1b geeksforgeeks Convert Ternary Expression to a Binary Tree \u4e3b\u8981\u8bb2\u5982\u4f55\u5c06\u4e00\u4e2a\u4e09\u5143\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3abinary tree\uff0c\u5b83\u4f7f\u7528\u7684\u662frecursion\uff1b","title":"Introduction"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/#geeksforgeeks#expression#tree","text":"geeksforgeeks Expression Tree \u5176\u4e2d\u63cf\u8ff0\u4e86 1\u3001evaluation 2\u3001construction of expression tree from postfix notation expression geeksforgeeks Building Expression tree from Prefix Expression \u5185\u5bb9\u76f8\u5bf9\u7b80\u5355: \u6839\u636eprefix expression\u6784\u9020expression tree geeksforgeeks Program to convert Infix notation to Expression Tree \u4e3b\u8981\u662f\u8bb2 shunting-yard-algorithm geeksforgeeks Convert ternary expression to Binary Tree using Stack \u4e3b\u8981\u8bb2\u5982\u4f55\u5c06\u4e00\u4e2a\u4e09\u5143\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3abinary tree\uff0c\u5b83\u4f7f\u7528\u7684\u662fstack\uff1b geeksforgeeks Convert Ternary Expression to a Binary Tree \u4e3b\u8981\u8bb2\u5982\u4f55\u5c06\u4e00\u4e2a\u4e09\u5143\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3abinary tree\uff0c\u5b83\u4f7f\u7528\u7684\u662frecursion\uff1b","title":"geeksforgeeks expression tree"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Building-Expression-tree-from-Prefix-Expression/","text":"geeksforgeeks Building Expression tree from Prefix Expression NOTE: \u4e00\u3001\u9700\u8981\u638c\u63e1prefix expression\u7684\u9012\u5f52\u5b9a\u4e49\uff0c\u7136\u540e\u6839\u636erecursive definition\u5c31\u53ef\u4ee5\u5199\u51fa\u5bf9\u5e94\u7684recursive function\uff0c\u4e0b\u9762\u7684\"Approach\"\u65e2\u662f\u5982\u6b64\u3002 \u5b83\u7684implementation\u8ba9\u6211\u60f3\u5230\u4e86\u6839\u636epreorder traversal\u6765\u53cd\u5e8f\u5217\u5316\u4e00\u68f5\u4e8c\u53c9\u6811 \u539f\u6587\u7ed9\u51fa\u7684\u5b9e\u73b0\u662f\u9012\u5f52\u81ea\u9876\u5411\u4e0b\u6784\u5efa\u4e8c\u53c9\u6811\u3001\u5148\u6784\u9020root node-\u7136\u540e\u9012\u5f52\u6784\u9020\u5de6\u53f3\u5b50\u6811 \u5728 infogalactic Polish notation \u4e2d\uff0c\u7ed9\u51fa\u4e86\u81ea\u5e95\u5411\u4e0aevaluate prefix expression Approach: If the character is an operand i.e. X then it\u2019ll be the leaf node of the required tree as all the operands are at the leaf in an expression tree. Else if the character is an operator and of the form OP X Y then it\u2019ll be an internal node with left child as the expressionTree(X) and right child as the expressionTree(Y) which can be solved using a recursive function. Examples Input : a [] = \u201c *+ ab - cd \u201d Output : The Infix expression is : a + b * c \u2013 d The Postfix expression is : a b + c d \u2013 * Input : a [] = \u201c + ab \u201d Output : The Infix expression is : a + b The Postfix expression is : a b + \u5b8c\u6574\u7a0b\u5e8f NOTE: \u4e0b\u9762\u7a0b\u5e8f\u662f\u6839\u636e\u539f\u6587\u7684\u7a0b\u5e8f\u7a0d\u5fae\u6574\u7406\u7684 // C program to construct an expression tree // from prefix expression #include <stdio.h> #include <stdlib.h> // Represents a node of the required tree typedef struct node { char data ; struct node * left , * right ; } node ; /** * @brief Function to recursively build the expression tree * * @param p * @param a \u5f53\u524d\u8282\u70b9\u7684data\u503c * @return \u4e0b\u4e00\u4e2a\u8282\u70b9\u7684data\u503c */ char * add ( node ** p , char * a ) { // If its the end of the expression if ( * a == '\\0' ) { return '\\0' ; } /** * \u7b2c\u4e00\u6b65: \u5148\u6784\u9020\u8282\u70b9\uff0c * \u7b2c\u4e8c\u6b65: * 1\u3001\u5982\u679c\u5bf9\u5e94\u7684\u8282\u70b9\u662f\u4e00\u4e2aoperator\uff0c\u5219\u9700\u8981\u9012\u5f52\u5411\u4e0b\u6784\u9020\u5de6\u53f3\u5b50\u6811 * 2\u3001\u5982\u679c\u5bf9\u5e94\u7684\u8282\u70b9\u662f\u4e00\u4e2aoperand\uff0c\u5219\u5b8c\u6210\u4e86\u6784\u9020 */ while ( 1 ) { if ( * p == NULL ) // \u7b2c\u4e00\u6b65: \u9700\u8981\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u8282\u70b9 { // Create a node with *a as the data and // both the children set to null node * nn = ( node * ) malloc ( sizeof ( node )); nn -> data = * a ; nn -> left = NULL ; nn -> right = NULL ; * p = nn ; } else // \u7b2c\u4e8c\u6b65 { // If the character is an operand if ( * a >= 'a' && * a <= 'z' ) // \u662f\u4e00\u4e2a\u64cd\u4f5c\u6570\uff0c\u663e\u7136\u4e0d\u9700\u8981\u9012\u5f52\u5411\u4e0b\u4e86 { return a ; } else // \u662f\u4e00\u4e2a\u8fd0\u7b97\u7b26\uff0c\u5219\u9700\u8981\u9012\u5f52\u7684\u6784\u9020\u5b83\u7684\u5de6\u53f3\u5b50\u6811 { char * q = \"null\" ; // Build the left sub-tree q = add ( & ( * p ) -> left , a + 1 ); // Build the right sub-tree q = add ( & ( * p ) -> right , q + 1 ); return q ; } } } } // Function to print the infix expression for the tree void inr ( node * p ) // recursion { if ( p == NULL ) { return ; } else { inr ( p -> left ); printf ( \"%c \" , p -> data ); inr ( p -> right ); } } // Function to print the postfix expression for the tree void postr ( node * p ) { if ( p == NULL ) { return ; } else { postr ( p -> left ); postr ( p -> right ); printf ( \"%c \" , p -> data ); } } // Driver code int main () { node * s = NULL ; char a [] = \"*+ab-cd\" ; add ( & s , a ); printf ( \"The Infix expression is: \\n \" ); inr ( s ); printf ( \" \\n \" ); printf ( \"The Postfix expression is: \\n \" ); postr ( s ); printf ( \" \\n \" ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra","title":"Introduction"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Building-Expression-tree-from-Prefix-Expression/#geeksforgeeks#building#expression#tree#from#prefix#expression","text":"NOTE: \u4e00\u3001\u9700\u8981\u638c\u63e1prefix expression\u7684\u9012\u5f52\u5b9a\u4e49\uff0c\u7136\u540e\u6839\u636erecursive definition\u5c31\u53ef\u4ee5\u5199\u51fa\u5bf9\u5e94\u7684recursive function\uff0c\u4e0b\u9762\u7684\"Approach\"\u65e2\u662f\u5982\u6b64\u3002 \u5b83\u7684implementation\u8ba9\u6211\u60f3\u5230\u4e86\u6839\u636epreorder traversal\u6765\u53cd\u5e8f\u5217\u5316\u4e00\u68f5\u4e8c\u53c9\u6811 \u539f\u6587\u7ed9\u51fa\u7684\u5b9e\u73b0\u662f\u9012\u5f52\u81ea\u9876\u5411\u4e0b\u6784\u5efa\u4e8c\u53c9\u6811\u3001\u5148\u6784\u9020root node-\u7136\u540e\u9012\u5f52\u6784\u9020\u5de6\u53f3\u5b50\u6811 \u5728 infogalactic Polish notation \u4e2d\uff0c\u7ed9\u51fa\u4e86\u81ea\u5e95\u5411\u4e0aevaluate prefix expression Approach: If the character is an operand i.e. X then it\u2019ll be the leaf node of the required tree as all the operands are at the leaf in an expression tree. Else if the character is an operator and of the form OP X Y then it\u2019ll be an internal node with left child as the expressionTree(X) and right child as the expressionTree(Y) which can be solved using a recursive function.","title":"geeksforgeeks Building Expression tree from Prefix Expression"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Building-Expression-tree-from-Prefix-Expression/#examples","text":"Input : a [] = \u201c *+ ab - cd \u201d Output : The Infix expression is : a + b * c \u2013 d The Postfix expression is : a b + c d \u2013 * Input : a [] = \u201c + ab \u201d Output : The Infix expression is : a + b The Postfix expression is : a b +","title":"Examples"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Building-Expression-tree-from-Prefix-Expression/#_1","text":"NOTE: \u4e0b\u9762\u7a0b\u5e8f\u662f\u6839\u636e\u539f\u6587\u7684\u7a0b\u5e8f\u7a0d\u5fae\u6574\u7406\u7684 // C program to construct an expression tree // from prefix expression #include <stdio.h> #include <stdlib.h> // Represents a node of the required tree typedef struct node { char data ; struct node * left , * right ; } node ; /** * @brief Function to recursively build the expression tree * * @param p * @param a \u5f53\u524d\u8282\u70b9\u7684data\u503c * @return \u4e0b\u4e00\u4e2a\u8282\u70b9\u7684data\u503c */ char * add ( node ** p , char * a ) { // If its the end of the expression if ( * a == '\\0' ) { return '\\0' ; } /** * \u7b2c\u4e00\u6b65: \u5148\u6784\u9020\u8282\u70b9\uff0c * \u7b2c\u4e8c\u6b65: * 1\u3001\u5982\u679c\u5bf9\u5e94\u7684\u8282\u70b9\u662f\u4e00\u4e2aoperator\uff0c\u5219\u9700\u8981\u9012\u5f52\u5411\u4e0b\u6784\u9020\u5de6\u53f3\u5b50\u6811 * 2\u3001\u5982\u679c\u5bf9\u5e94\u7684\u8282\u70b9\u662f\u4e00\u4e2aoperand\uff0c\u5219\u5b8c\u6210\u4e86\u6784\u9020 */ while ( 1 ) { if ( * p == NULL ) // \u7b2c\u4e00\u6b65: \u9700\u8981\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u8282\u70b9 { // Create a node with *a as the data and // both the children set to null node * nn = ( node * ) malloc ( sizeof ( node )); nn -> data = * a ; nn -> left = NULL ; nn -> right = NULL ; * p = nn ; } else // \u7b2c\u4e8c\u6b65 { // If the character is an operand if ( * a >= 'a' && * a <= 'z' ) // \u662f\u4e00\u4e2a\u64cd\u4f5c\u6570\uff0c\u663e\u7136\u4e0d\u9700\u8981\u9012\u5f52\u5411\u4e0b\u4e86 { return a ; } else // \u662f\u4e00\u4e2a\u8fd0\u7b97\u7b26\uff0c\u5219\u9700\u8981\u9012\u5f52\u7684\u6784\u9020\u5b83\u7684\u5de6\u53f3\u5b50\u6811 { char * q = \"null\" ; // Build the left sub-tree q = add ( & ( * p ) -> left , a + 1 ); // Build the right sub-tree q = add ( & ( * p ) -> right , q + 1 ); return q ; } } } } // Function to print the infix expression for the tree void inr ( node * p ) // recursion { if ( p == NULL ) { return ; } else { inr ( p -> left ); printf ( \"%c \" , p -> data ); inr ( p -> right ); } } // Function to print the postfix expression for the tree void postr ( node * p ) { if ( p == NULL ) { return ; } else { postr ( p -> left ); postr ( p -> right ); printf ( \"%c \" , p -> data ); } } // Driver code int main () { node * s = NULL ; char a [] = \"*+ab-cd\" ; add ( & s , a ); printf ( \"The Infix expression is: \\n \" ); inr ( s ); printf ( \" \\n \" ); printf ( \"The Postfix expression is: \\n \" ); postr ( s ); printf ( \" \\n \" ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra","title":"\u5b8c\u6574\u7a0b\u5e8f"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Convert-ternary-expression-to-Binary-Tree/","text":"Convert ternary expression to Binary Tree \u5c06 ternary expression \u8f6c\u6362\u4e3a Binary Tree\uff0c\u8fd9\u662f\u6700\u6700\u57fa\u672c\u7684parsing: 1\u3001geeksforgeeks Convert ternary expression to Binary Tree using Stack \u81ea\u5e95\u5411\u4e0a 2\u3001geeksforgeeks Convert Ternary Expression to a Binary Tree \u81ea\u9876\u5411\u4e0b geeksforgeeks Convert ternary expression to Binary Tree using Stack NOTE: \u8fd9\u662f\u5178\u578b\u7684bottom-up parsing Given a string str that contains a ternary expression which may be nested. The task is to convert the given ternary expression to a binary tree and return the root. Examples: Input: str = \"a?b:c\" Output: a b c a / \\ b c The preorder traversal of the above tree is a b c. Input: str = \"a?b?c:d:e\" Output: a b c d e a / \\ b e / \\ c d Approach: This is a stack-based approach to the given problem. Since the ternary operator has associativity from right-to-left, the string can be traversed from right to left. Take the letters one by one skipping the letters \u2018 ? \u2019 and \u2018 : \u2019 as these letters are used to decide whether the current letter (alphabet [ a to z ]) will go into the stack or be used to pop the top 2 elements from the top of the stack to make them the children of the current letter which is then itself pushed into the stack. This forms the tree in a bottom-up manner and the last remaining element in the stack after the entire string is processed is the root of the tree. \u5b8c\u6574\u5b9e\u73b0 Below is the implementation of the above approach: // C++ implementation of the approach #include <bits/stdc++.h> using namespace std ; // Node structure struct Node { char data ; Node * left , * right ; }; // Function to create a new node Node * createNewNode ( int data ) { Node * node = new Node ; node -> data = data ; node -> left = NULL , node -> right = NULL ; return node ; } // Function to print the preorder // traversal of the tree void preorder ( Node * root ) { if ( root == NULL ) return ; cout << root -> data << \" \" ; preorder ( root -> left ); preorder ( root -> right ); } // Function to convert the expression to a binary tree Node * convertExpression ( string str ) { stack < Node *> s ; // If the letter is the last letter of // the string or is of the type :letter: or ?letter: // we push the node pointer containing // the letter to the stack for ( int i = str . length () - 1 ; i >= 0 ;) { if (( i == str . length () - 1 ) || ( i != 0 && (( str [ i - 1 ] == ':' && str [ i + 1 ] == ':' ) || ( str [ i - 1 ] == '?' && str [ i + 1 ] == ':' )))) { s . push ( createNewNode ( str [ i ])); } // If we do not push the current letter node to stack, // it means the top 2 nodes in the stack currently are the // left and the right children of the current node // So pop these elements and assign them as the // children of the current letter node and then // push this node into the stack else { Node * lnode = s . top (); s . pop (); Node * rnode = s . top (); s . pop (); Node * node = createNewNode ( str [ i ]); node -> left = lnode ; node -> right = rnode ; s . push ( node ); } i -= 2 ; } // Finally, there will be only 1 element // in the stack which will be the // root of the binary tree return s . top (); } // Driver code int main () { string str = \"a?b?c:d:e\" ; // Convert expression Node * root = convertExpression ( str ); // Print the preorder traversal preorder ( root ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra geeksforgeeks Convert Ternary Expression to a Binary Tree \u5b8c\u6574\u5b9e\u73b0 // C++ program to convert a ternary expression to // a tree. #include <bits/stdc++.h> using namespace std ; // tree structure struct Node { char data ; Node * left , * right ; }; // function create a new node Node * newNode ( char Data ) { Node * new_node = new Node ; new_node -> data = Data ; new_node -> left = new_node -> right = NULL ; return new_node ; } // Function to convert Ternary Expression to a Binary // Tree. It return the root of tree //Notice that we pass index i by reference because we want to skip the characters in the subtree Node * convertExpression ( string str , int & i ) { // store current character of expression_string // [ 'a' to 'z'] Node * root = newNode ( str [ i ]); //If it was last character return //Base Case if ( i == str . length () -1 ) return root ; // Move ahead in str i ++ ; //If the next character is '?'.Then there will be subtree for the current node if ( str [ i ] == '?' ) { //skip the '?' i ++ ; //construct the left subtree //Notice after the below recursive call i will point to ':' just before the right child of current node since we pass i by reference root -> left = convertExpression ( str , i ); //skip the ':' character i ++ ; //construct the right subtree root -> right = convertExpression ( str , i ); return root ; } //If the next character is not '?' no subtree just return it else return root ; } // function print tree void printTree ( Node * root ) { if ( ! root ) return ; cout << root -> data << \" \" ; printTree ( root -> left ); printTree ( root -> right ); } // Driver program to test above function int main () { string expression = \"a?b?c:d:e\" ; int i = 0 ; Node * root = convertExpression ( expression , i ); printTree ( root ) ; return 0 ; }","title":"Introduction"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Convert-ternary-expression-to-Binary-Tree/#convert#ternary#expression#to#binary#tree","text":"\u5c06 ternary expression \u8f6c\u6362\u4e3a Binary Tree\uff0c\u8fd9\u662f\u6700\u6700\u57fa\u672c\u7684parsing: 1\u3001geeksforgeeks Convert ternary expression to Binary Tree using Stack \u81ea\u5e95\u5411\u4e0a 2\u3001geeksforgeeks Convert Ternary Expression to a Binary Tree \u81ea\u9876\u5411\u4e0b","title":"Convert ternary expression to Binary Tree"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Convert-ternary-expression-to-Binary-Tree/#geeksforgeeks#convert#ternary#expression#to#binary#tree#using#stack","text":"NOTE: \u8fd9\u662f\u5178\u578b\u7684bottom-up parsing Given a string str that contains a ternary expression which may be nested. The task is to convert the given ternary expression to a binary tree and return the root. Examples: Input: str = \"a?b:c\" Output: a b c a / \\ b c The preorder traversal of the above tree is a b c. Input: str = \"a?b?c:d:e\" Output: a b c d e a / \\ b e / \\ c d Approach: This is a stack-based approach to the given problem. Since the ternary operator has associativity from right-to-left, the string can be traversed from right to left. Take the letters one by one skipping the letters \u2018 ? \u2019 and \u2018 : \u2019 as these letters are used to decide whether the current letter (alphabet [ a to z ]) will go into the stack or be used to pop the top 2 elements from the top of the stack to make them the children of the current letter which is then itself pushed into the stack. This forms the tree in a bottom-up manner and the last remaining element in the stack after the entire string is processed is the root of the tree.","title":"geeksforgeeks Convert ternary expression to Binary Tree using Stack"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Convert-ternary-expression-to-Binary-Tree/#_1","text":"Below is the implementation of the above approach: // C++ implementation of the approach #include <bits/stdc++.h> using namespace std ; // Node structure struct Node { char data ; Node * left , * right ; }; // Function to create a new node Node * createNewNode ( int data ) { Node * node = new Node ; node -> data = data ; node -> left = NULL , node -> right = NULL ; return node ; } // Function to print the preorder // traversal of the tree void preorder ( Node * root ) { if ( root == NULL ) return ; cout << root -> data << \" \" ; preorder ( root -> left ); preorder ( root -> right ); } // Function to convert the expression to a binary tree Node * convertExpression ( string str ) { stack < Node *> s ; // If the letter is the last letter of // the string or is of the type :letter: or ?letter: // we push the node pointer containing // the letter to the stack for ( int i = str . length () - 1 ; i >= 0 ;) { if (( i == str . length () - 1 ) || ( i != 0 && (( str [ i - 1 ] == ':' && str [ i + 1 ] == ':' ) || ( str [ i - 1 ] == '?' && str [ i + 1 ] == ':' )))) { s . push ( createNewNode ( str [ i ])); } // If we do not push the current letter node to stack, // it means the top 2 nodes in the stack currently are the // left and the right children of the current node // So pop these elements and assign them as the // children of the current letter node and then // push this node into the stack else { Node * lnode = s . top (); s . pop (); Node * rnode = s . top (); s . pop (); Node * node = createNewNode ( str [ i ]); node -> left = lnode ; node -> right = rnode ; s . push ( node ); } i -= 2 ; } // Finally, there will be only 1 element // in the stack which will be the // root of the binary tree return s . top (); } // Driver code int main () { string str = \"a?b?c:d:e\" ; // Convert expression Node * root = convertExpression ( str ); // Print the preorder traversal preorder ( root ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra","title":"\u5b8c\u6574\u5b9e\u73b0"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Convert-ternary-expression-to-Binary-Tree/#geeksforgeeks#convert#ternary#expression#to#a#binary#tree","text":"","title":"geeksforgeeks Convert Ternary Expression to a Binary Tree"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Convert-ternary-expression-to-Binary-Tree/#_2","text":"// C++ program to convert a ternary expression to // a tree. #include <bits/stdc++.h> using namespace std ; // tree structure struct Node { char data ; Node * left , * right ; }; // function create a new node Node * newNode ( char Data ) { Node * new_node = new Node ; new_node -> data = Data ; new_node -> left = new_node -> right = NULL ; return new_node ; } // Function to convert Ternary Expression to a Binary // Tree. It return the root of tree //Notice that we pass index i by reference because we want to skip the characters in the subtree Node * convertExpression ( string str , int & i ) { // store current character of expression_string // [ 'a' to 'z'] Node * root = newNode ( str [ i ]); //If it was last character return //Base Case if ( i == str . length () -1 ) return root ; // Move ahead in str i ++ ; //If the next character is '?'.Then there will be subtree for the current node if ( str [ i ] == '?' ) { //skip the '?' i ++ ; //construct the left subtree //Notice after the below recursive call i will point to ':' just before the right child of current node since we pass i by reference root -> left = convertExpression ( str , i ); //skip the ':' character i ++ ; //construct the right subtree root -> right = convertExpression ( str , i ); return root ; } //If the next character is not '?' no subtree just return it else return root ; } // function print tree void printTree ( Node * root ) { if ( ! root ) return ; cout << root -> data << \" \" ; printTree ( root -> left ); printTree ( root -> right ); } // Driver program to test above function int main () { string expression = \"a?b?c:d:e\" ; int i = 0 ; Node * root = convertExpression ( expression , i ); printTree ( root ) ; return 0 ; }","title":"\u5b8c\u6574\u5b9e\u73b0"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Evaluation-of-Expression-Tree/","text":"geeksforgeeks Evaluation of Expression Tree NOTE: \u5178\u578b\u7684\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value As all the operators in the tree are binary hence each node will have either 0 or 2 children. As it can be inferred from the examples above , the integer values would appear at the leaf nodes , while the interior nodes represent the operators. To evaluate the syntax tree , a recursive approach can be followed . Algorithm : Let t be the syntax tree If t is not null then If t . info is operand then Return t . info Else A = solve ( t . left ) B = solve ( t . right ) return A operator B where operator is the info contained in t \u5b8c\u6574\u7a0b\u5e8f // C++ program to evaluate an expression tree #include <bits/stdc++.h> using namespace std ; // Class to represent the nodes of syntax tree class node { public : string info ; node * left = NULL , * right = NULL ; node ( string x ) { info = x ; } }; // Utility function to return the integer value // of a given string int toInt ( string s ) { int num = 0 ; // Check if the integral value is // negative or not // If it is not negative, generate the number // normally if ( s [ 0 ] != '-' ) for ( int i = 0 ; i < s . length (); i ++ ) num = num * 10 + ( int ( s [ i ]) - 48 ); // If it is negative, calculate the +ve number // first ignoring the sign and invert the // sign at the end else for ( int i = 1 ; i < s . length (); i ++ ) { num = num * 10 + ( int ( s [ i ]) - 48 ); num = num * -1 ; } return num ; } // This function receives a node of the syntax tree // and recursively evaluates it int eval ( node * root ) { // empty tree if ( ! root ) return 0 ; // leaf node i.e, an integer if ( ! root -> left && ! root -> right ) return toInt ( root -> info ); // Evaluate left subtree int l_val = eval ( root -> left ); // Evaluate right subtree int r_val = eval ( root -> right ); // Check which operator to apply if ( root -> info == \"+\" ) return l_val + r_val ; if ( root -> info == \"-\" ) return l_val - r_val ; if ( root -> info == \"*\" ) return l_val * r_val ; return l_val / r_val ; } //driver function to check the above program int main () { // create a syntax tree node * root = new node ( \"+\" ); root -> left = new node ( \"*\" ); root -> left -> left = new node ( \"5\" ); root -> left -> right = new node ( \"-4\" ); root -> right = new node ( \"-\" ); root -> right -> left = new node ( \"100\" ); root -> right -> right = new node ( \"20\" ); cout << eval ( root ) << endl ; delete ( root ); root = new node ( \"+\" ); root -> left = new node ( \"*\" ); root -> left -> left = new node ( \"5\" ); root -> left -> right = new node ( \"4\" ); root -> right = new node ( \"-\" ); root -> right -> left = new node ( \"100\" ); root -> right -> right = new node ( \"/\" ); root -> right -> right -> left = new node ( \"20\" ); root -> right -> right -> right = new node ( \"2\" ); cout << eval ( root ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra","title":"Introduction"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Evaluation-of-Expression-Tree/#geeksforgeeks#evaluation#of#expression#tree","text":"NOTE: \u5178\u578b\u7684\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value As all the operators in the tree are binary hence each node will have either 0 or 2 children. As it can be inferred from the examples above , the integer values would appear at the leaf nodes , while the interior nodes represent the operators. To evaluate the syntax tree , a recursive approach can be followed . Algorithm : Let t be the syntax tree If t is not null then If t . info is operand then Return t . info Else A = solve ( t . left ) B = solve ( t . right ) return A operator B where operator is the info contained in t","title":"geeksforgeeks Evaluation of Expression Tree"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Evaluation-of-Expression-Tree/#_1","text":"// C++ program to evaluate an expression tree #include <bits/stdc++.h> using namespace std ; // Class to represent the nodes of syntax tree class node { public : string info ; node * left = NULL , * right = NULL ; node ( string x ) { info = x ; } }; // Utility function to return the integer value // of a given string int toInt ( string s ) { int num = 0 ; // Check if the integral value is // negative or not // If it is not negative, generate the number // normally if ( s [ 0 ] != '-' ) for ( int i = 0 ; i < s . length (); i ++ ) num = num * 10 + ( int ( s [ i ]) - 48 ); // If it is negative, calculate the +ve number // first ignoring the sign and invert the // sign at the end else for ( int i = 1 ; i < s . length (); i ++ ) { num = num * 10 + ( int ( s [ i ]) - 48 ); num = num * -1 ; } return num ; } // This function receives a node of the syntax tree // and recursively evaluates it int eval ( node * root ) { // empty tree if ( ! root ) return 0 ; // leaf node i.e, an integer if ( ! root -> left && ! root -> right ) return toInt ( root -> info ); // Evaluate left subtree int l_val = eval ( root -> left ); // Evaluate right subtree int r_val = eval ( root -> right ); // Check which operator to apply if ( root -> info == \"+\" ) return l_val + r_val ; if ( root -> info == \"-\" ) return l_val - r_val ; if ( root -> info == \"*\" ) return l_val * r_val ; return l_val / r_val ; } //driver function to check the above program int main () { // create a syntax tree node * root = new node ( \"+\" ); root -> left = new node ( \"*\" ); root -> left -> left = new node ( \"5\" ); root -> left -> right = new node ( \"-4\" ); root -> right = new node ( \"-\" ); root -> right -> left = new node ( \"100\" ); root -> right -> right = new node ( \"20\" ); cout << eval ( root ) << endl ; delete ( root ); root = new node ( \"+\" ); root -> left = new node ( \"*\" ); root -> left -> left = new node ( \"5\" ); root -> left -> right = new node ( \"4\" ); root -> right = new node ( \"-\" ); root -> right -> left = new node ( \"100\" ); root -> right -> right = new node ( \"/\" ); root -> right -> right -> left = new node ( \"20\" ); root -> right -> right -> right = new node ( \"2\" ); cout << eval ( root ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra","title":"\u5b8c\u6574\u7a0b\u5e8f"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Expression-Tree/","text":"geeksforgeeks Expression Tree Expression tree is a binary tree in which each internal node corresponds to operator and each leaf node corresponds to operand so for example expression tree for 3 + ((5+9)*2) would be: Inorder traversal of expression tree produces infix version of given postfix expression (same with preorder traversal it gives prefix expression) Evaluating the expression represented by expression tree: Let t be the expression tree If t is not null then If t . value is operand then Return t . value A = solve ( t . left ) B = solve ( t . right ) // calculate applies operator 't.value' // on A and B, and returns value Return calculate ( A , B , t . value ) NOTE: \u5178\u578b\u7684\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value Construction of Expression Tree: Now For constructing expression tree we use a stack . We loop through input expression and do following for every character. 1) If character is operand push that into stack 2) If character is operator pop two values from stack make them its child and push current node again. At the end only element of stack will be root of expression tree. // C++ program for expression tree #include <bits/stdc++.h> using namespace std ; // An expression tree node struct et { char value ; et * left , * right ; }; // A utility function to check if 'c' // is an operator bool isOperator ( char c ) { if ( c == '+' || c == '-' || c == '*' || c == '/' || c == '^' ) return true ; return false ; } // Utility function to do inorder traversal void inorder ( et * t ) { if ( t ) { inorder ( t -> left ); printf ( \"%c \" , t -> value ); inorder ( t -> right ); } } // A utility function to create a new node et * newNode ( int v ) { et * temp = new et ; temp -> left = temp -> right = NULL ; temp -> value = v ; return temp ; } ; // Returns root of constructed tree for given // postfix expression et * constructTree ( char postfix []) { stack < et *> st ; et * t , * t1 , * t2 ; // Traverse through every character of // input expression for ( int i = 0 ; i < strlen ( postfix ); i ++ ) { // If operand, simply push into stack if ( ! isOperator ( postfix [ i ])) { t = newNode ( postfix [ i ]); st . push ( t ); } else // operator { t = newNode ( postfix [ i ]); // Pop two top nodes t1 = st . top (); // Store top st . pop (); // Remove top t2 = st . top (); st . pop (); // make them children t -> right = t1 ; t -> left = t2 ; // Add this subexpression to stack st . push ( t ); } } // only element will be root of expression // tree t = st . top (); st . pop (); return t ; } // Driver program to test above int main () { char postfix [] = \"ab+ef*g*-\" ; et * r = constructTree ( postfix ); printf ( \"infix expression is \\n \" ); inorder ( r ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra NOTE: \u81ea\u5e95\u5411\u4e0a\uff0c\u4e24\u4e24\u5408\u5e76 Output: infix expression is a + b - e * f * g This article is contributed by Utkarsh Trivedi . Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.","title":"Introduction"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Expression-Tree/#geeksforgeeks#expression#tree","text":"Expression tree is a binary tree in which each internal node corresponds to operator and each leaf node corresponds to operand so for example expression tree for 3 + ((5+9)*2) would be: Inorder traversal of expression tree produces infix version of given postfix expression (same with preorder traversal it gives prefix expression)","title":"geeksforgeeks Expression Tree"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Expression-Tree/#evaluating#the#expression#represented#by#expression#tree","text":"Let t be the expression tree If t is not null then If t . value is operand then Return t . value A = solve ( t . left ) B = solve ( t . right ) // calculate applies operator 't.value' // on A and B, and returns value Return calculate ( A , B , t . value ) NOTE: \u5178\u578b\u7684\u9012\u5f52\u3001\u81ea\u9876\u5411\u4e0b\u3001binary tree-DFS-post order\u540e\u5e8f-return value","title":"Evaluating the expression represented by expression tree:"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Expression-Tree/#construction#of#expression#tree","text":"Now For constructing expression tree we use a stack . We loop through input expression and do following for every character. 1) If character is operand push that into stack 2) If character is operator pop two values from stack make them its child and push current node again. At the end only element of stack will be root of expression tree. // C++ program for expression tree #include <bits/stdc++.h> using namespace std ; // An expression tree node struct et { char value ; et * left , * right ; }; // A utility function to check if 'c' // is an operator bool isOperator ( char c ) { if ( c == '+' || c == '-' || c == '*' || c == '/' || c == '^' ) return true ; return false ; } // Utility function to do inorder traversal void inorder ( et * t ) { if ( t ) { inorder ( t -> left ); printf ( \"%c \" , t -> value ); inorder ( t -> right ); } } // A utility function to create a new node et * newNode ( int v ) { et * temp = new et ; temp -> left = temp -> right = NULL ; temp -> value = v ; return temp ; } ; // Returns root of constructed tree for given // postfix expression et * constructTree ( char postfix []) { stack < et *> st ; et * t , * t1 , * t2 ; // Traverse through every character of // input expression for ( int i = 0 ; i < strlen ( postfix ); i ++ ) { // If operand, simply push into stack if ( ! isOperator ( postfix [ i ])) { t = newNode ( postfix [ i ]); st . push ( t ); } else // operator { t = newNode ( postfix [ i ]); // Pop two top nodes t1 = st . top (); // Store top st . pop (); // Remove top t2 = st . top (); st . pop (); // make them children t -> right = t1 ; t -> left = t2 ; // Add this subexpression to stack st . push ( t ); } } // only element will be root of expression // tree t = st . top (); st . pop (); return t ; } // Driver program to test above int main () { char postfix [] = \"ab+ef*g*-\" ; et * r = constructTree ( postfix ); printf ( \"infix expression is \\n \" ); inorder ( r ); return 0 ; } // g++ test.cpp --std=c++11 -pedantic -Wall -Wextra NOTE: \u81ea\u5e95\u5411\u4e0a\uff0c\u4e24\u4e24\u5408\u5e76 Output: infix expression is a + b - e * f * g This article is contributed by Utkarsh Trivedi . Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.","title":"Construction of Expression Tree:"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Program-to-convert-Infix-notation-to-Expression-Tree/","text":"geeksforgeeks Program to convert Infix notation to Expression Tree","title":"Introduction"},{"location":"Guide/Expression-tree/geeksforgeeks-Expression-Tree/Program-to-convert-Infix-notation-to-Expression-Tree/#geeksforgeeks#program#to#convert#infix#notation#to#expression#tree","text":"","title":"geeksforgeeks Program to convert Infix notation to Expression Tree"},{"location":"Guide/Name-binding/","text":"Name binding What is name binding? \u5176\u5b9e\uff0c\u4ecesymbol\u7684\u89d2\u5ea6\u6765\u7406\u89e3\u662f\u6bd4\u8f83\u5bb9\u6613\u7684: \u4e00\u4e2aname\u5c31\u662f\u4e00\u4e2asymbol\uff0csymbol\u662f\u7528\u4e8e\u6307\u4ee3\u7684\uff0c\u6240\u8c13binding\u662f\u7ed9\u8fd9\u4e2aname\u6307\u5b9avalue\u3002\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1utexas CS 378, Symbolic Programming \u3002 \u5173\u4e8esymbol\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Representation-and-computation\\Symbol-expression \u7ae0\u8282\u3002 wikipedia Name-binding For bound variables in mathematics, see free variables and bound variables . In programming languages , name binding is the association of entities (data and/or code) with identifiers .[ 1] An identifier bound to an object is said to reference that object(\u6b64\u5904\u7684object\u5c31\u662f\u4e0a\u4e00\u53e5\u63d0\u5230\u7684entity). Machine languages (\u53ef\u4ee5\u76f4\u63a5\u7531CPU\u8fd0\u884c\u7684\u8bed\u8a00) have no built-in notion of identifiers(\u673a\u5668\u8bed\u8a00\u6ca1\u6709\u5185\u7f6e\u7684identifier\u7684\u6982\u5ff5), but name-object bindings(name\u548cobject\u7684binding) as a service and notation for the programmer is implemented by programming languages(binding\u662f\u7531programming languages\u6765\u5b9e\u73b0\u7684). Binding is intimately(\u7d27\u5bc6\u5730) connected with scoping , as scope determines which names bind to which objects \u2013 at which locations in the program code ( lexically ) and in which one of the possible execution paths ( temporally )(binding\u4e0escope\u5bc6\u5207\u76f8\u5173\uff0c\u56e0\u4e3ascope \u786e\u5b9a\u54ea\u4e9bname\u7ed1\u5b9a\u5230\u54ea\u4e9bobject - \u7a0b\u5e8f\u4ee3\u7801\u4e2d\u7684\u54ea\u4e9b\u4f4d\u7f6e\uff08\u8bcd\u6cd5\uff09\u4ee5\u53ca\u54ea\u4e2a\u53ef\u80fd\u7684\u6267\u884c\u8def\u5f84\uff08\u4e34\u65f6\uff09). NOTE: \u5173\u4e8e \"data and/or code\"\uff0c\u53c2\u89c1\u5de5\u7a0bHardware\u7684 Computer-architecture\\Stored-program-computer \u7ae0\u8282\u7684\"Function and data model\"\u7ae0\u8282\u3002 \u5176\u5b9e\u6240\u8c13\u7684name binding\u662fcompiler\u6765\u505a\u7684\u4e00\u4ef6\u4e8b\u60c5\uff0ccompiler\u5c06\u5404\u4e2aidentifier\u548c\u5b83\u5bf9\u5e94\u7684entity\u8fdb\u884c\u5173\u8054\u4ece\u800c\u6b63\u786e\u5730\u7406\u89e3program\u3002 Use of an identifier id in a context that establishes a binding for id is called a binding (or defining) occurrence (\u7ed1\u5b9a\u4e8b\u4ef6\u6216\u5b9a\u4e49\u4e8b\u4ef6. In all other occurrences (e.g., in expressions, assignments, and subprogram calls), an identifier stands for what it is bound to; such occurrences are called applied occurrences (\u5728\u4e3aid\u5efa\u7acb\u7ed1\u5b9a\u7684\u4e0a\u4e0b\u6587\u4e2d\u4f7f\u7528\u6807\u8bc6\u7b26id\u79f0\u4e3a\u7ed1\u5b9a\uff08\u6216\u5b9a\u4e49\uff09\u4e8b\u4ef6\u3002 \u5728\u6240\u6709\u5176\u4ed6\u4e8b\u4ef6\u4e2d\uff08\u4f8b\u5982\uff0c\u5728\u8868\u8fbe\u5f0f\uff0c\u8d4b\u503c\u548c\u5b50\u7a0b\u5e8f\u8c03\u7528\u4e2d\uff09\uff0c\u6807\u8bc6\u7b26\u4ee3\u8868\u5b83\u6240\u7ed1\u5b9a\u7684\u5185\u5bb9; \u8fd9\u79cd\u4e8b\u4ef6\u79f0\u4e3a\u5e94\u7528\u4e8b\u4ef6). NOTE: \u601d\u8003\uff1aname binding\u548cscope\u7684\u5173\u7cfb \u603b\u7ed3\uff1a\u663e\u7136name-binding\u548cby reference\u662f\u5bc6\u5207\u76f8\u5173\u7684\u3002 Binding time Static binding (or early binding ) is name binding performed before the program is run [ 2] . Dynamic binding (or late binding or virtual binding ) is name binding performed as the program is running [ 2] . An example of a static binding is a direct C function call: the function referenced by the identifier cannot change at runtime. But an example of dynamic binding is dynamic dispatch , as in a C++ virtual method call. Since the specific type of a polymorphic object is not known before runtime (in general), the executed function is dynamically bound. Take, for example, the following Java code: public void foo ( java . util . List < String > list ) { list . add ( \"bar\" ); } List is an interface , so list must refer to a subtype of it. Is it a reference to a LinkedList , an ArrayList , or some other subtype of List ? The actual method referenced by add is not known until runtime. In C, such instance of dynamic binding may be a call to a function pointed by a variable or expression of a function pointer type whose value is unknown until it actually gets evaluated at run-time. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8fd8\u63cf\u8ff0\u4e86C\u8bed\u8a00\u4e2d\u5b9e\u73b0**dynamic binding**\u7684\u65b9\u5f0f\u3002 Rebinding and mutation Rebinding should not be confused with mutation (\u7a81\u53d8\uff0c\u6539\u53d8). Rebinding is a change to the referencing identifier. Mutation is a change to the referenced entity. Consider the following Java code: LinkedList < String > list ; list = new LinkedList < String > (); list . add ( \"foo\" ); list = null ; The identifier list initially references nothing (it is uninitialized ); it is then rebound to reference an object (a linked list of strings). The linked list referenced by list is then mutated, adding a string to the list. Lastly, list is rebound to null . Late static Late static binding is a variant of binding somewhere between static and dynamic binding. Consider the following PHP example: class A { static $word = \"hello\"; static function hello() { print self::$word; } } class B extends A { static $word = \"bye\"; } B::hello(); In this example, the PHP interpreter binds the keyword self inside A::hello() to class A , and so the call to B::hello() produces the string \"hello\". If the semantics of self::$word had been based on late static binding, then the result would have been \"bye\". Beginning with PHP version 5.3, late static binding is supported.[ 3] Specifically, if self::$word in the above were changed to static::$word as shown in the following block, where the keyword static would only be bound at runtime, then the result of the call to B::hello() would be \"bye\": class A { static $word = \"hello\" ; static function hello () { print static :: $word ; } } class B extends A { static $word = \"bye\" ; } B :: hello (); wikipedia Late binding Late binding , dynamic binding ,[ 1] or dynamic linkage [ 2] \u2014though not an identical process to dynamically linking imported code libraries\u2014is a computer programming mechanism in which the method being called upon an object, or the function being called with arguments, is looked up by name at runtime . In other words, a name is associated with a particular operation or object at runtime, rather than during compilation.","title":"Introduction"},{"location":"Guide/Name-binding/#name#binding","text":"","title":"Name binding"},{"location":"Guide/Name-binding/#what#is#name#binding","text":"\u5176\u5b9e\uff0c\u4ecesymbol\u7684\u89d2\u5ea6\u6765\u7406\u89e3\u662f\u6bd4\u8f83\u5bb9\u6613\u7684: \u4e00\u4e2aname\u5c31\u662f\u4e00\u4e2asymbol\uff0csymbol\u662f\u7528\u4e8e\u6307\u4ee3\u7684\uff0c\u6240\u8c13binding\u662f\u7ed9\u8fd9\u4e2aname\u6307\u5b9avalue\u3002\u5173\u4e8e\u6b64\uff0c\u53c2\u89c1utexas CS 378, Symbolic Programming \u3002 \u5173\u4e8esymbol\uff0c\u53c2\u89c1\u5de5\u7a0bdiscrete\u7684 Relation-structure-computation\\Representation-and-computation\\Symbol-expression \u7ae0\u8282\u3002","title":"What is name binding?"},{"location":"Guide/Name-binding/#wikipedia#name-binding","text":"For bound variables in mathematics, see free variables and bound variables . In programming languages , name binding is the association of entities (data and/or code) with identifiers .[ 1] An identifier bound to an object is said to reference that object(\u6b64\u5904\u7684object\u5c31\u662f\u4e0a\u4e00\u53e5\u63d0\u5230\u7684entity). Machine languages (\u53ef\u4ee5\u76f4\u63a5\u7531CPU\u8fd0\u884c\u7684\u8bed\u8a00) have no built-in notion of identifiers(\u673a\u5668\u8bed\u8a00\u6ca1\u6709\u5185\u7f6e\u7684identifier\u7684\u6982\u5ff5), but name-object bindings(name\u548cobject\u7684binding) as a service and notation for the programmer is implemented by programming languages(binding\u662f\u7531programming languages\u6765\u5b9e\u73b0\u7684). Binding is intimately(\u7d27\u5bc6\u5730) connected with scoping , as scope determines which names bind to which objects \u2013 at which locations in the program code ( lexically ) and in which one of the possible execution paths ( temporally )(binding\u4e0escope\u5bc6\u5207\u76f8\u5173\uff0c\u56e0\u4e3ascope \u786e\u5b9a\u54ea\u4e9bname\u7ed1\u5b9a\u5230\u54ea\u4e9bobject - \u7a0b\u5e8f\u4ee3\u7801\u4e2d\u7684\u54ea\u4e9b\u4f4d\u7f6e\uff08\u8bcd\u6cd5\uff09\u4ee5\u53ca\u54ea\u4e2a\u53ef\u80fd\u7684\u6267\u884c\u8def\u5f84\uff08\u4e34\u65f6\uff09). NOTE: \u5173\u4e8e \"data and/or code\"\uff0c\u53c2\u89c1\u5de5\u7a0bHardware\u7684 Computer-architecture\\Stored-program-computer \u7ae0\u8282\u7684\"Function and data model\"\u7ae0\u8282\u3002 \u5176\u5b9e\u6240\u8c13\u7684name binding\u662fcompiler\u6765\u505a\u7684\u4e00\u4ef6\u4e8b\u60c5\uff0ccompiler\u5c06\u5404\u4e2aidentifier\u548c\u5b83\u5bf9\u5e94\u7684entity\u8fdb\u884c\u5173\u8054\u4ece\u800c\u6b63\u786e\u5730\u7406\u89e3program\u3002 Use of an identifier id in a context that establishes a binding for id is called a binding (or defining) occurrence (\u7ed1\u5b9a\u4e8b\u4ef6\u6216\u5b9a\u4e49\u4e8b\u4ef6. In all other occurrences (e.g., in expressions, assignments, and subprogram calls), an identifier stands for what it is bound to; such occurrences are called applied occurrences (\u5728\u4e3aid\u5efa\u7acb\u7ed1\u5b9a\u7684\u4e0a\u4e0b\u6587\u4e2d\u4f7f\u7528\u6807\u8bc6\u7b26id\u79f0\u4e3a\u7ed1\u5b9a\uff08\u6216\u5b9a\u4e49\uff09\u4e8b\u4ef6\u3002 \u5728\u6240\u6709\u5176\u4ed6\u4e8b\u4ef6\u4e2d\uff08\u4f8b\u5982\uff0c\u5728\u8868\u8fbe\u5f0f\uff0c\u8d4b\u503c\u548c\u5b50\u7a0b\u5e8f\u8c03\u7528\u4e2d\uff09\uff0c\u6807\u8bc6\u7b26\u4ee3\u8868\u5b83\u6240\u7ed1\u5b9a\u7684\u5185\u5bb9; \u8fd9\u79cd\u4e8b\u4ef6\u79f0\u4e3a\u5e94\u7528\u4e8b\u4ef6). NOTE: \u601d\u8003\uff1aname binding\u548cscope\u7684\u5173\u7cfb \u603b\u7ed3\uff1a\u663e\u7136name-binding\u548cby reference\u662f\u5bc6\u5207\u76f8\u5173\u7684\u3002","title":"wikipedia Name-binding"},{"location":"Guide/Name-binding/#binding#time","text":"Static binding (or early binding ) is name binding performed before the program is run [ 2] . Dynamic binding (or late binding or virtual binding ) is name binding performed as the program is running [ 2] . An example of a static binding is a direct C function call: the function referenced by the identifier cannot change at runtime. But an example of dynamic binding is dynamic dispatch , as in a C++ virtual method call. Since the specific type of a polymorphic object is not known before runtime (in general), the executed function is dynamically bound. Take, for example, the following Java code: public void foo ( java . util . List < String > list ) { list . add ( \"bar\" ); } List is an interface , so list must refer to a subtype of it. Is it a reference to a LinkedList , an ArrayList , or some other subtype of List ? The actual method referenced by add is not known until runtime. In C, such instance of dynamic binding may be a call to a function pointed by a variable or expression of a function pointer type whose value is unknown until it actually gets evaluated at run-time. NOTE: \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u8fd8\u63cf\u8ff0\u4e86C\u8bed\u8a00\u4e2d\u5b9e\u73b0**dynamic binding**\u7684\u65b9\u5f0f\u3002","title":"Binding time"},{"location":"Guide/Name-binding/#rebinding#and#mutation","text":"Rebinding should not be confused with mutation (\u7a81\u53d8\uff0c\u6539\u53d8). Rebinding is a change to the referencing identifier. Mutation is a change to the referenced entity. Consider the following Java code: LinkedList < String > list ; list = new LinkedList < String > (); list . add ( \"foo\" ); list = null ; The identifier list initially references nothing (it is uninitialized ); it is then rebound to reference an object (a linked list of strings). The linked list referenced by list is then mutated, adding a string to the list. Lastly, list is rebound to null .","title":"Rebinding and mutation"},{"location":"Guide/Name-binding/#late#static","text":"Late static binding is a variant of binding somewhere between static and dynamic binding. Consider the following PHP example: class A { static $word = \"hello\"; static function hello() { print self::$word; } } class B extends A { static $word = \"bye\"; } B::hello(); In this example, the PHP interpreter binds the keyword self inside A::hello() to class A , and so the call to B::hello() produces the string \"hello\". If the semantics of self::$word had been based on late static binding, then the result would have been \"bye\". Beginning with PHP version 5.3, late static binding is supported.[ 3] Specifically, if self::$word in the above were changed to static::$word as shown in the following block, where the keyword static would only be bound at runtime, then the result of the call to B::hello() would be \"bye\": class A { static $word = \"hello\" ; static function hello () { print static :: $word ; } } class B extends A { static $word = \"bye\" ; } B :: hello ();","title":"Late static"},{"location":"Guide/Name-binding/#wikipedia#late#binding","text":"Late binding , dynamic binding ,[ 1] or dynamic linkage [ 2] \u2014though not an identical process to dynamically linking imported code libraries\u2014is a computer programming mechanism in which the method being called upon an object, or the function being called with arguments, is looked up by name at runtime . In other words, a name is associated with a particular operation or object at runtime, rather than during compilation.","title":"wikipedia Late binding"},{"location":"Guide/Parse-tree%26syntax-tree/","text":"Parse tree\u3001syntax tree parse tree\u548csyntax tree\u5728\u672c\u4e66\u4e2d\u591a\u6b21\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u5bf9\u6bd4\u4e00\u4e0b\u3002 \u4ecb\u7ecdparse tree\u7684\u4e3b\u8981\u7ae0\u8282\u6709\uff1a \u4e00\u30012.2.3 Parse Trees \u4e8c\u30012.4 Parsing \u4e09\u30012.9 Summary of Chapter 2 \u56db\u3001Chapter 4 Syntax Analysis \u4e94\u3001Chapter 5 Syntax-Directed Translation \u4ecb\u7ecdsyntax tree\u7684\u7ae0\u8282\u6709\uff1a \u4e00\u30012.1 Introduction \u4e8c\u30012.8.2 Construction of Syntax Trees \u4e09\u30012.8.4 Three-Address Code \u56db\u30015.3.1 Construction of Syntax Trees \u4e94\u30016.1 Variants of Syntax Trees TODO Parse tree VS DFA \u5728cpython\u7684 pgen \u4e2d\uff0c\u4f7f\u7528DFA\u6765\u8868\u793aproduction\u7684body\uff0c\u8fd9\u662f\u56e0\u4e3apython\u7684grammar\u6240\u4f7f\u7528\u7684\u662fEBNF\uff0c\u5176\u4e2d\u6269\u5c55\u4e86\u5bf9regular expression\u7684\u652f\u6301\u3002 \u5728 Natural Language Processing with Python \u7684 8. Analyzing Sentence Structure \u4e2d\u7ed9\u6211\u4eec\u6f14\u793a\u4e86\u5b9a\u4e49CFG\uff0c\u5e76\u6309\u7167\u8fd9\u4e2aCFG\u6765\u5bf9\u6587\u672c\u8fdb\u884c\u89e3\u6790\u5f97\u5230parse tree\u3002\u90a3\u5728nltk\u4e2d\u662f\u5982\u4f55\u6765\u8868\u793a\u5b83\u7684grammar\u7684\u5462\uff1fnltk\u7684parser\u662f\u5982\u4f55\u8fd0\u7528\u5b83\u7684grammar\u7684\u6765\u5b9e\u73b0parse\u7684\u5462\uff1f","title":"Introduction"},{"location":"Guide/Parse-tree%26syntax-tree/#parse#treesyntax#tree","text":"parse tree\u548csyntax tree\u5728\u672c\u4e66\u4e2d\u591a\u6b21\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u5bf9\u6bd4\u4e00\u4e0b\u3002 \u4ecb\u7ecdparse tree\u7684\u4e3b\u8981\u7ae0\u8282\u6709\uff1a \u4e00\u30012.2.3 Parse Trees \u4e8c\u30012.4 Parsing \u4e09\u30012.9 Summary of Chapter 2 \u56db\u3001Chapter 4 Syntax Analysis \u4e94\u3001Chapter 5 Syntax-Directed Translation \u4ecb\u7ecdsyntax tree\u7684\u7ae0\u8282\u6709\uff1a \u4e00\u30012.1 Introduction \u4e8c\u30012.8.2 Construction of Syntax Trees \u4e09\u30012.8.4 Three-Address Code \u56db\u30015.3.1 Construction of Syntax Trees \u4e94\u30016.1 Variants of Syntax Trees","title":"Parse tree\u3001syntax tree"},{"location":"Guide/Parse-tree%26syntax-tree/#todo","text":"","title":"TODO"},{"location":"Guide/Parse-tree%26syntax-tree/#parse#tree#vs#dfa","text":"\u5728cpython\u7684 pgen \u4e2d\uff0c\u4f7f\u7528DFA\u6765\u8868\u793aproduction\u7684body\uff0c\u8fd9\u662f\u56e0\u4e3apython\u7684grammar\u6240\u4f7f\u7528\u7684\u662fEBNF\uff0c\u5176\u4e2d\u6269\u5c55\u4e86\u5bf9regular expression\u7684\u652f\u6301\u3002 \u5728 Natural Language Processing with Python \u7684 8. Analyzing Sentence Structure \u4e2d\u7ed9\u6211\u4eec\u6f14\u793a\u4e86\u5b9a\u4e49CFG\uff0c\u5e76\u6309\u7167\u8fd9\u4e2aCFG\u6765\u5bf9\u6587\u672c\u8fdb\u884c\u89e3\u6790\u5f97\u5230parse tree\u3002\u90a3\u5728nltk\u4e2d\u662f\u5982\u4f55\u6765\u8868\u793a\u5b83\u7684grammar\u7684\u5462\uff1fnltk\u7684parser\u662f\u5982\u4f55\u8fd0\u7528\u5b83\u7684grammar\u7684\u6765\u5b9e\u73b0parse\u7684\u5462\uff1f","title":"Parse tree VS DFA"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/","text":"Abstract syntax tree wikipedia Abstract syntax tree In computer science , an abstract syntax tree ( AST ), or just syntax tree , is a tree representation of the abstract syntactic structure of source code written in a programming language . Each node of the tree denotes a construct occurring in the source code. NOTE : \u8868\u793a\u6e90\u4ee3\u7801\u7684\u62bd\u8c61\u8bed\u6cd5\u7ed3\u6784\uff0c\u6811\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u8868\u793a\u6e90\u4ee3\u7801\u4e2d\u7684\u4e00\u4e2aconstruct\u3002 The syntax is \"abstract\" in the sense that it does not represent every detail appearing in the real syntax , but rather just the structural or content-related details . For instance, grouping parentheses are implicit in the tree structure , so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. This distinguishes abstract syntax trees from concrete syntax trees , traditionally designated parse trees . Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis . NOTE: parse tree\u548cabstract syntax tree\u7684\u6784\u5efa Abstract syntax trees are also used in program analysis and program transformation systems. An abstract syntax tree for the following code for the Euclidean algorithm while b \u2260 0 if a > b a := a \u2212 b else b := b \u2212 a return a Application in compilers Abstract syntax trees are data structures widely used in compilers to represent the structure of program code. An AST is usually the result of the syntax analysis phase of a compiler. It often serves as an intermediate representation of the program through several stages that the compiler requires, and has a strong impact on the final output of the compiler. Motivation An AST has several properties that aid the further steps of the compilation process: An AST can be edited and enhanced with information such as properties and annotations for every element it contains. Such editing and annotation is impossible with the source code of a program, since it would imply changing it. Compared to the source code , an AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). An AST usually contains extra information about the program, due to the consecutive stages of analysis by the compiler. For example, it may store the position of each element in the source code, allowing the compiler to print useful error messages . ASTs are needed because of the inherent nature of programming languages and their documentation. Languages are often ambiguous by nature. In order to avoid this ambiguity, programming languages are often specified as a context-free grammar (CFG). However, there are often aspects of programming languages that a CFG can't express, but are part of the language and are documented in its specification. These are details that require a context to determine their validity and behaviour. For example, if a language allows new types to be declared, a CFG cannot predict the names of such types nor the way in which they should be used. Even if a language has a predefined set of types, enforcing proper usage usually requires some context. Another example is duck typing , where the type of an element can change depending on context. Operator overloading is yet another case where correct usage and final function are determined based on the context. Java provides an excellent example, where the '+' operator is both numerical addition and concatenation of strings. Although there are other data structures involved in the inner workings of a compiler, the AST performs a unique function. During the first stage, the syntax analysis stage, a compiler produces a parse tree. This parse tree can be used to perform almost all functions of a compiler by means of s**yntax-directed translation**. Although this method can lead to a more efficient compiler, it goes against the software engineering principles of writing and maintaining programs[ citation needed ]. Another advantage that the AST has over a parse tree is the size, particularly the smaller height of the AST and the smaller number of elements. Design The design of an AST is often closely linked with the design of a compiler and its expected features. Core requirements include the following: Variable types must be preserved, as well as the location of each declaration in source code. The order of executable statements must be explicitly represented and well defined. Left and right components of binary operations must be stored and correctly identified. Identifiers and their assigned values must be stored for assignment statements. These requirements can be used to design the data structure for the AST. Some operations will always require two elements, such as the two terms for addition. However, some language constructs require an arbitrarily large number of children, such as argument lists passed to programs from the command shell . As a result, an AST used to represent code written in such a language has to also be flexible enough to allow for quick addition of an unknown quantity of children. To support compiler verification it should be possible to unparse an AST into source code form. The source code produced should be sufficiently similar to the original in appearance and identical in execution, upon recompilation. Design patterns Due to the complexity of the requirements for an AST and the overall complexity of a compiler, it is beneficial to apply sound software development principles. One of these is to use proven design patterns to enhance modularity and ease of development. Different operations don't necessarily have different types, so it is important to have a sound node class hierarchy. This is crucial in the creation and the modification of the AST as the compiler progresses. Because the compiler traverses the tree several times to determine syntactic correctness, it is important to make traversing the tree a simple operation. The compiler executes a specific set of operations, depending on the type of each node, upon reaching it, so it often makes sense to use the visitor pattern . Usage The AST is used intensively during semantic analysis , where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program. After verifying correctness, the AST serves as the base for code generation. The AST is often used to generate an intermediate representation (IR), sometimes called an intermediate language , for the code generation.","title":"Introduction"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/#abstract#syntax#tree","text":"","title":"Abstract syntax tree"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/#wikipedia#abstract#syntax#tree","text":"In computer science , an abstract syntax tree ( AST ), or just syntax tree , is a tree representation of the abstract syntactic structure of source code written in a programming language . Each node of the tree denotes a construct occurring in the source code. NOTE : \u8868\u793a\u6e90\u4ee3\u7801\u7684\u62bd\u8c61\u8bed\u6cd5\u7ed3\u6784\uff0c\u6811\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u8868\u793a\u6e90\u4ee3\u7801\u4e2d\u7684\u4e00\u4e2aconstruct\u3002 The syntax is \"abstract\" in the sense that it does not represent every detail appearing in the real syntax , but rather just the structural or content-related details . For instance, grouping parentheses are implicit in the tree structure , so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. This distinguishes abstract syntax trees from concrete syntax trees , traditionally designated parse trees . Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis . NOTE: parse tree\u548cabstract syntax tree\u7684\u6784\u5efa Abstract syntax trees are also used in program analysis and program transformation systems. An abstract syntax tree for the following code for the Euclidean algorithm while b \u2260 0 if a > b a := a \u2212 b else b := b \u2212 a return a","title":"wikipedia Abstract syntax tree"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/#application#in#compilers","text":"Abstract syntax trees are data structures widely used in compilers to represent the structure of program code. An AST is usually the result of the syntax analysis phase of a compiler. It often serves as an intermediate representation of the program through several stages that the compiler requires, and has a strong impact on the final output of the compiler.","title":"Application in compilers"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/#motivation","text":"An AST has several properties that aid the further steps of the compilation process: An AST can be edited and enhanced with information such as properties and annotations for every element it contains. Such editing and annotation is impossible with the source code of a program, since it would imply changing it. Compared to the source code , an AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). An AST usually contains extra information about the program, due to the consecutive stages of analysis by the compiler. For example, it may store the position of each element in the source code, allowing the compiler to print useful error messages . ASTs are needed because of the inherent nature of programming languages and their documentation. Languages are often ambiguous by nature. In order to avoid this ambiguity, programming languages are often specified as a context-free grammar (CFG). However, there are often aspects of programming languages that a CFG can't express, but are part of the language and are documented in its specification. These are details that require a context to determine their validity and behaviour. For example, if a language allows new types to be declared, a CFG cannot predict the names of such types nor the way in which they should be used. Even if a language has a predefined set of types, enforcing proper usage usually requires some context. Another example is duck typing , where the type of an element can change depending on context. Operator overloading is yet another case where correct usage and final function are determined based on the context. Java provides an excellent example, where the '+' operator is both numerical addition and concatenation of strings. Although there are other data structures involved in the inner workings of a compiler, the AST performs a unique function. During the first stage, the syntax analysis stage, a compiler produces a parse tree. This parse tree can be used to perform almost all functions of a compiler by means of s**yntax-directed translation**. Although this method can lead to a more efficient compiler, it goes against the software engineering principles of writing and maintaining programs[ citation needed ]. Another advantage that the AST has over a parse tree is the size, particularly the smaller height of the AST and the smaller number of elements.","title":"Motivation"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/#design","text":"The design of an AST is often closely linked with the design of a compiler and its expected features. Core requirements include the following: Variable types must be preserved, as well as the location of each declaration in source code. The order of executable statements must be explicitly represented and well defined. Left and right components of binary operations must be stored and correctly identified. Identifiers and their assigned values must be stored for assignment statements. These requirements can be used to design the data structure for the AST. Some operations will always require two elements, such as the two terms for addition. However, some language constructs require an arbitrarily large number of children, such as argument lists passed to programs from the command shell . As a result, an AST used to represent code written in such a language has to also be flexible enough to allow for quick addition of an unknown quantity of children. To support compiler verification it should be possible to unparse an AST into source code form. The source code produced should be sufficiently similar to the original in appearance and identical in execution, upon recompilation.","title":"Design"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/#design#patterns","text":"Due to the complexity of the requirements for an AST and the overall complexity of a compiler, it is beneficial to apply sound software development principles. One of these is to use proven design patterns to enhance modularity and ease of development. Different operations don't necessarily have different types, so it is important to have a sound node class hierarchy. This is crucial in the creation and the modification of the AST as the compiler progresses. Because the compiler traverses the tree several times to determine syntactic correctness, it is important to make traversing the tree a simple operation. The compiler executes a specific set of operations, depending on the type of each node, upon reaching it, so it often makes sense to use the visitor pattern .","title":"Design patterns"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree/#usage","text":"The AST is used intensively during semantic analysis , where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program. After verifying correctness, the AST serves as the base for code generation. The AST is often used to generate an intermediate representation (IR), sometimes called an intermediate language , for the code generation.","title":"Usage"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/","text":"Abstract Syntax Tree VS Concrete Syntax Tree \u9f99\u4e66\u4e2d\u533a\u5206parse tree\u548csyntax tree\u7684\u7ae0\u8282\u6709\uff1a \u4e00\u30012.5.1 Abstract and Concrete Syntax Abstract syntax trees, or simply syntax trees , resemble(\u7c7b\u4f3c\u4e8e) parse trees to an extent. However, in the syntax tree , interior nodes represent programming constructs while in the parse tree , the interior nodes represent nonterminals . Many nonterminals of a grammar represent programming constructs, but others are \"helpers\" of one sort of another, such as those representing terms, factors, or other variations of expressions. In the syntax tree, these helpers typically are not needed and are hence dropped. To emphasize the contrast, a parse tree is sometimes called a concrete syntax tree , and the underlying grammar is called a concrete syntax for the language. stackoverflow What is the difference between an Abstract Syntax Tree and a Concrete Syntax Tree? I've been reading a bit about how interpreters/compilers work, and one area where I'm getting confused is the difference between an AST and a CST. My understanding is that the parser makes a CST, hands it to the semantic analyzer which turns it into an AST . However, my understanding is that the semantic analyzer simply ensures that rules are followed. I don't really understand why it would actually make any changes to make it abstract rather than concrete. Is there something that I'm missing about the semantic analyzer , or is the difference between an AST and CST somewhat artificial? A A concrete syntax tree represents the source text exactly in parsed form. In general, it conforms to the context-free grammar defining the source language. However, the concrete grammar and tree have a lot of things that are necessary to make source text unambiguously(\u65e0\u4e8c\u4e49\u7684) parseable , but do not contribute to actual meaning. For example, to implement operator precedence , your CFG usually has several levels of expression components (term, factor, etc.), with the operators connecting them at the different levels (you add terms to get expressions, terms are composed of factors optionally multipled, etc.). To actually interpret or compile the language, however, you don't need this; you just need Expression nodes that have operators and operands. The abstract syntax tree is the result of simplifying the concrete syntax tree down to this things actually needed to represent the meaning of the program. This tree has a much simpler definition and is thus easier to process in the later stages of execution . You usually don't need to actually build a concrete syntax tree . The action routines in your YACC (or Antlr, or Menhir, or whatever...) grammar can directly build the abstract syntax tree , so the concrete syntax tree only exists as a conceptual entity representing the parse structure of your source text. COMMENTS : Supplements: the Python interpreter first builds a CST and then converts to AST. \u2013 cgsdfc Dec 2 '18 at 12:18 A A concrete syntax tree matches what the grammar rules say is the syntax. The purpose of the abstract syntax tree is have a \"simple\" representation of what's essential in \"the syntax tree\". A real value in the AST IMHO is that it is smaller than the CST, and therefore takes less time to process. (You might say, who cares? But I work with a tool where we have tens of millions of nodes live at once!). Most parser generators that have any support for building syntax trees insist that you personally specify exactly how they get built under the assumption that your tree nodes will be \"simpler\" than the CST (and in that, they are generally right, as programmers are pretty lazy). Arguably it means you have to code fewer tree visitor functions, and that's valuable, too, in that it minimizes engineering energy. When you have 3500 rules (e.g., for COBOL) this matters. And this \"simpler\"ness leads to the good property of \"smallness\". But having such ASTs creates a problem that wasn't there: it doesn't match the grammar, and now you have to mentally track both of them. And when there are 1500 AST nodes for a 3500 rule grammar, this matters a lot. And if the grammar evolves (they always do!), now you have two giant sets of things to keep in synch. Another solution is to let the parser simply build CST nodes for you and just use those. This is a huge advantage when building the grammars: there's no need to invent 1500 special AST nodes to model 3500 grammar rules. Just think about the tree being isomorphic to the grammar. From the point of view of the grammar engineer this is completely brainless, which lets him focus on getting the grammar right and hacking at it to his heart's content. Arguably you have to write more node visitor rules, but that can be managed. More on this later. What we do with the DMS Software Reengineering Toolkit is to automatically build a CST based on the results of a (GLR) parsing process. DMS then automatically constructs an \"compressed\" CST for space efficiency reasons, by eliminating non-value carrying terminals (keywords, punctation), semantically useless unary productions, and forming lists for grammar rule pairs that are list like: L = e ; L = L e ; L2 = e2 ; L2 = L2 ',' e2 ; and a wide variety of variations of such forms. You think in terms of the grammar rules and the virtual CST; the tool operates on the compressed representation. Easy on your brain, faster/smaller at runtime. Remarkably, the compressed CST built this way looks a lot an AST that you might have designed by hand (see link at end to examples). In particular, the compressed CST doesn't carry any nodes that are just concrete syntax. There are minor bits of awkwardness: for example while the concrete nodes for '(' and ')' classically found in expression subgrammars are not in the tree, a \"parentheses node\" does appear in the compressed CST and has to be handled. A true AST would not have this. This seems like a pretty small price to pay for the convenience of not have to specify the AST construction, ever. And the documentation for the tree is always available and correct: the grammar is the documentation. How do we avoid \"extra visitors\"? We don't entirely, but DMS provides an AST library that walks the AST and handles the differences between the CST and the AST transparently. DMS also offers an \"attribute grammar\" evaluator (AGE), which is a method for passing values computed a nodes up and down the tree; the AGE handles all the tree representation issues and so the tool engineer only worries about writing computations effectively directly on the grammar rules themselves. Finally, DMS also provides \"surface-syntax\" patterns, which allows code fragments from the grammar to used to find specific types of subtrees, without knowing most of the node types involved. One of the other answers observes that if you want to build tools that can regenerate source, your AST will have to match the CST. That's not really right, but it is far easier to regenerate the source if you have CST nodes. DMS generates most of the prettyprinter automatically because it has access to both :-} Bottom line: ASTs are good for small, both phyiscal and conceptual. Automated AST construction from the CST provides both, and lets you avoid the problem of tracking two different sets. EDIT March 2015: Link to examples of CST vs. \"AST\" built this way A This blog post may be helpful. It seems to me that the AST \"throws away\" a lot of intermediate grammatical/structural information that wouldn't contribute to semantics. For example, you don't care that 3 is an atom is a term is a factor is a.... You just care that it's 3 when you're implementing the exponentiation expression or whatever. stackoverflow What's the difference between parse tree and AST? Are they generated by different phases of a compiling process? Or are they just different names for the same thing? COMMENTS : 1\u3001 Parse Tree is the result of your grammar with its artifacts (you can write an infinity of grammars for the same language), an AST reduce the Parse Tree the closest possible to the language. Several grammars for the same language will give different parse trees but should result to the same AST. (you can also reduce different scripts (different parse trees from the same grammar) to the same AST) \u2013 Guillaume86 Aug 29 '12 at 14:38 A This is based on the Expression Evaluator grammar by Terrence Parr. The grammar for this example: grammar Expr002; options { output=AST; ASTLabelType=CommonTree; // type of $stat.tree ref etc... } prog : ( stat )+ ; stat : expr NEWLINE -> expr | ID '=' expr NEWLINE -> ^('=' ID expr) | NEWLINE -> ; expr : multExpr (( '+'^ | '-'^ ) multExpr)* ; multExpr : atom ('*'^ atom)* ; atom : INT | ID | '('! expr ')'! ; ID : ('a'..'z' | 'A'..'Z' )+ ; INT : '0'..'9'+ ; NEWLINE : '\\r'? '\\n' ; WS : ( ' ' | '\\t' )+ { skip(); } ; Input x=1 y=2 3*(x+y) Parse Tree The parse tree is a concrete representation of the input. The parse tree retains all of the information of the input. The empty boxes represent whitespace, i.e. end of line. AST The AST is an abstract representation of the input. Notice that parens are not present in the AST because the associations are derivable from the tree structure. For a more through explanation see Compilers and Compiler Generators pg. 23 or Abstract Syntax Trees on pg. 21 in Syntax and Semantics of Programming Languages A From what I understand, the AST focuses more on the abstract relationships between the components of source code, while the parse tree focuses on the actual implementation of the grammar utilized by the language, including the nitpicky details. They are definitely not the same, since another term for \"parse tree\" is \"concrete syntax tree\". I found this page which attempts to resolve this exact question. A The DSL book from Martin Fowler explains this nicely. The AST only contains all 'useful' elements that will be used for further processing, while the parse tree contains all the artifacts (spaces, brackets, ...) from the original document you parse A Take the pascal assignment Age:= 42; The syntax tree would look just like the source code. Below I am putting brackets around the nodes. [Age][:=][42][;] An abstract tree would look like this [=][Age][42] The assignment becomes a node with 2 elements, Age and 42 . The idea is that you can execute the assignment. Also note that the pascal syntax disappears. Thus it is possible to have more than one language generate the same AST. This is useful for cross language script engines. TODO stackoverflow What would an AST (abstract syntax tree) for an object-oriented programming language look like? stackoverflow Compiling an AST back to source code thegreenplace Abstract vs. Concrete Syntax Trees draft \u5728\u300aCompilers Principles, Techniques, & Tools Second Edition\u300b4.2.4 Parse Trees and Derivations\u8282\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A parse tree is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal A in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation. expression tree\u7684interior node\u662foperator\u3002 \u4e24\u8005\u4e4b\u95f4\u65e2\u5b58\u5728\u7740\u76f8\u540c\u70b9\u4e5f\u5b58\u5728\u7740\u4e0d\u540c\u70b9\u3002","title":"Introduction"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#abstract#syntax#tree#vs#concrete#syntax#tree","text":"\u9f99\u4e66\u4e2d\u533a\u5206parse tree\u548csyntax tree\u7684\u7ae0\u8282\u6709\uff1a \u4e00\u30012.5.1 Abstract and Concrete Syntax Abstract syntax trees, or simply syntax trees , resemble(\u7c7b\u4f3c\u4e8e) parse trees to an extent. However, in the syntax tree , interior nodes represent programming constructs while in the parse tree , the interior nodes represent nonterminals . Many nonterminals of a grammar represent programming constructs, but others are \"helpers\" of one sort of another, such as those representing terms, factors, or other variations of expressions. In the syntax tree, these helpers typically are not needed and are hence dropped. To emphasize the contrast, a parse tree is sometimes called a concrete syntax tree , and the underlying grammar is called a concrete syntax for the language.","title":"Abstract Syntax Tree VS Concrete Syntax Tree"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#stackoverflow#what#is#the#difference#between#an#abstract#syntax#tree#and#a#concrete#syntax#tree","text":"I've been reading a bit about how interpreters/compilers work, and one area where I'm getting confused is the difference between an AST and a CST. My understanding is that the parser makes a CST, hands it to the semantic analyzer which turns it into an AST . However, my understanding is that the semantic analyzer simply ensures that rules are followed. I don't really understand why it would actually make any changes to make it abstract rather than concrete. Is there something that I'm missing about the semantic analyzer , or is the difference between an AST and CST somewhat artificial?","title":"stackoverflow What is the difference between an Abstract Syntax Tree and a Concrete Syntax Tree?"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#a","text":"A concrete syntax tree represents the source text exactly in parsed form. In general, it conforms to the context-free grammar defining the source language. However, the concrete grammar and tree have a lot of things that are necessary to make source text unambiguously(\u65e0\u4e8c\u4e49\u7684) parseable , but do not contribute to actual meaning. For example, to implement operator precedence , your CFG usually has several levels of expression components (term, factor, etc.), with the operators connecting them at the different levels (you add terms to get expressions, terms are composed of factors optionally multipled, etc.). To actually interpret or compile the language, however, you don't need this; you just need Expression nodes that have operators and operands. The abstract syntax tree is the result of simplifying the concrete syntax tree down to this things actually needed to represent the meaning of the program. This tree has a much simpler definition and is thus easier to process in the later stages of execution . You usually don't need to actually build a concrete syntax tree . The action routines in your YACC (or Antlr, or Menhir, or whatever...) grammar can directly build the abstract syntax tree , so the concrete syntax tree only exists as a conceptual entity representing the parse structure of your source text. COMMENTS : Supplements: the Python interpreter first builds a CST and then converts to AST. \u2013 cgsdfc Dec 2 '18 at 12:18","title":"A"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#a_1","text":"A concrete syntax tree matches what the grammar rules say is the syntax. The purpose of the abstract syntax tree is have a \"simple\" representation of what's essential in \"the syntax tree\". A real value in the AST IMHO is that it is smaller than the CST, and therefore takes less time to process. (You might say, who cares? But I work with a tool where we have tens of millions of nodes live at once!). Most parser generators that have any support for building syntax trees insist that you personally specify exactly how they get built under the assumption that your tree nodes will be \"simpler\" than the CST (and in that, they are generally right, as programmers are pretty lazy). Arguably it means you have to code fewer tree visitor functions, and that's valuable, too, in that it minimizes engineering energy. When you have 3500 rules (e.g., for COBOL) this matters. And this \"simpler\"ness leads to the good property of \"smallness\". But having such ASTs creates a problem that wasn't there: it doesn't match the grammar, and now you have to mentally track both of them. And when there are 1500 AST nodes for a 3500 rule grammar, this matters a lot. And if the grammar evolves (they always do!), now you have two giant sets of things to keep in synch. Another solution is to let the parser simply build CST nodes for you and just use those. This is a huge advantage when building the grammars: there's no need to invent 1500 special AST nodes to model 3500 grammar rules. Just think about the tree being isomorphic to the grammar. From the point of view of the grammar engineer this is completely brainless, which lets him focus on getting the grammar right and hacking at it to his heart's content. Arguably you have to write more node visitor rules, but that can be managed. More on this later. What we do with the DMS Software Reengineering Toolkit is to automatically build a CST based on the results of a (GLR) parsing process. DMS then automatically constructs an \"compressed\" CST for space efficiency reasons, by eliminating non-value carrying terminals (keywords, punctation), semantically useless unary productions, and forming lists for grammar rule pairs that are list like: L = e ; L = L e ; L2 = e2 ; L2 = L2 ',' e2 ; and a wide variety of variations of such forms. You think in terms of the grammar rules and the virtual CST; the tool operates on the compressed representation. Easy on your brain, faster/smaller at runtime. Remarkably, the compressed CST built this way looks a lot an AST that you might have designed by hand (see link at end to examples). In particular, the compressed CST doesn't carry any nodes that are just concrete syntax. There are minor bits of awkwardness: for example while the concrete nodes for '(' and ')' classically found in expression subgrammars are not in the tree, a \"parentheses node\" does appear in the compressed CST and has to be handled. A true AST would not have this. This seems like a pretty small price to pay for the convenience of not have to specify the AST construction, ever. And the documentation for the tree is always available and correct: the grammar is the documentation. How do we avoid \"extra visitors\"? We don't entirely, but DMS provides an AST library that walks the AST and handles the differences between the CST and the AST transparently. DMS also offers an \"attribute grammar\" evaluator (AGE), which is a method for passing values computed a nodes up and down the tree; the AGE handles all the tree representation issues and so the tool engineer only worries about writing computations effectively directly on the grammar rules themselves. Finally, DMS also provides \"surface-syntax\" patterns, which allows code fragments from the grammar to used to find specific types of subtrees, without knowing most of the node types involved. One of the other answers observes that if you want to build tools that can regenerate source, your AST will have to match the CST. That's not really right, but it is far easier to regenerate the source if you have CST nodes. DMS generates most of the prettyprinter automatically because it has access to both :-} Bottom line: ASTs are good for small, both phyiscal and conceptual. Automated AST construction from the CST provides both, and lets you avoid the problem of tracking two different sets. EDIT March 2015: Link to examples of CST vs. \"AST\" built this way","title":"A"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#a_2","text":"This blog post may be helpful. It seems to me that the AST \"throws away\" a lot of intermediate grammatical/structural information that wouldn't contribute to semantics. For example, you don't care that 3 is an atom is a term is a factor is a.... You just care that it's 3 when you're implementing the exponentiation expression or whatever.","title":"A"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#stackoverflow#whats#the#difference#between#parse#tree#and#ast","text":"Are they generated by different phases of a compiling process? Or are they just different names for the same thing? COMMENTS : 1\u3001 Parse Tree is the result of your grammar with its artifacts (you can write an infinity of grammars for the same language), an AST reduce the Parse Tree the closest possible to the language. Several grammars for the same language will give different parse trees but should result to the same AST. (you can also reduce different scripts (different parse trees from the same grammar) to the same AST) \u2013 Guillaume86 Aug 29 '12 at 14:38","title":"stackoverflow What's the difference between parse tree and AST?"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#a_3","text":"This is based on the Expression Evaluator grammar by Terrence Parr. The grammar for this example: grammar Expr002; options { output=AST; ASTLabelType=CommonTree; // type of $stat.tree ref etc... } prog : ( stat )+ ; stat : expr NEWLINE -> expr | ID '=' expr NEWLINE -> ^('=' ID expr) | NEWLINE -> ; expr : multExpr (( '+'^ | '-'^ ) multExpr)* ; multExpr : atom ('*'^ atom)* ; atom : INT | ID | '('! expr ')'! ; ID : ('a'..'z' | 'A'..'Z' )+ ; INT : '0'..'9'+ ; NEWLINE : '\\r'? '\\n' ; WS : ( ' ' | '\\t' )+ { skip(); } ; Input x=1 y=2 3*(x+y) Parse Tree The parse tree is a concrete representation of the input. The parse tree retains all of the information of the input. The empty boxes represent whitespace, i.e. end of line. AST The AST is an abstract representation of the input. Notice that parens are not present in the AST because the associations are derivable from the tree structure. For a more through explanation see Compilers and Compiler Generators pg. 23 or Abstract Syntax Trees on pg. 21 in Syntax and Semantics of Programming Languages","title":"A"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#a_4","text":"From what I understand, the AST focuses more on the abstract relationships between the components of source code, while the parse tree focuses on the actual implementation of the grammar utilized by the language, including the nitpicky details. They are definitely not the same, since another term for \"parse tree\" is \"concrete syntax tree\". I found this page which attempts to resolve this exact question.","title":"A"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#a_5","text":"The DSL book from Martin Fowler explains this nicely. The AST only contains all 'useful' elements that will be used for further processing, while the parse tree contains all the artifacts (spaces, brackets, ...) from the original document you parse","title":"A"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#a_6","text":"Take the pascal assignment Age:= 42; The syntax tree would look just like the source code. Below I am putting brackets around the nodes. [Age][:=][42][;] An abstract tree would look like this [=][Age][42] The assignment becomes a node with 2 elements, Age and 42 . The idea is that you can execute the assignment. Also note that the pascal syntax disappears. Thus it is possible to have more than one language generate the same AST. This is useful for cross language script engines.","title":"A"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#todo","text":"stackoverflow What would an AST (abstract syntax tree) for an object-oriented programming language look like? stackoverflow Compiling an AST back to source code thegreenplace Abstract vs. Concrete Syntax Trees","title":"TODO"},{"location":"Guide/Parse-tree%26syntax-tree/Abstract-syntax-tree-VS-parse-tree/#draft","text":"\u5728\u300aCompilers Principles, Techniques, & Tools Second Edition\u300b4.2.4 Parse Trees and Derivations\u8282\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A parse tree is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal A in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation. expression tree\u7684interior node\u662foperator\u3002 \u4e24\u8005\u4e4b\u95f4\u65e2\u5b58\u5728\u7740\u76f8\u540c\u70b9\u4e5f\u5b58\u5728\u7740\u4e0d\u540c\u70b9\u3002","title":"draft"},{"location":"Guide/Parse-tree%26syntax-tree/Parse-tree/","text":"Parse tree wikipedia Parse tree A parse tree or parsing tree [ 1] or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar . The term parse tree itself is used primarily in computational linguistics ; in theoretical syntax, the term syntax tree is more common. Parse trees concretely[ clarification needed ] reflect the syntax of the input language, making them distinct from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, parse trees do not use distinct symbol shapes for different types of constituents \uff08\u6210\u5206\uff09. Parse trees are usually constructed based on either the constituency relation of constituency grammars ( phrase structure grammars ) or the dependency relation of dependency grammars . Parse trees may be generated for sentences in natural languages (see natural language processing ), as well as during processing of computer languages, such as programming languages . A related concept is that of phrase marker or P-marker , as used in transformational generative grammar . A phrase marker is a linguistic expression marked as to its phrase structure. This may be presented in the form of a tree, or as a bracketed expression. Phrase markers are generated by applying phrase structure rules , and themselves are subject to further transformational rules. A set of possible parse trees for a syntactically ambiguous sentence is called a \"parse forest.\" Constituency-based parse trees","title":"Introduction"},{"location":"Guide/Parse-tree%26syntax-tree/Parse-tree/#parse#tree","text":"","title":"Parse tree"},{"location":"Guide/Parse-tree%26syntax-tree/Parse-tree/#wikipedia#parse#tree","text":"A parse tree or parsing tree [ 1] or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar . The term parse tree itself is used primarily in computational linguistics ; in theoretical syntax, the term syntax tree is more common. Parse trees concretely[ clarification needed ] reflect the syntax of the input language, making them distinct from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, parse trees do not use distinct symbol shapes for different types of constituents \uff08\u6210\u5206\uff09. Parse trees are usually constructed based on either the constituency relation of constituency grammars ( phrase structure grammars ) or the dependency relation of dependency grammars . Parse trees may be generated for sentences in natural languages (see natural language processing ), as well as during processing of computer languages, such as programming languages . A related concept is that of phrase marker or P-marker , as used in transformational generative grammar . A phrase marker is a linguistic expression marked as to its phrase structure. This may be presented in the form of a tree, or as a bracketed expression. Phrase markers are generated by applying phrase structure rules , and themselves are subject to further transformational rules. A set of possible parse trees for a syntactically ambiguous sentence is called a \"parse forest.\"","title":"wikipedia Parse tree"},{"location":"Guide/Parse-tree%26syntax-tree/Parse-tree/#constituency-based#parse#trees","text":"","title":"Constituency-based parse trees"},{"location":"Guide/Parse-tree%26syntax-tree/Parse-tree-to-AST/","text":"Parse tree to AST tomassetti Building a compiler for your own language: from the parse tree to the Abstract Syntax Tree","title":"Introduction"},{"location":"Guide/Parse-tree%26syntax-tree/Parse-tree-to-AST/#parse#tree#to#ast","text":"","title":"Parse tree to AST"},{"location":"Guide/Parse-tree%26syntax-tree/Parse-tree-to-AST/#tomassetti#building#a#compiler#for#your#own#language#from#the#parse#tree#to#the#abstract#syntax#tree","text":"","title":"tomassetti Building a compiler for your own language: from the parse tree to the Abstract Syntax Tree"},{"location":"Guide/Parsing-algorithm/","text":"Parsing algorithm wikipedia Parsing Parsing , syntax analysis , or syntactic analysis is the process of analysing a string of symbols , either in natural language , computer languages or data structures , conforming to the rules of a formal grammar . The term parsing comes from Latin pars ( orationis ), meaning part (of speech) .[ 1] NOTE: **syntax analysis**\u66f4\u52a0\u80fd\u591f\u8bf4\u660e Parsing \u7684\u542b\u4e49\uff1b The term has slightly different meanings in different branches of linguistics and computer science . Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams . It usually emphasizes the importance of grammatical divisions such as subject and predicate . Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input. NOTE: Compiler principle can also be classified into computational linguistics . The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\"[ 1] This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences . Within computer science, the term is used in the analysis of computer languages , referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters . The term may also be used to describe a split or separation. Types of parsers The task of the parser is essentially to determine if and how the input can be derived from the start symbol of the grammar. This can be done in essentially two ways: 1\u3001 Top-down parsing - Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse trees using a top-down expansion of the given formal grammar rules. Tokens are consumed from left to right. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules. 2\u3001 Bottom-up parsing - A parser can start with the input and attempt to rewrite it to the start symbol. Intuitively, the parser attempts to locate the most basic elements, then the elements containing these, and so on. LR parsers are examples of bottom-up parsers. Another term used for this type of parser is Shift-Reduce parsing. NOTE: Top-down: parser doc code LL parsers Recursive-descent parser Bottom-up: LR parsers \u3001 Shift-Reduce LL parsers and recursive-descent parser are examples of top-down parsers which cannot accommodate left recursive production rules . Although it has been believed that simple implementations of top-down parsing cannot accommodate direct and indirect left-recursion and may require exponential time and space complexity while parsing ambiguous context-free grammars , more sophisticated algorithms for top-down parsing have been created by Frost, Hafiz, and Callaghan[ 11] [ 12] which accommodate ambiguity and left recursion in polynomial time and which generate polynomial-size representations of the potentially exponential number of parse trees. Their algorithm is able to produce both left-most and right-most derivations of an input with regard to a given context-free grammar . An important distinction with regard to parsers is whether a parser generates a leftmost derivation or a rightmost derivation (see context-free grammar ). LL parsers will generate a leftmost derivation and LR parsers will generate a rightmost derivation (although usually in reverse). Some graphical parsing algorithms have been designed for visual programming languages .[ 13] [ 14] Parsers for visual languages are sometimes based on graph grammars .","title":"Introduction"},{"location":"Guide/Parsing-algorithm/#parsing#algorithm","text":"","title":"Parsing algorithm"},{"location":"Guide/Parsing-algorithm/#wikipedia#parsing","text":"Parsing , syntax analysis , or syntactic analysis is the process of analysing a string of symbols , either in natural language , computer languages or data structures , conforming to the rules of a formal grammar . The term parsing comes from Latin pars ( orationis ), meaning part (of speech) .[ 1] NOTE: **syntax analysis**\u66f4\u52a0\u80fd\u591f\u8bf4\u660e Parsing \u7684\u542b\u4e49\uff1b The term has slightly different meanings in different branches of linguistics and computer science . Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams . It usually emphasizes the importance of grammatical divisions such as subject and predicate . Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information. Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input. NOTE: Compiler principle can also be classified into computational linguistics . The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\"[ 1] This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences . Within computer science, the term is used in the analysis of computer languages , referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters . The term may also be used to describe a split or separation.","title":"wikipedia Parsing"},{"location":"Guide/Parsing-algorithm/#types#of#parsers","text":"The task of the parser is essentially to determine if and how the input can be derived from the start symbol of the grammar. This can be done in essentially two ways: 1\u3001 Top-down parsing - Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse trees using a top-down expansion of the given formal grammar rules. Tokens are consumed from left to right. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules. 2\u3001 Bottom-up parsing - A parser can start with the input and attempt to rewrite it to the start symbol. Intuitively, the parser attempts to locate the most basic elements, then the elements containing these, and so on. LR parsers are examples of bottom-up parsers. Another term used for this type of parser is Shift-Reduce parsing. NOTE: Top-down: parser doc code LL parsers Recursive-descent parser Bottom-up: LR parsers \u3001 Shift-Reduce LL parsers and recursive-descent parser are examples of top-down parsers which cannot accommodate left recursive production rules . Although it has been believed that simple implementations of top-down parsing cannot accommodate direct and indirect left-recursion and may require exponential time and space complexity while parsing ambiguous context-free grammars , more sophisticated algorithms for top-down parsing have been created by Frost, Hafiz, and Callaghan[ 11] [ 12] which accommodate ambiguity and left recursion in polynomial time and which generate polynomial-size representations of the potentially exponential number of parse trees. Their algorithm is able to produce both left-most and right-most derivations of an input with regard to a given context-free grammar . An important distinction with regard to parsers is whether a parser generates a leftmost derivation or a rightmost derivation (see context-free grammar ). LL parsers will generate a leftmost derivation and LR parsers will generate a rightmost derivation (although usually in reverse). Some graphical parsing algorithms have been designed for visual programming languages .[ 13] [ 14] Parsers for visual languages are sometimes based on graph grammars .","title":"Types of parsers"},{"location":"Guide/Parsing-algorithm/Bottom-up/","text":"Bottom-up parsing wikipedia Bottom-up parsing In computer science , parsing reveals the grammatical structure of linear input text, as a first step in working out its meaning. Bottom-up parsing recognizes the text's lowest-level small details first, before its mid-level structures, and leaving the highest-level overall structure to last. Bottom-up Versus Top-down The bottom-up name comes from the concept of a parse tree , in which the most detailed parts are at the bottom of the (upside-down) tree, and larger structures composed from them are in successively higher layers, until at the top or \"root\" of the tree a single unit describes the entire input stream. A bottom-up parse discovers and processes that tree starting from the bottom left end , and incrementally works its way upwards and rightwards.[ 2] A parser may act on the structure hierarchy's low, mid, and highest levels without ever creating an actual data tree; the tree is then merely implicit in the parser's actions. Bottom-up parsing patiently waits until it has scanned and parsed all parts of some construct before committing to what the combined construct is. The opposite of this is top-down parsing , in which the input's overall structure is decided (or guessed at) first, before dealing with mid-level parts, leaving completion of all lowest-level details to last. A top-down parser discovers and processes the hierarchical tree starting from the top, and incrementally works its way first downwards and then rightwards. Top-down parsing eagerly decides what a construct is much earlier, when it has only scanned the leftmost symbol of that construct and has not yet parsed any of its parts. Left corner parsing is a hybrid method which works bottom-up along the left edges of each subtree, and top-down on the rest of the parse tree. If a language grammar has multiple rules that may start with the same leftmost symbols but have different endings, then that grammar can be efficiently handled by a deterministic bottom-up parse but cannot be handled top-down without guesswork and backtracking . So bottom-up parsers handle a somewhat larger range of computer language grammars than do deterministic top-down parsers. Bottom-up parsing is sometimes done by backtracking . But much more commonly, bottom-up parsing is done by a shift-reduce parser such as a LALR parser . Examples Some of the parsers that use bottom-up parsing include: 1\u3001 Precedence parser - Simple precedence parser - Operator-precedence parser 2\u3001Bounded-context parser (BC) 3\u3001 LR parser (**L**eft-to-right, **R**ightmost derivation in reverse) - Simple LR parser (SLR) - LALR parser (Look-Ahead) - Canonical LR parser (LR(1)) - GLR parser (Generalized)[ 3] 4\u3001 CYK parser (Cocke\u2013Younger\u2013Kasami) 5\u3001Recursive ascent parser - Packrat parser 6\u3001 Shift-reduce parser","title":"Introduction"},{"location":"Guide/Parsing-algorithm/Bottom-up/#bottom-up#parsing","text":"","title":"Bottom-up parsing"},{"location":"Guide/Parsing-algorithm/Bottom-up/#wikipedia#bottom-up#parsing","text":"In computer science , parsing reveals the grammatical structure of linear input text, as a first step in working out its meaning. Bottom-up parsing recognizes the text's lowest-level small details first, before its mid-level structures, and leaving the highest-level overall structure to last.","title":"wikipedia Bottom-up parsing"},{"location":"Guide/Parsing-algorithm/Bottom-up/#bottom-up#versus#top-down","text":"The bottom-up name comes from the concept of a parse tree , in which the most detailed parts are at the bottom of the (upside-down) tree, and larger structures composed from them are in successively higher layers, until at the top or \"root\" of the tree a single unit describes the entire input stream. A bottom-up parse discovers and processes that tree starting from the bottom left end , and incrementally works its way upwards and rightwards.[ 2] A parser may act on the structure hierarchy's low, mid, and highest levels without ever creating an actual data tree; the tree is then merely implicit in the parser's actions. Bottom-up parsing patiently waits until it has scanned and parsed all parts of some construct before committing to what the combined construct is. The opposite of this is top-down parsing , in which the input's overall structure is decided (or guessed at) first, before dealing with mid-level parts, leaving completion of all lowest-level details to last. A top-down parser discovers and processes the hierarchical tree starting from the top, and incrementally works its way first downwards and then rightwards. Top-down parsing eagerly decides what a construct is much earlier, when it has only scanned the leftmost symbol of that construct and has not yet parsed any of its parts. Left corner parsing is a hybrid method which works bottom-up along the left edges of each subtree, and top-down on the rest of the parse tree. If a language grammar has multiple rules that may start with the same leftmost symbols but have different endings, then that grammar can be efficiently handled by a deterministic bottom-up parse but cannot be handled top-down without guesswork and backtracking . So bottom-up parsers handle a somewhat larger range of computer language grammars than do deterministic top-down parsers. Bottom-up parsing is sometimes done by backtracking . But much more commonly, bottom-up parsing is done by a shift-reduce parser such as a LALR parser .","title":"Bottom-up Versus Top-down"},{"location":"Guide/Parsing-algorithm/Bottom-up/#examples","text":"Some of the parsers that use bottom-up parsing include: 1\u3001 Precedence parser - Simple precedence parser - Operator-precedence parser 2\u3001Bounded-context parser (BC) 3\u3001 LR parser (**L**eft-to-right, **R**ightmost derivation in reverse) - Simple LR parser (SLR) - LALR parser (Look-Ahead) - Canonical LR parser (LR(1)) - GLR parser (Generalized)[ 3] 4\u3001 CYK parser (Cocke\u2013Younger\u2013Kasami) 5\u3001Recursive ascent parser - Packrat parser 6\u3001 Shift-reduce parser","title":"Examples"},{"location":"Guide/Parsing-algorithm/Bottom-up/CYK-algorithm/","text":"CYK algorithm wikipedia CYK algorithm In computer science , the Cocke\u2013Younger\u2013Kasami algorithm (alternatively called CYK , or CKY ) is a parsing algorithm for context-free grammars , named after its inventors, John Cocke , Daniel Younger and Tadao Kasami . It employs bottom-up parsing and dynamic programming .","title":"Introduction"},{"location":"Guide/Parsing-algorithm/Bottom-up/CYK-algorithm/#cyk#algorithm","text":"","title":"CYK algorithm"},{"location":"Guide/Parsing-algorithm/Bottom-up/CYK-algorithm/#wikipedia#cyk#algorithm","text":"In computer science , the Cocke\u2013Younger\u2013Kasami algorithm (alternatively called CYK , or CKY ) is a parsing algorithm for context-free grammars , named after its inventors, John Cocke , Daniel Younger and Tadao Kasami . It employs bottom-up parsing and dynamic programming .","title":"wikipedia CYK algorithm"},{"location":"Guide/Parsing-algorithm/Bottom-up/LR-parser/","text":"LR parser wikipedia LR parser In computer science , LR parsers are a type of bottom-up parser that analyses deterministic context-free languages in linear time . There are several variants of LR parsers: SLR parsers , LALR parsers , Canonical LR(1) parsers , Minimal LR(1) parsers , GLR parsers . LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages . An LR parser ( L**eft-to-right, **R**ightmost derivation in reverse) reads input text from left to right without backing up (this is true for most parsers), and produces a rightmost derivation in reverse: it does a bottom-up parse - not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in **LR(1) or sometimes LR( k ) . To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR . The LR( k ) condition for a grammar was suggested by Knuth to stand for \"translatable from left to right with bound k .\" LR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages, but LR parsers are not suited for human languages which need more flexible but inevitably slower methods. Some methods which can parse arbitrary context-free languages (e.g., Cocke\u2013Younger\u2013Kasami , Earley , GLR ) have worst-case performance of O(*n*3) time. Other methods which backtrack or yield multiple parses may even take exponential time when they guess badly. The above properties of L , R , and k are actually shared by all shift-reduce parsers , including precedence parsers . But by convention, the LR name stands for the form of parsing invented by Donald Knuth , and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser ).[ 1] LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing .[ 3] This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern. Overview Bottom-up parse tree for example A*2 + 1 An LR parser scans and parses the input text in one forward pass over the text. The parser builds up the parse tree incrementally, bottom up, and left to right, without guessing or backtracking. At every point in this pass, the parser has accumulated a list of subtrees or phrases of the input text that have been already parsed. Those subtrees are not yet joined together because the parser has not yet reached the right end of the syntax pattern that will combine them. At step 6 in an example parse, only \"A*2\" has been parsed, incompletely. Only the shaded lower-left corner of the parse tree exists. None of the parse tree nodes numbered 7 and above exist yet. Nodes 3, 4, and 6 are the roots of isolated subtrees for variable A, operator , and number 2, respectively. These three root nodes are temporarily held in a **parse stack* . The remaining unparsed portion of the input stream is \"+ 1\". Bottom-up parse tree built in numbered steps Shift and reduce actions As with other shift-reduce parsers, an LR parser works by doing some combination of Shift steps and Reduce steps. 1\u3001A Shift step advances in the input stream by one symbol. That shifted symbol becomes a new single-node parse tree. 2\u3001A Reduce step applies a completed grammar rule to some of the recent parse trees, joining them together as one tree with a new root symbol. If the input has no syntax errors, the parser continues with these steps until all of the input has been consumed and all of the parse trees have been reduced to a single tree representing an entire legal input. LR parsers differ from other shift-reduce parsers in how they decide when to reduce, and how to pick between rules with similar endings. But the final decisions and the sequence of shift or reduce steps are the same. Much of the LR parser's efficiency is from being deterministic. To avoid guessing, the LR parser often looks ahead (rightwards) at the next scanned symbol, before deciding what to do with previously scanned symbols. The lexical scanner works one or more symbols ahead of the parser. The lookahead symbols are the 'right-hand context' for the parsing decision.","title":"Introduction"},{"location":"Guide/Parsing-algorithm/Bottom-up/LR-parser/#lr#parser","text":"","title":"LR parser"},{"location":"Guide/Parsing-algorithm/Bottom-up/LR-parser/#wikipedia#lr#parser","text":"In computer science , LR parsers are a type of bottom-up parser that analyses deterministic context-free languages in linear time . There are several variants of LR parsers: SLR parsers , LALR parsers , Canonical LR(1) parsers , Minimal LR(1) parsers , GLR parsers . LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. They are widely used for the processing of computer languages . An LR parser ( L**eft-to-right, **R**ightmost derivation in reverse) reads input text from left to right without backing up (this is true for most parsers), and produces a rightmost derivation in reverse: it does a bottom-up parse - not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in **LR(1) or sometimes LR( k ) . To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR . The LR( k ) condition for a grammar was suggested by Knuth to stand for \"translatable from left to right with bound k .\" LR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages, but LR parsers are not suited for human languages which need more flexible but inevitably slower methods. Some methods which can parse arbitrary context-free languages (e.g., Cocke\u2013Younger\u2013Kasami , Earley , GLR ) have worst-case performance of O(*n*3) time. Other methods which backtrack or yield multiple parses may even take exponential time when they guess badly. The above properties of L , R , and k are actually shared by all shift-reduce parsers , including precedence parsers . But by convention, the LR name stands for the form of parsing invented by Donald Knuth , and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser ).[ 1] LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing .[ 3] This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern.","title":"wikipedia LR parser"},{"location":"Guide/Parsing-algorithm/Bottom-up/LR-parser/#overview","text":"","title":"Overview"},{"location":"Guide/Parsing-algorithm/Bottom-up/LR-parser/#bottom-up#parse#tree#for#example#a2#1","text":"An LR parser scans and parses the input text in one forward pass over the text. The parser builds up the parse tree incrementally, bottom up, and left to right, without guessing or backtracking. At every point in this pass, the parser has accumulated a list of subtrees or phrases of the input text that have been already parsed. Those subtrees are not yet joined together because the parser has not yet reached the right end of the syntax pattern that will combine them. At step 6 in an example parse, only \"A*2\" has been parsed, incompletely. Only the shaded lower-left corner of the parse tree exists. None of the parse tree nodes numbered 7 and above exist yet. Nodes 3, 4, and 6 are the roots of isolated subtrees for variable A, operator , and number 2, respectively. These three root nodes are temporarily held in a **parse stack* . The remaining unparsed portion of the input stream is \"+ 1\". Bottom-up parse tree built in numbered steps","title":"Bottom-up parse tree for example A*2 + 1"},{"location":"Guide/Parsing-algorithm/Bottom-up/LR-parser/#shift#and#reduce#actions","text":"As with other shift-reduce parsers, an LR parser works by doing some combination of Shift steps and Reduce steps. 1\u3001A Shift step advances in the input stream by one symbol. That shifted symbol becomes a new single-node parse tree. 2\u3001A Reduce step applies a completed grammar rule to some of the recent parse trees, joining them together as one tree with a new root symbol. If the input has no syntax errors, the parser continues with these steps until all of the input has been consumed and all of the parse trees have been reduced to a single tree representing an entire legal input. LR parsers differ from other shift-reduce parsers in how they decide when to reduce, and how to pick between rules with similar endings. But the final decisions and the sequence of shift or reduce steps are the same. Much of the LR parser's efficiency is from being deterministic. To avoid guessing, the LR parser often looks ahead (rightwards) at the next scanned symbol, before deciding what to do with previously scanned symbols. The lexical scanner works one or more symbols ahead of the parser. The lookahead symbols are the 'right-hand context' for the parsing decision.","title":"Shift and reduce actions"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/","text":"LL VS LR stackoverflow What is the difference between LL and LR parsing? A At a high level, the difference between LL parsing and LR parsing is that LL parsers begin at the start symbol and try to apply productions to arrive at the target string , whereas LR parsers begin at the target string and try to arrive back at the start symbol . An LL parse is a left-to-right, leftmost derivation. That is, we consider the input symbols from the left to the right and attempt to construct a leftmost derivation. This is done by beginning at the start symbol and repeatedly expanding out the leftmost nonterminal until we arrive at the target string . An LR parse is a left-to-right, rightmost derivation, meaning that we scan from the left to right and attempt to construct a rightmost derivation. The parser continuously picks a substring of the input and attempts to reverse it back to a nonterminal. During an LL parse, the parser continuously chooses between two actions: Predict : Based on the leftmost nonterminal and some number of lookahead tokens, choose which production ought to be applied to get closer to the input string. Match : Match the leftmost guessed terminal symbol with the leftmost unconsumed symbol of input. As an example, given this grammar: S \u2192 E E \u2192 T + E E \u2192 T T \u2192 int Then given the string int + int + int , an LL(2) parser (which uses two tokens of lookahead) would parse the string as follows: Production Input Action --------------------------------------------------------- S int + int + int Predict S -> E E int + int + int Predict E -> T + E T + E int + int + int Predict T -> int int + E int + int + int Match int + E + int + int Match + E int + int Predict E -> T + E T + E int + int Predict T -> int int + E int + int Match int + E + int Match + E int Predict E -> T T int Predict T -> int int int Match int Accept Notice that in each step we look at the leftmost symbol in our production. If it's a terminal, we match it, and if it's a nonterminal, we predict what it's going to be by choosing one of the rules. In an LR parser, there are two actions: Shift : Add the next token of input to a buffer for consideration. Reduce : Reduce a collection of terminals and nonterminals in this buffer back to some nonterminal by reversing a production. As an example, an LR(1) parser (with one token of lookahead) might parse that same string as follows: Workspace Input Action --------------------------------------------------------- int + int + int Shift int + int + int Reduce T -> int T + int + int Shift T + int + int Shift T + int + int Reduce T -> int T + T + int Shift T + T + int Shift T + T + int Reduce T -> int T + T + T Reduce E -> T T + T + E Reduce E -> T + E T + E Reduce E -> T + E E Reduce S -> E S Accept The two parsing algorithms you mentioned (LL and LR) are known to have different characteristics. LL parsers tend to be easier to write by hand, but they are less powerful than LR parsers and accept a much smaller set of grammars than LR parsers do. LR parsers come in many flavors (LR(0), SLR(1), LALR(1), LR(1), IELR(1), GLR(0), etc.) and are far more powerful. They also tend to have much more complex and are almost always generated by tools like yacc or bison . LL parsers also come in many flavors (including LL(*), which is used by the ANTLR tool), though in practice LL(1) is the most-widely used. As a shameless plug, if you'd like to learn more about LL and LR parsing, I just finished teaching a compilers course and have some handouts and lecture slides on parsing on the course website. I'd be glad to elaborate on any of them if you think it would be useful. A Josh Haberman in his article LL and LR Parsing Demystified claims that LL parsing directly corresponds with the Polish Notation , whereas LR corresponds to Reverse Polish Notation . The difference between PN and RPN is the order of traversing the binary tree of the equation: + 1 * 2 3 // Polish (prefix) expression; pre-order traversal. 1 2 3 * + // Reverse Polish (postfix) expression; post-order traversal. According to Haberman, this illustrates the main difference between LL and LR parsers: The primary difference between how LL and LR parsers operate is that an LL parser outputs a pre-order traversal of the parse tree and an LR parser outputs a post-order traversal. For the in-depth explanation, examples and conclusions check out Haberman's article . \u9884\u6d4b\u5206\u6790\u8868 VS LR\u8bed\u6cd5\u5206\u6790\u8868 LL\u548cLR\u90fd\u662f\u8868\u9a71\u52a8\u7684\uff0c\u8fd9\u4e24\u4e2a\u8868\u5c31\u662f\u5206\u522b\u9a71\u52a8\u4e24\u8005\u7684\u8868\u3002 \u8bed\u6cd5\u5206\u6790\u7684\u8fc7\u7a0b\u662f\u4e0d\u65ad\u6839\u636e\u4ea7\u751f\u5f0f\u8fdb\u884c\u8f6c\u6362\u7684\u8fc7\u7a0b\u3002 \u4e24\u8005\u7684\u6784\u9020\u90fd\u662f\u57fa\u4e8e\u5bf9grammar\u7684\u5206\u6790\uff0c\u4e24\u8005\u7684\u6784\u9020\u8fc7\u7a0b\u90fd\u662f\u6cbf\u7740\u4ea7\u751f\u5f0f\u8fdb\u884cderive\uff0c\u6240\u4e0d\u540c\u7684\u662f\uff0c\u9884\u6d4b\u5206\u6790\u8868\u4e00\u76f4derive\u5230\u4e86terminal\uff1b\u800cLR\u8bed\u6cd5\u5206\u6790\u8868\u5219\u662f\u5c06\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u5168\u90e8\u5305\u542b\u4e86\uff1a \u9884\u6d4b\u5206\u6790\u8868\u7684\u6784\u9020\u4f7f\u7528\u662fnon-terminal symbol\u7684FIRST\u548cFOLLOW\uff0cFIRST\u548cFOLLOW\u6240\u5305\u542b\u7684\u90fd\u662fterminal\uff0c\u5176\u5b9e\u5b83\u7684\u76ee\u7684\u662f\u77e5\u9053\u5f53\u9047\u5230\u67d0\u4e2aterminal\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u54ea\u4e2aproduction\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2aterminal\u3002\u5373\u5b83\u4fa7\u91cd\u7684\u662f\u5bf9\u4e8e\u4e00\u4e2anon-terminal\u7684production\uff0c\u5b83\u80fd\u591f\u63a8\u5bfc\u51fa\u4ec0\u4e48\uff0c\u8fd9\u6837\u5b83\u5c31\u80fd\u591f\u636e\u6b64\u6765\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u8fd9\u4e2aproduction\u3002 LR\u8bed\u6cd5\u5206\u6790\u8868\u6b63\u5982\u5176\u540d\u79f0\u6240\u63ed\u793a\u5730\uff0c\u5b83\u5176\u5b9e\u662f\u5bf9\u8bed\u6cd5\u7684\u5206\u6790\uff0c\u5bf9\u8bed\u6cd5\u4e2d\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u8fdb\u884c\u5206\u6790\uff0c\u6784\u9020\u51fa\u6765\u5b83\u7684\u8f6c\u6362\u6240\u5bf9\u5e94\u7684automaton\u3002\u663e\u7136\uff0c\u56e0\u4e3a\u5b83\u7684\u8f6c\u6362\u90fd\u662f\u57fa\u4e8egrammar\u6240\u6784\u9020\u51fa\u6765\u7684\uff0c\u56e0\u6b64\uff0c\u5b83\u7684\u6240\u6709\u7684\u8f6c\u6362\u90fd\u662f\u6709\u6548\u7684\u8f6c\u6362\uff0c\u56e0\u6b64\u53ea\u8981\u5f85\u5206\u6790\u7684\u4e32\u4e0d\u7b26\u5408\u8fd9\u4e2aautomaton\u7684\u8f6c\u6362\uff0c\u90a3\u4e48\u5b83\u5c31\u662f\u65e0\u6548\u7684\u4e86\u3002\u56e0\u6b64\u6211\u4eec\u7684\u5f85\u5206\u6790\u7684\u4e32\u4e00\u5b9a\u662f\u5bf9\u5e94\u4e86automaton\u4e2d\u7684\u4e00\u6761\u8def\u5f84\u3002\u4f7f\u7528\u6211\u4eec\u7684\u6587\u6cd5\uff0c\u6309\u7167\u67d0\u79cd\u63a8\u5bfc\u65b9\u5f0f\u662f\u4e00\u5b9a\u80fd\u591fderive\u8fd9\u4e2a\u4e32\u7684\u3002\u5728\u5f85\u5206\u6790\u7684\u4e32\u4e2d\uff0c\u4ec5\u4ec5\u5305\u542b\u7684\u662fterminal\uff0c\u800cgrammar\u4e2d\uff0c\u662f\u4e24\u8005\u90fd\u5305\u542b\u7684\uff0c\u6240\u4ee5\u5728\u8fdb\u884c\u5206\u6790\u7684\u65f6\u5019\uff0c\u4e00\u5b9a\u8981\u5c06terminal\u89c4\u7ea6\u4e3anon-terminal\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u4f7f\u5206\u6790\u7ee7\u7eed\u4e0b\u53bb\u3002LR\u8bed\u6cd5\u5206\u6790\u662f\u63a8\u5bfc\u7684\u9006\u8fc7\u7a0b\uff0c\u663e\u7136\u6211\u4eec\u662f\u8981\u6cbf\u7740\u63a8\u5bfc\u65b9\u5411\u9006\u6d41\u800c\u4e0a\uff0c\u56e0\u4e3a\u63a8\u5bfc\u7684\u8fc7\u7a0b\u7684\u4e2d\u95f4\u8fc7\u7a0b\u80af\u5b9a\u662f\u4f1a\u5b58\u5728non-terminal\u7684\uff0c\u800c\u6211\u4eec\u662f\u9006\u5411\u8fdb\u884c\u63a8\u5bfc\uff0c\u6240\u4ee5\u80af\u5b9a\u9700\u8981\u5c06\u4e00\u4e9bterminal\u89c4\u7ea6\u4e3anon-terminal\u3002 \u4e3a\u4ec0\u4e48LR\u662fright-most derivation\uff1f \u9884\u6d4b\u5206\u6790\u8868\u5176\u5b9e\u4e5f\u662f\u4e00\u4e2a\u8f6c\u6362\u51fd\u6570\uff0c\u8981\u4f7f\u7528\u54ea\u4e2a\u4ea7\u751f\u5f0f\u8fdb\u884cderivate LR\u662f\u4e00\u4e2aautomaton\uff0c\u72b6\u6001\uff0c\u8f6c\u6362 \u8bed\u6cd5\u5206\u6790\u8868\u7531\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210\uff1a action goto \u4eceLR(0)\u81ea\u52a8\u673a\u5230LR\u8bed\u6cd5\u5206\u6790\u8868 LR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u662f\u57fa\u4e8eLR(0)\u81ea\u52a8\u673a\u7684 \u672c\u8d28\u8fd8\u662f\u8f6c\u6362\uff0c\u65e0\u8bba\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\u8fd8\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362 action\u4e3b\u8981\u5b9a\u4e49\u7684\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\uff0c\u800cgoto\u5219\u5b9a\u4e49\u7684\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362\u3002 SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u8fc7\u7a0b\u4e3b\u8981\u8ba9\u6211\u611f\u5230\u56f0\u60d1\u7684\u662f\u5b83\u5c06action\u5b9a\u4e49\u6210\u4e86 \u72b6\u6001\u548cterminal\u7684\u51fd\u6570\uff1f SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684ACTION\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\u6709\u8fd9\u6837\u7684\u89c4\u5219\uff1a LR\u8bed\u6cd5\u5206\u6790\u5668\u7684\u683c\u5c40configuration VS LR(0)\u81ea\u52a8\u673a\u7684\u72b6\u6001","title":"Introduction"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/#ll#vs#lr","text":"","title":"LL VS LR"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/#stackoverflow#what#is#the#difference#between#ll#and#lr#parsing","text":"","title":"stackoverflow What is the difference between LL and LR parsing?"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/#a","text":"At a high level, the difference between LL parsing and LR parsing is that LL parsers begin at the start symbol and try to apply productions to arrive at the target string , whereas LR parsers begin at the target string and try to arrive back at the start symbol . An LL parse is a left-to-right, leftmost derivation. That is, we consider the input symbols from the left to the right and attempt to construct a leftmost derivation. This is done by beginning at the start symbol and repeatedly expanding out the leftmost nonterminal until we arrive at the target string . An LR parse is a left-to-right, rightmost derivation, meaning that we scan from the left to right and attempt to construct a rightmost derivation. The parser continuously picks a substring of the input and attempts to reverse it back to a nonterminal. During an LL parse, the parser continuously chooses between two actions: Predict : Based on the leftmost nonterminal and some number of lookahead tokens, choose which production ought to be applied to get closer to the input string. Match : Match the leftmost guessed terminal symbol with the leftmost unconsumed symbol of input. As an example, given this grammar: S \u2192 E E \u2192 T + E E \u2192 T T \u2192 int Then given the string int + int + int , an LL(2) parser (which uses two tokens of lookahead) would parse the string as follows: Production Input Action --------------------------------------------------------- S int + int + int Predict S -> E E int + int + int Predict E -> T + E T + E int + int + int Predict T -> int int + E int + int + int Match int + E + int + int Match + E int + int Predict E -> T + E T + E int + int Predict T -> int int + E int + int Match int + E + int Match + E int Predict E -> T T int Predict T -> int int int Match int Accept Notice that in each step we look at the leftmost symbol in our production. If it's a terminal, we match it, and if it's a nonterminal, we predict what it's going to be by choosing one of the rules. In an LR parser, there are two actions: Shift : Add the next token of input to a buffer for consideration. Reduce : Reduce a collection of terminals and nonterminals in this buffer back to some nonterminal by reversing a production. As an example, an LR(1) parser (with one token of lookahead) might parse that same string as follows: Workspace Input Action --------------------------------------------------------- int + int + int Shift int + int + int Reduce T -> int T + int + int Shift T + int + int Shift T + int + int Reduce T -> int T + T + int Shift T + T + int Shift T + T + int Reduce T -> int T + T + T Reduce E -> T T + T + E Reduce E -> T + E T + E Reduce E -> T + E E Reduce S -> E S Accept The two parsing algorithms you mentioned (LL and LR) are known to have different characteristics. LL parsers tend to be easier to write by hand, but they are less powerful than LR parsers and accept a much smaller set of grammars than LR parsers do. LR parsers come in many flavors (LR(0), SLR(1), LALR(1), LR(1), IELR(1), GLR(0), etc.) and are far more powerful. They also tend to have much more complex and are almost always generated by tools like yacc or bison . LL parsers also come in many flavors (including LL(*), which is used by the ANTLR tool), though in practice LL(1) is the most-widely used. As a shameless plug, if you'd like to learn more about LL and LR parsing, I just finished teaching a compilers course and have some handouts and lecture slides on parsing on the course website. I'd be glad to elaborate on any of them if you think it would be useful.","title":"A"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/#a_1","text":"Josh Haberman in his article LL and LR Parsing Demystified claims that LL parsing directly corresponds with the Polish Notation , whereas LR corresponds to Reverse Polish Notation . The difference between PN and RPN is the order of traversing the binary tree of the equation: + 1 * 2 3 // Polish (prefix) expression; pre-order traversal. 1 2 3 * + // Reverse Polish (postfix) expression; post-order traversal. According to Haberman, this illustrates the main difference between LL and LR parsers: The primary difference between how LL and LR parsers operate is that an LL parser outputs a pre-order traversal of the parse tree and an LR parser outputs a post-order traversal. For the in-depth explanation, examples and conclusions check out Haberman's article .","title":"A"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/#vs#lr","text":"LL\u548cLR\u90fd\u662f\u8868\u9a71\u52a8\u7684\uff0c\u8fd9\u4e24\u4e2a\u8868\u5c31\u662f\u5206\u522b\u9a71\u52a8\u4e24\u8005\u7684\u8868\u3002 \u8bed\u6cd5\u5206\u6790\u7684\u8fc7\u7a0b\u662f\u4e0d\u65ad\u6839\u636e\u4ea7\u751f\u5f0f\u8fdb\u884c\u8f6c\u6362\u7684\u8fc7\u7a0b\u3002 \u4e24\u8005\u7684\u6784\u9020\u90fd\u662f\u57fa\u4e8e\u5bf9grammar\u7684\u5206\u6790\uff0c\u4e24\u8005\u7684\u6784\u9020\u8fc7\u7a0b\u90fd\u662f\u6cbf\u7740\u4ea7\u751f\u5f0f\u8fdb\u884cderive\uff0c\u6240\u4e0d\u540c\u7684\u662f\uff0c\u9884\u6d4b\u5206\u6790\u8868\u4e00\u76f4derive\u5230\u4e86terminal\uff1b\u800cLR\u8bed\u6cd5\u5206\u6790\u8868\u5219\u662f\u5c06\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u5168\u90e8\u5305\u542b\u4e86\uff1a \u9884\u6d4b\u5206\u6790\u8868\u7684\u6784\u9020\u4f7f\u7528\u662fnon-terminal symbol\u7684FIRST\u548cFOLLOW\uff0cFIRST\u548cFOLLOW\u6240\u5305\u542b\u7684\u90fd\u662fterminal\uff0c\u5176\u5b9e\u5b83\u7684\u76ee\u7684\u662f\u77e5\u9053\u5f53\u9047\u5230\u67d0\u4e2aterminal\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u54ea\u4e2aproduction\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2aterminal\u3002\u5373\u5b83\u4fa7\u91cd\u7684\u662f\u5bf9\u4e8e\u4e00\u4e2anon-terminal\u7684production\uff0c\u5b83\u80fd\u591f\u63a8\u5bfc\u51fa\u4ec0\u4e48\uff0c\u8fd9\u6837\u5b83\u5c31\u80fd\u591f\u636e\u6b64\u6765\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u8fd9\u4e2aproduction\u3002 LR\u8bed\u6cd5\u5206\u6790\u8868\u6b63\u5982\u5176\u540d\u79f0\u6240\u63ed\u793a\u5730\uff0c\u5b83\u5176\u5b9e\u662f\u5bf9\u8bed\u6cd5\u7684\u5206\u6790\uff0c\u5bf9\u8bed\u6cd5\u4e2d\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u8fdb\u884c\u5206\u6790\uff0c\u6784\u9020\u51fa\u6765\u5b83\u7684\u8f6c\u6362\u6240\u5bf9\u5e94\u7684automaton\u3002\u663e\u7136\uff0c\u56e0\u4e3a\u5b83\u7684\u8f6c\u6362\u90fd\u662f\u57fa\u4e8egrammar\u6240\u6784\u9020\u51fa\u6765\u7684\uff0c\u56e0\u6b64\uff0c\u5b83\u7684\u6240\u6709\u7684\u8f6c\u6362\u90fd\u662f\u6709\u6548\u7684\u8f6c\u6362\uff0c\u56e0\u6b64\u53ea\u8981\u5f85\u5206\u6790\u7684\u4e32\u4e0d\u7b26\u5408\u8fd9\u4e2aautomaton\u7684\u8f6c\u6362\uff0c\u90a3\u4e48\u5b83\u5c31\u662f\u65e0\u6548\u7684\u4e86\u3002\u56e0\u6b64\u6211\u4eec\u7684\u5f85\u5206\u6790\u7684\u4e32\u4e00\u5b9a\u662f\u5bf9\u5e94\u4e86automaton\u4e2d\u7684\u4e00\u6761\u8def\u5f84\u3002\u4f7f\u7528\u6211\u4eec\u7684\u6587\u6cd5\uff0c\u6309\u7167\u67d0\u79cd\u63a8\u5bfc\u65b9\u5f0f\u662f\u4e00\u5b9a\u80fd\u591fderive\u8fd9\u4e2a\u4e32\u7684\u3002\u5728\u5f85\u5206\u6790\u7684\u4e32\u4e2d\uff0c\u4ec5\u4ec5\u5305\u542b\u7684\u662fterminal\uff0c\u800cgrammar\u4e2d\uff0c\u662f\u4e24\u8005\u90fd\u5305\u542b\u7684\uff0c\u6240\u4ee5\u5728\u8fdb\u884c\u5206\u6790\u7684\u65f6\u5019\uff0c\u4e00\u5b9a\u8981\u5c06terminal\u89c4\u7ea6\u4e3anon-terminal\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u4f7f\u5206\u6790\u7ee7\u7eed\u4e0b\u53bb\u3002LR\u8bed\u6cd5\u5206\u6790\u662f\u63a8\u5bfc\u7684\u9006\u8fc7\u7a0b\uff0c\u663e\u7136\u6211\u4eec\u662f\u8981\u6cbf\u7740\u63a8\u5bfc\u65b9\u5411\u9006\u6d41\u800c\u4e0a\uff0c\u56e0\u4e3a\u63a8\u5bfc\u7684\u8fc7\u7a0b\u7684\u4e2d\u95f4\u8fc7\u7a0b\u80af\u5b9a\u662f\u4f1a\u5b58\u5728non-terminal\u7684\uff0c\u800c\u6211\u4eec\u662f\u9006\u5411\u8fdb\u884c\u63a8\u5bfc\uff0c\u6240\u4ee5\u80af\u5b9a\u9700\u8981\u5c06\u4e00\u4e9bterminal\u89c4\u7ea6\u4e3anon-terminal\u3002 \u4e3a\u4ec0\u4e48LR\u662fright-most derivation\uff1f \u9884\u6d4b\u5206\u6790\u8868\u5176\u5b9e\u4e5f\u662f\u4e00\u4e2a\u8f6c\u6362\u51fd\u6570\uff0c\u8981\u4f7f\u7528\u54ea\u4e2a\u4ea7\u751f\u5f0f\u8fdb\u884cderivate LR\u662f\u4e00\u4e2aautomaton\uff0c\u72b6\u6001\uff0c\u8f6c\u6362 \u8bed\u6cd5\u5206\u6790\u8868\u7531\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210\uff1a action goto","title":"\u9884\u6d4b\u5206\u6790\u8868 VS LR\u8bed\u6cd5\u5206\u6790\u8868"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/#lr0lr","text":"LR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u662f\u57fa\u4e8eLR(0)\u81ea\u52a8\u673a\u7684 \u672c\u8d28\u8fd8\u662f\u8f6c\u6362\uff0c\u65e0\u8bba\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\u8fd8\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362 action\u4e3b\u8981\u5b9a\u4e49\u7684\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\uff0c\u800cgoto\u5219\u5b9a\u4e49\u7684\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362\u3002 SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u8fc7\u7a0b\u4e3b\u8981\u8ba9\u6211\u611f\u5230\u56f0\u60d1\u7684\u662f\u5b83\u5c06action\u5b9a\u4e49\u6210\u4e86 \u72b6\u6001\u548cterminal\u7684\u51fd\u6570\uff1f SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684ACTION\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\u6709\u8fd9\u6837\u7684\u89c4\u5219\uff1a","title":"\u4eceLR(0)\u81ea\u52a8\u673a\u5230LR\u8bed\u6cd5\u5206\u6790\u8868"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/#lrconfiguration#vs#lr0","text":"","title":"LR\u8bed\u6cd5\u5206\u6790\u5668\u7684\u683c\u5c40configuration VS LR(0)\u81ea\u52a8\u673a\u7684\u72b6\u6001"},{"location":"Guide/Parsing-algorithm/LL-VS-LR-parser/LR-parsers-VS-recursive-descent-parsers/","text":"Recursive descent parser use backtracking to match. LL parsers are table-based parsers, they use parsing table. Parsing table is constructed from the grammar and acts as a transformation function. Using parsing table and k tokens of lookahead , a LL parser can become predictive parser and avoid backtracking .","title":"LR parsers VS recursive descent parsers"},{"location":"Guide/Parsing-algorithm/Others/Chart-parser/","text":"Chart parser In computer science , a chart parser is a type of parser suitable for ambiguous grammars (including grammars of natural languages ). It uses the dynamic programming approach\u2014partial hypothesized results are stored in a structure called a chart and can be re-used. This eliminates backtracking and prevents a combinatorial explosion . Chart parsing is generally credited to Martin Kay . Types of chart parsers A common approach is to use a variant of the Viterbi algorithm . The Earley parser is a type of chart parser mainly used for parsing in computational linguistics , named for its inventor. Another chart parsing algorithm is the Cocke-Younger-Kasami (CYK) algorithm. Chart parsers can also be used for parsing computer languages. Earley parsers in particular have been used in compiler compilers where their ability to parse using arbitrary Context-free grammars eases the task of writing the grammar for a particular language. However their lower efficiency has led to people avoiding them for most compiler work. In bidirectional chart parsing, edges of the chart are marked with a direction, either forwards or backwards, and rules are enforced on the direction in which edges must point in order to be combined into further edges. In incremental chart parsing, the chart is constructed incrementally as the text is edited by the user, with each change to the text resulting in the minimal possible corresponding change to the chart. Chart parsers are distinguished between top-down and bottom-up , as well as active and passive.","title":"Chart-parser"},{"location":"Guide/Parsing-algorithm/Others/Chart-parser/#chart#parser","text":"In computer science , a chart parser is a type of parser suitable for ambiguous grammars (including grammars of natural languages ). It uses the dynamic programming approach\u2014partial hypothesized results are stored in a structure called a chart and can be re-used. This eliminates backtracking and prevents a combinatorial explosion . Chart parsing is generally credited to Martin Kay .","title":"Chart parser"},{"location":"Guide/Parsing-algorithm/Others/Chart-parser/#types#of#chart#parsers","text":"A common approach is to use a variant of the Viterbi algorithm . The Earley parser is a type of chart parser mainly used for parsing in computational linguistics , named for its inventor. Another chart parsing algorithm is the Cocke-Younger-Kasami (CYK) algorithm. Chart parsers can also be used for parsing computer languages. Earley parsers in particular have been used in compiler compilers where their ability to parse using arbitrary Context-free grammars eases the task of writing the grammar for a particular language. However their lower efficiency has led to people avoiding them for most compiler work. In bidirectional chart parsing, edges of the chart are marked with a direction, either forwards or backwards, and rules are enforced on the direction in which edges must point in order to be combined into further edges. In incremental chart parsing, the chart is constructed incrementally as the text is edited by the user, with each change to the text resulting in the minimal possible corresponding change to the chart. Chart parsers are distinguished between top-down and bottom-up , as well as active and passive.","title":"Types of chart parsers"},{"location":"Guide/Parsing-algorithm/Others/Earley-parser/","text":"Earley parser","title":"Earley-parser"},{"location":"Guide/Parsing-algorithm/Others/Earley-parser/#earley#parser","text":"","title":"Earley parser"},{"location":"Guide/Parsing-algorithm/Top-down/","text":"Top-down parsing wikipedia Top-down parsing In computer science , top-down parsing is a parsing strategy where one first looks at the highest level of the parse tree and works down the parse tree by using the rewriting rules of a formal grammar .[ 1] LL parsers are a type of parser that uses a top-down parsing strategy. NOTE: Rewriting rules means expanding, substituting an nonterminal symbol with production body . Top-down parsing is a strategy of analyzing unknown data relationships by hypothesizing general parse tree structures and then considering whether the known fundamental structures are compatible with the hypothesis. It occurs in the analysis of both natural languages and computer languages . Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse-trees using a top-down expansion of the given formal grammar rules. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules.[ 2] Simple implementations of top-down parsing do not terminate for left-recursive grammars, and top-down parsing with backtracking may have exponential time complexity with respect to the length of the input for ambiguous CFGs .[ 3] However, more sophisticated top-down parsers have been created by Frost, Hafiz, and Callaghan [ 4] [ 5] which do accommodate ambiguity and left recursion in polynomial time and which generate polynomial-sized representations of the potentially exponential number of parse trees.","title":"Introduction"},{"location":"Guide/Parsing-algorithm/Top-down/#top-down#parsing","text":"","title":"Top-down parsing"},{"location":"Guide/Parsing-algorithm/Top-down/#wikipedia#top-down#parsing","text":"In computer science , top-down parsing is a parsing strategy where one first looks at the highest level of the parse tree and works down the parse tree by using the rewriting rules of a formal grammar .[ 1] LL parsers are a type of parser that uses a top-down parsing strategy. NOTE: Rewriting rules means expanding, substituting an nonterminal symbol with production body . Top-down parsing is a strategy of analyzing unknown data relationships by hypothesizing general parse tree structures and then considering whether the known fundamental structures are compatible with the hypothesis. It occurs in the analysis of both natural languages and computer languages . Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for parse-trees using a top-down expansion of the given formal grammar rules. Inclusive choice is used to accommodate ambiguity by expanding all alternative right-hand-sides of grammar rules.[ 2] Simple implementations of top-down parsing do not terminate for left-recursive grammars, and top-down parsing with backtracking may have exponential time complexity with respect to the length of the input for ambiguous CFGs .[ 3] However, more sophisticated top-down parsers have been created by Frost, Hafiz, and Callaghan [ 4] [ 5] which do accommodate ambiguity and left recursion in polynomial time and which generate polynomial-sized representations of the potentially exponential number of parse trees.","title":"wikipedia Top-down parsing"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/","text":"wikipedia LL parser In computer science , an LL parser (**L**eft-to-right, **L**eftmost derivation) is a top-down parser for a subset of context-free languages . It parses the input from **L**eft to right, performing **L**eftmost derivation of the sentence. NOTE: Follow the **L**eftmost derivation , you will find an detailed explanation of leftmost derivation. An LL parser is called an LL( k ) parser if it uses k tokens of lookahead when parsing a sentence. A grammar is called an LL( k ) grammar if an LL( k ) parser can be constructed from it. A formal language is called an LL( k ) language if it has an LL( k ) grammar. The set of LL( k ) languages is properly contained in that of LL( k +1) languages, for each k \u2265 0. A corollary of this is that not all context-free languages can be recognized by an LL( k ) parser. An LL parser is called an LL( * ), or LL-regular, parser if it is not restricted to a finite number k of tokens of lookahead, but can make parsing decisions by recognizing whether the following tokens belong to a regular language (for example by means of a deterministic finite automaton ). LL grammars, particularly LL(1) grammars, are of great practical interest, as parsers for these grammars are easy to construct, and many computer languages are designed to be LL(1) for this reason.[ 3] LL parsers are table-based parsers, similar to LR parsers . LL grammars can also be parsed by recursive descent parsers . According to Waite and Goos (1984), LL( k ) grammars were introduced by Stearns and Lewis (1969). Overview For a given context-free grammar , the parser attempts to find the leftmost derivation . Given an example grammar $ G $: $ S\\to E $ $ E\\to (E+E) $ $ E\\to i $ the leftmost derivation for $ w=((i+i)+i) $ is: $ S {\\overset {(1)}{\\Rightarrow }} E {\\overset {(2)}{\\Rightarrow }} (E+E) {\\overset {(2)}{\\Rightarrow }} ((E+E)+E) {\\overset {(3)}{\\Rightarrow }} ((i+E)+E) {\\overset {(3)}{\\Rightarrow }} ((i+i)+E) {\\overset {(3)}{\\Rightarrow }} ((i+i)+i) $ Generally, there are multiple possibilities when selecting a rule to expand the leftmost non-terminal . In step 2 of the previous example, the parser must choose whether to apply rule 2 or rule 3: $ S {\\overset {(1)}{\\Rightarrow }} E {\\overset {(?)}{\\Rightarrow }} ? $ To be efficient, the parser must be able to make this choice deterministically when possible, without backtracking . For some grammars, it can do this by peeking on the unread input (without reading). In our example, if the parser knows that the next unread symbol is $ ( $ , the only correct rule that can be used is 2. NOTE: Peeking makes the parser predictive to choose an deterministic production and avoid choosing by backtracking . As you will see later, it is not enough to achieve predictability just by peeking , parsing table is also needed to make the parser predictive to avoid backtracking . Generally, an $ LL(k) $ parser can look ahead at $ k $ symbols. However, given a grammar, the problem of determining if there exists a $ LL(k) $ parser for some $ k $ that recognizes it is undecidable. For each $ k $, there is a language that cannot be recognized by an $ LL(k) $ parser, but can be by an $ LL(k+1) $. We can use the above analysis to give the following formal definition: Let $ G $ be a context-free grammar and $ k\\geq 1 $. We say that $ G $ is $ LL(k) $, if and only if for any two leftmost derivations: $ S \\Rightarrow \\dots \\Rightarrow wA\\alpha \\Rightarrow \\dots \\Rightarrow w\\beta \\alpha \\Rightarrow \\dots \\Rightarrow wu $ $ S \\Rightarrow \\dots \\Rightarrow wA\\alpha \\Rightarrow \\dots \\Rightarrow w\\gamma \\alpha \\Rightarrow \\dots \\Rightarrow wv $ the following condition holds: the prefix of the string $ u $ of length $ k $ equals the prefix of the string $ v $ of length $ k $ implies $ \\beta = \\gamma $. Do not understand. Parser The $ LL(k) $ parser is a deterministic pushdown automaton with the ability to peek on the next $ k $ input symbols without reading. This peek capability can be emulated by storing the lookahead buffer contents in the finite state space , since both buffer and input alphabet are finite in size. As a result, this does not make the automaton more powerful, but is a convenient abstraction. The stack alphabet is $ \\Gamma =N\\cup \\Sigma $, where: $ N $ is the set of non-terminals; $ \\Sigma $ the set of terminal (input) symbols with a special end-of-input (EOI) symbol $ $ $. The parser stack initially contains the starting symbol above the EOI: $ [ S $ ] $. During operation, the parser repeatedly replaces the symbol $ X $ on top of the stack: with some $ \\alpha $, if $ X\\in N $ and there is a rule $ X\\to \\alpha $ ( X X is a non-terminal); with $ \\epsilon $ (in some notations $ \\lambda $), i.e. $ X $ is popped off the stack, if $ X\\in \\Sigma $. In this case, an input symbol $ x $ is read and if $ x\\neq X , the parser rejects the input ( , the parser rejects the input ( X$ is a terminal, which means X X should equal to x x ). NOTE: At first step, X X is S S . If the last symbol to be removed from the stack is the EOI, the parsing is successful; the automaton accepts via an empty stack. The states and the transition function are not explicitly given; they are specified (generated) using a more convenient parse table instead. The table provides the following mapping: row: top-of-stack symbol $ X $ column: $ |w|\\leq k $ lookahead buffer contents cell: rule number for $ X\\to \\alpha $ or $ \\epsilon $ If the parser cannot perform a valid transition, the input is rejected (empty cells). To make the table more compact, only the non-terminal rows are commonly displayed, since the action is the same for terminals. NOTE: The LL parser consists of three parts parsing table parser stack input stream The following image is from a Wikipedia entry pushdown automaton The $ LL(k) $ parser is a deterministic pushdown automaton . Concrete example Set up To explain an LL(1) parser's workings we will consider the following small LL(1) grammar: S \u2192 F S \u2192 ( S + F ) F \u2192 a and parse the following input: ( a + a ) An LL(1) parsing table for a grammar has a row for each of the non-terminals and a column for each terminal (including the special terminal, represented here as $ , that is used to indicate the end of the input stream). Each cell of the table may point to at most one rule of the grammar (identified by its number). For example, in the parsing table for the above grammar, the cell for the non-terminal 'S' and terminal '(' points to the rule number 2: ( ) a + $ S 2 - 1 - - F - - 3 - - The algorithm to construct a parsing table is described in a later section, but first let's see how the parser uses the parsing table to process its input. Parsing procedure In each step, the parser reads the next-available symbol from the input stream, and the top-most symbol from the stack. If the input symbol and the stack-top symbol match, the parser discards them both, leaving only the unmatched symbols in the input stream and on the stack. Thus, in its first step, the parser reads the input symbol ( and the stack-top symbol S . The parsing table instruction comes from the column headed by the input symbol ( and the row headed by the stack-top symbol S ; this cell contains 2 , which instructs the parser to apply rule (2). The parser has to rewrite S to ( S + F ) on the stack by removing S from stack and pushing ) , F , + , S , ( onto the stack, and this writes the rule number 2 to the output. The stack then becomes: [ (, S, +, F, ), $ ] In the second step, the parser removes the ( from its input stream and from its stack , since they now match. The stack now becomes: [ S, +, F, ), $ ] Now the parser has an a on its input stream and an S as its stack top. The parsing table instructs it to apply rule (1) from the grammar and write the rule number 1 to the output stream. The stack becomes: [ F, +, F, ), $ ] The parser now has an ' a' on its input stream and an 'F' as its stack top. The parsing table instructs it to apply rule (3) from the grammar and write the rule number 3 to the output stream. The stack becomes: [ a, +, F, ), $ ] The parser now has an ' a' on the input stream and an 'a' at its stack top. Because they are the same, it removes it from the input stream and pops it from the top of the stack. The parser then has an ' +' on the input stream and '+' is at the top of the stack meaning, like with 'a', it is popped from the stack and removed from the input stream. This results in: [ F, ), $ ] In the next three steps the parser will replace ' F' on the stack by ' a' , write the rule number 3 to the output stream and remove the ' a' and ' )' from both the stack and the input stream. The parser thus ends with ' $' on both its stack and its input stream. In this case the parser will report that it has accepted the input string and write the following list of rule numbers to the output stream: [ 2, 1, 3, 3 ] This is indeed a list of rules for a leftmost derivation of the input string, which is: S \u2192 ( S + F ) \u2192 ( F + F ) \u2192 ( a + F ) \u2192 ( a + a ) implementation cpp python Remarks As can be seen from the example, the parser performs three types of steps depending on whether the top of the stack is a nonterminal, a terminal or the special symbol $ : If the top is a nonterminal then the parser looks up in the parsing table, on the basis of this nonterminal and the symbol on the input stream, which rule of the grammar it should use to replace nonterminal on the stack. The number of the rule is written to the output stream. If the parsing table indicates that there is no such rule then the parser reports an error and stops. If the top is a terminal then the parser compares it to the symbol on the input stream and if they are equal they are both removed. If they are not equal the parser reports an error and stops. If the top is ** and on the input stream there is also a ** ** and on the input stream there is also a ** then the parser reports that it has successfully parsed the input, otherwise it reports an error. In both cases the parser will stop. These steps are repeated until the parser stops, and then it will have either completely parsed the input and written a leftmost derivation to the output stream or it will have reported an error. Constructing an LL(1) parsing table In order to fill the parsing table , we have to establish what grammar rule the parser should choose if it sees a nonterminal A on the top of its stack and a symbol a on its input stream. It is easy to see that such a rule should be of the form A \u2192 w and that the language corresponding to w should have at least one string starting with a . For this purpose we define the First-set of w , written here as Fi (w) , as the set of terminals that can be found at the start of some string in w , plus \\epsilon \\epsilon if the empty string also belongs to w . Given a grammar with the rules A_1 \\to w_1, \\dots, A_n \\to w_n A_1 \\to w_1, \\dots, A_n \\to w_n , we can compute the Fi (wi) and Fi (Ai) for every rule as follows:","title":"Introduction"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#wikipedia#ll#parser","text":"In computer science , an LL parser (**L**eft-to-right, **L**eftmost derivation) is a top-down parser for a subset of context-free languages . It parses the input from **L**eft to right, performing **L**eftmost derivation of the sentence. NOTE: Follow the **L**eftmost derivation , you will find an detailed explanation of leftmost derivation. An LL parser is called an LL( k ) parser if it uses k tokens of lookahead when parsing a sentence. A grammar is called an LL( k ) grammar if an LL( k ) parser can be constructed from it. A formal language is called an LL( k ) language if it has an LL( k ) grammar. The set of LL( k ) languages is properly contained in that of LL( k +1) languages, for each k \u2265 0. A corollary of this is that not all context-free languages can be recognized by an LL( k ) parser. An LL parser is called an LL( * ), or LL-regular, parser if it is not restricted to a finite number k of tokens of lookahead, but can make parsing decisions by recognizing whether the following tokens belong to a regular language (for example by means of a deterministic finite automaton ). LL grammars, particularly LL(1) grammars, are of great practical interest, as parsers for these grammars are easy to construct, and many computer languages are designed to be LL(1) for this reason.[ 3] LL parsers are table-based parsers, similar to LR parsers . LL grammars can also be parsed by recursive descent parsers . According to Waite and Goos (1984), LL( k ) grammars were introduced by Stearns and Lewis (1969).","title":"wikipedia LL parser"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#overview","text":"For a given context-free grammar , the parser attempts to find the leftmost derivation . Given an example grammar $ G $: $ S\\to E $ $ E\\to (E+E) $ $ E\\to i $ the leftmost derivation for $ w=((i+i)+i) $ is: $ S {\\overset {(1)}{\\Rightarrow }} E {\\overset {(2)}{\\Rightarrow }} (E+E) {\\overset {(2)}{\\Rightarrow }} ((E+E)+E) {\\overset {(3)}{\\Rightarrow }} ((i+E)+E) {\\overset {(3)}{\\Rightarrow }} ((i+i)+E) {\\overset {(3)}{\\Rightarrow }} ((i+i)+i) $ Generally, there are multiple possibilities when selecting a rule to expand the leftmost non-terminal . In step 2 of the previous example, the parser must choose whether to apply rule 2 or rule 3: $ S {\\overset {(1)}{\\Rightarrow }} E {\\overset {(?)}{\\Rightarrow }} ? $ To be efficient, the parser must be able to make this choice deterministically when possible, without backtracking . For some grammars, it can do this by peeking on the unread input (without reading). In our example, if the parser knows that the next unread symbol is $ ( $ , the only correct rule that can be used is 2. NOTE: Peeking makes the parser predictive to choose an deterministic production and avoid choosing by backtracking . As you will see later, it is not enough to achieve predictability just by peeking , parsing table is also needed to make the parser predictive to avoid backtracking . Generally, an $ LL(k) $ parser can look ahead at $ k $ symbols. However, given a grammar, the problem of determining if there exists a $ LL(k) $ parser for some $ k $ that recognizes it is undecidable. For each $ k $, there is a language that cannot be recognized by an $ LL(k) $ parser, but can be by an $ LL(k+1) $. We can use the above analysis to give the following formal definition: Let $ G $ be a context-free grammar and $ k\\geq 1 $. We say that $ G $ is $ LL(k) $, if and only if for any two leftmost derivations: $ S \\Rightarrow \\dots \\Rightarrow wA\\alpha \\Rightarrow \\dots \\Rightarrow w\\beta \\alpha \\Rightarrow \\dots \\Rightarrow wu $ $ S \\Rightarrow \\dots \\Rightarrow wA\\alpha \\Rightarrow \\dots \\Rightarrow w\\gamma \\alpha \\Rightarrow \\dots \\Rightarrow wv $ the following condition holds: the prefix of the string $ u $ of length $ k $ equals the prefix of the string $ v $ of length $ k $ implies $ \\beta = \\gamma $. Do not understand.","title":"Overview"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#parser","text":"The $ LL(k) $ parser is a deterministic pushdown automaton with the ability to peek on the next $ k $ input symbols without reading. This peek capability can be emulated by storing the lookahead buffer contents in the finite state space , since both buffer and input alphabet are finite in size. As a result, this does not make the automaton more powerful, but is a convenient abstraction. The stack alphabet is $ \\Gamma =N\\cup \\Sigma $, where: $ N $ is the set of non-terminals; $ \\Sigma $ the set of terminal (input) symbols with a special end-of-input (EOI) symbol $ $ $. The parser stack initially contains the starting symbol above the EOI: $ [ S $ ] $. During operation, the parser repeatedly replaces the symbol $ X $ on top of the stack: with some $ \\alpha $, if $ X\\in N $ and there is a rule $ X\\to \\alpha $ ( X X is a non-terminal); with $ \\epsilon $ (in some notations $ \\lambda $), i.e. $ X $ is popped off the stack, if $ X\\in \\Sigma $. In this case, an input symbol $ x $ is read and if $ x\\neq X , the parser rejects the input ( , the parser rejects the input ( X$ is a terminal, which means X X should equal to x x ). NOTE: At first step, X X is S S . If the last symbol to be removed from the stack is the EOI, the parsing is successful; the automaton accepts via an empty stack. The states and the transition function are not explicitly given; they are specified (generated) using a more convenient parse table instead. The table provides the following mapping: row: top-of-stack symbol $ X $ column: $ |w|\\leq k $ lookahead buffer contents cell: rule number for $ X\\to \\alpha $ or $ \\epsilon $ If the parser cannot perform a valid transition, the input is rejected (empty cells). To make the table more compact, only the non-terminal rows are commonly displayed, since the action is the same for terminals. NOTE: The LL parser consists of three parts parsing table parser stack input stream The following image is from a Wikipedia entry pushdown automaton The $ LL(k) $ parser is a deterministic pushdown automaton .","title":"Parser"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#concrete#example","text":"","title":"Concrete example"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#set#up","text":"To explain an LL(1) parser's workings we will consider the following small LL(1) grammar: S \u2192 F S \u2192 ( S + F ) F \u2192 a and parse the following input: ( a + a ) An LL(1) parsing table for a grammar has a row for each of the non-terminals and a column for each terminal (including the special terminal, represented here as $ , that is used to indicate the end of the input stream). Each cell of the table may point to at most one rule of the grammar (identified by its number). For example, in the parsing table for the above grammar, the cell for the non-terminal 'S' and terminal '(' points to the rule number 2: ( ) a + $ S 2 - 1 - - F - - 3 - - The algorithm to construct a parsing table is described in a later section, but first let's see how the parser uses the parsing table to process its input.","title":"Set up"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#parsing#procedure","text":"In each step, the parser reads the next-available symbol from the input stream, and the top-most symbol from the stack. If the input symbol and the stack-top symbol match, the parser discards them both, leaving only the unmatched symbols in the input stream and on the stack. Thus, in its first step, the parser reads the input symbol ( and the stack-top symbol S . The parsing table instruction comes from the column headed by the input symbol ( and the row headed by the stack-top symbol S ; this cell contains 2 , which instructs the parser to apply rule (2). The parser has to rewrite S to ( S + F ) on the stack by removing S from stack and pushing ) , F , + , S , ( onto the stack, and this writes the rule number 2 to the output. The stack then becomes: [ (, S, +, F, ), $ ] In the second step, the parser removes the ( from its input stream and from its stack , since they now match. The stack now becomes: [ S, +, F, ), $ ] Now the parser has an a on its input stream and an S as its stack top. The parsing table instructs it to apply rule (1) from the grammar and write the rule number 1 to the output stream. The stack becomes: [ F, +, F, ), $ ] The parser now has an ' a' on its input stream and an 'F' as its stack top. The parsing table instructs it to apply rule (3) from the grammar and write the rule number 3 to the output stream. The stack becomes: [ a, +, F, ), $ ] The parser now has an ' a' on the input stream and an 'a' at its stack top. Because they are the same, it removes it from the input stream and pops it from the top of the stack. The parser then has an ' +' on the input stream and '+' is at the top of the stack meaning, like with 'a', it is popped from the stack and removed from the input stream. This results in: [ F, ), $ ] In the next three steps the parser will replace ' F' on the stack by ' a' , write the rule number 3 to the output stream and remove the ' a' and ' )' from both the stack and the input stream. The parser thus ends with ' $' on both its stack and its input stream. In this case the parser will report that it has accepted the input string and write the following list of rule numbers to the output stream: [ 2, 1, 3, 3 ] This is indeed a list of rules for a leftmost derivation of the input string, which is: S \u2192 ( S + F ) \u2192 ( F + F ) \u2192 ( a + F ) \u2192 ( a + a )","title":"Parsing procedure"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#implementation","text":"cpp python","title":"implementation"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#remarks","text":"As can be seen from the example, the parser performs three types of steps depending on whether the top of the stack is a nonterminal, a terminal or the special symbol $ : If the top is a nonterminal then the parser looks up in the parsing table, on the basis of this nonterminal and the symbol on the input stream, which rule of the grammar it should use to replace nonterminal on the stack. The number of the rule is written to the output stream. If the parsing table indicates that there is no such rule then the parser reports an error and stops. If the top is a terminal then the parser compares it to the symbol on the input stream and if they are equal they are both removed. If they are not equal the parser reports an error and stops. If the top is ** and on the input stream there is also a ** ** and on the input stream there is also a ** then the parser reports that it has successfully parsed the input, otherwise it reports an error. In both cases the parser will stop. These steps are repeated until the parser stops, and then it will have either completely parsed the input and written a leftmost derivation to the output stream or it will have reported an error.","title":"Remarks"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/#constructing#an#ll1#parsing#table","text":"In order to fill the parsing table , we have to establish what grammar rule the parser should choose if it sees a nonterminal A on the top of its stack and a symbol a on its input stream. It is easy to see that such a rule should be of the form A \u2192 w and that the language corresponding to w should have at least one string starting with a . For this purpose we define the First-set of w , written here as Fi (w) , as the set of terminals that can be found at the start of some string in w , plus \\epsilon \\epsilon if the empty string also belongs to w . Given a grammar with the rules A_1 \\to w_1, \\dots, A_n \\to w_n A_1 \\to w_1, \\dots, A_n \\to w_n , we can compute the Fi (wi) and Fi (Ai) for every rule as follows:","title":"Constructing an LL(1) parsing table"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/andrewbegel-LL%281%29-Parsing/","text":"LL(1) Parsing The following grammar comes from Written Assignment 2. E -> E + T | T T -> T * F | F F -> (E) | int This is a grammar for arithmetic. There are several orders of precedence here: () 's beat * , and * beats + . We'd like to parse sentences using this grammar using a top-down, recursive descent algorithm. left-recursion elimination This algorithm traverses the grammar looking for matches between terminals ( * , + , ( , ) , and int ) and the input sentence. We do this search depth-first, which presents a problem. If we start at the starting production E , and derive the production E + T , we have E still on the left. In recursive-descent parsing, we can only expand the left-most non-terminal at each step! We're going to infinitely loop if we try to parse sentences using this grammar. How do we fix it? We use a technique known as left-recursion elimination to get rid of the non-terminals on the left-hand side of each production that cause us to infinitely loop. (Note: not all grammars have left recursion. You can identify the ones that have immediate left recursion by looking at all of the productions -- if the non-terminal on the left side of the arrow is the same as the non-terminal in the left-most position of any phrase on the right side of the arrow, then this grammar is left-recursive. There are other forms of left recursion that can show up if you were to \"recurse\" down multiple rules in the grammar. If you eventually will cause any infinite loop, the grammar is left-recursive.) Let's take a look at the first one: E -> E + T | T What we do is this. For each production where the non-terminal on the left ( E ) of the arrow is the same as the left-side of a production on the right-hand side of the arrow ( E + T ), we take the part of the production without the E ( +T ) and move it down into its own new production (we'll call it E' ). E' -> + T We're not done yet. Now, after each of the new productions, add E' to the end. E' -> + T E' Nope, still not done yet. Now add an extra production to epsilon. E' -> + T E' | epsilon Good. Now we must fix up the original E productions. Here, we take all of the right-hand sides that didn't start with E , and add E' to the end of them. E -> T E' If we do this for the T productions as well, we get the following grammar: E -> T E' E' -> + T E' | epsilon T -> F T' T' -> * F T' | epsilon F -> (E) | int Note, the F production didn't get changed at all. That's because F didn't appear on the leftmost position of any of the productions on the right-hand side of the arrow. LL(1) parsing Once we have performed left-recursion elimination on our grammar, we need to construct our parser. We're going to use an algorithm called LL(1) parsing. This is an algorithm that utilizes a lookup table to parse an expression. On top, we list all of the terminals , and on the left, we list the non-terminals (we include $ to signify end-of-file). + * ( ) int $ E E' T T' F Our LL(1) parser also contains a stack of non-terminals . Initially, we'll only put the starting state ( E ) on the stack. As we parse, we'll be putting non-terminals on and popping them off this stack. When they're all gone, and our stack is empty (and there is no more input), we're done. At each step, there is a grammar symbol at the top of the stack. When we see an input token from the lexer, we look in the table to see what to do. Each cell in the table is going to tell our LL(1) parser what to do when it sees the terminal on the top when the non-terminal on the left is at the top of the stack. Right now, our table is empty. Let's fill it up. We do this by computing two functions called FIRST and FOLLOW . FIRST FIRST is a function on each non-terminal ( E , E' , T , T' , and F ) that tells us which terminals (tokens from the lexer) can appear as the first part of one of these non-terminals . Epsilon (neither a terminal nor a non-terminal) may also be included in this FIRST function. ( FIRST is also defined for terminals, but its value is just equal to the terminal itself, so we won't talk about it more.) What this means is that the parser is going to invoke some of these productions in the grammar. We need to know which one to pick when we see a particular token in the input stream. NOTE: To be predictive. So, let's start computing. What is the FIRST ( E )? What are the terminals that can appear at the beginning of the stream when we're looking for an E ? Well, E -> T E' , so whatever occurs at the beginning of E will be the same as what happens at the beginning of T . FIRST(E) => FIRST(T) How about FIRST ( E' )? This one is easy, we have the terminal + , and epsilon. FIRST(E') = { +, epsilon } And we'll continue with the others: FIRST(T) => FIRST(F) FIRST(T') = { *, epsilon } FIRST(F) = { (, int } See? FIRST ( F ) is just the set of terminals that are at the beginnings of its productions. So, to sum up: FIRST(E) = { (, int } FIRST(E') = { +, epsilon } FIRST(T) = { (, int } FIRST(T') = { *, epsilon } FIRST(F) = { (, int } FOLLOW Now, let's do FOLLOW . Just as FIRST shows us the terminals that can be at the beginning of a derived non-terminal, FOLLOW shows us the terminals that can come after a derived non-terminal. Note, this does not mean the last terminal derived from a non-terminal. It's the set of terminals that can come after it. We define FOLLOW for all the non-terminals in the grammar. How do we figure out FOLLOW ? Instead of looking at the first terminal for each phrase on the right side of the arrow, we find every place our non-terminal is located on the right side of any of the arrows. Then we look for some terminals . As we go through our example, you'll see almost all of the different ways we figure out the FOLLOW of a non-terminal. First, however, let's pretend that our grammar starts with a unique starting production (it's not really part of the grammar): S -> E We start our journey at S , but rewrite it a bit to reflect the EOF that can be at the end. In parser-land, EOF is represented by $ . So our production is really: S -> E $ What is FOLLOW ( E )? (Note: We don't really care about FOLLOW ( S ) because it's just imaginary.) Look on all of the right-hand sides (after the arrow) of all of the productions in the grammar. What terminals appear on the right of the E ? Well, I see a $ and a ). FOLLOW(E) = { $, ) }","title":"Introduction"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/andrewbegel-LL%281%29-Parsing/#ll1#parsing","text":"The following grammar comes from Written Assignment 2. E -> E + T | T T -> T * F | F F -> (E) | int This is a grammar for arithmetic. There are several orders of precedence here: () 's beat * , and * beats + . We'd like to parse sentences using this grammar using a top-down, recursive descent algorithm.","title":"LL(1) Parsing"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/andrewbegel-LL%281%29-Parsing/#left-recursion#elimination","text":"This algorithm traverses the grammar looking for matches between terminals ( * , + , ( , ) , and int ) and the input sentence. We do this search depth-first, which presents a problem. If we start at the starting production E , and derive the production E + T , we have E still on the left. In recursive-descent parsing, we can only expand the left-most non-terminal at each step! We're going to infinitely loop if we try to parse sentences using this grammar. How do we fix it? We use a technique known as left-recursion elimination to get rid of the non-terminals on the left-hand side of each production that cause us to infinitely loop. (Note: not all grammars have left recursion. You can identify the ones that have immediate left recursion by looking at all of the productions -- if the non-terminal on the left side of the arrow is the same as the non-terminal in the left-most position of any phrase on the right side of the arrow, then this grammar is left-recursive. There are other forms of left recursion that can show up if you were to \"recurse\" down multiple rules in the grammar. If you eventually will cause any infinite loop, the grammar is left-recursive.) Let's take a look at the first one: E -> E + T | T What we do is this. For each production where the non-terminal on the left ( E ) of the arrow is the same as the left-side of a production on the right-hand side of the arrow ( E + T ), we take the part of the production without the E ( +T ) and move it down into its own new production (we'll call it E' ). E' -> + T We're not done yet. Now, after each of the new productions, add E' to the end. E' -> + T E' Nope, still not done yet. Now add an extra production to epsilon. E' -> + T E' | epsilon Good. Now we must fix up the original E productions. Here, we take all of the right-hand sides that didn't start with E , and add E' to the end of them. E -> T E' If we do this for the T productions as well, we get the following grammar: E -> T E' E' -> + T E' | epsilon T -> F T' T' -> * F T' | epsilon F -> (E) | int Note, the F production didn't get changed at all. That's because F didn't appear on the leftmost position of any of the productions on the right-hand side of the arrow.","title":"left-recursion elimination"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/andrewbegel-LL%281%29-Parsing/#ll1#parsing_1","text":"Once we have performed left-recursion elimination on our grammar, we need to construct our parser. We're going to use an algorithm called LL(1) parsing. This is an algorithm that utilizes a lookup table to parse an expression. On top, we list all of the terminals , and on the left, we list the non-terminals (we include $ to signify end-of-file). + * ( ) int $ E E' T T' F Our LL(1) parser also contains a stack of non-terminals . Initially, we'll only put the starting state ( E ) on the stack. As we parse, we'll be putting non-terminals on and popping them off this stack. When they're all gone, and our stack is empty (and there is no more input), we're done. At each step, there is a grammar symbol at the top of the stack. When we see an input token from the lexer, we look in the table to see what to do. Each cell in the table is going to tell our LL(1) parser what to do when it sees the terminal on the top when the non-terminal on the left is at the top of the stack. Right now, our table is empty. Let's fill it up. We do this by computing two functions called FIRST and FOLLOW .","title":"LL(1) parsing"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/andrewbegel-LL%281%29-Parsing/#first","text":"FIRST is a function on each non-terminal ( E , E' , T , T' , and F ) that tells us which terminals (tokens from the lexer) can appear as the first part of one of these non-terminals . Epsilon (neither a terminal nor a non-terminal) may also be included in this FIRST function. ( FIRST is also defined for terminals, but its value is just equal to the terminal itself, so we won't talk about it more.) What this means is that the parser is going to invoke some of these productions in the grammar. We need to know which one to pick when we see a particular token in the input stream. NOTE: To be predictive. So, let's start computing. What is the FIRST ( E )? What are the terminals that can appear at the beginning of the stream when we're looking for an E ? Well, E -> T E' , so whatever occurs at the beginning of E will be the same as what happens at the beginning of T . FIRST(E) => FIRST(T) How about FIRST ( E' )? This one is easy, we have the terminal + , and epsilon. FIRST(E') = { +, epsilon } And we'll continue with the others: FIRST(T) => FIRST(F) FIRST(T') = { *, epsilon } FIRST(F) = { (, int } See? FIRST ( F ) is just the set of terminals that are at the beginnings of its productions. So, to sum up: FIRST(E) = { (, int } FIRST(E') = { +, epsilon } FIRST(T) = { (, int } FIRST(T') = { *, epsilon } FIRST(F) = { (, int }","title":"FIRST"},{"location":"Guide/Parsing-algorithm/Top-down/LL-parser/andrewbegel-LL%281%29-Parsing/#follow","text":"Now, let's do FOLLOW . Just as FIRST shows us the terminals that can be at the beginning of a derived non-terminal, FOLLOW shows us the terminals that can come after a derived non-terminal. Note, this does not mean the last terminal derived from a non-terminal. It's the set of terminals that can come after it. We define FOLLOW for all the non-terminals in the grammar. How do we figure out FOLLOW ? Instead of looking at the first terminal for each phrase on the right side of the arrow, we find every place our non-terminal is located on the right side of any of the arrows. Then we look for some terminals . As we go through our example, you'll see almost all of the different ways we figure out the FOLLOW of a non-terminal. First, however, let's pretend that our grammar starts with a unique starting production (it's not really part of the grammar): S -> E We start our journey at S , but rewrite it a bit to reflect the EOF that can be at the end. In parser-land, EOF is represented by $ . So our production is really: S -> E $ What is FOLLOW ( E )? (Note: We don't really care about FOLLOW ( S ) because it's just imaginary.) Look on all of the right-hand sides (after the arrow) of all of the productions in the grammar. What terminals appear on the right of the E ? Well, I see a $ and a ). FOLLOW(E) = { $, ) }","title":"FOLLOW"},{"location":"Guide/Parsing-algorithm/Top-down/Recursive-descent-parser/","text":"","title":"Introduction"},{"location":"Guide/Program-transformation/","text":"Program transformation \u662f\u5728\u9605\u8bfb Aspect Oriented Programming in C++ - Current supported alternatives # A \u65f6\uff0c\u53d1\u73b0\u7684: Aspect-oriented programming is a just a special kind of program transformation (\" find places that match this condition ('pointcut') and do this to the code at that place \"). So, if you have a program transformation tool, you can emulate AOP pretty easily. To do transformation on C++ you need a strong C++ front end and as well as ability to transform and regenerate code. OpenC++ was a project to do C++ transformations, where the transformations are coded purely as procedural AST-walks with procedural AST modifications. A particular issue had to do with which dialect of C++ was handled by OpenC++; in particular, I'm not sure OpenC++ handled templates or full common dialects (GCC, MS) of C+; however I have no direct experience with it, just a keen aficionado of such tools. \u770b\u5230\u4e86\u8fd9\u6bb5\u63cf\u8ff0\uff0c\u6211\u60f3\u5230\u4e86 JavaScript Babel (transcompiler) \uff0c\u73b0\u5728\u60f3\u60f3\uff0ccompiler\u6240\u505a\u7684\u5176\u5b9e\u5c31\u662f program transformation\u3002 semanticdesigns Program Transformation TODO 1\u3001Google Logica Logica\u4ee3\u7801\u53ef\u4ee5\u7f16\u8bd1\u6210SQL \u53c2\u89c1:","title":"Introduction"},{"location":"Guide/Program-transformation/#program#transformation","text":"\u662f\u5728\u9605\u8bfb Aspect Oriented Programming in C++ - Current supported alternatives # A \u65f6\uff0c\u53d1\u73b0\u7684: Aspect-oriented programming is a just a special kind of program transformation (\" find places that match this condition ('pointcut') and do this to the code at that place \"). So, if you have a program transformation tool, you can emulate AOP pretty easily. To do transformation on C++ you need a strong C++ front end and as well as ability to transform and regenerate code. OpenC++ was a project to do C++ transformations, where the transformations are coded purely as procedural AST-walks with procedural AST modifications. A particular issue had to do with which dialect of C++ was handled by OpenC++; in particular, I'm not sure OpenC++ handled templates or full common dialects (GCC, MS) of C+; however I have no direct experience with it, just a keen aficionado of such tools. \u770b\u5230\u4e86\u8fd9\u6bb5\u63cf\u8ff0\uff0c\u6211\u60f3\u5230\u4e86 JavaScript Babel (transcompiler) \uff0c\u73b0\u5728\u60f3\u60f3\uff0ccompiler\u6240\u505a\u7684\u5176\u5b9e\u5c31\u662f program transformation\u3002","title":"Program transformation"},{"location":"Guide/Program-transformation/#semanticdesigns#program#transformation","text":"","title":"semanticdesigns Program Transformation"},{"location":"Guide/Program-transformation/#todo","text":"1\u3001Google Logica Logica\u4ee3\u7801\u53ef\u4ee5\u7f16\u8bd1\u6210SQL \u53c2\u89c1:","title":"TODO"},{"location":"Guide/Program-transformation/DMS-Software-Reengineering-Toolkit/","text":"DMS\u00ae Software Reengineering Toolkit\u2122 \u662f\u5728\u9605\u8bfb Aspect Oriented Programming in C++ - Current supported alternatives # A \u65f6\uff0c\u53d1\u73b0\u7684: Aspect-oriented programming is a just a special kind of program transformation (\" find places that match this condition ('pointcut') and do this to the code at that place \"). So, if you have a program transformation tool, you can emulate AOP pretty easily. To do transformation on C++ you need a strong C++ front end and as well as ability to transform and regenerate code. OpenC++ was a project to do C++ transformations, where the transformations are coded purely as procedural AST-walks with procedural AST modifications. A particular issue had to do with which dialect of C++ was handled by OpenC++; in particular, I'm not sure OpenC++ handled templates or full common dialects (GCC, MS) of C+; however I have no direct experience with it, just a keen aficionado of such tools. Our DMS Software Reengineering Toolkit is a general purpose program transformation parameterized by language definitions. It has robust definitions for GCC and MS dialects of C++ . You can implement program transformations procedurally as OpenC++ does, or more conveniently you can write source-to-source pattern-directed transformations (or more usually, mix these to achieve complex effects). DMS has been used to carry out massive restructuring of large scale C++ codes (see Case study: Re-engineering C++ component models via automatic program transformation ). DMS is actively maintained, but it is commercial. semanticdesigns DMS\u00ae Software Reengineering Toolkit\u2122 wikipedia DMS Software Reengineering Toolkit","title":"Introduction"},{"location":"Guide/Program-transformation/DMS-Software-Reengineering-Toolkit/#dms#software#reengineering#toolkittm","text":"\u662f\u5728\u9605\u8bfb Aspect Oriented Programming in C++ - Current supported alternatives # A \u65f6\uff0c\u53d1\u73b0\u7684: Aspect-oriented programming is a just a special kind of program transformation (\" find places that match this condition ('pointcut') and do this to the code at that place \"). So, if you have a program transformation tool, you can emulate AOP pretty easily. To do transformation on C++ you need a strong C++ front end and as well as ability to transform and regenerate code. OpenC++ was a project to do C++ transformations, where the transformations are coded purely as procedural AST-walks with procedural AST modifications. A particular issue had to do with which dialect of C++ was handled by OpenC++; in particular, I'm not sure OpenC++ handled templates or full common dialects (GCC, MS) of C+; however I have no direct experience with it, just a keen aficionado of such tools. Our DMS Software Reengineering Toolkit is a general purpose program transformation parameterized by language definitions. It has robust definitions for GCC and MS dialects of C++ . You can implement program transformations procedurally as OpenC++ does, or more conveniently you can write source-to-source pattern-directed transformations (or more usually, mix these to achieve complex effects). DMS has been used to carry out massive restructuring of large scale C++ codes (see Case study: Re-engineering C++ component models via automatic program transformation ). DMS is actively maintained, but it is commercial.","title":"DMS\u00ae Software Reengineering Toolkit\u2122"},{"location":"Guide/Program-transformation/DMS-Software-Reengineering-Toolkit/#semanticdesigns#dms#software#reengineering#toolkittm","text":"","title":"semanticdesigns DMS\u00ae Software Reengineering Toolkit\u2122"},{"location":"Guide/Program-transformation/DMS-Software-Reengineering-Toolkit/#wikipedia#dms#software#reengineering#toolkit","text":"","title":"wikipedia DMS Software Reengineering Toolkit"},{"location":"Guide/Semantic-analysis/","text":"Semantic analysis wikipedia Semantic analysis (compilers) Semantic analysis or context sensitive analysis is a process in compiler construction, usually after parsing , to gather necessary semantic information from the source code .[ 1] It usually includes type checking , or makes sure a variable is declared before use which is impossible to describe in the extended Backus\u2013Naur form and thus not easily detected during parsing.","title":"Introduction"},{"location":"Guide/Semantic-analysis/#semantic#analysis","text":"","title":"Semantic analysis"},{"location":"Guide/Semantic-analysis/#wikipedia#semantic#analysis#compilers","text":"Semantic analysis or context sensitive analysis is a process in compiler construction, usually after parsing , to gather necessary semantic information from the source code .[ 1] It usually includes type checking , or makes sure a variable is declared before use which is impossible to describe in the extended Backus\u2013Naur form and thus not easily detected during parsing.","title":"wikipedia Semantic analysis (compilers)"},{"location":"Guide/Semantic-analysis/Formal-semantic/","text":"Formal semantic draft1 \u6211\u4e0d\u77e5\u9053\u672c\u8282\u6807\u9898\u7684formal semantic\u662f\u5426\u51c6\u786e\uff0c\u8fd9\u662f\u4e0a\u5468\u548c\u540c\u4e8b\u8ba8\u8bba\u7684\u65f6\u5019\uff0c\u4ed6\u63d0\u51fa\u7684\u4e00\u4e2a\u6982\u5ff5\u3002\u5176\u5b9e\u5bf9symbol expression\u7684evaluation\u5c31\u662f\u5bf9\u5b83\u7684\u8bed\u4e49\u7684\u7406\u89e3\uff0c\u5173\u4e8e\u6b64\uff0c\u6211\u9996\u5148\u60f3\u5230\u7684\u662f\u57fa\u4e8eAST\u7684evaluation\uff0c\u5982\u679c\u90fd\u662f\u57fa\u4e8eAST\u7684\u8bdd\uff0c\u90a3\u4e48operator\u7684\u5b9a\u4e49\u3001\u542b\u4e49\u7684\u89e3\u91ca\u5c31\u975e\u5e38\u91cd\u8981\u4e86\u3002 \u6211\u7684\u4e00\u4e2a\u95ee\u9898\u662f: \u662f\u5426\u90fd\u662f\u57fa\u4e8eAST\u7684evaluation\u6765\u5b9e\u73b0semantic\u7684\u7406\u89e3\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u9605\u8bfb: 1) programming-language\\docs\\Theory\\Programming-paradigm\\Symbolic-programming\\utexas-cs378.pdf 2) \u9f99\u4e66\u4e2d\u7684\u63cf\u8ff0 3) Google \"tree and formal semantic\" 4) Formal Semantics and Formal Pragmatics, Lecture 1 \u8fd9\u7bc7\u6587\u7ae0\u4e0d\u9519\uff0c\u5df2\u7ecf\u5c06\u4ed6\u4fdd\u5b58\u5230\u4e86\u672c\u5730 draft2 \u5c06sentence\u770b\u505a\u662fexpression\uff0c\u7136\u540e\u5c06expression\u8f6c\u6362\u4e3atree\uff0c\u7136\u540e\u57fa\u4e8etree\u8fdb\u884c\u7ffb\u8bd1\uff0c\u5c06\u5b83\u7ffb\u8bd1\u4e3a\u4e00\u4e9b\u5217\u7684\u6307\u4ee4\uff0c\u8fd9\u5c31\u5b8c\u6210\u4e86\u8bed\u4e49\u7684\u7406\u89e3\u3002\u6700\u6700\u5178\u578b\u7684\u6848\u4f8b\u5c31\u662f computer algebra \u3002","title":"Introduction"},{"location":"Guide/Semantic-analysis/Formal-semantic/#formal#semantic","text":"","title":"Formal semantic"},{"location":"Guide/Semantic-analysis/Formal-semantic/#draft1","text":"\u6211\u4e0d\u77e5\u9053\u672c\u8282\u6807\u9898\u7684formal semantic\u662f\u5426\u51c6\u786e\uff0c\u8fd9\u662f\u4e0a\u5468\u548c\u540c\u4e8b\u8ba8\u8bba\u7684\u65f6\u5019\uff0c\u4ed6\u63d0\u51fa\u7684\u4e00\u4e2a\u6982\u5ff5\u3002\u5176\u5b9e\u5bf9symbol expression\u7684evaluation\u5c31\u662f\u5bf9\u5b83\u7684\u8bed\u4e49\u7684\u7406\u89e3\uff0c\u5173\u4e8e\u6b64\uff0c\u6211\u9996\u5148\u60f3\u5230\u7684\u662f\u57fa\u4e8eAST\u7684evaluation\uff0c\u5982\u679c\u90fd\u662f\u57fa\u4e8eAST\u7684\u8bdd\uff0c\u90a3\u4e48operator\u7684\u5b9a\u4e49\u3001\u542b\u4e49\u7684\u89e3\u91ca\u5c31\u975e\u5e38\u91cd\u8981\u4e86\u3002 \u6211\u7684\u4e00\u4e2a\u95ee\u9898\u662f: \u662f\u5426\u90fd\u662f\u57fa\u4e8eAST\u7684evaluation\u6765\u5b9e\u73b0semantic\u7684\u7406\u89e3\uff1f\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\uff0c\u53ef\u4ee5\u9605\u8bfb: 1) programming-language\\docs\\Theory\\Programming-paradigm\\Symbolic-programming\\utexas-cs378.pdf 2) \u9f99\u4e66\u4e2d\u7684\u63cf\u8ff0 3) Google \"tree and formal semantic\" 4) Formal Semantics and Formal Pragmatics, Lecture 1 \u8fd9\u7bc7\u6587\u7ae0\u4e0d\u9519\uff0c\u5df2\u7ecf\u5c06\u4ed6\u4fdd\u5b58\u5230\u4e86\u672c\u5730","title":"draft1"},{"location":"Guide/Semantic-analysis/Formal-semantic/#draft2","text":"\u5c06sentence\u770b\u505a\u662fexpression\uff0c\u7136\u540e\u5c06expression\u8f6c\u6362\u4e3atree\uff0c\u7136\u540e\u57fa\u4e8etree\u8fdb\u884c\u7ffb\u8bd1\uff0c\u5c06\u5b83\u7ffb\u8bd1\u4e3a\u4e00\u4e9b\u5217\u7684\u6307\u4ee4\uff0c\u8fd9\u5c31\u5b8c\u6210\u4e86\u8bed\u4e49\u7684\u7406\u89e3\u3002\u6700\u6700\u5178\u578b\u7684\u6848\u4f8b\u5c31\u662f computer algebra \u3002","title":"draft2"},{"location":"Implementation/","text":"Implementation \u4e0b\u9762\u603b\u7ed3\u4e00\u4e0b\u6211\u76ee\u524d\u63a5\u89e6\u8fc7\u7684compiler: implementation \u94fe\u63a5 gcc \u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Implementation\\GCC \u7ae0\u8282 LLVM \u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Implementation\\LLVM \u7ae0\u8282 cpython \u5de5\u7a0bprogramming-language\u7684 Python \u7ae0\u8282 nltk","title":"Introduction"},{"location":"Implementation/#implementation","text":"\u4e0b\u9762\u603b\u7ed3\u4e00\u4e0b\u6211\u76ee\u524d\u63a5\u89e6\u8fc7\u7684compiler: implementation \u94fe\u63a5 gcc \u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Implementation\\GCC \u7ae0\u8282 LLVM \u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Implementation\\LLVM \u7ae0\u8282 cpython \u5de5\u7a0bprogramming-language\u7684 Python \u7ae0\u8282 nltk","title":"Implementation"},{"location":"Implementation/Architecture/","text":"\u7f16\u8bd1\u5668\u524d\u7aef \u7f16\u8bd1\u5668\u524d\u7aef\u53ef\u4ee5\u770b\u505a\u662f\u4e24\u68f5\u6811\uff08parse tree\u548cabstract syntax tree\uff09\uff0c\u4e00\u4e2a\u56fe\uff08\u63a7\u5236\u6d41\u56fe\uff09\u3002","title":"\u7f16\u8bd1\u5668\u524d\u7aef"},{"location":"Implementation/Architecture/#_1","text":"\u7f16\u8bd1\u5668\u524d\u7aef\u53ef\u4ee5\u770b\u505a\u662f\u4e24\u68f5\u6811\uff08parse tree\u548cabstract syntax tree\uff09\uff0c\u4e00\u4e2a\u56fe\uff08\u63a7\u5236\u6d41\u56fe\uff09\u3002","title":"\u7f16\u8bd1\u5668\u524d\u7aef"},{"location":"Implementation/impl/","text":"C compiler https://github.com/rui314/8cc https://github.com/rui314/9cc https://github.com/rui314/chibicc https://www.sigbus.info/ https://www.sigbus.info/how-i-wrote-a-self-hosting-c-compiler-in-40-days https://github.com/zakirullin/tiny-compiler https://github.com/jamiebuilds/the-super-tiny-compiler recursive descend parser https://github.com/codeplea/tinyexpr https://github.com/axilmar/parserlib LL parser https://github.com/sid24rane/LL1-parser https://github.com/scottfrazer/hermes https://github.com/dxhj/predictive-parser LR parser https://github.com/erezsh/plyplus https://github.com/lark-parser/lark https://github.com/igordejanovic/parglare https://github.com/bajdcc/clibparser https://github.com/igordejanovic/parglare automaton https://github.com/WojciechMula/pyahocorasick state machine https://github.com/pytransitions/transitions https://github.com/microsoft/BlingFire https://github.com/viewflow/django-fsm https://github.com/jtushman/state_machine https://github.com/boost-experimental/sml https://github.com/dabeaz/ply/tree/master/ply https://github.com/dabeaz/sly","title":"C compiler"},{"location":"Implementation/impl/#c#compiler","text":"https://github.com/rui314/8cc https://github.com/rui314/9cc https://github.com/rui314/chibicc https://www.sigbus.info/ https://www.sigbus.info/how-i-wrote-a-self-hosting-c-compiler-in-40-days https://github.com/zakirullin/tiny-compiler https://github.com/jamiebuilds/the-super-tiny-compiler","title":"C compiler"},{"location":"Implementation/impl/#recursive#descend#parser","text":"https://github.com/codeplea/tinyexpr https://github.com/axilmar/parserlib","title":"recursive descend parser"},{"location":"Implementation/impl/#ll#parser","text":"https://github.com/sid24rane/LL1-parser https://github.com/scottfrazer/hermes https://github.com/dxhj/predictive-parser","title":"LL parser"},{"location":"Implementation/impl/#lr#parser","text":"https://github.com/erezsh/plyplus https://github.com/lark-parser/lark https://github.com/igordejanovic/parglare https://github.com/bajdcc/clibparser https://github.com/igordejanovic/parglare","title":"LR parser"},{"location":"Implementation/impl/#automaton","text":"https://github.com/WojciechMula/pyahocorasick","title":"automaton"},{"location":"Implementation/impl/#state#machine","text":"https://github.com/pytransitions/transitions https://github.com/microsoft/BlingFire https://github.com/viewflow/django-fsm https://github.com/jtushman/state_machine https://github.com/boost-experimental/sml https://github.com/dabeaz/ply/tree/master/ply https://github.com/dabeaz/sly","title":"state machine"},{"location":"Optimization/","text":"Compiler optimizing wikipedia Optimizing compiler","title":"Introduction"},{"location":"Optimization/#compiler#optimizing","text":"","title":"Compiler optimizing"},{"location":"Optimization/#wikipedia#optimizing#compiler","text":"","title":"wikipedia Optimizing compiler"},{"location":"Optimization/Basic-block/","text":"Basic block wikipedia Basic block In compiler construction , a basic block is a straight-line code sequence with no branches in except to the entry and no branches out except at the exit.[ 1] [ 2] This restricted form makes a basic block highly amenable to analysis.[ 3] Compilers usually decompose programs into their basic blocks as a first step in the analysis process. Basic blocks form the vertices or nodes in a control flow graph . Definition The code in a basic block has: One entry point , meaning no code within it is the destination of a jump instruction anywhere in the program. One exit point, meaning only the last instruction can cause the program to begin executing code in a different basic block. Under these circumstances, whenever the first instruction in a basic block is executed, the rest of the instructions are necessarily executed exactly once, in order.[ 4] [ 5] The code may be source code , assembly code or some other sequence of instructions. More formally, a sequence of instructions forms a basic block if: The instruction in each position dominates , or always executes before, all those in later positions. No other instruction executes between two instructions in the sequence. This definition is more general than the intuitive one in some ways. For example, it allows unconditional jumps to labels not targeted by other jumps. This definition embodies the properties that make basic blocks easy to work with when constructing an algorithm. The blocks to which control may transfer after reaching the end of a block are called that block's successors , while the blocks from which control may have come when entering a block are called that block's predecessors . The start of a basic block may be jumped to from more than one location. gatevidyalay Basic Blocks and Flow Graphs | Examples Basic Blocks Basic block is a set of statements that always executes in a sequence one after the other. The characteristics of basic blocks are They do not contain any kind of jump statements in them. There is no possibility of branching or getting halt in the middle. All the statements execute in the same order they appear. They do not lose lose the flow control of the program. NOTE: \u6709\u70b9\u539f\u5b50\u7684\u542b\u4e49 Example Of Basic Block Three Address Code for the expression a = b + c + d is- Here, All the statements execute in a sequence one after the other. Thus, they form a basic block. Example Of Not A Basic Block Three Address Code for the expression If A<B then 1 else 0 is- Here, The statements do not execute in a sequence one after the other. Thus, they do not form a basic block. Partitioning Intermediate Code Into Basic Blocks Any given code can be partitioned into basic blocks using the following rules Rule-01: Determining Leaders Following statements of the code are called as Leaders First statement of the code. Statement that is a target of the conditional or unconditional goto statement. Statement that appears immediately after a goto statement. Rule-02: Determining Basic Blocks All the statements that follow the leader (including the leader ) till the next leader appears form one basic block. The first statement of the code is called as the first leader . The block containing the first leader is called as Initial block . Flow Graphs A flow graph is a directed graph with flow control information added to the basic blocks. The basic blocks serve as nodes of the flow graph. There is a directed edge from block B1 to block B2 if B2 appears immediately after B1 in the code. PRACTICE PROBLEMS BASED ON BASIC BLOCKS & FLOW GRAPHS Problem-01: Compute the basic blocks for the given three address statements (1) PROD = 0 (2) I = 1 (3) T2 = addr(A) \u2013 4 (4) T4 = addr(B) \u2013 4 (5) T1 = 4 x I (6) T3 = T2[T1] (7) T5 = T4[T1] (8) T6 = T3 x T5 (9) PROD = PROD + T6 (10) I = I + 1 (11) IF I <=20 GOTO (5) Solution We have- PROD = 0 is a leader since first statement of the code is a leader. T1 = 4 x I is a leader since target of the conditional goto statement is a leader. Now, the given code can be partitioned into two basic blocks as- Problem-02: Draw a flow graph for the three address statements given in problem-01. Solution Firstly, we compute the basic blocks (already done above). Secondly, we assign the flow control information. The required flow graph is-","title":"Introduction"},{"location":"Optimization/Basic-block/#basic#block","text":"","title":"Basic block"},{"location":"Optimization/Basic-block/#wikipedia#basic#block","text":"In compiler construction , a basic block is a straight-line code sequence with no branches in except to the entry and no branches out except at the exit.[ 1] [ 2] This restricted form makes a basic block highly amenable to analysis.[ 3] Compilers usually decompose programs into their basic blocks as a first step in the analysis process. Basic blocks form the vertices or nodes in a control flow graph .","title":"wikipedia Basic block"},{"location":"Optimization/Basic-block/#definition","text":"The code in a basic block has: One entry point , meaning no code within it is the destination of a jump instruction anywhere in the program. One exit point, meaning only the last instruction can cause the program to begin executing code in a different basic block. Under these circumstances, whenever the first instruction in a basic block is executed, the rest of the instructions are necessarily executed exactly once, in order.[ 4] [ 5] The code may be source code , assembly code or some other sequence of instructions. More formally, a sequence of instructions forms a basic block if: The instruction in each position dominates , or always executes before, all those in later positions. No other instruction executes between two instructions in the sequence. This definition is more general than the intuitive one in some ways. For example, it allows unconditional jumps to labels not targeted by other jumps. This definition embodies the properties that make basic blocks easy to work with when constructing an algorithm. The blocks to which control may transfer after reaching the end of a block are called that block's successors , while the blocks from which control may have come when entering a block are called that block's predecessors . The start of a basic block may be jumped to from more than one location.","title":"Definition"},{"location":"Optimization/Basic-block/#gatevidyalay#basic#blocks#and#flow#graphs#examples","text":"","title":"gatevidyalay Basic Blocks and Flow Graphs | Examples"},{"location":"Optimization/Basic-block/#basic#blocks","text":"Basic block is a set of statements that always executes in a sequence one after the other. The characteristics of basic blocks are They do not contain any kind of jump statements in them. There is no possibility of branching or getting halt in the middle. All the statements execute in the same order they appear. They do not lose lose the flow control of the program. NOTE: \u6709\u70b9\u539f\u5b50\u7684\u542b\u4e49","title":"Basic Blocks"},{"location":"Optimization/Basic-block/#example#of#basic#block","text":"Three Address Code for the expression a = b + c + d is- Here, All the statements execute in a sequence one after the other. Thus, they form a basic block.","title":"Example Of Basic Block"},{"location":"Optimization/Basic-block/#example#of#not#a#basic#block","text":"Three Address Code for the expression If A<B then 1 else 0 is- Here, The statements do not execute in a sequence one after the other. Thus, they do not form a basic block.","title":"Example Of Not A Basic Block"},{"location":"Optimization/Basic-block/#partitioning#intermediate#code#into#basic#blocks","text":"Any given code can be partitioned into basic blocks using the following rules","title":"Partitioning Intermediate Code Into Basic Blocks"},{"location":"Optimization/Basic-block/#rule-01#determining#leaders","text":"Following statements of the code are called as Leaders First statement of the code. Statement that is a target of the conditional or unconditional goto statement. Statement that appears immediately after a goto statement.","title":"Rule-01: Determining Leaders"},{"location":"Optimization/Basic-block/#rule-02#determining#basic#blocks","text":"All the statements that follow the leader (including the leader ) till the next leader appears form one basic block. The first statement of the code is called as the first leader . The block containing the first leader is called as Initial block .","title":"Rule-02: Determining Basic Blocks"},{"location":"Optimization/Basic-block/#flow#graphs","text":"A flow graph is a directed graph with flow control information added to the basic blocks. The basic blocks serve as nodes of the flow graph. There is a directed edge from block B1 to block B2 if B2 appears immediately after B1 in the code.","title":"Flow Graphs"},{"location":"Optimization/Basic-block/#practice#problems#based#on#basic#blocks#flow#graphs","text":"","title":"PRACTICE PROBLEMS BASED ON BASIC BLOCKS &amp; FLOW GRAPHS"},{"location":"Optimization/Basic-block/#problem-01","text":"Compute the basic blocks for the given three address statements (1) PROD = 0 (2) I = 1 (3) T2 = addr(A) \u2013 4 (4) T4 = addr(B) \u2013 4 (5) T1 = 4 x I (6) T3 = T2[T1] (7) T5 = T4[T1] (8) T6 = T3 x T5 (9) PROD = PROD + T6 (10) I = I + 1 (11) IF I <=20 GOTO (5)","title":"Problem-01:"},{"location":"Optimization/Basic-block/#solution","text":"We have- PROD = 0 is a leader since first statement of the code is a leader. T1 = 4 x I is a leader since target of the conditional goto statement is a leader. Now, the given code can be partitioned into two basic blocks as-","title":"Solution"},{"location":"Optimization/Basic-block/#problem-02","text":"Draw a flow graph for the three address statements given in problem-01.","title":"Problem-02:"},{"location":"Optimization/Basic-block/#solution_1","text":"Firstly, we compute the basic blocks (already done above). Secondly, we assign the flow control information. The required flow graph is-","title":"Solution"},{"location":"Optimization/Control-flow-analysis/","text":"Control flow analysis Control flow analysis","title":"Introduction"},{"location":"Optimization/Control-flow-analysis/#control#flow#analysis","text":"","title":"Control flow analysis"},{"location":"Optimization/Control-flow-analysis/Control-flow%26%26program-counter/","text":"\u5982\u4f55\u5b9e\u73b0\u63a7\u5236\u6d41 \u6240\u8c13\u7684 Control flow \u5176\u5b9e\u5c31\u662f program counter \u6211\u4eec\u5e38\u5e38\u542c\u5230Control flow\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Control flow \u5bf9\u5b83\u7684\u603b\u7ed3\u662f\u975e\u5e38\u5168\u9762\u7684\uff0c\u4ecehigh-level programming language\u7ea7\u522b\uff08\u5728high-level programming language\u4e2d\u6709control flow statement\uff0c\u6bd4\u5982return\u3001goto\u7b49\uff09\uff0c\u5230 machine language \u7ea7\u522b\uff08\u8fd9\u662f\u6700\u5e95\u5c42\u4e86\uff1b\u4ee5x86 \u4e3a\u4f8b\uff0c JMP \u6307\u4ee4\uff0c\u66f4\u591a\u53c2\u89c1 X86 Assembly/Control Flow \uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u7684\u662f machine language \u7ea7\u522b\uff0c\u6b63\u5982\u5176\u6240\u603b\u7ed3\u7684\uff1a At the level of machine language or assembly language , control flow instructions usually work by altering the program counter . For some central processing units (CPUs), the only control flow instructions available are conditional or unconditional branch instructions, also termed jumps. CPU\u7684 program counter \u9ed8\u8ba4\u884c\u4e3a\u662f\uff1a\u81ea\u52a01\u7684\uff0c\u6240\u4ee5\u7a0b\u5e8f\u9ed8\u8ba4\u662f\u987a\u5e8f\u6267\u884c\u5373\u53ef\uff08\u7f16\u8bd1\u5668\u7f16\u8bd1\u751f\u6210\u7684machine language program\u5176\u5b9e\u662f\u987a\u5e8f\u7684\uff09\uff0c\u901a\u8fc7control flow instruction\uff0c\u53ef\u7528\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u5404\u79cd\u6267\u884cflow\u3002 \u4e00\u4e2a\u4f8b\u5b50\u662f\u5728OS\u4e66\u76844.1. The Role of Interrupt Signals As the name suggests, interrupt signals provide a way to divert the processor to code outside the normal** flow of control**. When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter. \u6b63\u5728\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u770b\u5f85\u672c\u8d28\u4e0a\u76f8\u540c\u7684\u4e8b\u60c5\uff0c\u5728program language\u5c42\uff0c\u6211\u4eec\u628a\u5b83\u53eb\u505aflow of control\uff0c\u5728\u6307\u4ee4\u5c42\uff0c\u6211\u4eec\u5b83\u5176\u5b9e\u662fprogram counter\u3002","title":"Introduction"},{"location":"Optimization/Control-flow-analysis/Control-flow%26%26program-counter/#_1","text":"","title":"\u5982\u4f55\u5b9e\u73b0\u63a7\u5236\u6d41"},{"location":"Optimization/Control-flow-analysis/Control-flow%26%26program-counter/#control#flowprogram#counter","text":"\u6211\u4eec\u5e38\u5e38\u542c\u5230Control flow\uff0c\u7ef4\u57fa\u767e\u79d1\u7684 Control flow \u5bf9\u5b83\u7684\u603b\u7ed3\u662f\u975e\u5e38\u5168\u9762\u7684\uff0c\u4ecehigh-level programming language\u7ea7\u522b\uff08\u5728high-level programming language\u4e2d\u6709control flow statement\uff0c\u6bd4\u5982return\u3001goto\u7b49\uff09\uff0c\u5230 machine language \u7ea7\u522b\uff08\u8fd9\u662f\u6700\u5e95\u5c42\u4e86\uff1b\u4ee5x86 \u4e3a\u4f8b\uff0c JMP \u6307\u4ee4\uff0c\u66f4\u591a\u53c2\u89c1 X86 Assembly/Control Flow \uff09\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u91cd\u70b9\u5173\u6ce8\u7684\u662f machine language \u7ea7\u522b\uff0c\u6b63\u5982\u5176\u6240\u603b\u7ed3\u7684\uff1a At the level of machine language or assembly language , control flow instructions usually work by altering the program counter . For some central processing units (CPUs), the only control flow instructions available are conditional or unconditional branch instructions, also termed jumps. CPU\u7684 program counter \u9ed8\u8ba4\u884c\u4e3a\u662f\uff1a\u81ea\u52a01\u7684\uff0c\u6240\u4ee5\u7a0b\u5e8f\u9ed8\u8ba4\u662f\u987a\u5e8f\u6267\u884c\u5373\u53ef\uff08\u7f16\u8bd1\u5668\u7f16\u8bd1\u751f\u6210\u7684machine language program\u5176\u5b9e\u662f\u987a\u5e8f\u7684\uff09\uff0c\u901a\u8fc7control flow instruction\uff0c\u53ef\u7528\u6539\u53d8\u8fd9\u79cd\u9ed8\u8ba4\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u5404\u79cd\u6267\u884cflow\u3002 \u4e00\u4e2a\u4f8b\u5b50\u662f\u5728OS\u4e66\u76844.1. The Role of Interrupt Signals As the name suggests, interrupt signals provide a way to divert the processor to code outside the normal** flow of control**. When an interrupt signal arrives, the CPU must stop what it's currently doing and switch to a new activity; it does this by saving the current value of the program counter (i.e., the content of the eip and cs registers) in the Kernel Mode stack and by placing an address related to the interrupt type into the program counter. \u6b63\u5728\u4e0d\u540c\u7684\u5c42\u6b21\u6765\u770b\u5f85\u672c\u8d28\u4e0a\u76f8\u540c\u7684\u4e8b\u60c5\uff0c\u5728program language\u5c42\uff0c\u6211\u4eec\u628a\u5b83\u53eb\u505aflow of control\uff0c\u5728\u6307\u4ee4\u5c42\uff0c\u6211\u4eec\u5b83\u5176\u5b9e\u662fprogram counter\u3002","title":"\u6240\u8c13\u7684Control flow\u5176\u5b9e\u5c31\u662fprogram counter"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/","text":"Control-flow graph Definition Reachability Domination relationship Special edges Loop management Reducibility Loop connectedness Control-flow graph In computer science , a control-flow graph ( CFG ) is a representation , using graph notation, of all paths that might be traversed through a program during its execution . The control-flow graph is due to Frances E. Allen ,[ 1] who notes that Reese T. Prosser used boolean connectivity matrices for flow analysis before.[ 2] The CFG is essential to many compiler optimizations and static-analysis tools. Definition In a control-flow graph each node in the graph represents a basic block , i.e. a straight-line piece of code without any jumps or jump targets ; jump targets start a block, and jumps end a block. Directed edges are used to represent jumps in the control flow . There are, in most presentations, two specially designated blocks: the entry block , through which control enters into the flow graph, and the exit block , through which all control flow leaves.[ 3] NOTE: \u8868\u793ajump\u7684\u5173\u952e\u5b57\uff1a break goto Because of its construction procedure, in a CFG, every edge A\u2192B has the property that: outdegree (A) > 1 or indegree(B) > 1 (or both). The CFG can thus be obtained, at least conceptually, by starting from the program's (full) flow graph\u2014i.e. the graph in which every node represents an individual instruction\u2014and performing an edge contraction for every edge that falsifies the predicate above, i.e. contracting every edge whose source has a single exit and whose destination has a single entry. This contraction-based algorithm is of no practical importance, except as a visualization aid for understanding the CFG construction, because the CFG can be more efficiently constructed directly from the program by scanning it for basic blocks . Reachability Main article: Reachability Reachability is a graph property useful in optimization. If a subgraph is not connected from the subgraph containing the entry block, that subgraph is unreachable during any execution, and so is unreachable code ; under normal conditions it can be safely removed. If the exit block is unreachable from the entry block, an infinite loop may exist. Not all infinite loops are detectable, see Halting problem . A halting order may also exist there. Unreachable code and infinite loops are possible even if the programmer does not explicitly code them: optimizations like constant propagation and constant folding followed by jump threading can collapse multiple basic blocks into one, cause edges to be removed from a CFG, etc., thus possibly disconnecting parts of the graph. Domination relationship Main article: Dominator (graph theory) A block M dominates a block N if every path from the entry that reaches block N has to pass through block M. The entry block dominates all blocks. In the reverse direction, block M postdominates block N if every path from N to the exit has to pass through block M. The exit block postdominates all blocks. It is said that a block M immediately dominates block N if M dominates N, and there is no intervening block P such that M dominates P and P dominates N. In other words, M is the last dominator on all paths from entry to N. Each block has a unique immediate dominator. Similarly, there is a notion of immediate postdominator , analogous to immediate dominator . The dominator tree is an ancillary data structure depicting the dominator relationships. There is an arc from Block M to Block N if M is an immediate dominator of N. This graph is a tree, since each block has a unique immediate dominator. This tree is rooted at the entry block. The dominator tree can be calculated efficiently using Lengauer\u2013Tarjan's algorithm . A postdominator tree is analogous to the dominator tree . This tree is rooted at the exit block. Special edges A back edge is an edge that points to a block that has already been met during a depth-first ( DFS ) traversal of the graph. Back edges are typical of loops. A critical edge is an edge which is neither the only edge leaving its source block, nor the only edge entering its destination block. These edges must be split : a new block must be created in the middle of the edge, in order to insert computations on the edge without affecting any other edges. An abnormal edge is an edge whose destination is unknown. Exception handling constructs can produce them. These edges tend to inhibit optimization. An impossible edge (also known as a fake edge ) is an edge which has been added to the graph solely to preserve the property that the exit block postdominates all blocks. It cannot ever be traversed. Loop management A loop header (sometimes called the entry point of the loop) is a dominator that is the target of a loop-forming back edge. The loop header dominates all blocks in the loop body. A block may be a loop header for more than one loop. A loop may have multiple entry points, in which case it has no \"loop header\". Suppose block M is a dominator with several incoming edges, some of them being back edges (so M is a loop header). It is advantageous to several optimization passes to break M up into two blocks Mpre and Mloop. The contents of M and back edges are moved to Mloop, the rest of the edges are moved to point into Mpre, and a new edge from Mpre to Mloop is inserted (so that Mpre is the immediate dominator of Mloop). In the beginning, Mpre would be empty, but passes like loop-invariant code motion could populate it. Mpre is called the loop pre-header , and Mloop would be the loop header. Reducibility A reducible CFG is one with edges that can be partitioned into two disjoint sets: forward edges, and back edges, such that:[ 5] Forward edges form a directed acyclic graph with all nodes reachable from the entry node. For all back edges (A, B), node B dominates node A. Structured programming languages are often designed such that all CFGs they produce are reducible, and common structured programming statements such as IF, FOR, WHILE, BREAK, and CONTINUE produce reducible graphs. To produce irreducible graphs, statements such as GOTO are needed. Irreducible graphs may also be produced by some compiler optimizations. Loop connectedness The loop connectedness of a CFG is defined with respect to a given depth-first search tree (DFST) of the CFG. This DFST should be rooted at the start node and cover every node of the CFG. Edges in the CFG which run from a node to one of its DFST ancestors (including itself) are called back edges. The loop connectedness is the largest number of back edges found in any cycle-free path of the CFG. In a reducible CFG, the loop connectedness is independent of the DFST chosen.[ 6] [ 7] Loop connectedness has been used to reason about the time complexity of data-flow analysis .[ 6]","title":"Introduction"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#control-flow#graph","text":"In computer science , a control-flow graph ( CFG ) is a representation , using graph notation, of all paths that might be traversed through a program during its execution . The control-flow graph is due to Frances E. Allen ,[ 1] who notes that Reese T. Prosser used boolean connectivity matrices for flow analysis before.[ 2] The CFG is essential to many compiler optimizations and static-analysis tools.","title":"Control-flow graph"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#definition","text":"In a control-flow graph each node in the graph represents a basic block , i.e. a straight-line piece of code without any jumps or jump targets ; jump targets start a block, and jumps end a block. Directed edges are used to represent jumps in the control flow . There are, in most presentations, two specially designated blocks: the entry block , through which control enters into the flow graph, and the exit block , through which all control flow leaves.[ 3] NOTE: \u8868\u793ajump\u7684\u5173\u952e\u5b57\uff1a break goto Because of its construction procedure, in a CFG, every edge A\u2192B has the property that: outdegree (A) > 1 or indegree(B) > 1 (or both). The CFG can thus be obtained, at least conceptually, by starting from the program's (full) flow graph\u2014i.e. the graph in which every node represents an individual instruction\u2014and performing an edge contraction for every edge that falsifies the predicate above, i.e. contracting every edge whose source has a single exit and whose destination has a single entry. This contraction-based algorithm is of no practical importance, except as a visualization aid for understanding the CFG construction, because the CFG can be more efficiently constructed directly from the program by scanning it for basic blocks .","title":"Definition"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#reachability","text":"Main article: Reachability Reachability is a graph property useful in optimization. If a subgraph is not connected from the subgraph containing the entry block, that subgraph is unreachable during any execution, and so is unreachable code ; under normal conditions it can be safely removed. If the exit block is unreachable from the entry block, an infinite loop may exist. Not all infinite loops are detectable, see Halting problem . A halting order may also exist there. Unreachable code and infinite loops are possible even if the programmer does not explicitly code them: optimizations like constant propagation and constant folding followed by jump threading can collapse multiple basic blocks into one, cause edges to be removed from a CFG, etc., thus possibly disconnecting parts of the graph.","title":"Reachability"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#domination#relationship","text":"Main article: Dominator (graph theory) A block M dominates a block N if every path from the entry that reaches block N has to pass through block M. The entry block dominates all blocks. In the reverse direction, block M postdominates block N if every path from N to the exit has to pass through block M. The exit block postdominates all blocks. It is said that a block M immediately dominates block N if M dominates N, and there is no intervening block P such that M dominates P and P dominates N. In other words, M is the last dominator on all paths from entry to N. Each block has a unique immediate dominator. Similarly, there is a notion of immediate postdominator , analogous to immediate dominator . The dominator tree is an ancillary data structure depicting the dominator relationships. There is an arc from Block M to Block N if M is an immediate dominator of N. This graph is a tree, since each block has a unique immediate dominator. This tree is rooted at the entry block. The dominator tree can be calculated efficiently using Lengauer\u2013Tarjan's algorithm . A postdominator tree is analogous to the dominator tree . This tree is rooted at the exit block.","title":"Domination relationship"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#special#edges","text":"A back edge is an edge that points to a block that has already been met during a depth-first ( DFS ) traversal of the graph. Back edges are typical of loops. A critical edge is an edge which is neither the only edge leaving its source block, nor the only edge entering its destination block. These edges must be split : a new block must be created in the middle of the edge, in order to insert computations on the edge without affecting any other edges. An abnormal edge is an edge whose destination is unknown. Exception handling constructs can produce them. These edges tend to inhibit optimization. An impossible edge (also known as a fake edge ) is an edge which has been added to the graph solely to preserve the property that the exit block postdominates all blocks. It cannot ever be traversed.","title":"Special edges"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#loop#management","text":"A loop header (sometimes called the entry point of the loop) is a dominator that is the target of a loop-forming back edge. The loop header dominates all blocks in the loop body. A block may be a loop header for more than one loop. A loop may have multiple entry points, in which case it has no \"loop header\". Suppose block M is a dominator with several incoming edges, some of them being back edges (so M is a loop header). It is advantageous to several optimization passes to break M up into two blocks Mpre and Mloop. The contents of M and back edges are moved to Mloop, the rest of the edges are moved to point into Mpre, and a new edge from Mpre to Mloop is inserted (so that Mpre is the immediate dominator of Mloop). In the beginning, Mpre would be empty, but passes like loop-invariant code motion could populate it. Mpre is called the loop pre-header , and Mloop would be the loop header.","title":"Loop management"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#reducibility","text":"A reducible CFG is one with edges that can be partitioned into two disjoint sets: forward edges, and back edges, such that:[ 5] Forward edges form a directed acyclic graph with all nodes reachable from the entry node. For all back edges (A, B), node B dominates node A. Structured programming languages are often designed such that all CFGs they produce are reducible, and common structured programming statements such as IF, FOR, WHILE, BREAK, and CONTINUE produce reducible graphs. To produce irreducible graphs, statements such as GOTO are needed. Irreducible graphs may also be produced by some compiler optimizations.","title":"Reducibility"},{"location":"Optimization/Control-flow-analysis/Control-flow-graph/#loop#connectedness","text":"The loop connectedness of a CFG is defined with respect to a given depth-first search tree (DFST) of the CFG. This DFST should be rooted at the start node and cover every node of the CFG. Edges in the CFG which run from a node to one of its DFST ancestors (including itself) are called back edges. The loop connectedness is the largest number of back edges found in any cycle-free path of the CFG. In a reducible CFG, the loop connectedness is independent of the DFST chosen.[ 6] [ 7] Loop connectedness has been used to reason about the time complexity of data-flow analysis .[ 6]","title":"Loop connectedness"},{"location":"Optimization/Data-flow-analysis/","text":"Data-flow analysis","title":"Introduction"},{"location":"Optimization/Data-flow-analysis/#data-flow#analysis","text":"","title":"Data-flow analysis"},{"location":"Optimization/Inline/","text":"Inline inline\u662fC-family language compiler\u4e2d\u5e38\u7528\u7684\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u6240\u4ee5\u5c06\u5b83\u653e\u5230\u4e86\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Compile \u7ae0\u8282\u4e2d\u3002","title":"Introduction"},{"location":"Optimization/Inline/#inline","text":"inline\u662fC-family language compiler\u4e2d\u5e38\u7528\u7684\u4e00\u79cd\u4f18\u5316\u65b9\u6cd5\uff0c\u6240\u4ee5\u5c06\u5b83\u653e\u5230\u4e86\u5de5\u7a0bprogramming-language\u7684 C-family-language\\C-and-C++\\From-source-code-to-exec\\Compile \u7ae0\u8282\u4e2d\u3002","title":"Inline"},{"location":"Optimization/Static-single-assignment-form/","text":"Static single assignment form \u6839\u636e wikipedia valgrind \u4e2d\u7684\u4ecb\u7ecd\uff0cvalgrind \u7684 IR \u91c7\u7528\u7684\u5c31\u662f SSA -based form\u3002 wikipedia Static single assignment form In compiler design, static single assignment form (often abbreviated as SSA form or simply SSA ) is a property of an intermediate representation (IR), which requires that each variable be assigned exactly once, and every variable be defined before it is used. Existing variables in the original IR are split into versions , new variables typically indicated by the original name with a subscript in textbooks, so that every definition gets its own version. In SSA form, use-def chains are explicit and each contains a single element.","title":"Introduction"},{"location":"Optimization/Static-single-assignment-form/#static#single#assignment#form","text":"\u6839\u636e wikipedia valgrind \u4e2d\u7684\u4ecb\u7ecd\uff0cvalgrind \u7684 IR \u91c7\u7528\u7684\u5c31\u662f SSA -based form\u3002","title":"Static single assignment form"},{"location":"Optimization/Static-single-assignment-form/#wikipedia#static#single#assignment#form","text":"In compiler design, static single assignment form (often abbreviated as SSA form or simply SSA ) is a property of an intermediate representation (IR), which requires that each variable be assigned exactly once, and every variable be defined before it is used. Existing variables in the original IR are split into versions , new variables typically indicated by the original name with a subscript in textbooks, so that every definition gets its own version. In SSA form, use-def chains are explicit and each contains a single element.","title":"wikipedia Static single assignment form"},{"location":"TODO/TODO/","text":"TODO 20191231 symbol table and AST \u5728 Abstract syntax tree \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a The compiler also generates symbol tables based on the AST during semantic analysis. \u6211\u7684\u7591\u60d1\u662f\uff1asymbol table\u5230\u5e95\u662f\u7531\u8c01\u521b\u5efa\u7684\uff1f\u8bb0\u5f97\u4e0a\u5468\u672b\u7684\u65f6\u5019\uff0c\u8fd8\u5728\u770bpython\u7684symbol table\u7684\u5b9e\u73b0\u3002\u6240\u4ee5\uff0c\u6709\u5fc5\u8981\u5bf9symbol table\u505a\u4e00\u6b21\u4e13\u9898\u7684\u5206\u6790\u3002","title":"TODO"},{"location":"TODO/TODO/#todo","text":"","title":"TODO"},{"location":"TODO/TODO/#20191231","text":"","title":"20191231"},{"location":"TODO/TODO/#symbol#table#and#ast","text":"\u5728 Abstract syntax tree \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a The compiler also generates symbol tables based on the AST during semantic analysis. \u6211\u7684\u7591\u60d1\u662f\uff1asymbol table\u5230\u5e95\u662f\u7531\u8c01\u521b\u5efa\u7684\uff1f\u8bb0\u5f97\u4e0a\u5468\u672b\u7684\u65f6\u5019\uff0c\u8fd8\u5728\u770bpython\u7684symbol table\u7684\u5b9e\u73b0\u3002\u6240\u4ee5\uff0c\u6709\u5fc5\u8981\u5bf9symbol table\u505a\u4e00\u6b21\u4e13\u9898\u7684\u5206\u6790\u3002","title":"symbol table and AST"},{"location":"TODO/wikipedia-Alias-analysis/","text":"wikipedia Alias analysis Alias analysis is a technique in compiler theory , used to determine if a storage location may be accessed in more than one way. Two pointers are said to be aliased if they point to the same location. Alias analysis techniques are usually classified by flow-sensitivity and context-sensitivity. They may determine may-alias or must-alias information. The term alias analysis is often used interchangeably with points-to analysis , a specific case. Alias analysers intend to make and compute useful information for understanding aliasing in programs. Overview In general, alias analysis determines whether or not separate memory references point to the same area of memory. This allows the compiler to determine what variables in the program will be affected by a statement. For example, consider the following section of code that accesses members of structures: p . foo = 1 ; q . foo = 2 ; i = p . foo + 3 ; There are three possible alias cases here: The variables p and q cannot alias (i.e., they never point to the same memory location). The variables p and q must alias (i.e., they always point to the same memory location). It cannot be conclusively determined at compile time if p and q alias or not. If p and q cannot alias, then i = p.foo + 3; can be changed to i = 4 . If p and q must alias, then i = p.foo + 3; can be changed to i = 5 because p.foo + 3 = q.foo + 3 . In both cases, we are able to perform optimizations from the alias knowledge (assuming that no other thread updating the same locations can interleave with the current thread, or that the language memory model permits those updates to be not immediately visible to the current thread in absence of explicit synchronization constructs ). On the other hand, if it is not known if p and q alias or not, then no optimizations can be performed and the whole of the code must be executed to get the result. Two memory references are said to have a may-alias relation if their aliasing is unknown. Performing alias analysis In alias analysis , we divide the program's memory into alias classes . Alias classes are disjoint sets of locations that cannot alias to one another. For the discussion here, it is assumed that the optimizations done here occur on a low-level intermediate representation of the program. This is to say that the program has been compiled into binary operations, jumps, moves between registers, moves from registers to memory, moves from memory to registers, branches, and function calls/returns.","title":"wikipedia [Alias analysis](https://en.wikipedia.org/wiki/Alias_analysis)"},{"location":"TODO/wikipedia-Alias-analysis/#wikipedia#alias#analysis","text":"Alias analysis is a technique in compiler theory , used to determine if a storage location may be accessed in more than one way. Two pointers are said to be aliased if they point to the same location. Alias analysis techniques are usually classified by flow-sensitivity and context-sensitivity. They may determine may-alias or must-alias information. The term alias analysis is often used interchangeably with points-to analysis , a specific case. Alias analysers intend to make and compute useful information for understanding aliasing in programs.","title":"wikipedia Alias analysis"},{"location":"TODO/wikipedia-Alias-analysis/#overview","text":"In general, alias analysis determines whether or not separate memory references point to the same area of memory. This allows the compiler to determine what variables in the program will be affected by a statement. For example, consider the following section of code that accesses members of structures: p . foo = 1 ; q . foo = 2 ; i = p . foo + 3 ; There are three possible alias cases here: The variables p and q cannot alias (i.e., they never point to the same memory location). The variables p and q must alias (i.e., they always point to the same memory location). It cannot be conclusively determined at compile time if p and q alias or not. If p and q cannot alias, then i = p.foo + 3; can be changed to i = 4 . If p and q must alias, then i = p.foo + 3; can be changed to i = 5 because p.foo + 3 = q.foo + 3 . In both cases, we are able to perform optimizations from the alias knowledge (assuming that no other thread updating the same locations can interleave with the current thread, or that the language memory model permits those updates to be not immediately visible to the current thread in absence of explicit synchronization constructs ). On the other hand, if it is not known if p and q alias or not, then no optimizations can be performed and the whole of the code must be executed to get the result. Two memory references are said to have a may-alias relation if their aliasing is unknown.","title":"Overview"},{"location":"TODO/wikipedia-Alias-analysis/#performing#alias#analysis","text":"In alias analysis , we divide the program's memory into alias classes . Alias classes are disjoint sets of locations that cannot alias to one another. For the discussion here, it is assumed that the optimizations done here occur on a low-level intermediate representation of the program. This is to say that the program has been compiled into binary operations, jumps, moves between registers, moves from registers to memory, moves from memory to registers, branches, and function calls/returns.","title":"Performing alias analysis"},{"location":"Tools/Parser-generator/","text":"Parser generator \u5728\u9f99\u4e66\u76844.9 Parser Generators\u4e2d\uff0c\u4ecb\u7ecd\u4e86Yacc\uff0c\u672c\u6587\u5c06\u5bf9\u4e00\u4e9b\u5e38\u89c1\u7684parser generator\u8fdb\u884c\u603b\u7ed3\u3002 Lex wikipedia Lex (software) Yacc wikipedia Yacc Bison wikipedia GNU Bison ANTLR ANTLR The Lemon LALR(1) Parser Generator sqlite The Lemon LALR(1) Parser Generator wikipedia Comparison of parser generators This is a list of notable lexer generators and parser generators for various language classes.","title":"Introduction"},{"location":"Tools/Parser-generator/#parser#generator","text":"\u5728\u9f99\u4e66\u76844.9 Parser Generators\u4e2d\uff0c\u4ecb\u7ecd\u4e86Yacc\uff0c\u672c\u6587\u5c06\u5bf9\u4e00\u4e9b\u5e38\u89c1\u7684parser generator\u8fdb\u884c\u603b\u7ed3\u3002","title":"Parser generator"},{"location":"Tools/Parser-generator/#lex","text":"wikipedia Lex (software)","title":"Lex"},{"location":"Tools/Parser-generator/#yacc","text":"wikipedia Yacc","title":"Yacc"},{"location":"Tools/Parser-generator/#bison","text":"wikipedia GNU Bison","title":"Bison"},{"location":"Tools/Parser-generator/#antlr","text":"ANTLR","title":"ANTLR"},{"location":"Tools/Parser-generator/#the#lemon#lalr1#parser#generator","text":"sqlite The Lemon LALR(1) Parser Generator","title":"The Lemon LALR(1) Parser Generator"},{"location":"Tools/Parser-generator/#wikipedia#comparison#of#parser#generators","text":"This is a list of notable lexer generators and parser generators for various language classes.","title":"wikipedia Comparison of parser generators"},{"location":"Tools/Parser-generator/Lex%26yacc/","text":"Lex & Yacc compilertools The Lex & Yacc Page wikipedia Lex (software) Lex is a computer program that generates lexical analyzers (\"scanners\" or \"lexers\").[ 1] [ 2] Lex is commonly used with the yacc parser generator . Lex, originally written by Mike Lesk and Eric Schmidt [ 3] and described in 1975,[ 4] [ 5] is the standard lexical analyzer generator on many Unix systems, and an equivalent tool is specified as part of the POSIX standard.[ citation needed ] Lex reads an input stream specifying the lexical analyzer and outputs source code implementing the lexer in the C programming language . In addition to C, some old versions of Lex could also generate a lexer in Ratfor .[ 6] Open source Though originally distributed as proprietary software, some versions of Lex are now open source . Open source versions of Lex, based on the original AT&T code are now distributed as a part of open source operating systems such as OpenSolaris and Plan 9 from Bell Labs .[ clarification needed ] One popular open source version of Lex, called flex , or the \"fast lexical analyzer\", is not derived from proprietary coding. Structure of a Lex file The structure of a Lex file is intentionally similar to that of a yacc file; files are divided into three sections, separated by lines that contain only two percent signs, as follows 1\u3001The definition section defines macros and imports header files written in C . It is also possible to write any C code here, which will be copied verbatim into the generated source file. 2\u3001The rules section associates regular expression patterns with C statements . When the lexer sees text in the input matching a given pattern, it will execute the associated C code. 3\u3001The C code section contains C statements and functions that are copied verbatim to the generated source file. These statements presumably contain code called by the rules in the rules section. In large programs it is more convenient to place this code in a separate file linked in at compile time.","title":"Introduction"},{"location":"Tools/Parser-generator/Lex%26yacc/#lex#yacc","text":"","title":"Lex &amp; Yacc"},{"location":"Tools/Parser-generator/Lex%26yacc/#compilertools#the#lex#yacc#page","text":"","title":"compilertools The Lex &amp; Yacc Page"},{"location":"Tools/Parser-generator/Lex%26yacc/#wikipedia#lex#software","text":"Lex is a computer program that generates lexical analyzers (\"scanners\" or \"lexers\").[ 1] [ 2] Lex is commonly used with the yacc parser generator . Lex, originally written by Mike Lesk and Eric Schmidt [ 3] and described in 1975,[ 4] [ 5] is the standard lexical analyzer generator on many Unix systems, and an equivalent tool is specified as part of the POSIX standard.[ citation needed ] Lex reads an input stream specifying the lexical analyzer and outputs source code implementing the lexer in the C programming language . In addition to C, some old versions of Lex could also generate a lexer in Ratfor .[ 6]","title":"wikipedia Lex (software)"},{"location":"Tools/Parser-generator/Lex%26yacc/#open#source","text":"Though originally distributed as proprietary software, some versions of Lex are now open source . Open source versions of Lex, based on the original AT&T code are now distributed as a part of open source operating systems such as OpenSolaris and Plan 9 from Bell Labs .[ clarification needed ] One popular open source version of Lex, called flex , or the \"fast lexical analyzer\", is not derived from proprietary coding.","title":"Open source"},{"location":"Tools/Parser-generator/Lex%26yacc/#structure#of#a#lex#file","text":"The structure of a Lex file is intentionally similar to that of a yacc file; files are divided into three sections, separated by lines that contain only two percent signs, as follows 1\u3001The definition section defines macros and imports header files written in C . It is also possible to write any C code here, which will be copied verbatim into the generated source file. 2\u3001The rules section associates regular expression patterns with C statements . When the lexer sees text in the input matching a given pattern, it will execute the associated C code. 3\u3001The C code section contains C statements and functions that are copied verbatim to the generated source file. These statements presumably contain code called by the rules in the rules section. In large programs it is more convenient to place this code in a separate file linked in at compile time.","title":"Structure of a Lex file"}]}