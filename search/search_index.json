{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"compiler-principle # Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) is really an amazing book, as mentioned in Chapter 1 Introduction This book is is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages , machine architecture , language theory , algorithms , and software engineering . As mentioned in the paragraph above, the study of compiler is an interdiscipline involving computer science ( programming languages , machine architecture , algorithms , and software engineering ) and language theory , so the book may be hard to understand if lacking experience in programming, grasp of common data structure(especially tree which is mainly used in the front end, graph which is mainly used in the back end) and algorithms(especially recursion ). The language theory in the paragraph above mainly refers to the formal language theory , I have create a repository Automata-and-formal-language to record my knowledge of formal language theory and automata theory . A basic understanding of these theories will help understand the book. The data structure mentioned in the book can be found in my repository data-structure . The algorithm mentioned in the book can be found in my repository algorithm . Beside the book, this repository also contain some material supplemented to help understand.","title":"Home"},{"location":"#compiler-principle","text":"Compilers Principles, Techniques and Tools Second Edition(aka dragon book ) is really an amazing book, as mentioned in Chapter 1 Introduction This book is is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages , machine architecture , language theory , algorithms , and software engineering . As mentioned in the paragraph above, the study of compiler is an interdiscipline involving computer science ( programming languages , machine architecture , algorithms , and software engineering ) and language theory , so the book may be hard to understand if lacking experience in programming, grasp of common data structure(especially tree which is mainly used in the front end, graph which is mainly used in the back end) and algorithms(especially recursion ). The language theory in the paragraph above mainly refers to the formal language theory , I have create a repository Automata-and-formal-language to record my knowledge of formal language theory and automata theory . A basic understanding of these theories will help understand the book. The data structure mentioned in the book can be found in my repository data-structure . The algorithm mentioned in the book can be found in my repository algorithm . Beside the book, this repository also contain some material supplemented to help understand.","title":"compiler-principle"},{"location":"TODO/","text":"TODO # 20191231 # symbol table and AST # \u5728 Abstract syntax tree \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a The compiler also generates symbol tables based on the AST during semantic analysis. \u6211\u7684\u7591\u60d1\u662f\uff1asymbol table\u5230\u5e95\u662f\u7531\u8c01\u521b\u5efa\u7684\uff1f\u8bb0\u5f97\u4e0a\u5468\u672b\u7684\u65f6\u5019\uff0c\u8fd8\u5728\u770bpython\u7684symbol table\u7684\u5b9e\u73b0\u3002\u6240\u4ee5\uff0c\u6709\u5fc5\u8981\u5bf9symbol table\u505a\u4e00\u6b21\u4e13\u9898\u7684\u5206\u6790\u3002","title":"TODO"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#20191231","text":"","title":"20191231"},{"location":"TODO/#symbol-table-and-ast","text":"\u5728 Abstract syntax tree \u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a The compiler also generates symbol tables based on the AST during semantic analysis. \u6211\u7684\u7591\u60d1\u662f\uff1asymbol table\u5230\u5e95\u662f\u7531\u8c01\u521b\u5efa\u7684\uff1f\u8bb0\u5f97\u4e0a\u5468\u672b\u7684\u65f6\u5019\uff0c\u8fd8\u5728\u770bpython\u7684symbol table\u7684\u5b9e\u73b0\u3002\u6240\u4ee5\uff0c\u6709\u5fc5\u8981\u5bf9symbol table\u505a\u4e00\u6b21\u4e13\u9898\u7684\u5206\u6790\u3002","title":"symbol table and AST"},{"location":"Chapter-1-Introduction/","text":"Chapter 1 Introduction # How long have you been programming? Have you ever wondered what a programming language is? There are many ways to answer this question, here I will try to answer this question from some different perspectives. From formal language # A programming language is a formal language , so it is equipped with alphabet words and lexical grammar to defining the syntax of tokens . Syntax and formal grammar to describe its syntax semantics From the perspectives of a compiler # How dose compiler understand what the program mean? The table is a summary of the implementation of compiler. language phase grammar technique generator alphabet Lexical grammar Lexical analysis regular expressions Finite-state machine Scanner generators, for example Lex (software) Syntax (programming languages) syntax analysis Context-free grammar parse tree , abstract syntax tree Parser generators, for example Yacc Semantics (computer science) Semantic analysis (compilers) Lexical grammar , syntax and semantics define a programming language,.Only when specifying lexical grammar , syntax and semantics can the compiler understand what programmer have written and translate the language correctly. This book is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages, machine architecture, language theory, algorithms, and software engineering.","title":"1-Introduction"},{"location":"Chapter-1-Introduction/#chapter-1-introduction","text":"How long have you been programming? Have you ever wondered what a programming language is? There are many ways to answer this question, here I will try to answer this question from some different perspectives.","title":"Chapter 1 Introduction"},{"location":"Chapter-1-Introduction/#from-formal-language","text":"A programming language is a formal language , so it is equipped with alphabet words and lexical grammar to defining the syntax of tokens . Syntax and formal grammar to describe its syntax semantics","title":"From  formal language"},{"location":"Chapter-1-Introduction/#from-the-perspectives-of-a-compiler","text":"How dose compiler understand what the program mean? The table is a summary of the implementation of compiler. language phase grammar technique generator alphabet Lexical grammar Lexical analysis regular expressions Finite-state machine Scanner generators, for example Lex (software) Syntax (programming languages) syntax analysis Context-free grammar parse tree , abstract syntax tree Parser generators, for example Yacc Semantics (computer science) Semantic analysis (compilers) Lexical grammar , syntax and semantics define a programming language,.Only when specifying lexical grammar , syntax and semantics can the compiler understand what programmer have written and translate the language correctly. This book is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages, machine architecture, language theory, algorithms, and software engineering.","title":"From the perspectives of a compiler"},{"location":"Chapter-1-Introduction/1.1-Language-Processors/","text":"1.1 Language Processors # Simply stated, a compiler is a program that can read a program in one language--the source language--and translate it into an equivalent program in another language--the target language; see Fig. 1.1. An important role of the compiler is to report any errors in the source program that it detects during the translation process. See also: Compiler If the target program is an executable machine-language program , it can then be called by the user to process inputs and produce outputs; see Fig. 1.2. See also: Machine code Executable NOTE: The compiler definition given here is very broad because target programs can take many forms, not just executable machine-language programs . As stated in article Is Python interpreted or compiled? Yes. : Compiling is a more general idea: take a program in one language (or form), and convert it into another language or form. Usually the source form is a higher-level language than the destination form, such as when converting from C to machine code. But converting from JavaScript 8 to JavaScript 5 is also a kind of compiling. In Python, the source is compiled into a much simpler form called bytecode . An interpreter is another common kind of language processor. Instead of producing a target program as a translation, an interpreter appears to directly execute the operations specified in the source program on inputs supplied by the user, as shown in Fig. 1.3. The machine-language target program produced by a compiler is usually much faster than an interpreter at mapping inputs to outputs . An interpreter, however, can usually give better error diagnostics than a compiler, because it executes the source program statement by statement. NOTE: Executing the source program statement by statement is the feature of interpreter and it is impossible for advanced programming language to be so. Typical interpreter is shell: bash redis server's execution of command committed by the client can also be seen as an interpreter just as shell Java language processors combine compilation and interpretation , as shown in Fig. 1.4. A Java source program may first be compiled into an intermediate form called bytecode . The bytecodes are then interpreted by a virtual machine . A benefit of this arrangement is that bytecodes compiled on one machine can be interpreted on another machine, perhaps across a network. See also: Virtual machine In order to achieve faster processing of inputs to outputs, some Java compilers, called just-in-time compilers, translate the bytecodes into machine language immediately before they run the intermediate program to process the input. See also: Just-in-time compilation NOTE: language processor\uff1a compiler, such as c, c++ interpreter, such as shell script hybrid compiler, such as python and java NOTE: Python is similar to Java in combining compilation and interpretation , but there are difference between the two language. The following is an good article explaining python implementation: Is Python interpreted or compiled? Yes. This post is very informative and clear and can help understand the content in this chapter. In addition to a compiler, several other programs may be required to create an executable target program, as shown in Fig. 1.5. preprocessor assembler linker loader NOTE: This book focus only on compiler and the others is not included. NOTE: \u4ee5\u4e0b\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff0c\u5728\u4e00\u4e9b\u6587\u7ae0\u4e2d\u53ef\u80fd\u4f1a\u63d0\u5230\uff1a Dynamic compilation Dynamic vs Static Compiler (JavaScript) Why Static Compilation?","title":"1.1-Language-Processors.md"},{"location":"Chapter-1-Introduction/1.1-Language-Processors/#11-language-processors","text":"Simply stated, a compiler is a program that can read a program in one language--the source language--and translate it into an equivalent program in another language--the target language; see Fig. 1.1. An important role of the compiler is to report any errors in the source program that it detects during the translation process. See also: Compiler If the target program is an executable machine-language program , it can then be called by the user to process inputs and produce outputs; see Fig. 1.2. See also: Machine code Executable NOTE: The compiler definition given here is very broad because target programs can take many forms, not just executable machine-language programs . As stated in article Is Python interpreted or compiled? Yes. : Compiling is a more general idea: take a program in one language (or form), and convert it into another language or form. Usually the source form is a higher-level language than the destination form, such as when converting from C to machine code. But converting from JavaScript 8 to JavaScript 5 is also a kind of compiling. In Python, the source is compiled into a much simpler form called bytecode . An interpreter is another common kind of language processor. Instead of producing a target program as a translation, an interpreter appears to directly execute the operations specified in the source program on inputs supplied by the user, as shown in Fig. 1.3. The machine-language target program produced by a compiler is usually much faster than an interpreter at mapping inputs to outputs . An interpreter, however, can usually give better error diagnostics than a compiler, because it executes the source program statement by statement. NOTE: Executing the source program statement by statement is the feature of interpreter and it is impossible for advanced programming language to be so. Typical interpreter is shell: bash redis server's execution of command committed by the client can also be seen as an interpreter just as shell Java language processors combine compilation and interpretation , as shown in Fig. 1.4. A Java source program may first be compiled into an intermediate form called bytecode . The bytecodes are then interpreted by a virtual machine . A benefit of this arrangement is that bytecodes compiled on one machine can be interpreted on another machine, perhaps across a network. See also: Virtual machine In order to achieve faster processing of inputs to outputs, some Java compilers, called just-in-time compilers, translate the bytecodes into machine language immediately before they run the intermediate program to process the input. See also: Just-in-time compilation NOTE: language processor\uff1a compiler, such as c, c++ interpreter, such as shell script hybrid compiler, such as python and java NOTE: Python is similar to Java in combining compilation and interpretation , but there are difference between the two language. The following is an good article explaining python implementation: Is Python interpreted or compiled? Yes. This post is very informative and clear and can help understand the content in this chapter. In addition to a compiler, several other programs may be required to create an executable target program, as shown in Fig. 1.5. preprocessor assembler linker loader NOTE: This book focus only on compiler and the others is not included. NOTE: \u4ee5\u4e0b\u662f\u4e00\u4e9b\u8865\u5145\u5185\u5bb9\uff0c\u5728\u4e00\u4e9b\u6587\u7ae0\u4e2d\u53ef\u80fd\u4f1a\u63d0\u5230\uff1a Dynamic compilation Dynamic vs Static Compiler (JavaScript) Why Static Compilation?","title":"1.1 Language Processors"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/","text":"1.2 The Structure of a Compiler 1.2.1 Lexical Analysis 1.2.2 Syntax Analysis 1.2.3 Semantic Analysis 1.2.4 Intermediate Code Generation 1.2.5 Code Optimization 1.2.6 Code Generation 1.2.7 Symbol-Table Management 1.2.8 The Grouping of Phases into Passes 1.2.9 Compiler-Construction Tools 1.2 The Structure of a Compiler # Up to this point we have treated a compiler as a single box that maps a source program into a semantically equivalent target program. If we open up this box a little, we see that there are two parts to this mapping: analysis synthesis The analysis part breaks up the source program into constituent pieces and imposes a grammatical structure on them. It then uses this structure to create an intermediate representation of the source program. If the analysis part detects that the source program is either syntactically ill formed or semantically unsound, then it must provide informative messages, so the user can take corrective action. The analysis part also collects information about the source program and stores it in a data structure called a symbol table , which is passed along with the intermediate representation to the synthesis part . The synthesis part constructs the desired target program from the intermediate representation and the information in the symbol table. The analysis part is often called the front end of the compiler; the synthesis part is the back end . If we examine the compilation process in more detail, we see that it operates as a sequence of phases , each of which transforms one representation of the source program to another. A typical decomposition of a compiler into phases is shown in Fig. 1.6. In practice, several phases may be grouped together, and the intermediate representations between the grouped phases need not be constructed explicitly. The symbol table , which stores information about the entire source program, is used by all phases of the compiler. NOTE: \u7b26\u53f7\u8868\u4e5f\u662f\u4e00\u95e8\u79d1\u5b66\uff0c\u9700\u8981\u5bf9\u5b83\u4ed4\u7ec6\u7814\u7a76\uff0c\u672c\u4e66\u4e2d\u6d89\u53ca\u7b26\u53f7\u8868\u7684\u7ae0\u8282\u6709\uff1a 1.2.1 Lexical Analysis 1.2.7 Symbol-Table Management 2.7 Symbol Tables \u9664\u6b64\u4e4b\u5916\uff0c\u4e0b\u9762\u662f\u5173\u4e8e\u7b26\u53f7\u8868\u7684\u4e00\u4e9b\u6269\u5c55\u77e5\u8bc6\uff1a \u7ef4\u57fa\u767e\u79d1\u7684 Symbol table \uff0c\u5185\u5bb9\u975e\u5e38\u597d\uff0c\u672c\u7ae0\u8282\u4e2d\u6536\u5f55\u4e86\u8fd9\u7bc7\u6587\u7ae0\u3002 Questions: CPython symbol table Clang symbol table Some compilers have a machine-independent optimization phase between the front end and the back end . The purpose of this optimization phase is to perform transformations on the intermediate representation , so that the back end can produce a better target program than it would have otherwise produced from an unoptimized intermediate representation. Since optimization is optional, one or the other of the two optimization phases shown in Fig. 1.6 may be missing. NOTE: \u56fe1.6\u7ed9\u51fa\u4e86\u4e00\u4e2acompiler\u7684\u67b6\u6784\uff0c\u8fd9\u4e2a\u67b6\u6784\u662f\u6e05\u6670\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u601d\u8003\u4e00\u4e0b\u5982\u4f55\u6765\u5b9e\u73b0\u4e00\u4e2a\u7f16\u8bd1\u5668\uff0c\u5982\u4e0b\u662f\u4e24\u79cd\u7b56\u7565\uff1a \u5206\u5f00\u5b9e\u73b0\u5404\u4e2a\u90e8\u4ef6\u7136\u540e\u5c06\u5b83\u4eec\u7ec4\u88c5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u7f16\u8bd1\u5668\uff0c\u6bcf\u4e2a\u90e8\u4ef6\u53ef\u4ee5\u72ec\u7acb\u5de5\u4f5c\uff08\u8026\u5408\u5ea6\u4f4e\uff09 \u6240\u6709\u529f\u80fd\u96c6\u6210\u8d77\u6765\u5f62\u6210\u4e00\u4e2amonolithic compiler\uff0c\u4e0d\u80fd\u72ec\u7acb\u5730\u4f7f\u7528\u5176\u4e2d\u67d0\u4e2a\u90e8\u4ef6\uff08\u8026\u5408\u5ea6\u9ad8\uff09 \u76ee\u524dc\u7cfb\u8bed\u8a00\u4e2d\u6700\u6d41\u884c\u7684\u4e24\u6b3e\u7f16\u8bd1\u5668\uff1a gcc \u548c clang \u3002\u5728clang\u7684\u5b98\u65b9\u7f51\u7ad9\u7684\u6587\u7ae0 Clang vs Other Open Source Compilers \u4e2d\uff0c\u7ed9\u51fa\u4e86\u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u5bf9\u6bd4\uff08\u5404\u81ea\u7684\u957f\u5904\u4e0e\u77ed\u5904\uff09\uff0c\u4e0b\u9762\u662f\u4e24\u79cd\u7684\u5dee\u5f02\u4e4b\u4e00\uff1a Clang is designed as an API from its inception, allowing it to be reused by source analysis tools, refactoring, IDEs (etc) as well as for code generation. GCC is built as a monolithic static compiler, which makes it extremely difficult to use as an API and integrate into other tools. Further, its historic design and current policy makes it difficult to decouple the front-end from the rest of the compiler. \u663e\u7136\uff0cclang\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e00\u79cd\u7b56\u7565\uff0cgcc\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e8c\u79cd\u7b56\u7565\uff0c\u6b63\u5982\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4f5c\u8005\u63d0\u51fa\u7684\u89c2\u70b9 differences in goals lead to different strengths and weaknesses \u6211\u89c9\u5f97\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002\u5728clang\u7684\u5b98\u65b9\u6587\u6863\u4e2d\uff0c\u5c06clang\u7684\u8fd9\u79cd\u8bbe\u8ba1\u7b56\u7565\u79f0\u4e3a\uff1a Library Based Architecture \u3002 \u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u67b6\u6784\u6240\u91c7\u53d6\u7684\u622a\u7136\u4e0d\u540c\u7684\u4e24\u79cd\u7b56\u7565\u662f\u8f6f\u4ef6\u8bbe\u8ba1\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u5178\u578b\u7684\u95ee\u9898\u3002 \u629b\u5f00\u8f6f\u4ef6\u8bbe\u8ba1\u4e0d\u8c08\uff0c\u56de\u5f52\u672c\u4e66\u3002\u672c\u4e66\u6240\u8bb2\u8ff0\u7684\u662f\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u662f\u7eaf\u7406\u8bba\u4e0a\u7684\uff0c\u5982\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u6784\u6210\uff0c\u672c\u4e66\u4f1a\u5206\u7ae0\u8282\u6765\u4ecb\u7ecd\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u5206\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u4ece\u524d\u7aef\u5230\u540e\u7aef\u3002\u5b9e\u8df5\u662f\u6709\u52a9\u4e8e\u7406\u8bba\u7684\u7406\u89e3\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u5fc5\u987b\u8981\u4f7f\u7528\u4e00\u4e9b\u5de5\u5177\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u7406\u8bba\u3002clang\u4f5c\u4e3a\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u522b\u7684\u5b8c\u6574\u7684\u3001\u6210\u719f\u7684\u7f16\u8bd1\u5668\uff0c\u5b83\u5b9e\u73b0\u4e86\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u5404\u4e2a\u90e8\u5206\uff0c\u800c\u4e14\u5b83\u7684 Library Based Architecture \uff0c\u4f7f\u5f97\u57fa\u672c\u4e0a\u7f16\u8bd1\u5668\u7684\u6bcf\u4e2a\u90e8\u5206\u90fd\u5bf9\u5e94\u4e3a\u5b83\u7684\u4e00\u4e2a\u5e93\uff0c\u8fd9\u4e9b\u5e93\u6709\u7740\u7b80\u6d01\u7684API\uff0c\u5e76\u4e14\u6587\u6863\u8be6\u5c3d\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528clang\u6765\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u597d\u5de5\u5177\u3002 \u9664\u6b64\u4e4b\u5916\uff0c python \u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u539f\u7406\u7684\u975e\u5e38\u597d\u7684\u5de5\u5177\uff1a Design of CPython\u2019s Compiler \u00b6 \uff0c\u5176\u4e2d\u7ed9\u51facpython\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1 Python Language Services \u00b6 \uff0c\u8fd9\u662fpython\u6807\u51c6\u5e93\u63d0\u4f9b\u7684module\uff0cthese modules support tokenizing, parsing, syntax analysis, bytecode disassembly, and various other facilities. \u7ed3\u5408\u8fd9\u4e9b\u5de5\u5177\uff0c\u6211\u4eec\u80fd\u591f\u8be6\u5c3d\u5730\u89c2\u5bdf\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u5de5\u4f5c\u539f\u7406\u3002 1.2.1 Lexical Analysis # The first phase of a compiler is called lexical analysis or scanning . The lexical analyzer reads the stream of characters making up the source program and groups the characters into meaningful sequences called lexemes . For each lexeme , the lexical analyzer produces as output a token of the form <token-name, attribute-value> that it passes on to the subsequent phase, syntax analysis . In the token, the first component token-name is an abstract symbol that is used during syntax analysis , and the second component attribute-value points to an entry in the symbol table for this token. Information from the symbol-table entry is needed for semantic analysis and code generation . For example, suppose a source program contains the assignment statement position = initial + rate * 60 (1.1) The characters in this assignment could be grouped into the following lexemes and mapped into the following tokens passed on to the syntax analyzer : position is a lexeme that would be mapped into a token <id, 1> , where id is an abstract symbol standing for identifier and 1 points to the symbol-table entry for position . The symbol-table entry for an identifier holds information about the identifier, such as its name and type . The assignment symbol = is a lexeme that is mapped into the token <=> . Since this token needs no attribute-value, we have omitted the second component. We could have used any abstract symbol such as assign for the token-name, but for notational convenience we have chosen to use the lexeme itself as the name of the abstract symbol. initial is a lexeme that is mapped into the token <id, 2> , where 2 points to the symbol-table entry for initial . + is a lexeme that is mapped into the token <+> . rate is a lexeme that is mapped into the token <id, 3> , where 3 points to the symbol-table entry for rate . * is a lexeme that is mapped into the token <*> . 60 is a lexeme that is mapped into the token <60> . Technically speaking, for the lexeme 60 we should make up a token like <number, 4> , where 4 points to the symbol table for the internal representation of integer 60 but we shall defer the discussion of tokens for numbers until Chapter 2. Chapter 3 discusses techniques for building lexical analyzers. Blanks separating the lexemes would b e discarded by the lexical analyzer. Figure 1.7 shows the representation of the assignment statement (1.1) after lexical analysis as the sequence of tokens <id, 1> <=> <id, 2> <+> <id, 3> <?> <60> (1.2) NOTE: Lexical Analysis\u4e3b\u8981\u5728Chapter 3 Lexical Analysis\u4e2d\u8bb2\u8ff0\u3002 NOTE: Clang \u4e2d\u7531 liblex \u6765\u5b9e\u73b0Lexical Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Lexer and Preprocessor Library \u00b6 1.2.2 Syntax Analysis # The second phase of the compiler is syntax analysis or parsing . The parser uses the first components of the tokens produced by the lexical analyzer to create a tree-like intermediate representation that depicts the grammatical structure of the token stream. A typical representation is a syntax tree in which each interior node represents an operation and the children of the node represent the arguments of the operation. A syntax tree for the token stream (1.2) is shown as the output of the syntactic analyzer in Fig. 1.7. This tree shows the order in which the operations in the assignment position = initial + rate * 60 are to be performed. The tree has an interior node labeled * with <id, 3> as its left child and the integer 60 as its right child. The node <id, 3> represents the identifier rate . The node labeled * makes it explicit that we must first multiply the value of rate by 60 . The node labeled + indicates that we must add the result of this multiplication to the value of initial. The root of the tree, labeled = , indicates that we must store the result of this addition into the location for the identifier position . This ordering of operations is consistent with the usual conventions of arithmetic which tell us that multiplication has higher precedence than addition, and hence that the multiplication is to be performed before the addition. The subsequent phases of the compiler use the grammatical structure to help analyze the source program and generate the target program. In Chapter 4 we shall use context-free grammars to specify the grammatical structure of programming languages and discuss algorithms for constructing efficient syntax analyzers automatically from certain classes of grammars. In Chapters 2 and 5 we shall see that syntax-directed definitions can help specify the translation of programming language constructs. NOTE: Clang \u4e2d\u7531 libparse \u6765\u5b9e\u73b0Syntax Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Parser Library \u00b6 1.2.3 Semantic Analysis # The semantic analyzer uses the syntax tree and the information in the symbol table to check the source program for semantic consistency with the language definition. It also gathers type information and saves it in either the syntax tree or the symbol table, for subsequent use during intermediate-code generation. An important part of semantic analysis is type checking , where the compiler checks that each operator has matching operands. For example, many programming language definitions require an array index to be an integer; the compiler must report an error if a floating-point number is used to index an array. The language specification may permit some type conversions called coercions . For example, a binary arithmetic operator may be applied to either a pair of integers or to a pair of floating-point numbers. If the operator is applied to a floating-point number and an integer, the compiler may convert or coerce the integer into a floating-point number. In Fig. 1.7, notice that the output of the semantic analyzer has an extra node for the operator inttofloat , which explicitly converts its integer argument into a floating-point number. Type checking and semantic analysis are discussed in Chapter 6. NOTE: Clang \u4e2d\u7531 libsema \u6765\u5b9e\u73b0Semantic Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Sema Library \u00b6 1.2.4 Intermediate Code Generation # In the process of translating a source program into target code, a compiler may construct one or more intermediate representations , which can have a variety of forms. Syntax trees are a form of intermediate representation ; they are commonly used during syntax and semantic analysis. After syntax and semantic analysis of the source program, many compilers generate an explicit low-level or machine-like intermediate representation, which we can think of as a program for an abstract machine. This intermediate representation should have two important properties: it should be easy to produce and it should be easy to translate into the target machine. In Chapter 6, we consider an intermediate form called three-address code, which consists of a sequence of assembly-like instructions with three operands per instruction. Each operand can act like a register. The output of the intermediate code generator in Fig. 1.7 consists of the three-address code sequence In Chapter 6, we cover the principal intermediate representations used in compilers. Chapter 5 introduces techniques for syntax-directed translation that are applied in Chapter 6 to type checking and intermediate-code generation for typical programming language constructs such as expressions, flow-of-control constructs, and procedure calls. 1.2.5 Code Optimization # The machine-independent code-optimization phase attempts to improve the intermediate code so that better target code will result. Usually better means faster, but other objectives may be desired, such as shorter code, or target code that consumes less power. For example, a straightforward algorithm generates the intermediate code (1.3), using an instruction for each operator in the tree representation that comes from the semantic analyzer. A simple intermediate code generation algorithm followed by code optimization is a reasonable way to generate good target code. The optimizer can deduce that the conversion of 60 from integer to floating point can be done once and for all at compile time, so the inttofloat operation can be eliminated by replacing the integer 60 by the floating-point number 60.0. Moreover, t3 is used only once to transmit its value to id1 so the optimizer can transform (1.3) into the shorter sequence t1 = id3 * 60.0 id1 = id2 + t1 (1.4) There is a great variation in the amount of code optimization different compilers perform. In those that do the most, the so-called \"optimizing compilers,\" a significant amount of time is spent on this phase. There are simple optimizations that significantly improve the running time of the target program without slowing down compilation too much. The chapters from 8 on discuss machine-independent and machine-dependent optimizations in detail. 1.2.6 Code Generation # The code generator takes as input an intermediate representation of the source program and maps it into the target language. If the target language is machine code, registers or memory locations are selected for each of the variables used by the program. Then, the intermediate instructions are translated into sequences of machine instructions that perform the same task. A crucial aspect of code generation is the judicious assignment of registers to hold variables. For example, using registers R1 and R2 , the intermediate code in (1.4) might get translated into the machine code LDF R2, id3 MULF R2, R2, #60.0 LDF R1, id2 ADDF R1, R1, R2 STF id1, R1 (1.5) 1.2.7 Symbol-Table Management # An essential function of a compiler is to record the variable names used in the source program and collect information about various attributes of each name. These attributes may provide information about the storage allocated for a name, its type, its scope (where in the program its value may b e used), and in the case of procedure names, such things as the number and types of its arguments, the method of passing each argument (for example, by value or by reference), and the type returned. The symbol table is a data structure containing a record for each variable name, with fields for the attributes of the name. The data structure should be designed to allow the compiler to find the record for each name quickly and to store or retrieve data from that record quickly. Symbol tables are discussed in Chapter 2. 1.2.8 The Grouping of Phases into Passes # 1.2.9 Compiler-Construction Tools # The compiler writer, like any software developer, can profitably use modern software development environments containing tools such as language editors, debuggers, version managers, profilers, test harnesses, and so on. In addition to these general software-development tools, other more specialized tools have been created to help implement various phases of a compiler. These tools use specialized languages for specifying and implementing specific components, and many use quite sophisticated algorithms. The most successful tools are those that hide the details of the generation algorithm and produce components that can be easily integrated into the remainder of the compiler. Some commonly used compiler-construction tools include Parser generators that automatically produce syntax analyzers from a grammatical description of a programming language. Scanner generators that produce lexical analyzers from a regular-expression description of the tokens of a language. Syntax-directed translation engines that produce collections of routines for walking a parse tree and generating intermediate code . Code-generator generators that produce a code generator from a collection of rules for translating each operation of the intermediate language into the machine language for a target machine. Data-flow analysis engines that facilitate the gathering of information about how values are transmitted from one part of a program to each other part. Data-flow analysis is a key part of code optimization. Compiler-construction toolkits that provide an integrated set of routines for constructing various phases of a compiler. We shall describe many of these tools throughout this book. NOTE: Wikipedia\u7684 Comparison of parser generators \u603b\u7ed3\u4e86parser generator\u548cscanner generator\u3002","title":"1.2-The-Structure-of-a-Compiler.md"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#12-the-structure-of-a-compiler","text":"Up to this point we have treated a compiler as a single box that maps a source program into a semantically equivalent target program. If we open up this box a little, we see that there are two parts to this mapping: analysis synthesis The analysis part breaks up the source program into constituent pieces and imposes a grammatical structure on them. It then uses this structure to create an intermediate representation of the source program. If the analysis part detects that the source program is either syntactically ill formed or semantically unsound, then it must provide informative messages, so the user can take corrective action. The analysis part also collects information about the source program and stores it in a data structure called a symbol table , which is passed along with the intermediate representation to the synthesis part . The synthesis part constructs the desired target program from the intermediate representation and the information in the symbol table. The analysis part is often called the front end of the compiler; the synthesis part is the back end . If we examine the compilation process in more detail, we see that it operates as a sequence of phases , each of which transforms one representation of the source program to another. A typical decomposition of a compiler into phases is shown in Fig. 1.6. In practice, several phases may be grouped together, and the intermediate representations between the grouped phases need not be constructed explicitly. The symbol table , which stores information about the entire source program, is used by all phases of the compiler. NOTE: \u7b26\u53f7\u8868\u4e5f\u662f\u4e00\u95e8\u79d1\u5b66\uff0c\u9700\u8981\u5bf9\u5b83\u4ed4\u7ec6\u7814\u7a76\uff0c\u672c\u4e66\u4e2d\u6d89\u53ca\u7b26\u53f7\u8868\u7684\u7ae0\u8282\u6709\uff1a 1.2.1 Lexical Analysis 1.2.7 Symbol-Table Management 2.7 Symbol Tables \u9664\u6b64\u4e4b\u5916\uff0c\u4e0b\u9762\u662f\u5173\u4e8e\u7b26\u53f7\u8868\u7684\u4e00\u4e9b\u6269\u5c55\u77e5\u8bc6\uff1a \u7ef4\u57fa\u767e\u79d1\u7684 Symbol table \uff0c\u5185\u5bb9\u975e\u5e38\u597d\uff0c\u672c\u7ae0\u8282\u4e2d\u6536\u5f55\u4e86\u8fd9\u7bc7\u6587\u7ae0\u3002 Questions: CPython symbol table Clang symbol table Some compilers have a machine-independent optimization phase between the front end and the back end . The purpose of this optimization phase is to perform transformations on the intermediate representation , so that the back end can produce a better target program than it would have otherwise produced from an unoptimized intermediate representation. Since optimization is optional, one or the other of the two optimization phases shown in Fig. 1.6 may be missing. NOTE: \u56fe1.6\u7ed9\u51fa\u4e86\u4e00\u4e2acompiler\u7684\u67b6\u6784\uff0c\u8fd9\u4e2a\u67b6\u6784\u662f\u6e05\u6670\u7684\uff0c\u4f5c\u4e3a\u4e00\u540d\u8f6f\u4ef6\u5de5\u7a0b\u5e08\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u601d\u8003\u4e00\u4e0b\u5982\u4f55\u6765\u5b9e\u73b0\u4e00\u4e2a\u7f16\u8bd1\u5668\uff0c\u5982\u4e0b\u662f\u4e24\u79cd\u7b56\u7565\uff1a \u5206\u5f00\u5b9e\u73b0\u5404\u4e2a\u90e8\u4ef6\u7136\u540e\u5c06\u5b83\u4eec\u7ec4\u88c5\u6210\u4e00\u4e2a\u5b8c\u6574\u7684\u7f16\u8bd1\u5668\uff0c\u6bcf\u4e2a\u90e8\u4ef6\u53ef\u4ee5\u72ec\u7acb\u5de5\u4f5c\uff08\u8026\u5408\u5ea6\u4f4e\uff09 \u6240\u6709\u529f\u80fd\u96c6\u6210\u8d77\u6765\u5f62\u6210\u4e00\u4e2amonolithic compiler\uff0c\u4e0d\u80fd\u72ec\u7acb\u5730\u4f7f\u7528\u5176\u4e2d\u67d0\u4e2a\u90e8\u4ef6\uff08\u8026\u5408\u5ea6\u9ad8\uff09 \u76ee\u524dc\u7cfb\u8bed\u8a00\u4e2d\u6700\u6d41\u884c\u7684\u4e24\u6b3e\u7f16\u8bd1\u5668\uff1a gcc \u548c clang \u3002\u5728clang\u7684\u5b98\u65b9\u7f51\u7ad9\u7684\u6587\u7ae0 Clang vs Other Open Source Compilers \u4e2d\uff0c\u7ed9\u51fa\u4e86\u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u5bf9\u6bd4\uff08\u5404\u81ea\u7684\u957f\u5904\u4e0e\u77ed\u5904\uff09\uff0c\u4e0b\u9762\u662f\u4e24\u79cd\u7684\u5dee\u5f02\u4e4b\u4e00\uff1a Clang is designed as an API from its inception, allowing it to be reused by source analysis tools, refactoring, IDEs (etc) as well as for code generation. GCC is built as a monolithic static compiler, which makes it extremely difficult to use as an API and integrate into other tools. Further, its historic design and current policy makes it difficult to decouple the front-end from the rest of the compiler. \u663e\u7136\uff0cclang\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e00\u79cd\u7b56\u7565\uff0cgcc\u6240\u91c7\u53d6\u7684\u662f\u7b2c\u4e8c\u79cd\u7b56\u7565\uff0c\u6b63\u5982\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\u4f5c\u8005\u63d0\u51fa\u7684\u89c2\u70b9 differences in goals lead to different strengths and weaknesses \u6211\u89c9\u5f97\u662f\u975e\u5e38\u6b63\u786e\u7684\u3002\u5728clang\u7684\u5b98\u65b9\u6587\u6863\u4e2d\uff0c\u5c06clang\u7684\u8fd9\u79cd\u8bbe\u8ba1\u7b56\u7565\u79f0\u4e3a\uff1a Library Based Architecture \u3002 \u8fd9\u4e24\u6b3e\u7f16\u8bd1\u5668\u7684\u67b6\u6784\u6240\u91c7\u53d6\u7684\u622a\u7136\u4e0d\u540c\u7684\u4e24\u79cd\u7b56\u7565\u662f\u8f6f\u4ef6\u8bbe\u8ba1\u9886\u57df\u4e00\u4e2a\u975e\u5e38\u5178\u578b\u7684\u95ee\u9898\u3002 \u629b\u5f00\u8f6f\u4ef6\u8bbe\u8ba1\u4e0d\u8c08\uff0c\u56de\u5f52\u672c\u4e66\u3002\u672c\u4e66\u6240\u8bb2\u8ff0\u7684\u662f\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u662f\u7eaf\u7406\u8bba\u4e0a\u7684\uff0c\u5982\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u6784\u6210\uff0c\u672c\u4e66\u4f1a\u5206\u7ae0\u8282\u6765\u4ecb\u7ecd\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u5206\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u4ece\u524d\u7aef\u5230\u540e\u7aef\u3002\u5b9e\u8df5\u662f\u6709\u52a9\u4e8e\u7406\u8bba\u7684\u7406\u89e3\u7684\uff0c\u6240\u4ee5\u6211\u4eec\u5fc5\u987b\u8981\u4f7f\u7528\u4e00\u4e9b\u5de5\u5177\u6765\u9a8c\u8bc1\u8fd9\u4e9b\u7406\u8bba\u3002clang\u4f5c\u4e3a\u4e00\u4e2a\u5de5\u4e1a\u7ea7\u522b\u7684\u5b8c\u6574\u7684\u3001\u6210\u719f\u7684\u7f16\u8bd1\u5668\uff0c\u5b83\u5b9e\u73b0\u4e86\u56fe1.6\u6240\u5c55\u793a\u7684\u7f16\u8bd1\u5668\u7684\u5404\u4e2a\u90e8\u5206\uff0c\u800c\u4e14\u5b83\u7684 Library Based Architecture \uff0c\u4f7f\u5f97\u57fa\u672c\u4e0a\u7f16\u8bd1\u5668\u7684\u6bcf\u4e2a\u90e8\u5206\u90fd\u5bf9\u5e94\u4e3a\u5b83\u7684\u4e00\u4e2a\u5e93\uff0c\u8fd9\u4e9b\u5e93\u6709\u7740\u7b80\u6d01\u7684API\uff0c\u5e76\u4e14\u6587\u6863\u8be6\u5c3d\uff0c\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528clang\u6765\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u597d\u5de5\u5177\u3002 \u9664\u6b64\u4e4b\u5916\uff0c python \u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5b66\u4e60\u7f16\u8bd1\u539f\u7406\u7684\u975e\u5e38\u597d\u7684\u5de5\u5177\uff1a Design of CPython\u2019s Compiler \u00b6 \uff0c\u5176\u4e2d\u7ed9\u51facpython\u7f16\u8bd1\u5668\u7684\u8bbe\u8ba1 Python Language Services \u00b6 \uff0c\u8fd9\u662fpython\u6807\u51c6\u5e93\u63d0\u4f9b\u7684module\uff0cthese modules support tokenizing, parsing, syntax analysis, bytecode disassembly, and various other facilities. \u7ed3\u5408\u8fd9\u4e9b\u5de5\u5177\uff0c\u6211\u4eec\u80fd\u591f\u8be6\u5c3d\u5730\u89c2\u5bdf\u7f16\u8bd1\u5668\u5404\u4e2a\u90e8\u4ef6\u7684\u5de5\u4f5c\u539f\u7406\u3002","title":"1.2 The Structure of a Compiler"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#121-lexical-analysis","text":"The first phase of a compiler is called lexical analysis or scanning . The lexical analyzer reads the stream of characters making up the source program and groups the characters into meaningful sequences called lexemes . For each lexeme , the lexical analyzer produces as output a token of the form <token-name, attribute-value> that it passes on to the subsequent phase, syntax analysis . In the token, the first component token-name is an abstract symbol that is used during syntax analysis , and the second component attribute-value points to an entry in the symbol table for this token. Information from the symbol-table entry is needed for semantic analysis and code generation . For example, suppose a source program contains the assignment statement position = initial + rate * 60 (1.1) The characters in this assignment could be grouped into the following lexemes and mapped into the following tokens passed on to the syntax analyzer : position is a lexeme that would be mapped into a token <id, 1> , where id is an abstract symbol standing for identifier and 1 points to the symbol-table entry for position . The symbol-table entry for an identifier holds information about the identifier, such as its name and type . The assignment symbol = is a lexeme that is mapped into the token <=> . Since this token needs no attribute-value, we have omitted the second component. We could have used any abstract symbol such as assign for the token-name, but for notational convenience we have chosen to use the lexeme itself as the name of the abstract symbol. initial is a lexeme that is mapped into the token <id, 2> , where 2 points to the symbol-table entry for initial . + is a lexeme that is mapped into the token <+> . rate is a lexeme that is mapped into the token <id, 3> , where 3 points to the symbol-table entry for rate . * is a lexeme that is mapped into the token <*> . 60 is a lexeme that is mapped into the token <60> . Technically speaking, for the lexeme 60 we should make up a token like <number, 4> , where 4 points to the symbol table for the internal representation of integer 60 but we shall defer the discussion of tokens for numbers until Chapter 2. Chapter 3 discusses techniques for building lexical analyzers. Blanks separating the lexemes would b e discarded by the lexical analyzer. Figure 1.7 shows the representation of the assignment statement (1.1) after lexical analysis as the sequence of tokens <id, 1> <=> <id, 2> <+> <id, 3> <?> <60> (1.2) NOTE: Lexical Analysis\u4e3b\u8981\u5728Chapter 3 Lexical Analysis\u4e2d\u8bb2\u8ff0\u3002 NOTE: Clang \u4e2d\u7531 liblex \u6765\u5b9e\u73b0Lexical Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Lexer and Preprocessor Library \u00b6","title":"1.2.1 Lexical Analysis"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#122-syntax-analysis","text":"The second phase of the compiler is syntax analysis or parsing . The parser uses the first components of the tokens produced by the lexical analyzer to create a tree-like intermediate representation that depicts the grammatical structure of the token stream. A typical representation is a syntax tree in which each interior node represents an operation and the children of the node represent the arguments of the operation. A syntax tree for the token stream (1.2) is shown as the output of the syntactic analyzer in Fig. 1.7. This tree shows the order in which the operations in the assignment position = initial + rate * 60 are to be performed. The tree has an interior node labeled * with <id, 3> as its left child and the integer 60 as its right child. The node <id, 3> represents the identifier rate . The node labeled * makes it explicit that we must first multiply the value of rate by 60 . The node labeled + indicates that we must add the result of this multiplication to the value of initial. The root of the tree, labeled = , indicates that we must store the result of this addition into the location for the identifier position . This ordering of operations is consistent with the usual conventions of arithmetic which tell us that multiplication has higher precedence than addition, and hence that the multiplication is to be performed before the addition. The subsequent phases of the compiler use the grammatical structure to help analyze the source program and generate the target program. In Chapter 4 we shall use context-free grammars to specify the grammatical structure of programming languages and discuss algorithms for constructing efficient syntax analyzers automatically from certain classes of grammars. In Chapters 2 and 5 we shall see that syntax-directed definitions can help specify the translation of programming language constructs. NOTE: Clang \u4e2d\u7531 libparse \u6765\u5b9e\u73b0Syntax Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Parser Library \u00b6","title":"1.2.2 Syntax Analysis"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#123-semantic-analysis","text":"The semantic analyzer uses the syntax tree and the information in the symbol table to check the source program for semantic consistency with the language definition. It also gathers type information and saves it in either the syntax tree or the symbol table, for subsequent use during intermediate-code generation. An important part of semantic analysis is type checking , where the compiler checks that each operator has matching operands. For example, many programming language definitions require an array index to be an integer; the compiler must report an error if a floating-point number is used to index an array. The language specification may permit some type conversions called coercions . For example, a binary arithmetic operator may be applied to either a pair of integers or to a pair of floating-point numbers. If the operator is applied to a floating-point number and an integer, the compiler may convert or coerce the integer into a floating-point number. In Fig. 1.7, notice that the output of the semantic analyzer has an extra node for the operator inttofloat , which explicitly converts its integer argument into a floating-point number. Type checking and semantic analysis are discussed in Chapter 6. NOTE: Clang \u4e2d\u7531 libsema \u6765\u5b9e\u73b0Semantic Analysis\uff0c\u5982\u4e0b\u662f\u6587\u6863: The Sema Library \u00b6","title":"1.2.3 Semantic Analysis"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#124-intermediate-code-generation","text":"In the process of translating a source program into target code, a compiler may construct one or more intermediate representations , which can have a variety of forms. Syntax trees are a form of intermediate representation ; they are commonly used during syntax and semantic analysis. After syntax and semantic analysis of the source program, many compilers generate an explicit low-level or machine-like intermediate representation, which we can think of as a program for an abstract machine. This intermediate representation should have two important properties: it should be easy to produce and it should be easy to translate into the target machine. In Chapter 6, we consider an intermediate form called three-address code, which consists of a sequence of assembly-like instructions with three operands per instruction. Each operand can act like a register. The output of the intermediate code generator in Fig. 1.7 consists of the three-address code sequence In Chapter 6, we cover the principal intermediate representations used in compilers. Chapter 5 introduces techniques for syntax-directed translation that are applied in Chapter 6 to type checking and intermediate-code generation for typical programming language constructs such as expressions, flow-of-control constructs, and procedure calls.","title":"1.2.4 Intermediate Code Generation"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#125-code-optimization","text":"The machine-independent code-optimization phase attempts to improve the intermediate code so that better target code will result. Usually better means faster, but other objectives may be desired, such as shorter code, or target code that consumes less power. For example, a straightforward algorithm generates the intermediate code (1.3), using an instruction for each operator in the tree representation that comes from the semantic analyzer. A simple intermediate code generation algorithm followed by code optimization is a reasonable way to generate good target code. The optimizer can deduce that the conversion of 60 from integer to floating point can be done once and for all at compile time, so the inttofloat operation can be eliminated by replacing the integer 60 by the floating-point number 60.0. Moreover, t3 is used only once to transmit its value to id1 so the optimizer can transform (1.3) into the shorter sequence t1 = id3 * 60.0 id1 = id2 + t1 (1.4) There is a great variation in the amount of code optimization different compilers perform. In those that do the most, the so-called \"optimizing compilers,\" a significant amount of time is spent on this phase. There are simple optimizations that significantly improve the running time of the target program without slowing down compilation too much. The chapters from 8 on discuss machine-independent and machine-dependent optimizations in detail.","title":"1.2.5 Code Optimization"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#126-code-generation","text":"The code generator takes as input an intermediate representation of the source program and maps it into the target language. If the target language is machine code, registers or memory locations are selected for each of the variables used by the program. Then, the intermediate instructions are translated into sequences of machine instructions that perform the same task. A crucial aspect of code generation is the judicious assignment of registers to hold variables. For example, using registers R1 and R2 , the intermediate code in (1.4) might get translated into the machine code LDF R2, id3 MULF R2, R2, #60.0 LDF R1, id2 ADDF R1, R1, R2 STF id1, R1 (1.5)","title":"1.2.6 Code Generation"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#127-symbol-table-management","text":"An essential function of a compiler is to record the variable names used in the source program and collect information about various attributes of each name. These attributes may provide information about the storage allocated for a name, its type, its scope (where in the program its value may b e used), and in the case of procedure names, such things as the number and types of its arguments, the method of passing each argument (for example, by value or by reference), and the type returned. The symbol table is a data structure containing a record for each variable name, with fields for the attributes of the name. The data structure should be designed to allow the compiler to find the record for each name quickly and to store or retrieve data from that record quickly. Symbol tables are discussed in Chapter 2.","title":"1.2.7 Symbol-Table Management"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#128-the-grouping-of-phases-into-passes","text":"","title":"1.2.8 The Grouping of Phases into Passes"},{"location":"Chapter-1-Introduction/1.2-The-Structure-of-a-Compiler/#129-compiler-construction-tools","text":"The compiler writer, like any software developer, can profitably use modern software development environments containing tools such as language editors, debuggers, version managers, profilers, test harnesses, and so on. In addition to these general software-development tools, other more specialized tools have been created to help implement various phases of a compiler. These tools use specialized languages for specifying and implementing specific components, and many use quite sophisticated algorithms. The most successful tools are those that hide the details of the generation algorithm and produce components that can be easily integrated into the remainder of the compiler. Some commonly used compiler-construction tools include Parser generators that automatically produce syntax analyzers from a grammatical description of a programming language. Scanner generators that produce lexical analyzers from a regular-expression description of the tokens of a language. Syntax-directed translation engines that produce collections of routines for walking a parse tree and generating intermediate code . Code-generator generators that produce a code generator from a collection of rules for translating each operation of the intermediate language into the machine language for a target machine. Data-flow analysis engines that facilitate the gathering of information about how values are transmitted from one part of a program to each other part. Data-flow analysis is a key part of code optimization. Compiler-construction toolkits that provide an integrated set of routines for constructing various phases of a compiler. We shall describe many of these tools throughout this book. NOTE: Wikipedia\u7684 Comparison of parser generators \u603b\u7ed3\u4e86parser generator\u548cscanner generator\u3002","title":"1.2.9 Compiler-Construction Tools"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/","text":"1.5 Applications of Compiler Technology # Compiler design is not only about compilers, and many people use the technology learned by studying compilers in school, yet have never, strictly speaking, written (even part of ) a compiler for a major programming language. Compiler technology has other important uses as well. Additionally, compiler design impacts several other areas of computer science. In this section, we review the most important interactions and applications of the technology. 1.5.1 Implementation of High-Level Programming Languages # A high-level programming language defines a programming abstraction: the programmer expresses an algorithm using the language, and the compiler must translate that program to the target language. 1.5.2 Optimizations for Computer Architectures # The rapid evolution of computer architectures has also led to an insatiable demand for new compiler technology. Almost all high-performance systems take advantage of the same two basic techniques: parallelism and memory hierarchies . Parallelism can be found at several levels: at the instruction level, where multiple operations are executed simultaneously and at the processor level, where different threads of the same application are run on different processors. Memory hierarchies are a response to the basic limitation that we can build very fast storage or very large storage, but not storage that is both fast and large. 1.5.3 Design of New Computer Architectures # In the early days of computer architecture design, compilers were developed after the machines were built. That has changed. Since programming in high-level languages is the norm, the performance of a computer system is determined not by its raw speed but also by how well compilers can exploit its features. Thus, in modern computer architecture development, compilers are developed in the processor-design stage, and compiled code, running on simulators, is used to evaluate the proposed architectural features. 1.5.4 Program Translations # While we normally think of compiling as a translation from a high-level language to the machine level, the same technology can be applied to translate between different kinds of languages. The following are some of the important applications of program-translation techniques. 1.5.5 Software Productivity Tools # Programs are arguably the most complicated engineering artifacts ever produced; they consist of many many details, every one of which must be correct before the program will work completely. As a result, errors are rampant in programs; errors may crash a system, produce wrong results, render a system vulnerable to security attacks, or even lead to catastrophic failures in critical systems. Testing is the primary technique for locating errors in programs. An interesting and promising complementary approach is to use data-flow analysis to locate errors statically (that is, before the program is run). Data-flow analysis can find errors along all the possible execution paths, and not just those exercised by the input data sets, as in the case of program testing. Many of the data-flow-analysis techniques, originally developed for compiler optimizations , can be used to create tools that assist programmers in their software engineering tasks. The problem of finding all program errors is undecidable. A data-flow analysis may be designed to warn the programmers of all possible statements with a particular category of errors. But if most of these warnings are false alarms, users will not use the tool. Thus, practical error detectors are often neither sound nor complete. That is, they may not find all the errors in the program, and not all errors reported are guaranteed to be real errors. Nonetheless, various static analyses have been developed and shown to be effective in finding errors, such as dereferencing null or freed pointers, in real programs. The fact that error detectors may be unsound makes them significantly different from compiler optimizations. Optimizers must be conservative and cannot alter the semantics of the program under any circumstances. In the balance of this section, we shall mention several ways in which program analysis, building up on techniques originally develop ed to optimize code in compilers, have improved software productivity. Of special importance are techniques that detect statically when a program might have a security vulnerability. Type Checking # Bounds Checking # Memory-Management Tools # Garbage collection is another excellent example of the tradeoff between efficiency and a combination of ease of programming and software reliability. Automatic memory management obliterates all memory-management errors (e.g., \"memory leaks\"), which are a major source of problems in C and C++ programs. Various tools have been developed to help programmers find memory management errors. For example, Purify is a widely used tool that dynamically catches memory management errors as they occur. Tools that help identify some of these problems statically have also been developed. NOTE: \u73b0\u4ee3IDE\u4e00\u822c\u90fd\u4f1a\u96c6\u6210\u5f88\u591asoftware productivity tools\uff0c\u6bd4\u5982\u5728 pycharm \u4e2d\u3002 See also\uff1a List of tools for static code analysis Source Code Analysis Tools code inspection tools \u5728\u5b66\u4e60\u4e86\u672c\u4e66\u4e2d\u7684\u5185\u5bb9\u540e\uff0c\u5176\u5b9e\u8bfb\u8005\u53ef\u4ee5\u5927\u81f4\u63a8\u6d4b\u8fd9\u4e9b\u5de5\u5177\u7684\u539f\u7406\u4e86\u3002 \u663e\u7136\u8fd9\u4e9b\u90fd\u662f\u5e2e\u52a9 Software quality assurance \u7684\u6709\u6548\u5de5\u5177\u3002","title":"1.5-Applications-of-Compiler-Technology.md"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#15-applications-of-compiler-technology","text":"Compiler design is not only about compilers, and many people use the technology learned by studying compilers in school, yet have never, strictly speaking, written (even part of ) a compiler for a major programming language. Compiler technology has other important uses as well. Additionally, compiler design impacts several other areas of computer science. In this section, we review the most important interactions and applications of the technology.","title":"1.5 Applications of Compiler Technology"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#151-implementation-of-high-level-programming-languages","text":"A high-level programming language defines a programming abstraction: the programmer expresses an algorithm using the language, and the compiler must translate that program to the target language.","title":"1.5.1 Implementation of High-Level Programming Languages"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#152-optimizations-for-computer-architectures","text":"The rapid evolution of computer architectures has also led to an insatiable demand for new compiler technology. Almost all high-performance systems take advantage of the same two basic techniques: parallelism and memory hierarchies . Parallelism can be found at several levels: at the instruction level, where multiple operations are executed simultaneously and at the processor level, where different threads of the same application are run on different processors. Memory hierarchies are a response to the basic limitation that we can build very fast storage or very large storage, but not storage that is both fast and large.","title":"1.5.2 Optimizations for Computer Architectures"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#153-design-of-new-computer-architectures","text":"In the early days of computer architecture design, compilers were developed after the machines were built. That has changed. Since programming in high-level languages is the norm, the performance of a computer system is determined not by its raw speed but also by how well compilers can exploit its features. Thus, in modern computer architecture development, compilers are developed in the processor-design stage, and compiled code, running on simulators, is used to evaluate the proposed architectural features.","title":"1.5.3 Design of New Computer Architectures"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#154-program-translations","text":"While we normally think of compiling as a translation from a high-level language to the machine level, the same technology can be applied to translate between different kinds of languages. The following are some of the important applications of program-translation techniques.","title":"1.5.4 Program Translations"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#155-software-productivity-tools","text":"Programs are arguably the most complicated engineering artifacts ever produced; they consist of many many details, every one of which must be correct before the program will work completely. As a result, errors are rampant in programs; errors may crash a system, produce wrong results, render a system vulnerable to security attacks, or even lead to catastrophic failures in critical systems. Testing is the primary technique for locating errors in programs. An interesting and promising complementary approach is to use data-flow analysis to locate errors statically (that is, before the program is run). Data-flow analysis can find errors along all the possible execution paths, and not just those exercised by the input data sets, as in the case of program testing. Many of the data-flow-analysis techniques, originally developed for compiler optimizations , can be used to create tools that assist programmers in their software engineering tasks. The problem of finding all program errors is undecidable. A data-flow analysis may be designed to warn the programmers of all possible statements with a particular category of errors. But if most of these warnings are false alarms, users will not use the tool. Thus, practical error detectors are often neither sound nor complete. That is, they may not find all the errors in the program, and not all errors reported are guaranteed to be real errors. Nonetheless, various static analyses have been developed and shown to be effective in finding errors, such as dereferencing null or freed pointers, in real programs. The fact that error detectors may be unsound makes them significantly different from compiler optimizations. Optimizers must be conservative and cannot alter the semantics of the program under any circumstances. In the balance of this section, we shall mention several ways in which program analysis, building up on techniques originally develop ed to optimize code in compilers, have improved software productivity. Of special importance are techniques that detect statically when a program might have a security vulnerability.","title":"1.5.5 Software Productivity Tools"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#type-checking","text":"","title":"Type Checking"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#bounds-checking","text":"","title":"Bounds Checking"},{"location":"Chapter-1-Introduction/1.5-Applications-of-Compiler-Technology/#memory-management-tools","text":"Garbage collection is another excellent example of the tradeoff between efficiency and a combination of ease of programming and software reliability. Automatic memory management obliterates all memory-management errors (e.g., \"memory leaks\"), which are a major source of problems in C and C++ programs. Various tools have been developed to help programmers find memory management errors. For example, Purify is a widely used tool that dynamically catches memory management errors as they occur. Tools that help identify some of these problems statically have also been developed. NOTE: \u73b0\u4ee3IDE\u4e00\u822c\u90fd\u4f1a\u96c6\u6210\u5f88\u591asoftware productivity tools\uff0c\u6bd4\u5982\u5728 pycharm \u4e2d\u3002 See also\uff1a List of tools for static code analysis Source Code Analysis Tools code inspection tools \u5728\u5b66\u4e60\u4e86\u672c\u4e66\u4e2d\u7684\u5185\u5bb9\u540e\uff0c\u5176\u5b9e\u8bfb\u8005\u53ef\u4ee5\u5927\u81f4\u63a8\u6d4b\u8fd9\u4e9b\u5de5\u5177\u7684\u539f\u7406\u4e86\u3002 \u663e\u7136\u8fd9\u4e9b\u90fd\u662f\u5e2e\u52a9 Software quality assurance \u7684\u6709\u6548\u5de5\u5177\u3002","title":"Memory-Management Tools"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/","text":"1.6 Programming Language Basics 1.6.1 The Static/Dynamic Distinction 1.6.2 Environments and States 1.6.3 Static Scope and Block Structure 1.6.4 Explicit Access Control 1.6.5 Dynamic Scope 1.6.6 Parameter Passing Mechanisms Call-by-Value Call-by-Reference Call-by-Name 1.6.7 Aliasing 1.6 Programming Language Basics # In this section, we shall cover the most important terminology and distinctions that appear in the study of programming languages. It is not our purpose to cover all concepts or all the popular programming languages. We assume that the reader is familiar with at least one of C, C++ , C#, or Java, and may have encountered other languages as well. 1.6.1 The Static/Dynamic Distinction # Among the most important issues that we face when designing a compiler for a language is what decisions can the compiler make about a program. If a language uses a policy that allows the compiler to decide an issue, then we say that the language uses a static policy or that the issue can be decided at compile time . On the other hand, a policy that only allows a decision to be made when we execute the program is said to be a dynamic policy or to require a decision at run time . One issue on which we shall concentrate is the scope of declarations . The scope of a declaration of x is the region of the program in which uses of x refer to this declaration. A language uses static scope or lexical scope if it is possible to determine the scope of a declaration by looking only at the program. Otherwise, the language uses dynamic scope . With dynamic scope , as the program runs, the same use of x could refer to any of several different declarations of x . NOTE:\u4e0a\u9762\u5173\u4e8e scope \u7684\u5b9a\u4e49\u6240\u6307\u4ee3\u7684\u662fdeclaration\uff0c\u800c\u975evariable\u3002\u663e\u7136declaration\u80fd\u591f\u6307\u4ee3\u7684\u8303\u56f4\u8fdc\u8fdc\u5927\u4e8evariable\u3002 See also: Scope (computer science) Most languages, such as C and Java, use static scope . We shall discuss static scoping in Section 1.6.3. Example 1.3 : As another example of the static/dynamic distinction , consider the use of the term \"static\" as it applies to data in a Java class declaration. In Java, a variable is a name for a location in memory used to hold a data value . Here, \"static\" refers not to the scope of the variable, but rather to the ability of the compiler to determine the location in memory where the declared variable can be found. A declaration like public static int x; makes x a class variable and says that there is only one copy of x , no matter how many objects of this class are created. Moreover, the compiler can determine a location in memory where this integer x will be held. In contrast, had \" static \" been omitted from this declaration, then each object of the class would have its own location where x would be held, and the compiler could not determine all these places in advance of running the program. NOTE: \u901a\u8fc7\u5173\u952e\u5b57 static \u6765\u544a\u8bc9compiler\uff1a\u4f60\u53ef\u4ee5\u5728\u8fdb\u884c\u7f16\u8bd1\u7684\u65f6\u5019\u5c31\u7ed9 x \u5206\u914dmemory\uff1b\u800c\u4e0d\u662f\u5728\u8fd0\u884c\u7a0b\u5e8f\u7684\u65f6\u5019\u4f7f\u7528 new \u6765\u521b\u5efa\u5bf9\u8c61\u65f6\u518d\u5206\u914d x \u7684memory\uff0c\u663e\u7136\u8fd9\u662f\u6240\u8c13\u7684static policy\u3002 See also: Static (keyword) 1.6.2 Environments and States # Another important distinction we must make when discussing programming languages is whether changes occurring as the program runs affect the values of data elements or affect the interpretation of names for that data. For example,the execution of an assignment such as x = y + 1 changes the value denoted by the name x . More specifically, the assignment changes the value in whatever location is denoted by x . It may be less clear that the location denoted by x can change at run time . For instance, as we discussed in Example 1.3, if x is not a static (or \" class \") variable, then every object of the class has its own location for an instance of variable x. In that case, the assignment to x can change any of those \"instance\" variables, depending on the object to which a method containing that assignment is applied. The association of names with locations in memory (the store) and then with values can be described by two mappings that change as the program runs (see Fig. 1.8): 1. The environment is a mapping from names to locations in the store. Since variables refer to locations (\"l-values\" in the terminology of C), we could alternatively define an environment as a mapping from names to variables ( location ). 2. The state is a mapping from locations in store to their values . That is, the state maps l-values to their corresponding r-values , in the terminology of C. Environments change according to the scope rules of a language. NOTE: environments \u548c scope \u662f\u4e0d\u540c\u7684\u6982\u5ff5\u3002 Example 1.4 : Consider the C program fragment in Fig. 1.9. Integer i is declared a global variable, and also declared as a variable local to function f .When f is executing, the environment adjusts so that name i refers to the location reserved for the i that is local to f , and any use of i , such as the assignment i = 3 shown explicitly, refers to that location. Typically, the local i is given a place on the run-time stack int i; /* global i */ void f(...) { int i; /* local i */ ... i = 3; /* use of local i */ ... } ... x = i + 1; /* use of global i */ Whenever a function g other than f is executing, uses of i cannot refer to the i that is local to f . Uses of name i in g must be within the scope of some other declaration of i . An example is the explicitly shown statement x = i+1 ,which is inside some procedure whose definition is not shown. The i in i + 1 presumably\uff08\u5927\u6982\uff09 refers to the global i . As in most languages, declarations in C must precede their use, so a function that comes before the global i cannot refer to it. The environment and state mappings in Fig. 1.8 are dynamic , but there are a few exceptions: Static versus dynamic binding of names to locations . Most binding of names to locations is dynamic , and we discuss several approaches to this binding throughout the section. Some declarations, such as the global i in Fig. 1.9, can be given a location in the store once and for all, as the compiler generates object code(\u4e5f\u5c31\u662fglobal i \u662fstatic). Technically, the C compiler will assign a location in virtual memory for the global i ,leaving it to the loader and the operating system to determine where in the physical memory of the machine i will be located. However, we shall not worry about relocation issues such as these, which have no impact on compiling. Instead, we treat the address space that the compiler uses for its output code as if it gave physical memory locations . Static versus dynamic binding of locations to values . The binding of locations to values (the second stage in Fig. 1.8), is generally dynamic as well, since we cannot tell the value in a location until we run the program. Declared constants are an exception. For instance, the C definition c #define ARRAYSIZE 1000 binds the name ARRAYSIZE to the value 1000 statically. We can determine this binding by looking at the statement, and we know that it is impossible for this binding to change when the program executes. 1.6.3 Static Scope and Block Structure # Most languages, including C and its family, use static scope _. The scope rules for C are based on program structure ; the scope of a declaration is determined implicitly(\u9690\u5f0f\u5730) by where the declaration appears in the program. Later languages,such as C++, Java, and C#, also provide explicit control over scopes through the use of keywords like public , private , and protected . In this section we consider static-scope rules for a language with blocks , where a block is a grouping of declarations and statements . C uses braces { and } to delimit(\u5b9a\u754c) a block ; the alternative use of begin and end for the same purpose dates back to Algol. NOTE : \u5728 c++ \u4e2d\uff0cblock\u9664\u4e86\u53ef\u4ee5\u7528\u4e8e\u786e\u5b9ascope\u5916\uff0cblock\u4e5f\u53ef\u4ee5\u51b3\u5b9a storage duration \u3002\u4ee5\u4e0b\u662f\u6458\u81eacppreference \u7684 Storage class specifiers \u7ae0\u8282\uff1a automatic storage duration. The storage for the object is allocated at the beginning of the enclosing code block and deallocated at the end. All local objects have this storage duration, except those declared static , extern or thread_local . Names , Identifiers , and Variables Although the terms \"name\" and \"variable,\" often refer to the same thing,we use them carefully to distinguish between compile-time names and the run-time locations denoted by names. An identifier is a string of characters, typically letters or digits, that refers to (identifies) an entity , such as a data object , a procedure , a class ,or a type . All identifiers are names , but not all names are identifiers .Names can also be expressions. For example, the name x.y might denote the field y of a structure denoted by x . Here, x and y are identifiers, while x.y is a name, but not an identifier . Composite names like x.y are called qualified names . SUMMARY :\u663e\u7136\uff0cname\u662f\u6bd4identifier\u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\u3002 A variable refers to a particular location of the store. It is common for the same identifier to be declared more than once; each such declaration introduces a new variable . Even if each identifier is declared just once, an identifier local to a recursive procedure will refer to different locations of the store at different times. SUMMARY :\u663e\u7136\uff0cvariable\u66f4\u52a0\u503e\u5411\u4e8e\u8fd0\u884c\u65f6\u6982\u5ff5 Example 1.5 \uff1aC static-scope policy is as follows A C program consists of a sequence of top-level declarations of variables and functions Functions may have variable declarations within them, where variables include local variables and parameters . The scope of each such declaration is restricted to the function in which it appears. The scope of a top-level declaration of a name x consists of the entire program that follows, with the exception of those statements that lie within a function that also has a declaration of x The additional detail regarding the C static-scope policy deals with variable declarations within statements . We examine such declarations next and in Example 1.6. 2 Procedures, Functions, and Methods To avoid saying procedures, functions, or methods, each time we want to talk about a subprogram that may be called, we shall usually refer to all of them as procedures .The exception is that when talking explicitly of programs in languages like C that have only functions, we shall refer to them as functions .Or, if we are discussing a language like Java that has only methods , we shall use that term instead. A function generally returns a value of some type (the return type), while a procedure does not return any value. C and similar languages, which have only functions , treat procedures as functions that have a special return type void , to signify no return value. Object-oriented languages like Java and C++ use the term methods .These can behave like either functions or procedures, but are associated with a particular class. In C, the syntax of blocks is given by One type of statement is a block(block\u662f\u4e00\u79cdstatement). Blocks can appear anywhere that other types of statements, such as assignment statements, can appear. A block is a sequence of declarations followed by a sequence of statements,all surrounded by braces Note that this syntax allows blocks to be nested inside each other. This nesting property is referred to as block structure . The C family of languages has block structure, except that a function may not be defined inside another function. We say that a declaration D belongs to a block B if B is the most closely nested block containing D ; that is, D is located within B , but not within any block that is nested within B . The static-scope rule for variable declarations in block-structured languages is as follows. If declaration D of name x belongs to block B , then the scope of D is all of B , except for any blocks B' nested to any depth within B , in which x is redeclared. Here, x is redeclared in B' if some other declaration D of the same name x belongs to B' . An equivalent way to express this rule is to focus on a use of a name x .Let $B_1$, $B_2$, ... $B_k$ be all the blocks that surround this use of x , with $B_k$ the smallest, nested within $B_{k-1}$ , which is nested within $B_{k-2}$ , and so on. Search for the largest i such that there is a declaration of x belonging to $B_i$. This use of x refers to the declaration in $B_i$ . Alternatively, this use of x is within the scope of the declaration in $B_i$. 1.6.4 Explicit Access Control # Classes and structures introduce a new scope for their members. If p is an object of a class with a field (member) x , then the use of x in p.x refers to field x in the class definition. In analogy with block structure, the scope of a member declaration x in a class C extends to any subclass C' , except if C' has a local declaration of the same name x . Through the use of keywords like public , private , and protected , object-oriented languages such as C++ or Java provide explicit control over access to member names in a superclass. These keywords support encapsulation by restricting access. Thus, private names are purposely given a scope that includes only the method declarations and definitions associated with that class and any friend classes (the C++ term). Protected names are accessible to subclasses. Public names are accessible from outside the class. In C++ , a class definition may be separated from the definitions of some or all of its methods. Therefore, a name x associated with the class C may have a region of the code that is outside its scope, followed by another region (a method definition) that is within its scope. In fact, regions inside and outside the scope may alternate, until all the methods have been defined Declarations and Definitions The apparently similar terms declaration and definition for programming-language concepts are actually quite different. Declarations tell us about the types of things, while definitions tell us about their values . Thus, int i is a declaration of i , while i = 1 is a definition of i. The difference is more significant when we deal with methods or other procedures. In C++, a method is declared in a class definition , by giving the types of the arguments and result of the method (often called the signature for the method). The method is then defined, i.e., the code for executing the method is given, in another place. Similarly, it is common to define a C function in one file and declare it in other files where the function is used. 1.6.5 Dynamic Scope # Technically, any scoping policy is dynamic if it is based on factor(s) that can be known only when the program executes. The term dynamic scope , however,usually refers to the following policy: a use of a name x refers to the declaration of x in the most recently called, not-yet-terminated, procedure with such a declaration. Dynamic scoping of this type appears only in special situations.We shall consider two examples of dynamic policies: macro expansion in the C preprocessor and method resolution in object-oriented programming. Example 1.7 : In the C program of Fig. 1.12, identifier a is a macro that stands for expression (x + 1) . But what is x ? We cannot resolve x statically,that is, in terms of the program text #define a (x+1) int x = 2; void b() { int x = 1; printf(\"%d\\n\", a); } void c() { printf(\"%d\\n\", a); } void main() { b(); c();} In fact, in order to interpret x, we must use the usual dynamic-scope rule.We examine all the function calls that are currently active , and we take the most recently called function that has a declaration of x. It is to this declaration that the use of x refers. In the example of Fig. 1.12, the function main first calls function b . As b executes, it prints the value of the macro a . Since (x + 1) must be substituted for a , we resolve this use of x to the declaration int x=1 in function b . The reason is that b has a declaration of x , so the (x + 1) in the printf in b refers to this x . Thus, the value printed is 2. After b finishes, and c is called, we again need to print the value of macro a . However, the only x accessible to c is the global x . The printf statement in c thus refers to this declaration of x , and value 3 is printed. Dynamic scope resolution is also essential for polymorphic procedures , those that have two or more definitions for the same name , depending only on the types of the arguments(inheritance and override).In some languages, such as ML (see Section 7.3.3), it is possible to determine statically types for all uses of names, in which case the compiler can replace each use of a procedure name p by a reference to the code for the proper procedure. However, in other languages, such as Java and C++,there are times when the compiler cannot make that determination. Example 1.8 : A distinguishing feature of object-oriented programming is the ability of each object to invoke the appropriate method in response to a message.In other words, the procedure called when x.m() is executed depends on the class of the object denoted by x at that time. A typical example is as follows: There is a class C with a method named m() . D is a subclass of C , and D has its own method named m() . There is a use of m of the form x.m() , where x is an object of class C Normally, it is impossible to tell at compile time whether x will be of class C or of the subclass D . If the method application occurs several times, it is highly likely that some will be on objects denoted by x that are in class C but not D , while others will be in class D . It is not until run-time that it can be decided which definition of m is the right one. Thus, the code generated by the compiler must determine the class of the object x , and call one or the other method named m . Analogy Between Static and Dynamic Scoping While there could be any number of static or dynamic policies for scoping,there is an interesting relationship between the normal (block-structured) static scoping rule and the normal dynamic policy. In a sense, the dynamic rule is to time as the static rule is to space . While the static rule asks us to find the declaration whose unit (block) most closely surrounds the physical location of the use, the dynamic rule asks us to find the declaration whose unit (procedure invocation) most closely surrounds the time of the use. \u7ffb\u8bd1\uff1a\u52a8\u6001\u4f5c\u7528\u57df\u5904\u7406\u65f6\u95f4\u7684\u65b9\u5f0f\u7c7b\u4f3c\u4e8e\u9759\u6001\u4f5c\u7528\u57df\u5904\u7406\u7a7a\u95f4\u7684\u65b9\u5f0f\u3002 1.6.6 Parameter Passing Mechanisms # All programming languages have a notion of a procedure, but they can differ in how these procedures get their arguments . In this section, we shall consider how the actual parameters (the parameters used in the call of a procedure) are associated with the formal parameters (those used in the procedure definition). Which mechanism is used determines how the calling-sequence code treats parameters. The great majority of languages use either call-by-value ,or call-by-reference , or both. We shall explain these terms, and another method known as call-by-name ,that is primarily of historical interest. Call-by-Value # In call-by-value , the actual parameter is evaluated (if it is an expression ) or copied (if it is a variable). The value is placed in the location belonging to the corresponding formal parameter of the called procedure. This method is used in C and Java, and is a common option in C++ , as well as in most other languages. Call-by-value has the effect that all computation involving the formal parameters done by the called procedure is local to that procedure, and the actual parameters themselves cannot be changed. Note, however, that in C we can pass a pointer to a variable to allow that variable to be changed by the callee . Likewise, array names passed as parameters in C, C++ , or Java give the called procedure what is in effect a pointer or reference to the array itself. Thus, if a is the name of an array of the calling procedure, and it is passed by value to corresponding formal parameter x , then an assignment such as x[i] = 2 really changes the array element a[i] to 2. The reason is that, although x gets a copy of the value of a, that value is really a pointer to the beginning of the area of the store where the array named a is located. Similarly, in Java, many variables are really references, or pointers, to the things they stand for. This observation(\u7ed3\u8bba) applies to arrays, strings, and objects of all classes. Even though Java uses call-by-value exclusively(\u4ec5\u4ec5), whenever we pass the name of an object to a called procedure, the value received by that procedure is in effect a pointer to the object. Thus, the called procedure is able to affect the value of the object itself. Call-by-Reference # In call-by-reference, the address of the actual parameter is passed to the callee as the value of the corresponding formal parameter. Uses of the formal parameter in the code of the callee are implemented by following this pointer to the location indicated by the caller. Changes to the formal parameter thus appear as changes to the actual parameter.If the actual parameter is an expression , however, then the expression is evaluated before the call, and its value stored in a location of its own. Changes to the formal parameter change the value in this location, but can have no effect on the data of the caller. Call-by-reference is used for \"ref \" parameters in C++ and is an option in many other languages. It is almost essential when the formal parameter is a large object, array, or structure. The reason is that strict call-by-value requires that the caller copy the entire actual parameter into the space belonging to the corresponding formal parameter. This copying gets expensive when then parameter is large. As we noted when discussing call-by-value, languages such as Java solve the problem of passing arrays, strings, or other objects by copying only a reference to those objects. The effect is that Java behaves as if it used call-by-reference for anything other than a basic type such as an integer or real. Call-by-Name # A third mechanism call-by-name was used in the early programming language Algol 60. It requires that the callee execute as if the actual parameter were substituted literally for the formal parameter in the code of the callee, as if the formal parameter were a macro standing for the actual parameter (with renaming of local names in the called procedure, to keep them distinct). When the actual parameter is an expression rather than a variable, some unintuitive behaviors occur, which is one reason this mechanism is not favored to day. 1.6.7 Aliasing # There is an interesting consequence of call-by-reference parameter passing or its simulation, as in Java, where references to objects are passed by value. It is possible that two formal parameters can refer to the same location; such variables are said to be aliases of one another. As a result, any two variables, which may appear to take their values from two distinct formal parameters, can become aliases of each other, as well. #","title":"1.6-Programming-Language-Basics.md"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#16-programming-language-basics","text":"In this section, we shall cover the most important terminology and distinctions that appear in the study of programming languages. It is not our purpose to cover all concepts or all the popular programming languages. We assume that the reader is familiar with at least one of C, C++ , C#, or Java, and may have encountered other languages as well.","title":"1.6 Programming Language Basics"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#161-the-staticdynamic-distinction","text":"Among the most important issues that we face when designing a compiler for a language is what decisions can the compiler make about a program. If a language uses a policy that allows the compiler to decide an issue, then we say that the language uses a static policy or that the issue can be decided at compile time . On the other hand, a policy that only allows a decision to be made when we execute the program is said to be a dynamic policy or to require a decision at run time . One issue on which we shall concentrate is the scope of declarations . The scope of a declaration of x is the region of the program in which uses of x refer to this declaration. A language uses static scope or lexical scope if it is possible to determine the scope of a declaration by looking only at the program. Otherwise, the language uses dynamic scope . With dynamic scope , as the program runs, the same use of x could refer to any of several different declarations of x . NOTE:\u4e0a\u9762\u5173\u4e8e scope \u7684\u5b9a\u4e49\u6240\u6307\u4ee3\u7684\u662fdeclaration\uff0c\u800c\u975evariable\u3002\u663e\u7136declaration\u80fd\u591f\u6307\u4ee3\u7684\u8303\u56f4\u8fdc\u8fdc\u5927\u4e8evariable\u3002 See also: Scope (computer science) Most languages, such as C and Java, use static scope . We shall discuss static scoping in Section 1.6.3. Example 1.3 : As another example of the static/dynamic distinction , consider the use of the term \"static\" as it applies to data in a Java class declaration. In Java, a variable is a name for a location in memory used to hold a data value . Here, \"static\" refers not to the scope of the variable, but rather to the ability of the compiler to determine the location in memory where the declared variable can be found. A declaration like public static int x; makes x a class variable and says that there is only one copy of x , no matter how many objects of this class are created. Moreover, the compiler can determine a location in memory where this integer x will be held. In contrast, had \" static \" been omitted from this declaration, then each object of the class would have its own location where x would be held, and the compiler could not determine all these places in advance of running the program. NOTE: \u901a\u8fc7\u5173\u952e\u5b57 static \u6765\u544a\u8bc9compiler\uff1a\u4f60\u53ef\u4ee5\u5728\u8fdb\u884c\u7f16\u8bd1\u7684\u65f6\u5019\u5c31\u7ed9 x \u5206\u914dmemory\uff1b\u800c\u4e0d\u662f\u5728\u8fd0\u884c\u7a0b\u5e8f\u7684\u65f6\u5019\u4f7f\u7528 new \u6765\u521b\u5efa\u5bf9\u8c61\u65f6\u518d\u5206\u914d x \u7684memory\uff0c\u663e\u7136\u8fd9\u662f\u6240\u8c13\u7684static policy\u3002 See also: Static (keyword)","title":"1.6.1 The Static/Dynamic Distinction"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#162-environments-and-states","text":"Another important distinction we must make when discussing programming languages is whether changes occurring as the program runs affect the values of data elements or affect the interpretation of names for that data. For example,the execution of an assignment such as x = y + 1 changes the value denoted by the name x . More specifically, the assignment changes the value in whatever location is denoted by x . It may be less clear that the location denoted by x can change at run time . For instance, as we discussed in Example 1.3, if x is not a static (or \" class \") variable, then every object of the class has its own location for an instance of variable x. In that case, the assignment to x can change any of those \"instance\" variables, depending on the object to which a method containing that assignment is applied. The association of names with locations in memory (the store) and then with values can be described by two mappings that change as the program runs (see Fig. 1.8): 1. The environment is a mapping from names to locations in the store. Since variables refer to locations (\"l-values\" in the terminology of C), we could alternatively define an environment as a mapping from names to variables ( location ). 2. The state is a mapping from locations in store to their values . That is, the state maps l-values to their corresponding r-values , in the terminology of C. Environments change according to the scope rules of a language. NOTE: environments \u548c scope \u662f\u4e0d\u540c\u7684\u6982\u5ff5\u3002 Example 1.4 : Consider the C program fragment in Fig. 1.9. Integer i is declared a global variable, and also declared as a variable local to function f .When f is executing, the environment adjusts so that name i refers to the location reserved for the i that is local to f , and any use of i , such as the assignment i = 3 shown explicitly, refers to that location. Typically, the local i is given a place on the run-time stack int i; /* global i */ void f(...) { int i; /* local i */ ... i = 3; /* use of local i */ ... } ... x = i + 1; /* use of global i */ Whenever a function g other than f is executing, uses of i cannot refer to the i that is local to f . Uses of name i in g must be within the scope of some other declaration of i . An example is the explicitly shown statement x = i+1 ,which is inside some procedure whose definition is not shown. The i in i + 1 presumably\uff08\u5927\u6982\uff09 refers to the global i . As in most languages, declarations in C must precede their use, so a function that comes before the global i cannot refer to it. The environment and state mappings in Fig. 1.8 are dynamic , but there are a few exceptions: Static versus dynamic binding of names to locations . Most binding of names to locations is dynamic , and we discuss several approaches to this binding throughout the section. Some declarations, such as the global i in Fig. 1.9, can be given a location in the store once and for all, as the compiler generates object code(\u4e5f\u5c31\u662fglobal i \u662fstatic). Technically, the C compiler will assign a location in virtual memory for the global i ,leaving it to the loader and the operating system to determine where in the physical memory of the machine i will be located. However, we shall not worry about relocation issues such as these, which have no impact on compiling. Instead, we treat the address space that the compiler uses for its output code as if it gave physical memory locations . Static versus dynamic binding of locations to values . The binding of locations to values (the second stage in Fig. 1.8), is generally dynamic as well, since we cannot tell the value in a location until we run the program. Declared constants are an exception. For instance, the C definition c #define ARRAYSIZE 1000 binds the name ARRAYSIZE to the value 1000 statically. We can determine this binding by looking at the statement, and we know that it is impossible for this binding to change when the program executes.","title":"1.6.2 Environments and States"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#163-static-scope-and-block-structure","text":"Most languages, including C and its family, use static scope _. The scope rules for C are based on program structure ; the scope of a declaration is determined implicitly(\u9690\u5f0f\u5730) by where the declaration appears in the program. Later languages,such as C++, Java, and C#, also provide explicit control over scopes through the use of keywords like public , private , and protected . In this section we consider static-scope rules for a language with blocks , where a block is a grouping of declarations and statements . C uses braces { and } to delimit(\u5b9a\u754c) a block ; the alternative use of begin and end for the same purpose dates back to Algol. NOTE : \u5728 c++ \u4e2d\uff0cblock\u9664\u4e86\u53ef\u4ee5\u7528\u4e8e\u786e\u5b9ascope\u5916\uff0cblock\u4e5f\u53ef\u4ee5\u51b3\u5b9a storage duration \u3002\u4ee5\u4e0b\u662f\u6458\u81eacppreference \u7684 Storage class specifiers \u7ae0\u8282\uff1a automatic storage duration. The storage for the object is allocated at the beginning of the enclosing code block and deallocated at the end. All local objects have this storage duration, except those declared static , extern or thread_local . Names , Identifiers , and Variables Although the terms \"name\" and \"variable,\" often refer to the same thing,we use them carefully to distinguish between compile-time names and the run-time locations denoted by names. An identifier is a string of characters, typically letters or digits, that refers to (identifies) an entity , such as a data object , a procedure , a class ,or a type . All identifiers are names , but not all names are identifiers .Names can also be expressions. For example, the name x.y might denote the field y of a structure denoted by x . Here, x and y are identifiers, while x.y is a name, but not an identifier . Composite names like x.y are called qualified names . SUMMARY :\u663e\u7136\uff0cname\u662f\u6bd4identifier\u66f4\u52a0\u5bbd\u6cdb\u7684\u6982\u5ff5\u3002 A variable refers to a particular location of the store. It is common for the same identifier to be declared more than once; each such declaration introduces a new variable . Even if each identifier is declared just once, an identifier local to a recursive procedure will refer to different locations of the store at different times. SUMMARY :\u663e\u7136\uff0cvariable\u66f4\u52a0\u503e\u5411\u4e8e\u8fd0\u884c\u65f6\u6982\u5ff5 Example 1.5 \uff1aC static-scope policy is as follows A C program consists of a sequence of top-level declarations of variables and functions Functions may have variable declarations within them, where variables include local variables and parameters . The scope of each such declaration is restricted to the function in which it appears. The scope of a top-level declaration of a name x consists of the entire program that follows, with the exception of those statements that lie within a function that also has a declaration of x The additional detail regarding the C static-scope policy deals with variable declarations within statements . We examine such declarations next and in Example 1.6. 2 Procedures, Functions, and Methods To avoid saying procedures, functions, or methods, each time we want to talk about a subprogram that may be called, we shall usually refer to all of them as procedures .The exception is that when talking explicitly of programs in languages like C that have only functions, we shall refer to them as functions .Or, if we are discussing a language like Java that has only methods , we shall use that term instead. A function generally returns a value of some type (the return type), while a procedure does not return any value. C and similar languages, which have only functions , treat procedures as functions that have a special return type void , to signify no return value. Object-oriented languages like Java and C++ use the term methods .These can behave like either functions or procedures, but are associated with a particular class. In C, the syntax of blocks is given by One type of statement is a block(block\u662f\u4e00\u79cdstatement). Blocks can appear anywhere that other types of statements, such as assignment statements, can appear. A block is a sequence of declarations followed by a sequence of statements,all surrounded by braces Note that this syntax allows blocks to be nested inside each other. This nesting property is referred to as block structure . The C family of languages has block structure, except that a function may not be defined inside another function. We say that a declaration D belongs to a block B if B is the most closely nested block containing D ; that is, D is located within B , but not within any block that is nested within B . The static-scope rule for variable declarations in block-structured languages is as follows. If declaration D of name x belongs to block B , then the scope of D is all of B , except for any blocks B' nested to any depth within B , in which x is redeclared. Here, x is redeclared in B' if some other declaration D of the same name x belongs to B' . An equivalent way to express this rule is to focus on a use of a name x .Let $B_1$, $B_2$, ... $B_k$ be all the blocks that surround this use of x , with $B_k$ the smallest, nested within $B_{k-1}$ , which is nested within $B_{k-2}$ , and so on. Search for the largest i such that there is a declaration of x belonging to $B_i$. This use of x refers to the declaration in $B_i$ . Alternatively, this use of x is within the scope of the declaration in $B_i$.","title":"1.6.3 Static Scope and Block Structure"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#164-explicit-access-control","text":"Classes and structures introduce a new scope for their members. If p is an object of a class with a field (member) x , then the use of x in p.x refers to field x in the class definition. In analogy with block structure, the scope of a member declaration x in a class C extends to any subclass C' , except if C' has a local declaration of the same name x . Through the use of keywords like public , private , and protected , object-oriented languages such as C++ or Java provide explicit control over access to member names in a superclass. These keywords support encapsulation by restricting access. Thus, private names are purposely given a scope that includes only the method declarations and definitions associated with that class and any friend classes (the C++ term). Protected names are accessible to subclasses. Public names are accessible from outside the class. In C++ , a class definition may be separated from the definitions of some or all of its methods. Therefore, a name x associated with the class C may have a region of the code that is outside its scope, followed by another region (a method definition) that is within its scope. In fact, regions inside and outside the scope may alternate, until all the methods have been defined Declarations and Definitions The apparently similar terms declaration and definition for programming-language concepts are actually quite different. Declarations tell us about the types of things, while definitions tell us about their values . Thus, int i is a declaration of i , while i = 1 is a definition of i. The difference is more significant when we deal with methods or other procedures. In C++, a method is declared in a class definition , by giving the types of the arguments and result of the method (often called the signature for the method). The method is then defined, i.e., the code for executing the method is given, in another place. Similarly, it is common to define a C function in one file and declare it in other files where the function is used.","title":"1.6.4 Explicit Access Control"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#165-dynamic-scope","text":"Technically, any scoping policy is dynamic if it is based on factor(s) that can be known only when the program executes. The term dynamic scope , however,usually refers to the following policy: a use of a name x refers to the declaration of x in the most recently called, not-yet-terminated, procedure with such a declaration. Dynamic scoping of this type appears only in special situations.We shall consider two examples of dynamic policies: macro expansion in the C preprocessor and method resolution in object-oriented programming. Example 1.7 : In the C program of Fig. 1.12, identifier a is a macro that stands for expression (x + 1) . But what is x ? We cannot resolve x statically,that is, in terms of the program text #define a (x+1) int x = 2; void b() { int x = 1; printf(\"%d\\n\", a); } void c() { printf(\"%d\\n\", a); } void main() { b(); c();} In fact, in order to interpret x, we must use the usual dynamic-scope rule.We examine all the function calls that are currently active , and we take the most recently called function that has a declaration of x. It is to this declaration that the use of x refers. In the example of Fig. 1.12, the function main first calls function b . As b executes, it prints the value of the macro a . Since (x + 1) must be substituted for a , we resolve this use of x to the declaration int x=1 in function b . The reason is that b has a declaration of x , so the (x + 1) in the printf in b refers to this x . Thus, the value printed is 2. After b finishes, and c is called, we again need to print the value of macro a . However, the only x accessible to c is the global x . The printf statement in c thus refers to this declaration of x , and value 3 is printed. Dynamic scope resolution is also essential for polymorphic procedures , those that have two or more definitions for the same name , depending only on the types of the arguments(inheritance and override).In some languages, such as ML (see Section 7.3.3), it is possible to determine statically types for all uses of names, in which case the compiler can replace each use of a procedure name p by a reference to the code for the proper procedure. However, in other languages, such as Java and C++,there are times when the compiler cannot make that determination. Example 1.8 : A distinguishing feature of object-oriented programming is the ability of each object to invoke the appropriate method in response to a message.In other words, the procedure called when x.m() is executed depends on the class of the object denoted by x at that time. A typical example is as follows: There is a class C with a method named m() . D is a subclass of C , and D has its own method named m() . There is a use of m of the form x.m() , where x is an object of class C Normally, it is impossible to tell at compile time whether x will be of class C or of the subclass D . If the method application occurs several times, it is highly likely that some will be on objects denoted by x that are in class C but not D , while others will be in class D . It is not until run-time that it can be decided which definition of m is the right one. Thus, the code generated by the compiler must determine the class of the object x , and call one or the other method named m . Analogy Between Static and Dynamic Scoping While there could be any number of static or dynamic policies for scoping,there is an interesting relationship between the normal (block-structured) static scoping rule and the normal dynamic policy. In a sense, the dynamic rule is to time as the static rule is to space . While the static rule asks us to find the declaration whose unit (block) most closely surrounds the physical location of the use, the dynamic rule asks us to find the declaration whose unit (procedure invocation) most closely surrounds the time of the use. \u7ffb\u8bd1\uff1a\u52a8\u6001\u4f5c\u7528\u57df\u5904\u7406\u65f6\u95f4\u7684\u65b9\u5f0f\u7c7b\u4f3c\u4e8e\u9759\u6001\u4f5c\u7528\u57df\u5904\u7406\u7a7a\u95f4\u7684\u65b9\u5f0f\u3002","title":"1.6.5 Dynamic Scope"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#166-parameter-passing-mechanisms","text":"All programming languages have a notion of a procedure, but they can differ in how these procedures get their arguments . In this section, we shall consider how the actual parameters (the parameters used in the call of a procedure) are associated with the formal parameters (those used in the procedure definition). Which mechanism is used determines how the calling-sequence code treats parameters. The great majority of languages use either call-by-value ,or call-by-reference , or both. We shall explain these terms, and another method known as call-by-name ,that is primarily of historical interest.","title":"1.6.6 Parameter Passing Mechanisms"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#call-by-value","text":"In call-by-value , the actual parameter is evaluated (if it is an expression ) or copied (if it is a variable). The value is placed in the location belonging to the corresponding formal parameter of the called procedure. This method is used in C and Java, and is a common option in C++ , as well as in most other languages. Call-by-value has the effect that all computation involving the formal parameters done by the called procedure is local to that procedure, and the actual parameters themselves cannot be changed. Note, however, that in C we can pass a pointer to a variable to allow that variable to be changed by the callee . Likewise, array names passed as parameters in C, C++ , or Java give the called procedure what is in effect a pointer or reference to the array itself. Thus, if a is the name of an array of the calling procedure, and it is passed by value to corresponding formal parameter x , then an assignment such as x[i] = 2 really changes the array element a[i] to 2. The reason is that, although x gets a copy of the value of a, that value is really a pointer to the beginning of the area of the store where the array named a is located. Similarly, in Java, many variables are really references, or pointers, to the things they stand for. This observation(\u7ed3\u8bba) applies to arrays, strings, and objects of all classes. Even though Java uses call-by-value exclusively(\u4ec5\u4ec5), whenever we pass the name of an object to a called procedure, the value received by that procedure is in effect a pointer to the object. Thus, the called procedure is able to affect the value of the object itself.","title":"Call-by-Value"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#call-by-reference","text":"In call-by-reference, the address of the actual parameter is passed to the callee as the value of the corresponding formal parameter. Uses of the formal parameter in the code of the callee are implemented by following this pointer to the location indicated by the caller. Changes to the formal parameter thus appear as changes to the actual parameter.If the actual parameter is an expression , however, then the expression is evaluated before the call, and its value stored in a location of its own. Changes to the formal parameter change the value in this location, but can have no effect on the data of the caller. Call-by-reference is used for \"ref \" parameters in C++ and is an option in many other languages. It is almost essential when the formal parameter is a large object, array, or structure. The reason is that strict call-by-value requires that the caller copy the entire actual parameter into the space belonging to the corresponding formal parameter. This copying gets expensive when then parameter is large. As we noted when discussing call-by-value, languages such as Java solve the problem of passing arrays, strings, or other objects by copying only a reference to those objects. The effect is that Java behaves as if it used call-by-reference for anything other than a basic type such as an integer or real.","title":"Call-by-Reference"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#call-by-name","text":"A third mechanism call-by-name was used in the early programming language Algol 60. It requires that the callee execute as if the actual parameter were substituted literally for the formal parameter in the code of the callee, as if the formal parameter were a macro standing for the actual parameter (with renaming of local names in the called procedure, to keep them distinct). When the actual parameter is an expression rather than a variable, some unintuitive behaviors occur, which is one reason this mechanism is not favored to day.","title":"Call-by-Name"},{"location":"Chapter-1-Introduction/1.6-Programming-Language-Basics/#167-aliasing","text":"There is an interesting consequence of call-by-reference parameter passing or its simulation, as in Java, where references to objects are passed by value. It is possible that two formal parameters can refer to the same location; such variables are said to be aliases of one another. As a result, any two variables, which may appear to take their values from two distinct formal parameters, can become aliases of each other, as well.","title":"1.6.7 Aliasing"},{"location":"Chapter-1-Introduction/wikipedia-Comparison-of-parser-generators/","text":"Comparison of parser generators Comparison of parser generators # This is a list of notable lexer generators and parser generators for various language classes.","title":"wikipedia Comparison of parser generators"},{"location":"Chapter-1-Introduction/wikipedia-Comparison-of-parser-generators/#comparison-of-parser-generators","text":"This is a list of notable lexer generators and parser generators for various language classes.","title":"Comparison of parser generators"},{"location":"Chapter-1-Introduction/wikipedia-Semantic-analysis(compilers)/","text":"Semantic analysis (compilers) Semantic analysis (compilers) # Semantic analysis or context sensitive analysis is a process in compiler construction, usually after parsing , to gather necessary semantic information from the source code .[ 1] It usually includes type checking , or makes sure a variable is declared before use which is impossible to describe in the extended Backus\u2013Naur form and thus not easily detected during parsing.","title":"wikipedia Semantic analysis(compilers)"},{"location":"Chapter-1-Introduction/wikipedia-Semantic-analysis(compilers)/#semantic-analysis-compilers","text":"Semantic analysis or context sensitive analysis is a process in compiler construction, usually after parsing , to gather necessary semantic information from the source code .[ 1] It usually includes type checking , or makes sure a variable is declared before use which is impossible to describe in the extended Backus\u2013Naur form and thus not easily detected during parsing.","title":"Semantic analysis (compilers)"},{"location":"Chapter-1-Introduction/wikipedia-Symbol-table/","text":"Symbol table # In computer science , a symbol table is a data structure used by a language translator such as a compiler or interpreter , where each identifier (a.k.a. symbol ) in a program's source code is associated with information relating to its declaration or appearance in the source. In other words, the entries of a symbol table store the information related to the entry's corresponding symbol.","title":"[Symbol table](https://en.wikipedia.org/wiki/Symbol_table)"},{"location":"Chapter-1-Introduction/wikipedia-Symbol-table/#symbol-table","text":"In computer science , a symbol table is a data structure used by a language translator such as a compiler or interpreter , where each identifier (a.k.a. symbol ) in a program's source code is associated with information relating to its declaration or appearance in the source. In other words, the entries of a symbol table store the information related to the entry's corresponding symbol.","title":"Symbol table"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/","text":"Introduction # This chapter is an introduction to the compiling techniques in Chapters 3 through 6 of this book. It illustrates the techniques by developing a working Java program that translates representative programming language statements into three-address code , an intermediate representation. We start small by creating a syntax-directed translator that maps infix arithmetic expressions into postfix expressions. We then extend this translator to map code fragments as shown in Fig. 2.1 into three-address code of the form in Fig. 2.2. TIPS: It is not easy to implement mapping infix arithmetic expressions into postfix expressions, there are some algorithms Shunting-yard algorithm { int i; int j; float[100] a; float v; float x; while ( true ) { do i = i+1; while ( a[i] < v ); do j = j-1; while ( a[j] > v ); if ( i >= j ) break; x = a[i]; a[i] = a[j]; a[j] = x; } } Figure 2.1: A code fragment to be translated 1: i = i + 1 2: t1 = a [ i ] 3: if t1 < v goto 1 4: j = j - 1 5: t2 = a [ j ] 6: if t2 > v goto 4 7: ifFalse i >= j goto 9 8: goto 14 9: x = a [ i ] 10: t3 = a [ j ] 11: a [ i ] = t3 12: a [ j ] = x 13: goto 1 14: Figure 2.2: Simplified intermediate code for the program fragment in Fig. 2.1","title":2},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/#introduction","text":"This chapter is an introduction to the compiling techniques in Chapters 3 through 6 of this book. It illustrates the techniques by developing a working Java program that translates representative programming language statements into three-address code , an intermediate representation. We start small by creating a syntax-directed translator that maps infix arithmetic expressions into postfix expressions. We then extend this translator to map code fragments as shown in Fig. 2.1 into three-address code of the form in Fig. 2.2. TIPS: It is not easy to implement mapping infix arithmetic expressions into postfix expressions, there are some algorithms Shunting-yard algorithm { int i; int j; float[100] a; float v; float x; while ( true ) { do i = i+1; while ( a[i] < v ); do j = j-1; while ( a[j] > v ); if ( i >= j ) break; x = a[i]; a[i] = a[j]; a[j] = x; } } Figure 2.1: A code fragment to be translated 1: i = i + 1 2: t1 = a [ i ] 3: if t1 < v goto 1 4: j = j - 1 5: t2 = a [ j ] 6: if t2 > v goto 4 7: ifFalse i >= j goto 9 8: goto 14 9: x = a [ i ] 10: t3 = a [ j ] 11: a [ i ] = t3 12: a [ j ] = x 13: goto 1 14: Figure 2.2: Simplified intermediate code for the program fragment in Fig. 2.1","title":"Introduction"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.1-Introduction/","text":"2.1 Introduction 2.1 Introduction # For compiler, what can context-free grammar do? Specify syntax as described in Section 2.2 Help guide the translation of programs, a grammar-oriented compiling technique known as syntax-directed translation introduced in Section 2.4 Parsing or syntax analysis is introduced in Section 2.4.","title":"2.1-Introduction"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.1-Introduction/#21-introduction","text":"For compiler, what can context-free grammar do? Specify syntax as described in Section 2.2 Help guide the translation of programs, a grammar-oriented compiling technique known as syntax-directed translation introduced in Section 2.4 Parsing or syntax analysis is introduced in Section 2.4.","title":"2.1 Introduction"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/","text":"2.2 Syntax Definition 2.2.1 Definition of Grammars 2.2.2 Derivations 2.2.3 Parse Trees 2.2.4 Ambiguity 2.2.5 Associativity of Operators 2.2.6 Precedence of Operators Generalizing the Expression Grammar of Example 2.6 2.2 Syntax Definition # In this section, we introduce a notation \u2014 the \" context-free grammar ,\" or \"grammar\" for short \u2014 that is used to specify the syntax of a language. Grammars will be used throughout this book to organize compiler front ends . TIPS: Wikipedia has a good explanation of context-free grammar TIPS: It is a convention that there is a section called language reference or language specification to describe grammar of the programming language in the official document of the programming language , such as The Python Language Reference use extended BNF notation to describe grammar of the language. The Java\u00ae Language Specification 2.2.1 Definition of Grammars # A context-free grammar has four components: 1. A set of terminal symbols , sometimes referred to as \"tokens.\" The terminals are the elementary symbols of the language defined by the grammar. 2. A set of nonterminals , sometimes called \"syntactic variables.\" Each nonterminal represents a set of strings of terminals, in a manner we shall describe. 3. A set of productions , where each production consists of a nonterminal, called the head or left side of the production, an arrow, and a sequence of terminals and/or nonterminals, called the body or right side of the production. 4. A designation of one of the nonterminals as the start symbol. Example 2.1 : $$ \\begin{align} list \\to list + digit \\tag {2.1}\\ list \\to list - digit \\tag {2.2}\\ list \\to digit \\tag {2.3}\\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\tag {2.4} \\end{align} $$ TIPS: Programmer should be sensitive of recursion , it is obvious that Context-free grammar above has the property of recursion , a grammar is informally called a recursive grammar if it contains production rules that are recursive . 2.2.2 Derivations # TIPS: In addition to using the word derivation /derive, word expand is also used a lot. I think word expand can visually reflect the process of expanding an inner nodes of the parse tree describing in 2.2.3. A grammar derives strings by beginning with the start symbol and repeatedly replacing a nonterminal by the body of a production for that nonterminal. The terminal strings that can be derived from the start symbol form the language defined by the grammar. TIPS: Here is the definition of language , please recall my question in the first introduction that what a programming language is. Parsing is the problem of taking a string of terminals and figuring out how to derive it from the start symbol of the grammar, and if it cannot be derived from the start symbol of the grammar, then reporting syntax errors within the string. TIPS: The process of parsing is the inverse of the process of deriving. The process of deriving is to generate terminal strings in the direction of generation while the process of parsing is to find productions in the opposite direction of production. They are like recursion and corecursion . TIPS: Implementation of parsing is described in chapter 2.4. 2.2.3 Parse Trees # TIPS: Parse tree reveal us of the power of tree structure, and it is an typical application of tree . Tree data structures figure prominently in compiling. TIPS: Parse tree is the software implementation of grammar. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0c so do tree structure. This is one of reason why tree structure is selected as implementation of grammar. TIPS: This chapter gives the algorithm of build a parse tree according to the grammar. 2.2.4 Ambiguity # TIPS: Programming language should be accurate and the designer should be careful to design grammar to avoid ambiguity. Source of ambiguity includes precedence of operators associativity of operators 2.2.5 Associativity of Operators # TIPS: We can define associativity by grammar, this reveals us of the power if context free grammar. By convention, $9+5+2$ is equivalent to $(9+5)+2$ and $9-5-2$ is equivalent to $(9-5)-2$. When an operand like 5 has operators to its left and right, conventions are needed for deciding which operator applies to that operand. We say that the operator + associates to the left, because an operand with plus signs on both sides of it belongs to the operator to its left. In most programming languages the four arithmetic operators, addition, subtraction, multiplication, and division are left-associative. Some common operators such as exponentiation are right-associative. As another example, the assignment operator in C and its descendants is right-associative; that is, the expression a=b=c is treated in the same way as the expression a=(b=c) . For clarity, sample grammar for left-associative operator and sample grammar for right-associative operator are listed below along their parse tree. Strings like a=b=c with a right-associative operator are generated by the following grammar: $$ \\begin{align} right \\to letter=right|letter \\ letter \\to a|b| \\dots |z \\end{align} $$ Strings like 1+2 , 1-2 with a left-associative operator are generated by the following grammar: $$ \\begin{align} list \\to list + digit \\ list \\to list - digit \\ list \\to digit \\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\end{align} $$ The contrast between a parse tree for a left-associative operator like - and a parse tree for a right-associative operator like = is shown by Fig. 2.7. Note that the parse tree for 9-5-2 grows down towards the left, whereas the parse tree for a=b=c grows down towards the right. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0ca grammar is informally called a recursive grammar if it contains production rules that are recursive . Here we can further subdivide, a grammar for a context-free language is left recursive if there exists a non-terminal symbol A that can be put through the production rules to produce a string with A (as the leftmost symbol), so grammars for left-associative operator are left recursive while grammars for right-associative operator are right recursive . For comparison, here are the ambiguous grammars and its parse tree. $$ list \\to list + digit | list - digit | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 $$ 2.2.6 Precedence of Operators # We say that * has higher precedence than + if * takes its operands before + does. In ordinary arithmetic, multiplication and division have higher precedence than addition and subtraction. Therefore, 5 is taken by * in b oth 9+5*2 and 9*5+2 ; i.e., the expressions are equivalent to 9+(5*2) and (9*5)+2 , respectively. Example 2.6 : A grammar for arithmetic expressions can be constructed from a table showing the associativity and precedence of operators. We start with the four common arithmetic operators and a precedence table, showing the operators in order of increasing precedence. Operators on the same line have the same associativity and precedence: left-associative + - left-associative: * / We create two nonterminals expr and term for the two levels of precedence( expr for + - and term for * / ), and an extra nonterminal factor for generating basic units in expressions. The basic units in expressions are presently digits and parenthesized expressions. $$ factor \\to digit | ( expr ) $$ Now consider the binary operators, * and / , that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\ | term / factor \\ | factor $$ Similarly, expr generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\ | expr - term \\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\ term \\to term * factor | term / factor | factor \\ factor \\to digit | ( expr ) $$ TIPS: The priorities decrease from bottom to top Generalizing the Expression Grammar of Example 2.6 # We can think of a factor as an expression that cannot b e \"torn apart\" by any operator. By \"torn apart,\" we mean that placing an operator next to any factor, on either side, does not cause any piece of the factor , other than the whole , to become an operand of that operator. If the factor is a parenthesized expression , the parentheses protect against such \"tearing,\" while if the factor is a single operand, it cannot be torn apart. A term (that is not also a factor) is an expression that can be torn apart by operators of the highest precedence: * and / , but not by the lower-precedence operators. An expression (that is not a term or factor) can be torn apart by any operator. We can generalize this idea to any number n of precedence levels. We need n + 1 nonterminals. The first, like factor in Example 2.6, can never be torn apart. Typically, the production bodies for this nonterminal are only single operands and parenthesized expressions . Then, for each precedence level, there is one nonterminal representing expressions that can be torn apart only by operators at that level or higher. Typically, the productions for this nonterminal have bodies representing uses of the operators at that level, plus one body that is just the nonterminal for the next higher level. TIPS: The reason one more is needed is that it is for factor TIPS: When generating a parse tree from a grammar for expression, the higher the priority of operator, the earlier it is built, and the deeper it is","title":"2.2-Syntax-Definition"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#22-syntax-definition","text":"In this section, we introduce a notation \u2014 the \" context-free grammar ,\" or \"grammar\" for short \u2014 that is used to specify the syntax of a language. Grammars will be used throughout this book to organize compiler front ends . TIPS: Wikipedia has a good explanation of context-free grammar TIPS: It is a convention that there is a section called language reference or language specification to describe grammar of the programming language in the official document of the programming language , such as The Python Language Reference use extended BNF notation to describe grammar of the language. The Java\u00ae Language Specification","title":"2.2 Syntax Definition"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#221-definition-of-grammars","text":"A context-free grammar has four components: 1. A set of terminal symbols , sometimes referred to as \"tokens.\" The terminals are the elementary symbols of the language defined by the grammar. 2. A set of nonterminals , sometimes called \"syntactic variables.\" Each nonterminal represents a set of strings of terminals, in a manner we shall describe. 3. A set of productions , where each production consists of a nonterminal, called the head or left side of the production, an arrow, and a sequence of terminals and/or nonterminals, called the body or right side of the production. 4. A designation of one of the nonterminals as the start symbol. Example 2.1 : $$ \\begin{align} list \\to list + digit \\tag {2.1}\\ list \\to list - digit \\tag {2.2}\\ list \\to digit \\tag {2.3}\\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\tag {2.4} \\end{align} $$ TIPS: Programmer should be sensitive of recursion , it is obvious that Context-free grammar above has the property of recursion , a grammar is informally called a recursive grammar if it contains production rules that are recursive .","title":"2.2.1 Definition of Grammars"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#222-derivations","text":"TIPS: In addition to using the word derivation /derive, word expand is also used a lot. I think word expand can visually reflect the process of expanding an inner nodes of the parse tree describing in 2.2.3. A grammar derives strings by beginning with the start symbol and repeatedly replacing a nonterminal by the body of a production for that nonterminal. The terminal strings that can be derived from the start symbol form the language defined by the grammar. TIPS: Here is the definition of language , please recall my question in the first introduction that what a programming language is. Parsing is the problem of taking a string of terminals and figuring out how to derive it from the start symbol of the grammar, and if it cannot be derived from the start symbol of the grammar, then reporting syntax errors within the string. TIPS: The process of parsing is the inverse of the process of deriving. The process of deriving is to generate terminal strings in the direction of generation while the process of parsing is to find productions in the opposite direction of production. They are like recursion and corecursion . TIPS: Implementation of parsing is described in chapter 2.4.","title":"2.2.2 Derivations"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#223-parse-trees","text":"TIPS: Parse tree reveal us of the power of tree structure, and it is an typical application of tree . Tree data structures figure prominently in compiling. TIPS: Parse tree is the software implementation of grammar. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0c so do tree structure. This is one of reason why tree structure is selected as implementation of grammar. TIPS: This chapter gives the algorithm of build a parse tree according to the grammar.","title":"2.2.3 Parse Trees"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#224-ambiguity","text":"TIPS: Programming language should be accurate and the designer should be careful to design grammar to avoid ambiguity. Source of ambiguity includes precedence of operators associativity of operators","title":"2.2.4 Ambiguity"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#225-associativity-of-operators","text":"TIPS: We can define associativity by grammar, this reveals us of the power if context free grammar. By convention, $9+5+2$ is equivalent to $(9+5)+2$ and $9-5-2$ is equivalent to $(9-5)-2$. When an operand like 5 has operators to its left and right, conventions are needed for deciding which operator applies to that operand. We say that the operator + associates to the left, because an operand with plus signs on both sides of it belongs to the operator to its left. In most programming languages the four arithmetic operators, addition, subtraction, multiplication, and division are left-associative. Some common operators such as exponentiation are right-associative. As another example, the assignment operator in C and its descendants is right-associative; that is, the expression a=b=c is treated in the same way as the expression a=(b=c) . For clarity, sample grammar for left-associative operator and sample grammar for right-associative operator are listed below along their parse tree. Strings like a=b=c with a right-associative operator are generated by the following grammar: $$ \\begin{align} right \\to letter=right|letter \\ letter \\to a|b| \\dots |z \\end{align} $$ Strings like 1+2 , 1-2 with a left-associative operator are generated by the following grammar: $$ \\begin{align} list \\to list + digit \\ list \\to list - digit \\ list \\to digit \\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\end{align} $$ The contrast between a parse tree for a left-associative operator like - and a parse tree for a right-associative operator like = is shown by Fig. 2.7. Note that the parse tree for 9-5-2 grows down towards the left, whereas the parse tree for a=b=c grows down towards the right. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0ca grammar is informally called a recursive grammar if it contains production rules that are recursive . Here we can further subdivide, a grammar for a context-free language is left recursive if there exists a non-terminal symbol A that can be put through the production rules to produce a string with A (as the leftmost symbol), so grammars for left-associative operator are left recursive while grammars for right-associative operator are right recursive . For comparison, here are the ambiguous grammars and its parse tree. $$ list \\to list + digit | list - digit | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 $$","title":"2.2.5 Associativity of Operators"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#226-precedence-of-operators","text":"We say that * has higher precedence than + if * takes its operands before + does. In ordinary arithmetic, multiplication and division have higher precedence than addition and subtraction. Therefore, 5 is taken by * in b oth 9+5*2 and 9*5+2 ; i.e., the expressions are equivalent to 9+(5*2) and (9*5)+2 , respectively. Example 2.6 : A grammar for arithmetic expressions can be constructed from a table showing the associativity and precedence of operators. We start with the four common arithmetic operators and a precedence table, showing the operators in order of increasing precedence. Operators on the same line have the same associativity and precedence: left-associative + - left-associative: * / We create two nonterminals expr and term for the two levels of precedence( expr for + - and term for * / ), and an extra nonterminal factor for generating basic units in expressions. The basic units in expressions are presently digits and parenthesized expressions. $$ factor \\to digit | ( expr ) $$ Now consider the binary operators, * and / , that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\ | term / factor \\ | factor $$ Similarly, expr generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\ | expr - term \\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\ term \\to term * factor | term / factor | factor \\ factor \\to digit | ( expr ) $$ TIPS: The priorities decrease from bottom to top","title":"2.2.6 Precedence of Operators"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.2-Syntax-Definition/#generalizing-the-expression-grammar-of-example-26","text":"We can think of a factor as an expression that cannot b e \"torn apart\" by any operator. By \"torn apart,\" we mean that placing an operator next to any factor, on either side, does not cause any piece of the factor , other than the whole , to become an operand of that operator. If the factor is a parenthesized expression , the parentheses protect against such \"tearing,\" while if the factor is a single operand, it cannot be torn apart. A term (that is not also a factor) is an expression that can be torn apart by operators of the highest precedence: * and / , but not by the lower-precedence operators. An expression (that is not a term or factor) can be torn apart by any operator. We can generalize this idea to any number n of precedence levels. We need n + 1 nonterminals. The first, like factor in Example 2.6, can never be torn apart. Typically, the production bodies for this nonterminal are only single operands and parenthesized expressions . Then, for each precedence level, there is one nonterminal representing expressions that can be torn apart only by operators at that level or higher. Typically, the productions for this nonterminal have bodies representing uses of the operators at that level, plus one body that is just the nonterminal for the next higher level. TIPS: The reason one more is needed is that it is for factor TIPS: When generating a parse tree from a grammar for expression, the higher the priority of operator, the earlier it is built, and the deeper it is","title":"Generalizing the Expression Grammar of Example 2.6"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.3-Syntax-Directed-Translation/","text":"2.3 Syntax-Directed Translation 2.3 Syntax-Directed Translation #","title":"2.3-Syntax-Directed-Translation"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.3-Syntax-Directed-Translation/#23-syntax-directed-translation","text":"","title":"2.3 Syntax-Directed Translation"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/","text":"2.4 Parsing 2.4.1 Top-Down Parsing 2.4.2 Predictive Parsing 2.4.3 When to Use $\\epsilon$-Productions 2.4 Parsing # Parsing is the process of determining how a string of terminals can be generated by a grammar . In discussing this problem, it is helpful to think of a parse tree being constructed, even though a compiler may not construct one, in practice. However, a parser must be capable of constructing the tree in principle, or else the translation cannot be guaranteed correct. NOTE: The concept of parsing is introduced in chapter 2.2.2 Derivations This section introduces a parsing method called \" recursive descent ,\" which can be used both to parse and to implement syntax-directed translators. A complete Java program, implementing the translation scheme of Fig. 2.15, appears in the next section. A viable alternative is to use a software tool to generate a translator directly from a translation scheme. Section 4.9 describes such a tool Yacc; it can implement the translation scheme of Fig. 2.15 without modification. For any context-free grammar there is a parser that takes at most $O (n^3)$ time to parse a string of n terminals. But cubic time is generally too expensive. Fortunately, for real programming languages, we can generally design a grammar that can be parsed quickly. Linear-time algorithms suffice to parse essentially all languages that arise in practice. Programming-language parsers almost always make a single left-to-right scan over the input, looking ahead one terminal at a time, and constructing pieces of the parse tree as they go. Most parsing methods fall into one of two classes, called the top-down and bottom-up methods. These terms refer to the order in which nodes in the parse tree are constructed. In top-down parsers, construction starts at the root and proceeds towards the leaves, while in bottom-up parsers, construction starts at the leaves and proceeds towards the root. The popularity of top-down parsers is due to the fact that efficient parsers can be constructed more easily by hand using top-down methods. Bottom-up parsing, however, can handle a larger class of grammars and translation schemes, so software tools for generating parsers directly from grammars often use bottom-up methods. TIPS: Wikipedia has a special section on parsing algorithms Parsing algorithms 2.4.1 Top-Down Parsing # We introduce top-down parsing by considering a grammar that is well-suited for this class of methods. Later in this section, we consider the construction of top-down parsers in general. The grammar in Fig. 2.16 generates a subset of the statements of C or Java. We use the boldface terminals if and for for the keywords \"if\" and \"for\", respectively, to emphasize that these character sequences are treated as units, i.e., as single terminal symbols. Further, the terminal expr represents expressions; a more complete grammar would use a nonterminal expr and have productions for nonterminal expr . Similarly, other is a terminal representing other statement constructs. The top-down construction of a parse tree like the one in Fig. 2.17, is done by starting with the root, labeled with the starting nonterminal stmt , and repeatedly performing the following two steps. At node N , labeled with nonterminal A , select one of the productions for A and construct children at N for the symbols in the production body. Find the next node at which a subtree is to be constructed, typically the leftmost unexpanded nonterminal of the tree. For some grammars, the above steps can be implemented during a single left-to-right scan of the input string. The current terminal being scanned in the input is frequently referred to as the lookahead symbol . Initially, the lookahead symbol is the first, i.e., leftmost, terminal of the input string. Figure 2.18 illustrates the construction of the parse tree in Fig. 2.17 for the input string for ( ; expr ; expr ) other Initially, the terminal for is the lookahead symbol , and the known part of the parse tree consists of the root, labeled with the starting nonterminal stmt in Fig. 2.18(a). The objective is to construct the remainder of the parse tree in such a way that the string generated by the parse tree matches the input string. For a match to occur, the nonterminal stmt in Fig. 2.18(a) must derive a string that starts with the lookahead symbol for . In the grammar of Fig. 2.16, there is just one production for stmt that can derive such a string, so we select it, and construct the children of the root labeled with the symbols in the production body. This expansion of the parse tree is shown in Fig. 2.18(b). Each of the three snapshots in Fig. 2.18 has arrows marking the lookahead symbol in the input and the node in the parse tree that is being considered. Once children are constructed at a node, we next consider the leftmost child. In Fig. 2.18(b), children have just been constructed at the root, and the leftmost child labeled with for is being considered. At the nonterminal node labeled optexpr , we repeat the process of selecting a production for a nonterminal. Productions with $\\epsilon$ as the body (\" $\\epsilon$ -productions\") require special treatment. For the moment, we use them as a default when no other production can be used; we return to them in Section 2.4.3. With nonterminal optexpr and lookahead ; , the $\\epsilon$ -productions is used, since ; does not match the only other production for optexpr , which has terminal expr as its body. In general, the selection of a production for a nonterminal may involve trial-and-error ; that is, we may have to try a production and backtrack to try another production if the first is found to be unsuitable. A production is unsuitable if, after using the production, we cannot complete the tree to match the input string. Backtracking is not needed, however, in an important special case called predictive parsing, which we discuss next. NOTE: There is a good illustration of Top-Down Parser in this blog . 2.4.2 Predictive Parsing # Recursive-descent parsing is a top-down method of syntax analysis in which a set of recursive procedures is used to process the input. One procedure is associated with each nonterminal of a grammar. Here, we consider a simple form of recursive-descent parsing , called predictive parsing , in which the lookahead symbol unambiguously determines the flow of control through the procedure body for each nonterminal . The sequence of procedure calls during the analysis of an input string implicitly defines a parse tree for the input, and can be used to build an explicit parse tree , if desired. The predictive parser in Fig. 2.19 consists of procedures for the nonterminals stmt and optexpr of the grammar in Fig. 2.16 and an additional procedure match, used to simplify the code for stmt and optexpr . Procedure match(t) compares its argument t with the lookahead symbol and advances to the next input terminal if they match. Thus match changes the value of variable lookahead, a global variable that holds the currently scanned input terminal. void stmt () { switch ( lookahead ) { case expr: match (expr); match (';'); break; case 'if': match ('if'); match ('('); match (expr); match (')'); stmt (); break; case 'for': match ('for'); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); break; case other; match (other); break; default: report (\"syntax error\"); } void optexpr () { if ( lookahead == expr ) match (expr); } void match (terminal t) { if ( lookahead == t ) lookahead = nextTerminal; else report (\"syntax error\"); } Figure 2.19: Pseudo code for a predictive parser NOTE: A predictive parsing approach is one that hardcodes the grammar into the program Parsing begins with a call of the procedure for the starting nonterminal stmt . With the same input as in Fig. 2.18, lookahead is initially the first terminal for . Procedure stmt executes code corresponding to the production $$ stmt \\to for ( optexpr ; optexpr ; optexpr ) stmt $$ In the code for the production body -- that is, the for case of procedure stmt --each terminal is matched with the lookahead symbol, and each nonterminal leads to a call of its procedure, in the following sequence of calls: match (for); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); Predictive parsing relies on information about the first symbols that can be generated by a production body. More precisely, let $\\alpha$ be a string of grammar symbols (terminals and/or nonterminals). We define $FIRST (\\alpha)$ to be the set of terminals that appear as the first symbols of one or more strings of terminals generated from $\\alpha$. If $\\alpha$ is $\\epsilon$ or can generate $\\epsilon$ , then $\\epsilon$ is also in $FIRST (\\alpha)$. NOTE: Only when $FIRST (\\alpha)$ is known, can flow of control determine. The details of how one computes $FIRST (\\alpha)$ are in Section 4.4.2. Here, we shall just use ad hoc reasoning to deduce the symbols in $FIRST (\\alpha)$; typically, $\\alpha$ will either begin with a terminal, which is therefore the only symbol in $FIRST (\\alpha)$, or $\\alpha$ will begin with a nonterminal whose production bodies begin with terminals, in which case these terminals are the only members of $FIRST (\\alpha)$. For example, with respect to the grammar of Fig. 2.16, the following are correct calculations of $FIRST$. FIRST (stmt) = {expr; if; for; other} FIRST(expr ;) = {expr} NOTE: FIRST (stmt) corresponds to the case of void stmt () . The FIRST sets must be considered if there are two pro ductions $A \\to \\alpha$ and $A \\to \\beta$ . Ignoring $\\epsilon$-productions for the moment, predictive parsing requires $FIRST (\\alpha)$ and $FIRST(\\beta )$ to b e disjoint. The lookahead symbol can then be used to decide which production to use; if the lookahead symbol is in $FIRST (\\alpha)$, then $\\alpha$ is used. Otherwise, if the lookahead symbol is in $FIRST (\\beta)$, then $\\beta$ is used. 2.4.3 When to Use $\\epsilon$-Productions #","title":"2.4-Parsing"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#24-parsing","text":"Parsing is the process of determining how a string of terminals can be generated by a grammar . In discussing this problem, it is helpful to think of a parse tree being constructed, even though a compiler may not construct one, in practice. However, a parser must be capable of constructing the tree in principle, or else the translation cannot be guaranteed correct. NOTE: The concept of parsing is introduced in chapter 2.2.2 Derivations This section introduces a parsing method called \" recursive descent ,\" which can be used both to parse and to implement syntax-directed translators. A complete Java program, implementing the translation scheme of Fig. 2.15, appears in the next section. A viable alternative is to use a software tool to generate a translator directly from a translation scheme. Section 4.9 describes such a tool Yacc; it can implement the translation scheme of Fig. 2.15 without modification. For any context-free grammar there is a parser that takes at most $O (n^3)$ time to parse a string of n terminals. But cubic time is generally too expensive. Fortunately, for real programming languages, we can generally design a grammar that can be parsed quickly. Linear-time algorithms suffice to parse essentially all languages that arise in practice. Programming-language parsers almost always make a single left-to-right scan over the input, looking ahead one terminal at a time, and constructing pieces of the parse tree as they go. Most parsing methods fall into one of two classes, called the top-down and bottom-up methods. These terms refer to the order in which nodes in the parse tree are constructed. In top-down parsers, construction starts at the root and proceeds towards the leaves, while in bottom-up parsers, construction starts at the leaves and proceeds towards the root. The popularity of top-down parsers is due to the fact that efficient parsers can be constructed more easily by hand using top-down methods. Bottom-up parsing, however, can handle a larger class of grammars and translation schemes, so software tools for generating parsers directly from grammars often use bottom-up methods. TIPS: Wikipedia has a special section on parsing algorithms Parsing algorithms","title":"2.4 Parsing"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#241-top-down-parsing","text":"We introduce top-down parsing by considering a grammar that is well-suited for this class of methods. Later in this section, we consider the construction of top-down parsers in general. The grammar in Fig. 2.16 generates a subset of the statements of C or Java. We use the boldface terminals if and for for the keywords \"if\" and \"for\", respectively, to emphasize that these character sequences are treated as units, i.e., as single terminal symbols. Further, the terminal expr represents expressions; a more complete grammar would use a nonterminal expr and have productions for nonterminal expr . Similarly, other is a terminal representing other statement constructs. The top-down construction of a parse tree like the one in Fig. 2.17, is done by starting with the root, labeled with the starting nonterminal stmt , and repeatedly performing the following two steps. At node N , labeled with nonterminal A , select one of the productions for A and construct children at N for the symbols in the production body. Find the next node at which a subtree is to be constructed, typically the leftmost unexpanded nonterminal of the tree. For some grammars, the above steps can be implemented during a single left-to-right scan of the input string. The current terminal being scanned in the input is frequently referred to as the lookahead symbol . Initially, the lookahead symbol is the first, i.e., leftmost, terminal of the input string. Figure 2.18 illustrates the construction of the parse tree in Fig. 2.17 for the input string for ( ; expr ; expr ) other Initially, the terminal for is the lookahead symbol , and the known part of the parse tree consists of the root, labeled with the starting nonterminal stmt in Fig. 2.18(a). The objective is to construct the remainder of the parse tree in such a way that the string generated by the parse tree matches the input string. For a match to occur, the nonterminal stmt in Fig. 2.18(a) must derive a string that starts with the lookahead symbol for . In the grammar of Fig. 2.16, there is just one production for stmt that can derive such a string, so we select it, and construct the children of the root labeled with the symbols in the production body. This expansion of the parse tree is shown in Fig. 2.18(b). Each of the three snapshots in Fig. 2.18 has arrows marking the lookahead symbol in the input and the node in the parse tree that is being considered. Once children are constructed at a node, we next consider the leftmost child. In Fig. 2.18(b), children have just been constructed at the root, and the leftmost child labeled with for is being considered. At the nonterminal node labeled optexpr , we repeat the process of selecting a production for a nonterminal. Productions with $\\epsilon$ as the body (\" $\\epsilon$ -productions\") require special treatment. For the moment, we use them as a default when no other production can be used; we return to them in Section 2.4.3. With nonterminal optexpr and lookahead ; , the $\\epsilon$ -productions is used, since ; does not match the only other production for optexpr , which has terminal expr as its body. In general, the selection of a production for a nonterminal may involve trial-and-error ; that is, we may have to try a production and backtrack to try another production if the first is found to be unsuitable. A production is unsuitable if, after using the production, we cannot complete the tree to match the input string. Backtracking is not needed, however, in an important special case called predictive parsing, which we discuss next. NOTE: There is a good illustration of Top-Down Parser in this blog .","title":"2.4.1 Top-Down Parsing"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#242-predictive-parsing","text":"Recursive-descent parsing is a top-down method of syntax analysis in which a set of recursive procedures is used to process the input. One procedure is associated with each nonterminal of a grammar. Here, we consider a simple form of recursive-descent parsing , called predictive parsing , in which the lookahead symbol unambiguously determines the flow of control through the procedure body for each nonterminal . The sequence of procedure calls during the analysis of an input string implicitly defines a parse tree for the input, and can be used to build an explicit parse tree , if desired. The predictive parser in Fig. 2.19 consists of procedures for the nonterminals stmt and optexpr of the grammar in Fig. 2.16 and an additional procedure match, used to simplify the code for stmt and optexpr . Procedure match(t) compares its argument t with the lookahead symbol and advances to the next input terminal if they match. Thus match changes the value of variable lookahead, a global variable that holds the currently scanned input terminal. void stmt () { switch ( lookahead ) { case expr: match (expr); match (';'); break; case 'if': match ('if'); match ('('); match (expr); match (')'); stmt (); break; case 'for': match ('for'); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); break; case other; match (other); break; default: report (\"syntax error\"); } void optexpr () { if ( lookahead == expr ) match (expr); } void match (terminal t) { if ( lookahead == t ) lookahead = nextTerminal; else report (\"syntax error\"); } Figure 2.19: Pseudo code for a predictive parser NOTE: A predictive parsing approach is one that hardcodes the grammar into the program Parsing begins with a call of the procedure for the starting nonterminal stmt . With the same input as in Fig. 2.18, lookahead is initially the first terminal for . Procedure stmt executes code corresponding to the production $$ stmt \\to for ( optexpr ; optexpr ; optexpr ) stmt $$ In the code for the production body -- that is, the for case of procedure stmt --each terminal is matched with the lookahead symbol, and each nonterminal leads to a call of its procedure, in the following sequence of calls: match (for); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); Predictive parsing relies on information about the first symbols that can be generated by a production body. More precisely, let $\\alpha$ be a string of grammar symbols (terminals and/or nonterminals). We define $FIRST (\\alpha)$ to be the set of terminals that appear as the first symbols of one or more strings of terminals generated from $\\alpha$. If $\\alpha$ is $\\epsilon$ or can generate $\\epsilon$ , then $\\epsilon$ is also in $FIRST (\\alpha)$. NOTE: Only when $FIRST (\\alpha)$ is known, can flow of control determine. The details of how one computes $FIRST (\\alpha)$ are in Section 4.4.2. Here, we shall just use ad hoc reasoning to deduce the symbols in $FIRST (\\alpha)$; typically, $\\alpha$ will either begin with a terminal, which is therefore the only symbol in $FIRST (\\alpha)$, or $\\alpha$ will begin with a nonterminal whose production bodies begin with terminals, in which case these terminals are the only members of $FIRST (\\alpha)$. For example, with respect to the grammar of Fig. 2.16, the following are correct calculations of $FIRST$. FIRST (stmt) = {expr; if; for; other} FIRST(expr ;) = {expr} NOTE: FIRST (stmt) corresponds to the case of void stmt () . The FIRST sets must be considered if there are two pro ductions $A \\to \\alpha$ and $A \\to \\beta$ . Ignoring $\\epsilon$-productions for the moment, predictive parsing requires $FIRST (\\alpha)$ and $FIRST(\\beta )$ to b e disjoint. The lookahead symbol can then be used to decide which production to use; if the lookahead symbol is in $FIRST (\\alpha)$, then $\\alpha$ is used. Otherwise, if the lookahead symbol is in $FIRST (\\beta)$, then $\\beta$ is used.","title":"2.4.2 Predictive Parsing"},{"location":"Chapter-2-A-Simple-Syntax-Directed-Translator/2.4-Parsing/#243-when-to-use-epsilon-productions","text":"","title":"2.4.3 When to Use $\\epsilon$-Productions"},{"location":"Chapter-3-Lexical-Analysis/","text":"Chapter 3 Lexical Analysis # In this chapter we show how to construct a lexical analyzer . To implement a lexical analyzer by hand, it helps to start with a diagram or other description for the lexemes of each token. We can then write code to identify each occurrence of each lexeme on the input and to return information about the token identified. NOTE: Manual implementation just to make it easier for the readers to understand how the lexical analyzer works, what should be focused on is how does it work automatically and how to implement it. We can also produce a lexical analyzer automatically by specifying the lexeme patterns to a lexical-analyzer generator and compiling those patterns into code that functions as a lexical analyzer . This approach makes it easier to modify a lexical analyzer , since we have only to rewrite the affected patterns, not the entire program. It also speeds up the process of implementing the lexical analyzer, since the programmer specifies the software at the very high level of patterns and relies on the generator to produce the detailed code. We shall introduce in Section 3.5 a lexical-analyzer generator called Lex (or Flex in a more recent embodiment). NOTE: In fact, the essence lies in how lex works. We begin the study of lexical-analyzer generators by introducing regular expressions, a convenient notation for specifying lexeme patterns. We show how this notation can be transformed, first into nondeterministic automata and then into deterministic automata. The latter two notations can be used as input to a \"driver,\" that is, code which simulates these automata and uses them as a guide to determining the next token. This driver and the specification of the automaton form the nucleus of the lexical analyzer. NOTE: Nucleus of the lexical analyzer is what we should focus on. NOTE: This chapter is mainly about algorithms of lexical analysis, I extend to include the algorithms relevant to lexical analysis in wikipedia which is listed in section wikipedia String","title":3},{"location":"Chapter-3-Lexical-Analysis/#chapter-3-lexical-analysis","text":"In this chapter we show how to construct a lexical analyzer . To implement a lexical analyzer by hand, it helps to start with a diagram or other description for the lexemes of each token. We can then write code to identify each occurrence of each lexeme on the input and to return information about the token identified. NOTE: Manual implementation just to make it easier for the readers to understand how the lexical analyzer works, what should be focused on is how does it work automatically and how to implement it. We can also produce a lexical analyzer automatically by specifying the lexeme patterns to a lexical-analyzer generator and compiling those patterns into code that functions as a lexical analyzer . This approach makes it easier to modify a lexical analyzer , since we have only to rewrite the affected patterns, not the entire program. It also speeds up the process of implementing the lexical analyzer, since the programmer specifies the software at the very high level of patterns and relies on the generator to produce the detailed code. We shall introduce in Section 3.5 a lexical-analyzer generator called Lex (or Flex in a more recent embodiment). NOTE: In fact, the essence lies in how lex works. We begin the study of lexical-analyzer generators by introducing regular expressions, a convenient notation for specifying lexeme patterns. We show how this notation can be transformed, first into nondeterministic automata and then into deterministic automata. The latter two notations can be used as input to a \"driver,\" that is, code which simulates these automata and uses them as a guide to determining the next token. This driver and the specification of the automaton form the nucleus of the lexical analyzer. NOTE: Nucleus of the lexical analyzer is what we should focus on. NOTE: This chapter is mainly about algorithms of lexical analysis, I extend to include the algorithms relevant to lexical analysis in wikipedia which is listed in section wikipedia String","title":"Chapter 3 Lexical Analysis"},{"location":"Chapter-3-Lexical-Analysis/3.1-The-Role-of-the-Lexical-Analyzer/","text":"3.1 The Role of the Lexical Analyzer # As the first phase of a compiler, the main task of the lexical analyzer is to read the input characters of the source program, group them into lexemes, and produce as output a sequence of tokens for each lexeme in the source program. The stream of tokens is sent to the parser for syntax analysis. It is common for the lexical analyzer to interact with the symbol table as well. When the lexical analyzer discovers a lexeme constituting an identifier, it needs to enter that lexeme into the symbol table. In some cases, information regarding the kind of identifier may be read from the symbol table by the lexical analyzer to assist it in determining the proper token it must pass to the parser.","title":"3.1-The-Role-of-the-Lexical-Analyzer"},{"location":"Chapter-3-Lexical-Analysis/3.1-The-Role-of-the-Lexical-Analyzer/#31-the-role-of-the-lexical-analyzer","text":"As the first phase of a compiler, the main task of the lexical analyzer is to read the input characters of the source program, group them into lexemes, and produce as output a sequence of tokens for each lexeme in the source program. The stream of tokens is sent to the parser for syntax analysis. It is common for the lexical analyzer to interact with the symbol table as well. When the lexical analyzer discovers a lexeme constituting an identifier, it needs to enter that lexeme into the symbol table. In some cases, information regarding the kind of identifier may be read from the symbol table by the lexical analyzer to assist it in determining the proper token it must pass to the parser.","title":"3.1 The Role of the Lexical Analyzer"},{"location":"Chapter-3-Lexical-Analysis/3.4-Recognition-of-Tokens/","text":"3.4 Recognition of Tokens Aho-Corasick algorithm 3.4 Recognition of Tokens # NOTE: This chapter describes conversion from regular-expression patterns to transition diagrams by hand, which is intended to help reader to understand the finite automaton (finite state machine) widely used in engineering described in section 3.5. In essence, a transition diagram is a visual representation of a finite state machine. In the previous section we learned how to express patterns using regular expressions. Now, we must study how to take the patterns for all the needed tokens and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns. Our discussion will make use of the following running example. $$ \\begin{align} stmt \\to if \\; expr \\; then \\; stmt \\ | if \\; expr \\; then \\; stmt \\; else \\; stmt \\ | ? \\ expr \\to term \\; relop \\; term \\ | term \\ term \\to id \\ | number \\end{align} $$ Figure 3.10: A grammar for branching statements Aho-Corasick algorithm # NOTE: The main content of this chapter is passed, but in the exercise, the author introduce Aho-Corasick algorithm and KMP algorithm which are interesting. The following exercises, up to Exercise 3.4.12, introduce the Aho-Corasick algorithm for recognizing a collection of keywords in a text string in time proportional to the length of the text and the sum of the length of the keywords. This algorithm uses a special form of transition diagram called a trie . A trie is a tree-structured transition diagram with distinct labels on the edges leading from a node to its children. Leaves of the trie represent recognized keywords. NOTE: A trie is a tree-structured transition diagram or finite automaton , do you see the power of tree structure here as I do Knuth, Morris, and Pratt presented an algorithm for recognizing a single keyword $b_1 b_2 \\dots b_n$(length of the word is n) in a text string. Here the trie is a transition diagram with n+ 1 states, 0 through n. State 0 is the initial state, and state n represents acceptance, that is, discovery of the keyword. From each state s from 0 through n - 1, there is a transition to state s + 1, labeled by symbol $b_{s+1}$ . For example, the trie for the keyword ababaa is: In order to process text strings rapidly and search those strings for a keyword, it is useful to define, for keyword $b_1 b_2 \\dots b_n$ and position s in that keyword (corresponding to state s of its trie), a failure function , f(s) , computed as in Fig. 3.19. The objective is that $b_1 b_2 \\dots b_{f(s)}$ is the longest proper prefix of $b_1 b_2 \\dots b_s$ that is also a suffix of $b_1 b_2 \\dots b_s$. The reason $f (s)$ is important is that if we are trying to match a text string for $b_1 b_2 \\dots b_n$, and we have matched the first s positions, but we then fail (i.e., the next position of the text string does not hold $b_{s+1}$ ), then $f (s)$ is the longest prefix of $b_1 b_2 \\dots b_n$ that could possibly match the text string up to the point we are at. Of course, the next character of the text string must be $b_{f (s)+1}$, or else we still have problems and must consider a yet shorter prefix, which will be $b_{f (f (s))}$. t = 0; f (1) = 0; for (s = 1; s < n; s + +) f while (t > 0 && b[s+1] ! = b[t+1]) t = f (t); if (b[s+1] == b[t+1]) { t = t + 1; f (s + 1) = t; } else f (s + 1) = 0; } Figure 3.19: Algorithm to compute the failure function for keyword $b_1 b_2 \\dots b_n$ As an example, the failure function for the trie constructed above for ababaa is: s 1 2 3 4 5 6 f(s) 0 0 1 2 3 1 For instance, states 3 and 1 represent prefixes aba and a , respectively. f (3) = 1 because a is the longest proper prefix of aba that is also a suffix of aba . Also, f (2) = 0 , because the longest proper prefix of ab that is also a suffix is the empty string. Aho and Corasick generalized the KMP algorithm to recognize any of a set of keywords in a text string. In this case, the trie is a true tree, with branching from the root. There is one state for every string that is a prefix (not necessarily proper) of any keyword. The parent of a state corresponding to string $b_1 b_2 \\dots b_n$ is the state that corresponds to $b_1 b_2 \\dots b_k$. A state is accepting if it corresponds to a complete keyword. For example, Fig. 3.21 shows the trie for the keywords he , she , his , and hers .","title":"3.4-Recognition-of-Tokens"},{"location":"Chapter-3-Lexical-Analysis/3.4-Recognition-of-Tokens/#34-recognition-of-tokens","text":"NOTE: This chapter describes conversion from regular-expression patterns to transition diagrams by hand, which is intended to help reader to understand the finite automaton (finite state machine) widely used in engineering described in section 3.5. In essence, a transition diagram is a visual representation of a finite state machine. In the previous section we learned how to express patterns using regular expressions. Now, we must study how to take the patterns for all the needed tokens and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns. Our discussion will make use of the following running example. $$ \\begin{align} stmt \\to if \\; expr \\; then \\; stmt \\ | if \\; expr \\; then \\; stmt \\; else \\; stmt \\ | ? \\ expr \\to term \\; relop \\; term \\ | term \\ term \\to id \\ | number \\end{align} $$ Figure 3.10: A grammar for branching statements","title":"3.4 Recognition of Tokens"},{"location":"Chapter-3-Lexical-Analysis/3.4-Recognition-of-Tokens/#aho-corasick-algorithm","text":"NOTE: The main content of this chapter is passed, but in the exercise, the author introduce Aho-Corasick algorithm and KMP algorithm which are interesting. The following exercises, up to Exercise 3.4.12, introduce the Aho-Corasick algorithm for recognizing a collection of keywords in a text string in time proportional to the length of the text and the sum of the length of the keywords. This algorithm uses a special form of transition diagram called a trie . A trie is a tree-structured transition diagram with distinct labels on the edges leading from a node to its children. Leaves of the trie represent recognized keywords. NOTE: A trie is a tree-structured transition diagram or finite automaton , do you see the power of tree structure here as I do Knuth, Morris, and Pratt presented an algorithm for recognizing a single keyword $b_1 b_2 \\dots b_n$(length of the word is n) in a text string. Here the trie is a transition diagram with n+ 1 states, 0 through n. State 0 is the initial state, and state n represents acceptance, that is, discovery of the keyword. From each state s from 0 through n - 1, there is a transition to state s + 1, labeled by symbol $b_{s+1}$ . For example, the trie for the keyword ababaa is: In order to process text strings rapidly and search those strings for a keyword, it is useful to define, for keyword $b_1 b_2 \\dots b_n$ and position s in that keyword (corresponding to state s of its trie), a failure function , f(s) , computed as in Fig. 3.19. The objective is that $b_1 b_2 \\dots b_{f(s)}$ is the longest proper prefix of $b_1 b_2 \\dots b_s$ that is also a suffix of $b_1 b_2 \\dots b_s$. The reason $f (s)$ is important is that if we are trying to match a text string for $b_1 b_2 \\dots b_n$, and we have matched the first s positions, but we then fail (i.e., the next position of the text string does not hold $b_{s+1}$ ), then $f (s)$ is the longest prefix of $b_1 b_2 \\dots b_n$ that could possibly match the text string up to the point we are at. Of course, the next character of the text string must be $b_{f (s)+1}$, or else we still have problems and must consider a yet shorter prefix, which will be $b_{f (f (s))}$. t = 0; f (1) = 0; for (s = 1; s < n; s + +) f while (t > 0 && b[s+1] ! = b[t+1]) t = f (t); if (b[s+1] == b[t+1]) { t = t + 1; f (s + 1) = t; } else f (s + 1) = 0; } Figure 3.19: Algorithm to compute the failure function for keyword $b_1 b_2 \\dots b_n$ As an example, the failure function for the trie constructed above for ababaa is: s 1 2 3 4 5 6 f(s) 0 0 1 2 3 1 For instance, states 3 and 1 represent prefixes aba and a , respectively. f (3) = 1 because a is the longest proper prefix of aba that is also a suffix of aba . Also, f (2) = 0 , because the longest proper prefix of ab that is also a suffix is the empty string. Aho and Corasick generalized the KMP algorithm to recognize any of a set of keywords in a text string. In this case, the trie is a true tree, with branching from the root. There is one state for every string that is a prefix (not necessarily proper) of any keyword. The parent of a state corresponding to string $b_1 b_2 \\dots b_n$ is the state that corresponds to $b_1 b_2 \\dots b_k$. A state is accepting if it corresponds to a complete keyword. For example, Fig. 3.21 shows the trie for the keywords he , she , his , and hers .","title":"Aho-Corasick algorithm"},{"location":"Chapter-3-Lexical-Analysis/3.6-Finite-Automata/","text":"3.6 Finite Automata # We shall now discover how Lex turns its input program into a lexical analyzer. At the heart of the transition is the formalism known as finite automata . These are essentially graphs, like transition diagrams, with a few differences: Finite automata are recognizers ; they simply say \"yes\" or \"no\" about each possible input string. Finite automata come in two favors: (a) Nondeterministic finite automata (NFA) have no restrictions on the labels of their edges. A symbol can label several edges out of the same state, and $\\epsilon$, the empty string, is a possible label. (b) Deterministic finite automata (DFA) have, for each state, and for each symbol of its input alphabet exactly one edge with that symbol leaving that state. Both deterministic and nondeterministic finite automata are capable of recognizing the same languages. In fact these languages are exactly the same languages, called the regular languages , that regular expressions can describe. There is a small lacuna: as we defined them, regular expressions cannot describe the empty language, since we would never want to use this pattern in practice. However, finite automata can define the empty language. In the theory, $\\phi$ is treated as an additional regular expression for the sole purpose of defining the empty language. NOTE: In chapter 3.4 Recognition of Tokens, there are description as below A trie is a tree-structured transition diagram or finite automaton , It is known to us all that tree is graph. In first paragraph, the author also regard finite automata as graph. 3.6.1 Nondeterministic Finite Automata # A nondeterministic finite automaton (NFA) consists of: A finite set of states S . A set of input symbols $ \\Sigma $, the input alphabet . We assume that $\\epsilon$, which stands for the empty string, is never a member of $ \\Sigma $. A transition function that gives, for each state, and for each symbol in $ \\Sigma \\cup {\\epsilon}$ a set of next states . A state $s_0$ from $S$ that is distinguished as the start state (or initial state). A set of states $F$ , a subset of $S$ , that is distinguished as the accepting states (or final states ). We can represent either an NFA or DFA by a transition graph , where the nodes are states and the labeled edges represent the transition function. There is an edge labeled a from state $s$ to state $t$ if and only if $t$ is one of the next states for state $s$ and input $a$. This graph(transition graph) is very much like a transition diagram , except: a) The same symbol can label edges from one state to several different states, and b) An edge may be labeled by $\\epsilon$, the empty string, instead of, or in addition to, symbols from the input alphabet. NOTE: Transition diagram is formally described in chapter 3.4.1 Transition Diagrams Example 3.14 : The transition graph for an NFA recognizing the language of regular expression $(a|b)*abb$ is shown in Fig. 3.24. This abstract example, describing all strings of a 's and b 's ending in the particular string abb , will be used throughout this section. It is similar to regular expressions that describe languages of real interest, however. For instance, an expression describing all files whose name ends in .o is any *.o , where any stands for any printable character. NOTE: There are two edges labeled with a from state 0 while this is not allowed in DFA. Below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. Following our convention for transition diagrams, the double circle around state 3 indicates that this state is accepting. Notice that the only ways to get from the start state 0 to the accepting state is to follow some path that stays in state 0 for a while, then go es to states 1, 2, and 3 by reading abb from the input. Thus, the only strings getting to the accepting state are those that end in abb . 3.6.2 Transition Tables # We can also represent an NFA by a transition table , whose rows correspond to states, and whose columns correspond to the input symbols and $\\epsilon$. The entry for a given state and input is the value of the transition function applied to those arguments. If the transition function has no information about that state-input pair, we put $\\phi$ in the table for the pair. Example 3.15 : The transition table for the NFA of Fig. 3.24 is shown in Fig. 3.25. The transition table has the advantage that we can easily find the transitions on a given state and input. Its disadvantage is that it takes a lot of space, when the input alphabet is large, yet most states do not have any moves on most of the input symbols. NOTE: Transition diagrams and transition table correspond to two kind of data structure describing finite automata 3.6.4 Deterministic Finite Automata # A deterministic finite automaton (DFA) is a special case of an NFA where: There are no moves on input $\\epsilon$ , and For each state s and input symbol a , there is exactly one edge out of s labeled a . If we are using a transition table to represent a DFA, then each entry is a single state. We may therefore represent this state without the curly braces that we use to form sets. While the NFA is an abstract representation of an algorithm to recognize the strings of a certain language, the DFA is a simple, concrete algorithm for recognizing strings. It is fortunate indeed that every regular expression and every NFA can be converted to a DFA accepting the same language, because it is the DFA that we really implement or simulate when building lexical analyzers. The following algorithm shows how to apply a DFA to a string. Algorithm 3.18 : Simulating a DFA. INPUT : An input string x terminated by an end-of-file character eof . A DFA D with start state $s_0$, accepting states F , and transition function move . OUTPUT : Answer \"yes\" if D accepts x ;\"no\" otherwise. METHOD : Apply the algorithm in Fig. 3.27 to the input string x . The function move(s, c) gives the state to which there is an edge from state s on input c . The function nextChar returns the next character of the input string x . Example 3.19 : In Fig. 3.28 we see the transition graph of a DFA accepting the language $(a|b)*abb$ , the same as that accepted by the NFA of Fig. 3.24. Given the input string ababb , this DFA enters the sequence of states 0, 1, 2, 1, 2, 3 and returns \" yes.\"","title":"3.6-Finite-Automata"},{"location":"Chapter-3-Lexical-Analysis/3.6-Finite-Automata/#36-finite-automata","text":"We shall now discover how Lex turns its input program into a lexical analyzer. At the heart of the transition is the formalism known as finite automata . These are essentially graphs, like transition diagrams, with a few differences: Finite automata are recognizers ; they simply say \"yes\" or \"no\" about each possible input string. Finite automata come in two favors: (a) Nondeterministic finite automata (NFA) have no restrictions on the labels of their edges. A symbol can label several edges out of the same state, and $\\epsilon$, the empty string, is a possible label. (b) Deterministic finite automata (DFA) have, for each state, and for each symbol of its input alphabet exactly one edge with that symbol leaving that state. Both deterministic and nondeterministic finite automata are capable of recognizing the same languages. In fact these languages are exactly the same languages, called the regular languages , that regular expressions can describe. There is a small lacuna: as we defined them, regular expressions cannot describe the empty language, since we would never want to use this pattern in practice. However, finite automata can define the empty language. In the theory, $\\phi$ is treated as an additional regular expression for the sole purpose of defining the empty language. NOTE: In chapter 3.4 Recognition of Tokens, there are description as below A trie is a tree-structured transition diagram or finite automaton , It is known to us all that tree is graph. In first paragraph, the author also regard finite automata as graph.","title":"3.6 Finite Automata"},{"location":"Chapter-3-Lexical-Analysis/3.6-Finite-Automata/#361-nondeterministic-finite-automata","text":"A nondeterministic finite automaton (NFA) consists of: A finite set of states S . A set of input symbols $ \\Sigma $, the input alphabet . We assume that $\\epsilon$, which stands for the empty string, is never a member of $ \\Sigma $. A transition function that gives, for each state, and for each symbol in $ \\Sigma \\cup {\\epsilon}$ a set of next states . A state $s_0$ from $S$ that is distinguished as the start state (or initial state). A set of states $F$ , a subset of $S$ , that is distinguished as the accepting states (or final states ). We can represent either an NFA or DFA by a transition graph , where the nodes are states and the labeled edges represent the transition function. There is an edge labeled a from state $s$ to state $t$ if and only if $t$ is one of the next states for state $s$ and input $a$. This graph(transition graph) is very much like a transition diagram , except: a) The same symbol can label edges from one state to several different states, and b) An edge may be labeled by $\\epsilon$, the empty string, instead of, or in addition to, symbols from the input alphabet. NOTE: Transition diagram is formally described in chapter 3.4.1 Transition Diagrams Example 3.14 : The transition graph for an NFA recognizing the language of regular expression $(a|b)*abb$ is shown in Fig. 3.24. This abstract example, describing all strings of a 's and b 's ending in the particular string abb , will be used throughout this section. It is similar to regular expressions that describe languages of real interest, however. For instance, an expression describing all files whose name ends in .o is any *.o , where any stands for any printable character. NOTE: There are two edges labeled with a from state 0 while this is not allowed in DFA. Below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. Following our convention for transition diagrams, the double circle around state 3 indicates that this state is accepting. Notice that the only ways to get from the start state 0 to the accepting state is to follow some path that stays in state 0 for a while, then go es to states 1, 2, and 3 by reading abb from the input. Thus, the only strings getting to the accepting state are those that end in abb .","title":"3.6.1 Nondeterministic Finite Automata"},{"location":"Chapter-3-Lexical-Analysis/3.6-Finite-Automata/#362-transition-tables","text":"We can also represent an NFA by a transition table , whose rows correspond to states, and whose columns correspond to the input symbols and $\\epsilon$. The entry for a given state and input is the value of the transition function applied to those arguments. If the transition function has no information about that state-input pair, we put $\\phi$ in the table for the pair. Example 3.15 : The transition table for the NFA of Fig. 3.24 is shown in Fig. 3.25. The transition table has the advantage that we can easily find the transitions on a given state and input. Its disadvantage is that it takes a lot of space, when the input alphabet is large, yet most states do not have any moves on most of the input symbols. NOTE: Transition diagrams and transition table correspond to two kind of data structure describing finite automata","title":"3.6.2 Transition Tables"},{"location":"Chapter-3-Lexical-Analysis/3.6-Finite-Automata/#364-deterministic-finite-automata","text":"A deterministic finite automaton (DFA) is a special case of an NFA where: There are no moves on input $\\epsilon$ , and For each state s and input symbol a , there is exactly one edge out of s labeled a . If we are using a transition table to represent a DFA, then each entry is a single state. We may therefore represent this state without the curly braces that we use to form sets. While the NFA is an abstract representation of an algorithm to recognize the strings of a certain language, the DFA is a simple, concrete algorithm for recognizing strings. It is fortunate indeed that every regular expression and every NFA can be converted to a DFA accepting the same language, because it is the DFA that we really implement or simulate when building lexical analyzers. The following algorithm shows how to apply a DFA to a string. Algorithm 3.18 : Simulating a DFA. INPUT : An input string x terminated by an end-of-file character eof . A DFA D with start state $s_0$, accepting states F , and transition function move . OUTPUT : Answer \"yes\" if D accepts x ;\"no\" otherwise. METHOD : Apply the algorithm in Fig. 3.27 to the input string x . The function move(s, c) gives the state to which there is an edge from state s on input c . The function nextChar returns the next character of the input string x . Example 3.19 : In Fig. 3.28 we see the transition graph of a DFA accepting the language $(a|b)*abb$ , the same as that accepted by the NFA of Fig. 3.24. Given the input string ababb , this DFA enters the sequence of states 0, 1, 2, 1, 2, 3 and returns \" yes.\"","title":"3.6.4 Deterministic Finite Automata"},{"location":"Chapter-3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/","text":"3.7 From Regular Expressions to Automata # NOTE: What this chapter describe is mainly three algorithms Name Function chapter Thompson's construction algorithm transforming a regular expression into an equivalent nondeterministic finite automaton (NFA) 3.7.4 Construction of an NFA from a Regular Expression subset construction algorithm converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language 3.7.1 Conversion of an NFA to a DFA DFA minimization transforming a given deterministic finite automaton (DFA) into an equivalent DFA that has a minimum number of states. Using the algorithms above, a regular expression can be converted to the corresponding best DFA. The regular expression is the notation of choice for describing lexical analyzers and other pattern-processing software, as was reflected in Section 3.5. However, implementation of that software requires the simulation of a DFA, as in Algorithm 3.18, or perhaps simulation of an NFA. Because an NFA often has a choice of move on an input symbol (as Fig. 3.24 does on input a from state 0) or on $\\epsilon$ (as Fig. 3.26 does from state 0), or even a choice of making a transition on $\\epsilon$ or on a real input symbol, its simulation is less straightforward than for a DFA. Thus often it is important to convert an NFA to a DFA that accepts the same language. 3.7.1 Conversion of an NFA to a DFA # NOTE: The subset construction explained in this book is too abstract to grasp, wikipedia's explanation about this algorithm is more intuitionistic. The general idea behind the subset construction is that each state of the constructed DFA corresponds to a set of NFA states. After reading input $a_1 a_2 \\dots a_n$, the DFA is in that state which corresponds to the set of states that the NFA can reach, from its start state, following paths labeled $a_1 a_2 \\dots a_n$. It is p ossible that the number of DFA states is exponential in the number of NFA states, which could lead to difficulties when we try to implement this DFA. However, part of the p ower of the automaton-based approach to lexical analysis is that for real languages, the NFA and DFA have approximately the same number of states, and the exponential behavior is not seen. Algorithm 3.20 : The subset construction of a DFA from an NFA. INPUT : An NFA N . OUTPUT : A DFA D accepting the same language as N . METHOD : Our algorithm constructs a transition table $D_{tran}$ for D . Each state of D is a set of NFA states, and we construct $D_{tran}$ so D will simulate \"in parallel\" all possible moves N can make on a given input string. Our first problem is to deal with $\\epsilon$-transitions of N properly. In Fig. 3.31 we see the definitions of several functions that describe basic computations on the states of N that are needed in the algorithm. Note that s is a single state of N , while T is a set of states of N . OPERATION DESCRIPTION $\\epsilon-closure(s)$ Set of NFA states reachable from NFA state s on $\\epsilon$-transitions alone. $\\epsilon-closure(T)$ Set of NFA states reachable from some NFA state s in set T on $\\epsilon$-transitions alone; $= \\cup _{s \\in T} {\\epsilon-closure(s)}$ move(T, a) Set of NFA states to which there is a transition on input symbol a from some state s in T . Figure 3.31: Operations on NFA states We must explore those sets of states that N can be in after seeing some input string. As a basis, before reading the first input symbol, N can be in any of the states of $\\epsilon-closure(s_0)$, where $s_0$ is its start state . For the induction, suppose that N can be in set of states T after reading input string x . If it next reads input a , then N can immediately go to any of the states in $move (T , a)$. However, after reading a , it may also make several $\\epsilon-transitions$; thus N could be in any state of $\\epsilon-closure(move(T, a))$ after reading input xa . Following these ideas, the construction of the set of D 's states, $D_{states}$, and its transition function $D_{tran}$, is shown in Fig. 3.32. The start state of D is $\\epsilon-closure(s_0)$, and the accepting states of D are all those sets of N 's states that include at least one accepting state of N . To complete our description of the subset construction, we need only to show how $\\epsilon-closure(T)$ is computed for any set of NFA states T . This process, shown in Fig. 3.33, is a straightforward search in a graph from a set of states. In this case, imagine that only the $\\epsilon$-labeled edges are available in the graph. 3.7.4 Construction of an NFA from a Regular Expression # We now give an algorithm for converting any regular expression to an NFA that defines the same language. The algorithm is syntax-directed , in the sense that it works recursively up the parse tree for the regular expression. For each sub expression the algorithm constructs an NFA with a single accepting state. NOTE: bottom-up NOTE: How to build a parse tree for a regular expression? Efficiently building a parse tree from a regular expression Algorithm 3.23 : The McNaughton-Yamada-Thompson algorithm to convert a regular expression to an NFA. INPUT : A regular expression r over alphabet $\\Sigma$. OUTPUT : An NFA N accepting L(r ) . METHOD: Begin by parsing r into its constituent subexpressions. The rules for constructing an NFA consist of basis rules for handling subexpressions with no operators, and inductive rules for constructing larger NFA's from the NFA's for the immediate subexpressions of a given expression. BASIS : For expression $\\epsilon$ construct the NFA Here, i is a new state, the start state of this NFA, and f is another new state, the accepting state for the NFA. For any subexpression a in $\\Sigma$, construct the NFA where again i and f are new states, the start and accepting states, respectively. Note that in both of the basis constructions, we construct a distinct NFA, with new states, for every occurrence of $\\epsilon$ or some a as a subexpression of r . INDUCTION : Suppose N (s) and N (t) are NFA's for regular expressions s and t , respectively. a) Suppose r = s|t . Then N (r ) , the NFA for r , is constructed as in Fig. 3.40. Here, i and f are new states, the start and accepting states of N (r ) , respectively. There are $\\epsilon$-transitions from i to the start states of N (s) and N (t) , and each of their accepting states have $\\epsilon$-transitions to the accepting state f . Note that the accepting states of N (s) and N (t) are not accepting in N (r) . Since any path from i to f must pass through either N (s) or N (t) exclusively, and since the label of that path is not changed by the $\\epsilon$'s leaving i or entering f , we conclude that N (r ) accepts $L(s) \\cup L(t)$, which is the same as L(r ) . That is, Fig. 3.40 is a correct construction for the union operator. b) Suppose r = st . Then construct N (r ) as in Fig. 3.41. The start state of N (s) becomes the start state of N (r ) , and the accepting state of N (t) is the only accepting state of N (r ) . The accepting state of N(s) and the start state of N (t) are merged into a single state, with all the transitions in or out of either state. A path from i to f in Fig. 3.41 must go first through N (s) , and therefore its label will begin with some string in L(s) . The path then continues through N (t) , so the path's label finishes with a string in L(t) . As we shall so on argue, accepting states never have edges out and start states never have edges in, so it is not possible for a path to re-enter N (s) after leaving it. Thus, N (r ) accepts exactly L(s)L(t) , and is a correct NFA for r = st . c) Suppose $r = s^ $. Then for r we construct the NFA N (r ) shown in Fig. 3.42. Here, i and f are new states, the start state and lone accepting state of N (r ) . To get from i to f , we can either follow the introduced path labeled $\\epsilon$, which takes care of the one string in $L(s)^0$ , or we can go to the start state of N(s) , through that NFA, then from its accepting state back to its start state zero or more times. These options allow N (r ) to accept all the strings in $L(s)^1$, $L(s)^2$, and so on, so the entire set of strings accepted by N (r ) is $L(s^ )$. d) Finally, suppose r = (s) . Then L(r ) = L(s) , and we can use the NFA N (s) as N (r ) . NOTE: Implementing a regular expression engine is far more complex than the algorithm described here. The method description in Algorithm 3.23 contains hints as to why the inductive construction works as it should. We shall not give a formal correctness proof, but we shall list several properties of the constructed NFA's, in addition to the all-important fact that N (r ) accepts language L(r ) . These properties are interesting in their own right, and helpful in making a formal proof. N (r ) has at most twice as many states as there are operators and operands in r . This bound follows from the fact that each step of the algorithm creates at most two new states. N (r ) has one start state and one accepting state . The accepting state has no outgoing transitions, and the start state has no incoming transitions. Each state of N (r ) other than the accepting state has either one outgoing transition on a symbol in $\\Sigma$ or two outgoing transitions, both on $\\epsilon$. Example 3.24 : Let us use Algorithm 3.23 to construct an NFA for $r = (a | b) ^*abb$. Figure 3.43 shows a parse tree for r that is analogous to the parse trees constructed for arithmetic expressions in Section 2.2.3. For subexpression $r_1$ , the first a , we construct the NFA: State numbers have been chosen for consistency with what follows. For $r_2$ we construct: We can now combine $N (r_1)$ and $N (r_2)$, using the construction of Fig. 3.40 to obtain the NFA for $r_3 =r_1|r_2$; this NFA is shown in Fig. 3.44. The NFA for $r_4= (r_3)$ is the same as that for $r_3$. The NFA for $r_5= (r_3)$ is then as shown in Fig. 3.45. We have used the construction in Fig. 3.42 to build this NFA from the NFA in Fig. 3.44. Now, consider subexpression $r_6$, which is another a . We use the basis construction for a again, but we must use new states. It is not permissible to reuse the NFA we constructed for $r_1$, even though $r_1$ and $r_6$ are the same expression. The NFA for $r_6$ is: To obtain the NFA for $r_7= r_5r_6$, we apply the construction of Fig. 3.41. We merge states 7 and $7^{'}$, yielding the NFA of Fig. 3.46. Continuing in this fashion with new NFA's for the two subexpressions b called $r_8$ and $r_{10}$, we eventually construct the NFA for tha$r = (a | b) ^*abb$ that we first met in Fig. 3.34.","title":"3.7-From-Regular-Expressions-to-Automata"},{"location":"Chapter-3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/#37-from-regular-expressions-to-automata","text":"NOTE: What this chapter describe is mainly three algorithms Name Function chapter Thompson's construction algorithm transforming a regular expression into an equivalent nondeterministic finite automaton (NFA) 3.7.4 Construction of an NFA from a Regular Expression subset construction algorithm converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language 3.7.1 Conversion of an NFA to a DFA DFA minimization transforming a given deterministic finite automaton (DFA) into an equivalent DFA that has a minimum number of states. Using the algorithms above, a regular expression can be converted to the corresponding best DFA. The regular expression is the notation of choice for describing lexical analyzers and other pattern-processing software, as was reflected in Section 3.5. However, implementation of that software requires the simulation of a DFA, as in Algorithm 3.18, or perhaps simulation of an NFA. Because an NFA often has a choice of move on an input symbol (as Fig. 3.24 does on input a from state 0) or on $\\epsilon$ (as Fig. 3.26 does from state 0), or even a choice of making a transition on $\\epsilon$ or on a real input symbol, its simulation is less straightforward than for a DFA. Thus often it is important to convert an NFA to a DFA that accepts the same language.","title":"3.7 From Regular Expressions to Automata"},{"location":"Chapter-3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/#371-conversion-of-an-nfa-to-a-dfa","text":"NOTE: The subset construction explained in this book is too abstract to grasp, wikipedia's explanation about this algorithm is more intuitionistic. The general idea behind the subset construction is that each state of the constructed DFA corresponds to a set of NFA states. After reading input $a_1 a_2 \\dots a_n$, the DFA is in that state which corresponds to the set of states that the NFA can reach, from its start state, following paths labeled $a_1 a_2 \\dots a_n$. It is p ossible that the number of DFA states is exponential in the number of NFA states, which could lead to difficulties when we try to implement this DFA. However, part of the p ower of the automaton-based approach to lexical analysis is that for real languages, the NFA and DFA have approximately the same number of states, and the exponential behavior is not seen. Algorithm 3.20 : The subset construction of a DFA from an NFA. INPUT : An NFA N . OUTPUT : A DFA D accepting the same language as N . METHOD : Our algorithm constructs a transition table $D_{tran}$ for D . Each state of D is a set of NFA states, and we construct $D_{tran}$ so D will simulate \"in parallel\" all possible moves N can make on a given input string. Our first problem is to deal with $\\epsilon$-transitions of N properly. In Fig. 3.31 we see the definitions of several functions that describe basic computations on the states of N that are needed in the algorithm. Note that s is a single state of N , while T is a set of states of N . OPERATION DESCRIPTION $\\epsilon-closure(s)$ Set of NFA states reachable from NFA state s on $\\epsilon$-transitions alone. $\\epsilon-closure(T)$ Set of NFA states reachable from some NFA state s in set T on $\\epsilon$-transitions alone; $= \\cup _{s \\in T} {\\epsilon-closure(s)}$ move(T, a) Set of NFA states to which there is a transition on input symbol a from some state s in T . Figure 3.31: Operations on NFA states We must explore those sets of states that N can be in after seeing some input string. As a basis, before reading the first input symbol, N can be in any of the states of $\\epsilon-closure(s_0)$, where $s_0$ is its start state . For the induction, suppose that N can be in set of states T after reading input string x . If it next reads input a , then N can immediately go to any of the states in $move (T , a)$. However, after reading a , it may also make several $\\epsilon-transitions$; thus N could be in any state of $\\epsilon-closure(move(T, a))$ after reading input xa . Following these ideas, the construction of the set of D 's states, $D_{states}$, and its transition function $D_{tran}$, is shown in Fig. 3.32. The start state of D is $\\epsilon-closure(s_0)$, and the accepting states of D are all those sets of N 's states that include at least one accepting state of N . To complete our description of the subset construction, we need only to show how $\\epsilon-closure(T)$ is computed for any set of NFA states T . This process, shown in Fig. 3.33, is a straightforward search in a graph from a set of states. In this case, imagine that only the $\\epsilon$-labeled edges are available in the graph.","title":"3.7.1 Conversion of an NFA to a DFA"},{"location":"Chapter-3-Lexical-Analysis/3.7-From-Regular-Expressions-to-Automata/#374-construction-of-an-nfa-from-a-regular-expression","text":"We now give an algorithm for converting any regular expression to an NFA that defines the same language. The algorithm is syntax-directed , in the sense that it works recursively up the parse tree for the regular expression. For each sub expression the algorithm constructs an NFA with a single accepting state. NOTE: bottom-up NOTE: How to build a parse tree for a regular expression? Efficiently building a parse tree from a regular expression Algorithm 3.23 : The McNaughton-Yamada-Thompson algorithm to convert a regular expression to an NFA. INPUT : A regular expression r over alphabet $\\Sigma$. OUTPUT : An NFA N accepting L(r ) . METHOD: Begin by parsing r into its constituent subexpressions. The rules for constructing an NFA consist of basis rules for handling subexpressions with no operators, and inductive rules for constructing larger NFA's from the NFA's for the immediate subexpressions of a given expression. BASIS : For expression $\\epsilon$ construct the NFA Here, i is a new state, the start state of this NFA, and f is another new state, the accepting state for the NFA. For any subexpression a in $\\Sigma$, construct the NFA where again i and f are new states, the start and accepting states, respectively. Note that in both of the basis constructions, we construct a distinct NFA, with new states, for every occurrence of $\\epsilon$ or some a as a subexpression of r . INDUCTION : Suppose N (s) and N (t) are NFA's for regular expressions s and t , respectively. a) Suppose r = s|t . Then N (r ) , the NFA for r , is constructed as in Fig. 3.40. Here, i and f are new states, the start and accepting states of N (r ) , respectively. There are $\\epsilon$-transitions from i to the start states of N (s) and N (t) , and each of their accepting states have $\\epsilon$-transitions to the accepting state f . Note that the accepting states of N (s) and N (t) are not accepting in N (r) . Since any path from i to f must pass through either N (s) or N (t) exclusively, and since the label of that path is not changed by the $\\epsilon$'s leaving i or entering f , we conclude that N (r ) accepts $L(s) \\cup L(t)$, which is the same as L(r ) . That is, Fig. 3.40 is a correct construction for the union operator. b) Suppose r = st . Then construct N (r ) as in Fig. 3.41. The start state of N (s) becomes the start state of N (r ) , and the accepting state of N (t) is the only accepting state of N (r ) . The accepting state of N(s) and the start state of N (t) are merged into a single state, with all the transitions in or out of either state. A path from i to f in Fig. 3.41 must go first through N (s) , and therefore its label will begin with some string in L(s) . The path then continues through N (t) , so the path's label finishes with a string in L(t) . As we shall so on argue, accepting states never have edges out and start states never have edges in, so it is not possible for a path to re-enter N (s) after leaving it. Thus, N (r ) accepts exactly L(s)L(t) , and is a correct NFA for r = st . c) Suppose $r = s^ $. Then for r we construct the NFA N (r ) shown in Fig. 3.42. Here, i and f are new states, the start state and lone accepting state of N (r ) . To get from i to f , we can either follow the introduced path labeled $\\epsilon$, which takes care of the one string in $L(s)^0$ , or we can go to the start state of N(s) , through that NFA, then from its accepting state back to its start state zero or more times. These options allow N (r ) to accept all the strings in $L(s)^1$, $L(s)^2$, and so on, so the entire set of strings accepted by N (r ) is $L(s^ )$. d) Finally, suppose r = (s) . Then L(r ) = L(s) , and we can use the NFA N (s) as N (r ) . NOTE: Implementing a regular expression engine is far more complex than the algorithm described here. The method description in Algorithm 3.23 contains hints as to why the inductive construction works as it should. We shall not give a formal correctness proof, but we shall list several properties of the constructed NFA's, in addition to the all-important fact that N (r ) accepts language L(r ) . These properties are interesting in their own right, and helpful in making a formal proof. N (r ) has at most twice as many states as there are operators and operands in r . This bound follows from the fact that each step of the algorithm creates at most two new states. N (r ) has one start state and one accepting state . The accepting state has no outgoing transitions, and the start state has no incoming transitions. Each state of N (r ) other than the accepting state has either one outgoing transition on a symbol in $\\Sigma$ or two outgoing transitions, both on $\\epsilon$. Example 3.24 : Let us use Algorithm 3.23 to construct an NFA for $r = (a | b) ^*abb$. Figure 3.43 shows a parse tree for r that is analogous to the parse trees constructed for arithmetic expressions in Section 2.2.3. For subexpression $r_1$ , the first a , we construct the NFA: State numbers have been chosen for consistency with what follows. For $r_2$ we construct: We can now combine $N (r_1)$ and $N (r_2)$, using the construction of Fig. 3.40 to obtain the NFA for $r_3 =r_1|r_2$; this NFA is shown in Fig. 3.44. The NFA for $r_4= (r_3)$ is the same as that for $r_3$. The NFA for $r_5= (r_3)$ is then as shown in Fig. 3.45. We have used the construction in Fig. 3.42 to build this NFA from the NFA in Fig. 3.44. Now, consider subexpression $r_6$, which is another a . We use the basis construction for a again, but we must use new states. It is not permissible to reuse the NFA we constructed for $r_1$, even though $r_1$ and $r_6$ are the same expression. The NFA for $r_6$ is: To obtain the NFA for $r_7= r_5r_6$, we apply the construction of Fig. 3.41. We merge states 7 and $7^{'}$, yielding the NFA of Fig. 3.46. Continuing in this fashion with new NFA's for the two subexpressions b called $r_8$ and $r_{10}$, we eventually construct the NFA for tha$r = (a | b) ^*abb$ that we first met in Fig. 3.34.","title":"3.7.4 Construction of an NFA from a Regular Expression"},{"location":"Chapter-3-Lexical-Analysis/wikipedia-Lex(software)/","text":"Lex (software) Open source Structure of a Lex file Lex (software) # Lex is a computer program that generates lexical analyzers (\"scanners\" or \"lexers\").[ 1] [ 2] Lex is commonly used with the yacc parser generator . Lex, originally written by Mike Lesk and Eric Schmidt [ 3] and described in 1975,[ 4] [ 5] is the standard lexical analyzer generator on many Unix systems, and an equivalent tool is specified as part of the POSIX standard.[ citation needed ] Lex reads an input stream specifying the lexical analyzer and outputs source code implementing the lexer in the C programming language . In addition to C, some old versions of Lex could also generate a lexer in Ratfor .[ 6] Open source # Though originally distributed as proprietary software, some versions of Lex are now open source . Open source versions of Lex, based on the original AT&T code are now distributed as a part of open source operating systems such as OpenSolaris and Plan 9 from Bell Labs .[ clarification needed ] One popular open source version of Lex, called flex , or the \"fast lexical analyzer\", is not derived from proprietary coding. Structure of a Lex file # The structure of a Lex file is intentionally similar to that of a yacc file; files are divided into three sections, separated by lines that contain only two percent signs, as follows The definition section defines macros and imports header files written in C . It is also possible to write any C code here, which will be copied verbatim into the generated source file. The rules section associates regular expression patterns with C statements . When the lexer sees text in the input matching a given pattern, it will execute the associated C code. The C code section contains C statements and functions that are copied verbatim to the generated source file. These statements presumably contain code called by the rules in the rules section. In large programs it is more convenient to place this code in a separate file linked in at compile time.","title":"wikipedia Lex(software)"},{"location":"Chapter-3-Lexical-Analysis/wikipedia-Lex(software)/#lex-software","text":"Lex is a computer program that generates lexical analyzers (\"scanners\" or \"lexers\").[ 1] [ 2] Lex is commonly used with the yacc parser generator . Lex, originally written by Mike Lesk and Eric Schmidt [ 3] and described in 1975,[ 4] [ 5] is the standard lexical analyzer generator on many Unix systems, and an equivalent tool is specified as part of the POSIX standard.[ citation needed ] Lex reads an input stream specifying the lexical analyzer and outputs source code implementing the lexer in the C programming language . In addition to C, some old versions of Lex could also generate a lexer in Ratfor .[ 6]","title":"Lex (software)"},{"location":"Chapter-3-Lexical-Analysis/wikipedia-Lex(software)/#open-source","text":"Though originally distributed as proprietary software, some versions of Lex are now open source . Open source versions of Lex, based on the original AT&T code are now distributed as a part of open source operating systems such as OpenSolaris and Plan 9 from Bell Labs .[ clarification needed ] One popular open source version of Lex, called flex , or the \"fast lexical analyzer\", is not derived from proprietary coding.","title":"Open source"},{"location":"Chapter-3-Lexical-Analysis/wikipedia-Lex(software)/#structure-of-a-lex-file","text":"The structure of a Lex file is intentionally similar to that of a yacc file; files are divided into three sections, separated by lines that contain only two percent signs, as follows The definition section defines macros and imports header files written in C . It is also possible to write any C code here, which will be copied verbatim into the generated source file. The rules section associates regular expression patterns with C statements . When the lexer sees text in the input matching a given pattern, it will execute the associated C code. The C code section contains C statements and functions that are copied verbatim to the generated source file. These statements presumably contain code called by the rules in the rules section. In large programs it is more convenient to place this code in a separate file linked in at compile time.","title":"Structure of a Lex file"},{"location":"Chapter-4-Syntax-Analysis/","text":"Chapter 4 Syntax Analysis # NOTE: This chapter is mainly about algorithms of parsing or syntax analysis, I extend it to include algorithms relevant to parsing in wikipedia which is listed in section Wikipedia Parsing algorithms","title":4},{"location":"Chapter-4-Syntax-Analysis/#chapter-4-syntax-analysis","text":"NOTE: This chapter is mainly about algorithms of parsing or syntax analysis, I extend it to include algorithms relevant to parsing in wikipedia which is listed in section Wikipedia Parsing algorithms","title":"Chapter 4 Syntax Analysis"},{"location":"Chapter-4-Syntax-Analysis/4.2-Context-Free-Grammars/","text":"4.2 Context-Free Grammars # 4.2.3 Derivations # NOTE: Derivation means \u63a8\u5bfc in Chinese. The construction of a parse tree can be made precise by taking a derivational view, in which productions are treated as rewriting rules. Beginning with the start symbol, each rewriting step replaces a nonterminal by the body of one of its productions. This derivational view corresponds to the top-down construction of a parse tree , but the precision afforded by derivations will be especially helpful when bottom-up parsing is discussed. As we shall see, bottom-up parsing is related to a class of derivations known as \" rightmost \" derivations, in which the rightmost nonterminal is rewritten at each step. NOTE: I still do not understand why bottom-up parsing corresponds \" rightmost \" derivations. $\\Rightarrow$ If $S \\xrightarrow []{ \\ast} \\alpha$ where $S$ is the start symbol of a grammar $G$, we say that $\\alpha$ is a sentential form of G. Note that a sentential form may contain both terminals and nonterminals , and may be empty. A sentence of G is a sentential form with no nonterminals . The language generated by a grammar is its set of sentences . Thus, a string of terminals $w$ is in $L(G)$, the language generated by G, if and only if $w$ is a sentence of G (or $S \\xrightarrow []{ \\ast} w $. A language that can be generated by a grammar is said to be a context-free language . If two grammars generate the same language, the grammars are said to be equivalent . NOTE: Natural language is not context-free language . To understand how parsers work, we shall consider derivations in which the nonterminal to be replaced at each step is chosen as follows: In leftmost derivations , the leftmost nonterminal in each sentential is always chosen. If $\\alpha \\to \\beta $ is a step in which the leftmost nonterminal in $\\alpha$ is replaced, we write $\\alpha \\xrightarrow [ \\text{lm} ]{} \\beta$ . In rightmost derivations , the rightmost nonterminal is always chosen; we write $\\alpha \\xrightarrow [ \\text{rm} ]{} \\beta$ in this case. If , then we say that is a left-sentential form of the grammar at hand. Analogous definitions hold for rightmost derivations . Rightmost derivations are sometimes called canonical derivations . 4.2.4 Parse Trees and Derivations # A parse tree is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal A in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation. NOTE: Left-most derivation and right-most derivation yield the same tree in the end but the order in which productions are applied to replace nonterminals is different. 4.2.7 Context-Free Grammars Versus Regular Expressions # NOTE: \u4e0b\u9762\u8fd9\u4e9b\u5185\u5bb9\u662f\u5728\u9605\u8bfb Comparison of parser generators \u4e2d Regular languages \u7ae0\u8282\u65f6\uff0c\u53d1\u73b0\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u6b63\u597d\u548c\u672c\u8282\u5185\u5bb9\u76f8\u5173\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u8865\u5145\uff0c\u5e76\u4e14\u5b83\u7684\u8bb2\u89e3\u4e5f\u6bd4\u8f83\u6613\u61c2\uff1a Regular languages are a category of languages (sometimes termed Chomsky Type 3 ) which can be matched by a state machine (more specifically, by a deterministic finite automaton or a nondeterministic finite automaton ) constructed from a regular expression . In particular, a regular language can match constructs like \"A follows B\", \"Either A or B\", \"A, followed by zero or more instances of B\", but cannot match constructs which require consistency between non-adjacent elements, such as \"some instances of A followed by the same number of instances of B\", and also cannot express the concept of recursive \"nesting\" (\"every A is eventually followed by a matching B\"). A classic example of a problem which a regular grammar cannot handle is the question of whether a given string contains correctly-nested parentheses. (This is typically handled by a Chomsky Type 2 grammar, also termed a context-free grammar .) regular expression\u5c5e\u4e8e Regular languages \uff0c\u4f7f\u7528 regular grammar \uff0c\u6309\u7167 Chomsky hierarchy : regular grammar \u5c5e\u4e8e Type-3 grammars context-free grammar \u5c5e\u4e8e Type-2 grammars Every regular language is context-free , every context-free language is context-sensitive , every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.","title":"4.2-Context-Free-Grammars"},{"location":"Chapter-4-Syntax-Analysis/4.2-Context-Free-Grammars/#42-context-free-grammars","text":"","title":"4.2 Context-Free Grammars"},{"location":"Chapter-4-Syntax-Analysis/4.2-Context-Free-Grammars/#423-derivations","text":"NOTE: Derivation means \u63a8\u5bfc in Chinese. The construction of a parse tree can be made precise by taking a derivational view, in which productions are treated as rewriting rules. Beginning with the start symbol, each rewriting step replaces a nonterminal by the body of one of its productions. This derivational view corresponds to the top-down construction of a parse tree , but the precision afforded by derivations will be especially helpful when bottom-up parsing is discussed. As we shall see, bottom-up parsing is related to a class of derivations known as \" rightmost \" derivations, in which the rightmost nonterminal is rewritten at each step. NOTE: I still do not understand why bottom-up parsing corresponds \" rightmost \" derivations. $\\Rightarrow$ If $S \\xrightarrow []{ \\ast} \\alpha$ where $S$ is the start symbol of a grammar $G$, we say that $\\alpha$ is a sentential form of G. Note that a sentential form may contain both terminals and nonterminals , and may be empty. A sentence of G is a sentential form with no nonterminals . The language generated by a grammar is its set of sentences . Thus, a string of terminals $w$ is in $L(G)$, the language generated by G, if and only if $w$ is a sentence of G (or $S \\xrightarrow []{ \\ast} w $. A language that can be generated by a grammar is said to be a context-free language . If two grammars generate the same language, the grammars are said to be equivalent . NOTE: Natural language is not context-free language . To understand how parsers work, we shall consider derivations in which the nonterminal to be replaced at each step is chosen as follows: In leftmost derivations , the leftmost nonterminal in each sentential is always chosen. If $\\alpha \\to \\beta $ is a step in which the leftmost nonterminal in $\\alpha$ is replaced, we write $\\alpha \\xrightarrow [ \\text{lm} ]{} \\beta$ . In rightmost derivations , the rightmost nonterminal is always chosen; we write $\\alpha \\xrightarrow [ \\text{rm} ]{} \\beta$ in this case. If , then we say that is a left-sentential form of the grammar at hand. Analogous definitions hold for rightmost derivations . Rightmost derivations are sometimes called canonical derivations .","title":"4.2.3 Derivations"},{"location":"Chapter-4-Syntax-Analysis/4.2-Context-Free-Grammars/#424-parse-trees-and-derivations","text":"A parse tree is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal A in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation. NOTE: Left-most derivation and right-most derivation yield the same tree in the end but the order in which productions are applied to replace nonterminals is different.","title":"4.2.4 Parse Trees and Derivations"},{"location":"Chapter-4-Syntax-Analysis/4.2-Context-Free-Grammars/#427-context-free-grammars-versus-regular-expressions","text":"NOTE: \u4e0b\u9762\u8fd9\u4e9b\u5185\u5bb9\u662f\u5728\u9605\u8bfb Comparison of parser generators \u4e2d Regular languages \u7ae0\u8282\u65f6\uff0c\u53d1\u73b0\u8fd9\u4e00\u6bb5\u7684\u5185\u5bb9\u6b63\u597d\u548c\u672c\u8282\u5185\u5bb9\u76f8\u5173\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u8865\u5145\uff0c\u5e76\u4e14\u5b83\u7684\u8bb2\u89e3\u4e5f\u6bd4\u8f83\u6613\u61c2\uff1a Regular languages are a category of languages (sometimes termed Chomsky Type 3 ) which can be matched by a state machine (more specifically, by a deterministic finite automaton or a nondeterministic finite automaton ) constructed from a regular expression . In particular, a regular language can match constructs like \"A follows B\", \"Either A or B\", \"A, followed by zero or more instances of B\", but cannot match constructs which require consistency between non-adjacent elements, such as \"some instances of A followed by the same number of instances of B\", and also cannot express the concept of recursive \"nesting\" (\"every A is eventually followed by a matching B\"). A classic example of a problem which a regular grammar cannot handle is the question of whether a given string contains correctly-nested parentheses. (This is typically handled by a Chomsky Type 2 grammar, also termed a context-free grammar .) regular expression\u5c5e\u4e8e Regular languages \uff0c\u4f7f\u7528 regular grammar \uff0c\u6309\u7167 Chomsky hierarchy : regular grammar \u5c5e\u4e8e Type-3 grammars context-free grammar \u5c5e\u4e8e Type-2 grammars Every regular language is context-free , every context-free language is context-sensitive , every context-sensitive language is recursive and every recursive language is recursively enumerable. These are all proper inclusions, meaning that there exist recursively enumerable languages that are not context-sensitive, context-sensitive languages that are not context-free and context-free languages that are not regular.","title":"4.2.7 Context-Free Grammars Versus Regular Expressions"},{"location":"Chapter-4-Syntax-Analysis/4.3-Writing-a-Grammar/","text":"","title":"4.3-Writing-a-Grammar"},{"location":"Chapter-4-Syntax-Analysis/4.5-Bottom-Up-Parsing/","text":"4.5 Bottom-Up Parsing # A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). It is convenient to describe parsing as the process of building parse trees, although a front end may in fact carry out a translation directly without building an explicit tree. The sequence of tree snapshots in Fig. 4.25 illustrates a bottom-up parse of the token stream $id \\ast id$, with respect to the expression grammar (4.1). NOTE: Thinking of parsing as the process of building parse trees can help us grasp the content more easily. This section introduces a general style of bottom-up parsing known as shift-reduce parsing . The largest class of grammars for which shift-reduce parsers can be built, the LR grammars , will be discussed in Sections 4.6 and 4.7. Although it is too much work to build an LR parser by hand, tools called automatic parser generators make it easy to construct efficient LR parsers from suitable grammars. The concepts in this section are helpful for writing suitable grammars to make effective use of an LR parser generator. Algorithms for implementing parser generators appear in Section 4.7. NOTE: LR parser uses shift-reduce parsing . 4.5.1 Reductions # NOTE: Reduction is the reverse of derivation. We can think of bottom-up parsing as the process of \"reducing\" a string $w$ to the start symbol of the grammar. At each reduction step, a specific substring matching the body of a production is replaced by the nonterminal at the head of that production. The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds. Example 4.37 : The snapshots in Fig. 4.25 illustrate a sequence of reductions; the grammar is the expression grammar (4.1). The reductions will be discussed in terms of the sequence of strings $$ id \\ast id, F \\ast id, T \\ast id, T \\ast F, T, E $$ The strings in this sequence are formed from the roots of all the subtrees in the snapshots. The sequence starts with the input string $id \\ast id$. The first reduction produces $F \\ast id$ by reducing the leftmost $id$ to $F$ , using the production $F \\to id$. The second reduction produces $T \\ast id$ by reducing $F$ to $T$ . By definition, a reduction is the reverse of a step in a derivation (recall that in a derivation, a nonterminal in a sentential form is replaced by the body of one of its productions). The goal of bottom-up parsing is therefore to construct a derivation in reverse. The following corresponds to the parse in Fig. 4.25: $$ E \\Rightarrow T \\Rightarrow T \\ast F \\Rightarrow T \\ast id \\Rightarrow F \\ast id \\Rightarrow id \\ast id $$ This derivation is in fact a rightmost derivation . NOTE: We know that Reduction is the reverse of derivation, so what is done first in reduction is done later in production. The last paragraph concludes with a very important conclusion but the reasoning is a bit murky. I didn't get it right the first time I read it. Here is my analysis of the conclusion. Given grammar, we can choose leftmost derivation or rightmost derivation to derivate the parse tree. No matter order of derivation, the resulting parse tree and the resulting sentence is the same, but the derivation process is different. This is equivalent to that the shape of the parse tree is the same but the process of construction is different. In leftmost derivation , the left subtree is always constructed first and then the right subtree, so the right of the sentence is always derivated first while in rightmost derivation the order is reverse. Now let's turn our attention to reduction. A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). Fig. 4.25 illustrate the process of bottom-up construction of the parse tree. Given a sentence, bottom-up parser choose to reduce from left symbol to right symbol, so it always construct the subtree from lower left corner to upper right corner until the complete parse tree. It is obvious that eventually it will be able to construct a complete parse tree. The order of productions used in its construction is the exact opposite of the rightmost derivation. parsing\u662f\u6839\u636einput symbol\u6765\u9006\u63a8\u5176derivation\u7684\u8fc7\u7a0b\uff0cparser\u5bf9input symbol\u7684\u5904\u7406\u662f\u56fa\u5b9a\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u4e00\u6b21\u5904\u7406\u4e00\u4e2a\u5b57\u7b26\uff0c\u56e0\u6b64\u5b83\u53ea\u80fd\u591f\u5c06input symbol\u4e2d\u5de6\u4fa7\u7684terminal\u9010\u4e2a\u89c4\u7ea6\u4e3anon-terminal\uff0c\u7136\u540e\u518d\u8ddfinput symbol\u4e2d\u4f59\u4e0b\u7684terminal\u4e00\u8d77\u8fdb\u884c\u89c4\u7ea6\u3002\u663e\u7136\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u89c4\u7ea6\u662fderivation\u7684\u9006\u8fc7\u7a0b\uff0c\u4ece\u5de6\u81f3\u53f3\u7684reduction\u5bf9\u5e94\u7684\u662f\u4ece\u53f3\u81f3\u5de6\u7684derivation\uff0c\u6240\u4ee5right-most derivation\u88ab\u79f0\u4e3acanonical derivation\u3002 \u4e3a\u4ec0\u4e48right-most derivation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u800cleft-most derivation\u7684\u8fc7\u7a0b\u65e0\u6cd5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u8fd8\u662f\u8981\u628a\u63e1\u4e00\u70b9\uff1aparser\u5bf9input symbol\u662f\u4ece\u5de6\u81f3\u53f3\u8fdb\u884c\u5904\u7406\u7684\uff0c\u6240\u4ee5\u5b83\u53ea\u80fd\u591f\u4ec5\u4ec5\u6839\u636e\u5df2\u7ecf\u8bfb\u5230\u7684terminal symbol\u6765\u8fdb\u884creduction\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4e8e\u540e\u9762\u7684\u672a\u8bfb\u5165\u7684\u5b57\u7b26\u3002\u5728left-most derivation\u4e2d\uff0c\u4f18\u5148derivate \u7684\u662fleft-most\u7684non-terminal\u3002 4.5.2 Handle Pruning # Bottom-up parsing during a left-to-right scan of the input constructs a right-most derivation in reverse. Formally, if $S \\xrightarrow [rm]{\\ast} \\alpha A w \\xrightarrow [rm]{\\ast} \\alpha \\beta w $ , as in Fig. 4.27, then production $A \\to \\beta $ in the position following $\\alpha$ is a handle of $\\alpha \\beta w$ . Alternatively, a handle of a right-sentential form $\\gamma $ is a production $A \\to \\beta$ and a position of $\\gamma$ where the string $\\beta$ may be found, such that replacing $\\beta$ at that position by $A$ produces the previous right-sentential form in a rightmost derivation of $\\gamma$ . Notice that the string $w$ to the right of the handle must contain only terminal symbols. NOTE: This is because rightmost derivation always choose the rightmost nonterminal to expand; A rightmost derivation in reverse can be obtained by \"handle pruning.\" That is, we start with a string of terminals $w$ to be parsed. If $w$ is a sentence of the grammar at hand, then let $w = \\gamma n$, where $\\gamma _n$ is the $n$th right-sentential form of some as yet unknown rightmost derivation $$ S = \\gamma 0 \\xrightarrow [rm] {} \\gamma 1 \\xrightarrow [rm] {} \\gamma 2 \\xrightarrow [rm] {} \\ldots \\xrightarrow [rm] {}\\gamma {n-1} \\xrightarrow [rm] {} \\gamma n = w $$ To reconstruct this derivation in reverse order, we locate the handle $\\beta_n$ in $\\gamma_n$ and replace $\\beta_n$ by the head of the relevant production $A_n \\to \\beta_n $ to obtain the previous right-sentential form $\\gamma {n-1}$. Note that we do not yet know how handles are to be found, but we shall see methods of doing so shortly. We then repeat this process. That is, we locate the handle $\\beta_{n-1}$ in $\\gamma_{n-1}$ and reduce this handle to obtain the right-sentential form $\\gamma_{n-2}$. If by continuing this process we produce a right-sentential form consisting only of the start symbol $S$ , then we halt and announce successful completion of parsing. The reverse of the sequence of productions used in the reductions is a rightmost derivation for the input string. 4.5.3 Shift-Reduce Parsing # Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. As we shall see, the handle always appears at the top of the stack just before it is identified as the handle . Up on entering this configuration , the parser halts and announces successful completion of parsing. Figure 4.28 steps through the actions a shift-reduce parser might take in parsing the input string $id \\ast id$ according to the expression grammar (4.1). While the primary operations are shift and reduce, there are actually four possible actions a shift-reduce parser can make: (1) shift, (2) reduce, (3) accept, and (4) error. Shift . Shift the next input symbol onto the top of the stack. Reduce . The right end of the string to be reduced must be at the top of the stack. Locate the left end of the string within the stack and decide with what nonterminal to replace the string. Accept . Announce successful completion of parsing. Error . Discover a syntax error and call an error recovery routine. The use of a stack in shift-reduce parsing is justified by an important fact: the handle will always eventually appear on top of the stack, never inside. This fact can be shown by considering the possible forms of two successive steps in any rightmost derivation . Figure 4.29 illustrates the two possible cases. In case (1), $A$ is replaced by $\\beta B y$ , and then the rightmost nonterminal $B$ in the body $\\beta B y$ is replaced by $gamma$ . In case (2), $A$ is again expanded first, but this time the body is a string $y$ of terminals only. The next rightmost nonterminal $B$ will be somewhere to the left of $y$ . NOTE: The proof is omitted because I think it's a natural conclusion. If you read the note in 4.5.1 Reductions, you will find the conclusion is easy to understand. Here the author enumerates one example to support this. 4.5.4 Conflicts During Shift-Reduce Parsing # There are context-free grammars for which shift-reduce parsing cannot be used. Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack and also the next k input symbols, cannot decide whether to shift or to reduce (a shift/reduce conflict ), or cannot decide which of several reductions to make (a reduce/reduce conflict ). We now give some examples of syntactic constructs that give rise to such grammars. Technically, these grammars are not in the LR(k) class of grammars defined in Section 4.7; we refer to them as non-LR grammars . The k in LR(k) refers to the number of symbols of lookahead on the input. Grammars used in compiling usually fall in the LR(1) class, with one symbol of lookahead at most.","title":"4.5-Bottom-Up-Parsing"},{"location":"Chapter-4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#45-bottom-up-parsing","text":"A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). It is convenient to describe parsing as the process of building parse trees, although a front end may in fact carry out a translation directly without building an explicit tree. The sequence of tree snapshots in Fig. 4.25 illustrates a bottom-up parse of the token stream $id \\ast id$, with respect to the expression grammar (4.1). NOTE: Thinking of parsing as the process of building parse trees can help us grasp the content more easily. This section introduces a general style of bottom-up parsing known as shift-reduce parsing . The largest class of grammars for which shift-reduce parsers can be built, the LR grammars , will be discussed in Sections 4.6 and 4.7. Although it is too much work to build an LR parser by hand, tools called automatic parser generators make it easy to construct efficient LR parsers from suitable grammars. The concepts in this section are helpful for writing suitable grammars to make effective use of an LR parser generator. Algorithms for implementing parser generators appear in Section 4.7. NOTE: LR parser uses shift-reduce parsing .","title":"4.5 Bottom-Up Parsing"},{"location":"Chapter-4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#451-reductions","text":"NOTE: Reduction is the reverse of derivation. We can think of bottom-up parsing as the process of \"reducing\" a string $w$ to the start symbol of the grammar. At each reduction step, a specific substring matching the body of a production is replaced by the nonterminal at the head of that production. The key decisions during bottom-up parsing are about when to reduce and about what production to apply, as the parse proceeds. Example 4.37 : The snapshots in Fig. 4.25 illustrate a sequence of reductions; the grammar is the expression grammar (4.1). The reductions will be discussed in terms of the sequence of strings $$ id \\ast id, F \\ast id, T \\ast id, T \\ast F, T, E $$ The strings in this sequence are formed from the roots of all the subtrees in the snapshots. The sequence starts with the input string $id \\ast id$. The first reduction produces $F \\ast id$ by reducing the leftmost $id$ to $F$ , using the production $F \\to id$. The second reduction produces $T \\ast id$ by reducing $F$ to $T$ . By definition, a reduction is the reverse of a step in a derivation (recall that in a derivation, a nonterminal in a sentential form is replaced by the body of one of its productions). The goal of bottom-up parsing is therefore to construct a derivation in reverse. The following corresponds to the parse in Fig. 4.25: $$ E \\Rightarrow T \\Rightarrow T \\ast F \\Rightarrow T \\ast id \\Rightarrow F \\ast id \\Rightarrow id \\ast id $$ This derivation is in fact a rightmost derivation . NOTE: We know that Reduction is the reverse of derivation, so what is done first in reduction is done later in production. The last paragraph concludes with a very important conclusion but the reasoning is a bit murky. I didn't get it right the first time I read it. Here is my analysis of the conclusion. Given grammar, we can choose leftmost derivation or rightmost derivation to derivate the parse tree. No matter order of derivation, the resulting parse tree and the resulting sentence is the same, but the derivation process is different. This is equivalent to that the shape of the parse tree is the same but the process of construction is different. In leftmost derivation , the left subtree is always constructed first and then the right subtree, so the right of the sentence is always derivated first while in rightmost derivation the order is reverse. Now let's turn our attention to reduction. A bottom-up parse corresponds to the construction of a parse tree for an input string beginning at the leaves (the bottom) and working up towards the root(the top). Fig. 4.25 illustrate the process of bottom-up construction of the parse tree. Given a sentence, bottom-up parser choose to reduce from left symbol to right symbol, so it always construct the subtree from lower left corner to upper right corner until the complete parse tree. It is obvious that eventually it will be able to construct a complete parse tree. The order of productions used in its construction is the exact opposite of the rightmost derivation. parsing\u662f\u6839\u636einput symbol\u6765\u9006\u63a8\u5176derivation\u7684\u8fc7\u7a0b\uff0cparser\u5bf9input symbol\u7684\u5904\u7406\u662f\u56fa\u5b9a\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u4e00\u6b21\u5904\u7406\u4e00\u4e2a\u5b57\u7b26\uff0c\u56e0\u6b64\u5b83\u53ea\u80fd\u591f\u5c06input symbol\u4e2d\u5de6\u4fa7\u7684terminal\u9010\u4e2a\u89c4\u7ea6\u4e3anon-terminal\uff0c\u7136\u540e\u518d\u8ddfinput symbol\u4e2d\u4f59\u4e0b\u7684terminal\u4e00\u8d77\u8fdb\u884c\u89c4\u7ea6\u3002\u663e\u7136\u8fd9\u4e2a\u8fc7\u7a0b\u662f\u4ece\u5de6\u81f3\u53f3\u7684\uff0c\u89c4\u7ea6\u662fderivation\u7684\u9006\u8fc7\u7a0b\uff0c\u4ece\u5de6\u81f3\u53f3\u7684reduction\u5bf9\u5e94\u7684\u662f\u4ece\u53f3\u81f3\u5de6\u7684derivation\uff0c\u6240\u4ee5right-most derivation\u88ab\u79f0\u4e3acanonical derivation\u3002 \u4e3a\u4ec0\u4e48right-most derivation\u7684\u8fc7\u7a0b\u53ef\u4ee5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u800cleft-most derivation\u7684\u8fc7\u7a0b\u65e0\u6cd5\u901a\u8fc7input symbol\u5012\u63a8\u51fa\u6765\u5462\uff1f\u8fd8\u662f\u8981\u628a\u63e1\u4e00\u70b9\uff1aparser\u5bf9input symbol\u662f\u4ece\u5de6\u81f3\u53f3\u8fdb\u884c\u5904\u7406\u7684\uff0c\u6240\u4ee5\u5b83\u53ea\u80fd\u591f\u4ec5\u4ec5\u6839\u636e\u5df2\u7ecf\u8bfb\u5230\u7684terminal symbol\u6765\u8fdb\u884creduction\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u4e8e\u540e\u9762\u7684\u672a\u8bfb\u5165\u7684\u5b57\u7b26\u3002\u5728left-most derivation\u4e2d\uff0c\u4f18\u5148derivate \u7684\u662fleft-most\u7684non-terminal\u3002","title":"4.5.1 Reductions"},{"location":"Chapter-4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#452-handle-pruning","text":"Bottom-up parsing during a left-to-right scan of the input constructs a right-most derivation in reverse. Formally, if $S \\xrightarrow [rm]{\\ast} \\alpha A w \\xrightarrow [rm]{\\ast} \\alpha \\beta w $ , as in Fig. 4.27, then production $A \\to \\beta $ in the position following $\\alpha$ is a handle of $\\alpha \\beta w$ . Alternatively, a handle of a right-sentential form $\\gamma $ is a production $A \\to \\beta$ and a position of $\\gamma$ where the string $\\beta$ may be found, such that replacing $\\beta$ at that position by $A$ produces the previous right-sentential form in a rightmost derivation of $\\gamma$ . Notice that the string $w$ to the right of the handle must contain only terminal symbols. NOTE: This is because rightmost derivation always choose the rightmost nonterminal to expand; A rightmost derivation in reverse can be obtained by \"handle pruning.\" That is, we start with a string of terminals $w$ to be parsed. If $w$ is a sentence of the grammar at hand, then let $w = \\gamma n$, where $\\gamma _n$ is the $n$th right-sentential form of some as yet unknown rightmost derivation $$ S = \\gamma 0 \\xrightarrow [rm] {} \\gamma 1 \\xrightarrow [rm] {} \\gamma 2 \\xrightarrow [rm] {} \\ldots \\xrightarrow [rm] {}\\gamma {n-1} \\xrightarrow [rm] {} \\gamma n = w $$ To reconstruct this derivation in reverse order, we locate the handle $\\beta_n$ in $\\gamma_n$ and replace $\\beta_n$ by the head of the relevant production $A_n \\to \\beta_n $ to obtain the previous right-sentential form $\\gamma {n-1}$. Note that we do not yet know how handles are to be found, but we shall see methods of doing so shortly. We then repeat this process. That is, we locate the handle $\\beta_{n-1}$ in $\\gamma_{n-1}$ and reduce this handle to obtain the right-sentential form $\\gamma_{n-2}$. If by continuing this process we produce a right-sentential form consisting only of the start symbol $S$ , then we halt and announce successful completion of parsing. The reverse of the sequence of productions used in the reductions is a rightmost derivation for the input string.","title":"4.5.2 Handle Pruning"},{"location":"Chapter-4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#453-shift-reduce-parsing","text":"Shift-reduce parsing is a form of bottom-up parsing in which a stack holds grammar symbols and an input buffer holds the rest of the string to be parsed. As we shall see, the handle always appears at the top of the stack just before it is identified as the handle . Up on entering this configuration , the parser halts and announces successful completion of parsing. Figure 4.28 steps through the actions a shift-reduce parser might take in parsing the input string $id \\ast id$ according to the expression grammar (4.1). While the primary operations are shift and reduce, there are actually four possible actions a shift-reduce parser can make: (1) shift, (2) reduce, (3) accept, and (4) error. Shift . Shift the next input symbol onto the top of the stack. Reduce . The right end of the string to be reduced must be at the top of the stack. Locate the left end of the string within the stack and decide with what nonterminal to replace the string. Accept . Announce successful completion of parsing. Error . Discover a syntax error and call an error recovery routine. The use of a stack in shift-reduce parsing is justified by an important fact: the handle will always eventually appear on top of the stack, never inside. This fact can be shown by considering the possible forms of two successive steps in any rightmost derivation . Figure 4.29 illustrates the two possible cases. In case (1), $A$ is replaced by $\\beta B y$ , and then the rightmost nonterminal $B$ in the body $\\beta B y$ is replaced by $gamma$ . In case (2), $A$ is again expanded first, but this time the body is a string $y$ of terminals only. The next rightmost nonterminal $B$ will be somewhere to the left of $y$ . NOTE: The proof is omitted because I think it's a natural conclusion. If you read the note in 4.5.1 Reductions, you will find the conclusion is easy to understand. Here the author enumerates one example to support this.","title":"4.5.3 Shift-Reduce Parsing"},{"location":"Chapter-4-Syntax-Analysis/4.5-Bottom-Up-Parsing/#454-conflicts-during-shift-reduce-parsing","text":"There are context-free grammars for which shift-reduce parsing cannot be used. Every shift-reduce parser for such a grammar can reach a configuration in which the parser, knowing the entire stack and also the next k input symbols, cannot decide whether to shift or to reduce (a shift/reduce conflict ), or cannot decide which of several reductions to make (a reduce/reduce conflict ). We now give some examples of syntactic constructs that give rise to such grammars. Technically, these grammars are not in the LR(k) class of grammars defined in Section 4.7; we refer to them as non-LR grammars . The k in LR(k) refers to the number of symbols of lookahead on the input. Grammars used in compiling usually fall in the LR(1) class, with one symbol of lookahead at most.","title":"4.5.4 Conflicts During Shift-Reduce Parsing"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/","text":"4.6 Introduction to LR Parsing: Simple LR # The most prevalent type of bottom-up parser to day is based on a concept called LR(k ) parsing; the \"L\" is for left-to-right scanning of the input, the \"R\" for constructing a rightmost derivation in reverse, and the k for the number of input symbols of lookahead that are used in making parsing decisions. The cases k = 0 or k = 1 are of practical interest, and we shall only consider LR parsers with $k \\le 1$ here. When (k ) is omitted, k is assumed to be 1. This section introduces the basic concepts of LR parsing and the easiest method for constructing shift-reduce parsers, called \"simple LR\" (or SLR, for short). Some familiarity with the basic concepts is helpful even if the LR parser itself is constructed using an automatic parser generator. We begin with \"items\" and \"parser states;\" the diagnostic output from an LR parser generator typically includes parser states , which can be used to isolate the sources of parsing conflicts. Section 4.7 introduces two, more complex methods canonical-LR and LALR that are used in the majority of LR parsers. 4.6.1 Why LR Parsers? # LR parsers are table-driven, much like the nonrecursive LL parsers of Section 4.4.4. A grammar for which we can construct a parsing table using one of the methods in this section and the next is said to be an LR grammar . Intuitively, for a grammar to be LR it is sufficient that a left-to-right shift-reduce parser be able to recognize handles of right-sentential forms when they appear on top of the stack. 4.6.2 Items and the LR(0) Automaton # How do es a shift-reduce parser know when to shift and when to reduce? For example, with stack contents $T and next input symbol * in Fig. 4.28, how does the parser know that T on the top of the stack is not a handle, so the appropriate action is to shift and not to reduce T to E ? An LR parser makes shift-reduce decisions by maintaining states to keep track of where we are in a parse. States represent sets of \"items.\" An LR(0) item ( item for short) of a grammar G is a production of G with a dot at some position of the body. Thus, production $A \\to XYZ$ yields the four items The production $A \\to \\epsilon$ generates only one item, $A \\to \\bullet$ . Intuitively, an item indicates how much of a production we have seen at a given point in the parsing process. For example, the item $A \\to X Y Z$ indicates that we hope to see a string derivable from $X Y Z$ next on the input. Item $A \\to X \\cdot Y Z$ indicates that we have just seen on the input a string derivable from $X$ and that we hope next to see a string derivable from $Y Z$ . Item $A \\to X Y Z$ indicates that we have seen the body $X Y Z$ and that it may b e time to reduce $X Y Z$ to $A$. One collection of sets of LR(0) items, called the canonical LR(0) collection, provides the basis for constructing a deterministic finite automaton that is used to make parsing decisions. Such an automaton is called an LR(0) automaton . In particular, each state of the LR(0) automaton represents a set of items in the canonical LR(0) collection . The automaton for the expression grammar (4.1), shown in Fig. 4.31, will serve as the running example for discussing the canonical LR(0) collection for a grammar. To construct the canonical LR(0) collection for a grammar, we define an augmented grammar and two functions, CLOSURE and GOTO. If $G$ is a grammar with start symbol $S$ , then $\\acute{G}$ , the augmented grammar for G, is $G$ with a new start symbol $\\acute{S}$ and production $\\acute{S} \\to S $. The purpose of this new starting production is to indicate to the parser when it should stop parsing and announce acceptance of the input. That is, acceptance occurs when and only when the parser is about to reduce by $\\acute{S} \\to S $. Closure of Item Sets # The Function GOTO # Use of the LR(0) Automaton # The central idea behind \u201cSimple LR,\" or SLR, parsing is the construction from the grammar of the LR(0) automaton. The states of this automaton are the sets of items from the canonical LR(0) collection, and the transitions are given by the GOTO function. The LR(0) automaton for the expression grammar (4.1) appeared earlier in Fig. 4.31. The start state of the LR(0) automaton is $CLOSURE({[ \\acute{S} \\to S]})$, where $\\acute{S}$ is the start symbol of the augmented grammar. All states are accepting states. We say \"state j \" to refer to the state corresponding to the set of items $I_j$. \u6587\u6cd5\u548cLR(0)\u81ea\u52a8\u673a # \u4e00\u4e2a\u4ea7\u751f\u5f0f\u53ef\u80fd\u6709\u591a\u4e2a\u72b6\u6001\uff0c\u5b83\u4eec\u88ab\u6210\u4e3a\u9879\u3002\u6839\u636e\u9879\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u770b\u51fa\u5b83\u662f\u4e3areduction\u800c\u751f\u7684\uff0c\u5b83\u8868\u793a\u4e86\u5206\u6790\u5668\u5728\u5206\u6790\u7684\u8fc7\u7a0b\u4e2d\u5df2\u7ecf\u770b\u5230\u4e86 \u4ea7\u751f\u5f0f\u4f53 \u7684\u54ea\u4e9b\u90e8\u5206\uff0c\u4ee5\u53ca\u5b83\u6240\u671f\u671b\u770b\u5230\u7684\u5408\u4e4e\u5b83\u7684\u6587\u6cd5\u7684\u7b26\u53f7\u4e32\u3002\u5df2\u7ecf\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u5de6\u8fb9\uff0c\u671f\u671b\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u53f3\u8fb9\u3002\u663e\u7136\uff0c\u5f53\u5df2\u7ecf\u770b\u5230\u4e86\u4e00\u4e2a\u4ea7\u751f\u5f0f\u4f53\u7684\u5168\u90e8\u7b26\u53f7\u540e\uff0c\u5c31\u53ef\u4ee5\u8fdb\u884c\u89c4\u7ea6\u4e86\u3002 \u65e2\u7136\u5b9a\u4e49\u4e86\u72b6\u6001\uff0c\u90a3\u4e48\u80af\u5b9a\u5c31\u4f1a\u6d89\u53ca\u5230\u72b6\u6001\u7684\u8f6c\u6362\uff1a\u72b6\u6001\u7684\u8f6c\u6362\u6216\u8005\u8bf4\u9879\u7684\u8f6c\u6362\u662f\u7531\u5206\u6790\u5668\u5728\u5206\u6790\u8fc7\u7a0b\u4e2d\u770b\u5230\u7684\u6587\u6cd5\u7b26\u53f7\u800c\u89e6\u53d1\u7684\uff0c\u6bcf\u6b21\u770b\u5230\u4e00\u4e2a\u6587\u6cd5\u7b26\u53f7\uff0c\u5b83\u5c31\u53ef\u4ee5\u5c06\u4ea7\u751f\u5f0f\u4f53\u4e2d\u7684$\\bullet$\u5411\u53f3\u79fb\u52a8\u4e00\u6b21\uff0c\u4ece\u800c\u8fdb\u5165\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u72b6\u6001\u3002 \u90a3\u7ed9\u5b9a\u6587\u6cd5\uff0c\u6211\u4eec\u80fd\u5426\u63d0\u524d\u5c31\u5206\u6790\u51fa\u5b83\u4f1a\u6709\u54ea\u4e9b\u72b6\u6001\uff0c\u54ea\u4e9b\u8f6c\u6362\u5462\uff1f\u7b54\u6848\u662f\u80af\u5b9a\u7684\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u6784\u9020\u51fa\u7684LR(0)\u81ea\u52a8\u673a\u3002\u663e\u7136\uff0c\u4e00\u4e2aLR(0)\u81ea\u52a8\u673a\u7ed9\u51fa\u4e86\u7ed9\u5b9a\u6587\u6cd5\u4e2d\u7684\u6240\u6709\u7684\u53ef\u80fd\u7684\u6709\u6548\u7684\u8f6c\u6362\u3002 \u4eceLR(0)\u81ea\u52a8\u673a\u6765\u770b\u5f85LR\u8bed\u6cd5\u5206\u6790\u7b97\u6cd5 # LR(0)\u81ea\u52a8\u673a\u662f \u786e\u5b9a\u6709\u7a77\u72b6\u6001\u673a \uff0c\u5b83\u4ece \u72b6\u60010 \u5f00\u59cb\u5728\u6bcf\u4e2a\u7b26\u53f7\u4e0a\u90fd\u6709\u8f6c\u6362\u3002\u5982\u679c\u4e00\u4e2a\u72b6\u6001\u8868\u793a\u7684\u4ea7\u751f\u5f0f\u7684\u4f53\u5df2\u7ecf\u5168\u90e8\u90fd\u770b\u5230\u4e86\uff0c\u90a3\u4e48\u663e\u7136\u8fd9\u4e2a\u72b6\u6001\u5c31\u4e0d\u4f1a\u518d\u6709\u8f6c\u6362\u4e86\uff0c\u56e0\u6b64\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9009\u62e9\u5bf9\u5b83\u8fdb\u884c \u89c4\u7ea6 \u3002Example 4.43 \u4e2d\u7ed9\u51fa\u7684LR parser\u53ef\u4ee5\u8ba4\u4e3a\u6709\u4e24\u4e2a\u6808: \u5b57\u7b26\u6808 \u72b6\u6001\u6808 \u79fb\u5165\u64cd\u4f5c \u5bf9\u5e94\u7684\u662f\u5728LR(0)\u72b6\u6001\u673a\u8fdb\u884c\u72b6\u6001\u8f6c\u6362\uff0c\u5373\u4ece\u4e00\u4e2a\u72b6\u6001\u8f6c\u79fb\u5230\u53e6\u5916\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u5c06INPUT\u4e2d\u7684symbol\u79fb\u5165\u5230\u5b57\u7b26\u6808\u4e2d\uff0c\u76f8\u5e94\u7684\uff0c\u4e5f\u4f1a\u5c06\u65b0\u5230\u8fbe\u7684\u72b6\u6001\u538b\u5165\u5230\u72b6\u6001\u6808\u4e2d\u3002\u663e\u7136\uff0c\u72b6\u6001\u6808\u8bb0\u5f55\u4e86\u72b6\u6001\u8f6c\u6362\u7684\u8def\u5f84\uff0c\u5373\u6808\u4e2d\u6bcf\u6761\u8bb0\u5f55\u662f\u4ece\u5b83\u540e\u9762\u7684\u4e00\u6761\u8bb0\u5f55\u8f6c\u6362\u800c\u6765\u7684\u3002 \u90a3\u4e48\u89c4\u7ea6\u610f\u5473\u7740\u5bf9\u4e0a\u8ff0\u4e24\u4e2a\u6808\u6267\u884c\u4ec0\u4e48\u64cd\u4f5c\u5462\uff1f\u89c4\u7ea6\u610f\u5473\u7740\u5c06\u5b57\u7b26\u6808\u4e2d \u4ea7\u751f\u5f0f\u7684\u4f53 \u5f39\u51fa\u6808\uff0c\u76f8\u5e94\u7684\u4e5f\u8981\u4ece\u72b6\u6001\u6808\u4e2d\u5f39\u51fa\u76f8\u5e94\u6570\u91cf\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u5c06\u4ea7\u751f\u5f0f\u7684\u5934\u538b\u5165\u6808\u4e2d\uff0c\u90a3\u4e48\u6b64\u65f6\u8981\u538b\u5165\u4ec0\u4e48\u72b6\u6001\u5462\uff1f \u79fb\u5165\u662f\u6cbf\u7740\u81ea\u52a8\u673a\u7684\u67d0\u6761\u8def\u5f84\u8fdb\u884c\u8f6c\u6362\uff0c\u89c4\u7ea6\u5219\u662f\u56de\u5230\u8fd9\u6761\u8def\u5f84\u7684\u8d77\u70b9\uff0c\u663e\u7136\u89c4\u7ea6\u4f1a\u5f97\u5230\u4e00\u4e2anon-terminal\uff08\u5176\u5b9e\u8fd9\u5c31\u76f8\u5f53\u4e8e\u5df2\u7ecf\u6784\u9020\u597d\u4e86\u5b50\u6811\u4e86\uff09\u3002\u7136\u540e\u5728\u5f53\u524d\u6808\u9876\u7684\u72b6\u6001\u57fa\u4e8e\u524d\u9762\u89c4\u7ea6\u7684\u7b26\u53f7\u8fdb\u884c\u8f6c\u6362\uff0c\u7136\u540e\u5c06\u8f6c\u6362\u5f97\u5230\u7684\u65b0\u72b6\u6001\u538b\u5165\u6808\u4e2d\u3002\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u56e0\u4e3aparser\u5df2\u7ecf\u770b\u5230\u4e86\u8fd9\u4e2anon-terminal\u4e86\uff0c\u6240\u4ee5\u5fc5\u7136\u8981\u8fdb\u884c\u72b6\u6001\u7684\u8f6c\u6362\uff1b \u6811\u4e0e\u6808 # \u4eceFigure 4.31: LR(0) automaton for the expression grammar (4.1)\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cLR(0) automaton\u4e5f\u662f\u6811\u5f62\u7684\uff0cLR\u8bed\u6cd5\u5206\u6790\u5668\u5728\u8fd9\u68f5\u6811\u4e0a\u7684\u64cd\u4f5c\u4e5f\u662f\u57fa\u4e8e \u6808 \u7684\u3002\u5176\u5b9e\u5728\u601d\u8003LR(0) parser\u7684\u65f6\u5019\uff0c\u6211\u60f3\u5230\u4e86\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u5982\u679c\u5c06\u7a0b\u5e8f\u6267\u884c\u8fc7\u7a0b\u7cbe\u7b80\u6210\u51fd\u6570\u8c03\u7528\u7684\u8bdd\uff0c\u5176\u5b9e\u6574\u4e2a\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u753b\u6210\u6811\u5f62\u7684\uff0c\u5373\u7a0b\u5e8f\u7684 \u51fd\u6570\u8c03\u7528\u6811 \uff1a\u8fd9\u68f5\u6811\u7684 root\u8282\u70b9\u5c31\u662fmain\u51fd\u6570\uff0cmain\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u8fd9\u68f5\u6811\u7684\u7b2c\u4e00\u5c42\uff0c\u7b2c\u4e00\u5c42\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u7b2c\u4e8c\u5c42\uff0c\u4f9d\u6b21\u9012\u63a8\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u68f5\u5b8c\u6574\u7684\u6811\u4e86\uff1b\u5e76\u4e14\u548cLR(0) parser\u4e00\u6837\uff0c\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u662f\u4f7f\u7528\u7684stack\u3002\u663e\u7136\uff0c\u4e24\u79cd\u60c5\u51b5\u90fd\u6d89\u53ca\u4e86tree\u548cstack\uff1a \u5728LR parser\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u7b26\u53f7\u538b\u6808\u76f4\u81f3\u770b\u5230\u4e86\u5b8c\u6574\u7684\u4ea7\u751f\u5f0f\u4f53\u624d\u5c06\u4ea7\u751f\u5f0f\u4f53\u5f39\u51fa\u6808\u3001\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5f0f\u7684\u5934\u90e8->\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5934\u4f9d\u8d56\u4e8e\u5f97\u5230\u6240\u6709\u7684\u4ea7\u751f\u4f53->\u4e0d\u65ad\u5730\u5c06\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u538b\u6808\uff0c\u76f4\u5230\u89c1\u5230\u5168\u90e8\u7684\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u5c31\u51fa\u6808 \u5728\u51fd\u6570\u8c03\u7528\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u5165call stack\u4e2d\uff0c\u76f4\u81f3\u6240\u6709\u7684\u5b50\u51fd\u6570\u90fd\u8fd4\u56de\u4e3b\u51fd\u6570\u624d\u5f97\u4ee5\u6267\u884c\u5b8c\u6210 ->\u4e3b\u51fd\u6570\u7684\u503c\u4f9d\u8d56\u4e8e\u6240\u6709\u7684\u5b50\u51fd\u6570->\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u6808\uff0c\u77e5\u9053\u6700\u5e95\u5c42\u7684\u5b50\u51fd\u6570\u6c42\u89e3\uff0c\u624d\u4f9d\u6b21\u51fa\u6808 \u770b\uff0c\u4e24\u8005\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u591a\u4e48\u5730\u7c7b\u4f3c\uff1b \u4ece\u6811\u7684\u6784\u9020\u7684\u89d2\u5ea6\u6765\u770b\u5f85LR parser\u4e2d\u7684tree\u548c\u51fd\u6570\u8c03\u7528\u4e2d\u7684\u6811\uff1aLR parser\u4e2d\uff0ctree\u7684\u6784\u9020\u662f\u81ea\u5e95\u5411\u4e0a\u7684\uff0c\u800c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u81ea\u5b9a\u5411\u4e0b\u7684\uff0c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u6bd4\u8f83\u7c7b\u4f3c\u4e8eLR(0) parser\u4e2d\u6811\u7684\u6784\u9020\u3002 \u65e0\u8bba\u662f LR(0)\u81ea\u52a8\u673a \u4ee5\u53ca \u51fd\u6570\u8c03\u7528\u6811 \uff0c\u5b83\u4eec\u90fd\u662f\u662f\u6211\u4eec\u4ece\u5168\u5c40\u7684\u89d2\u5ea6\uff08\u6574\u4f53\u7684\u89d2\u5ea6\uff0c\u5206\u6790\u7684\u89d2\u5ea6\uff09\u6765\u5206\u6790\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b83\u4eec\u662f\u7406\u8bba\u5c42\u9762\u7684\u5206\u6790\uff0c\u800c\u4e0d\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\uff0c\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u538b\u6839\u5c31\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u6784\u9020\u51fa\u8fd9\u6837\u7684\u4e00\u68f5\u6811\uff0c\u5e76\u4e14\u538b\u6839\u5c31\u65e0\u9700\u77e5\u9053\u6574\u4e2a\u6811\u662f\u600e\u6837\u7684\u3002\u6bd4\u5982\u5728LR parser\u4e2d\uff0cparser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u51fd\u6570\u7684\u6267\u884c\u662f\u987a\u5e8f\u6267\u884c\u7684\uff0c\u4e00\u6b21\u53ea\u4f1a\u6267\u884c\u4e00\u4e2a\u51fd\u6570\uff1b\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u6211\u89c9\u5f97\u8fd9\u662f\u7531\u8ba1\u7b97\u673a\u7684\u4f53\u7cfb\u7ed3\u6784\u6240\u51b3\u5b9a\u7684\uff0c\u6b63\u5982\u5404\u79cdautomaton\u6a21\u578b\u6240\u5c55\u793a\u7684\u90a3\u6837\uff0c\u8ba1\u7b97\u673a\u5c31\u662f\u8fd9\u6837\u7684\u89c4\u5219\uff0c\u5c31\u662f\u8fd9\u6837\u7684\u987a\u5e8f\uff0c\u6240\u4ee5\u6211\u4eec\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e5f\u662f\u9700\u8981\u5bfb\u627e\u89c4\u5219\uff0c\u987a\u5e8f\uff0c\u8fd9\u662f\u4e00\u79cd\u8ba1\u7b97\u601d\u7ef4\uff1b \u6240\u4ee5\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\uff08\u6709\u8d77\u70b9\uff0c\u6709\u7ec8\u70b9\uff09\uff0c\u663e\u7136\u8fd9\u6761\u8def\u5f84\u662f \u7ebf\u6027\u7684 \uff0c\u662f \u8fde\u7eed\u7684 \uff08\u80fd\u591f\u4ece\u7ec8\u70b9\u518d\u8fd4\u56de\u5230\u8d77\u70b9\uff09\u3002\u5982\u679c\u6211\u4eec\u5c06\u6267\u884c\u7684\u8def\u5f84\u8fde\u63a5\u8d77\u6765\uff08\u56e0\u4e3a\u8fd9\u4e9b\u8def\u5f84\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5c06\u5b83\u4eec\u8fde\u63a5\u8d77\u6765\uff09\uff0c\u4ee5\u9002\u5f53\u7684\u65b9\u5f0f\u753b\u51fa\u4e86\uff08\u6b63\u5982Fig. 4.31\uff09\uff0c\u90a3\u4e48\u5b83\u5c31\u80fd\u591f\u5c55\u73b0\u51fa\u6211\u4eec\u7684\u5728\u7406\u8bba\u5c42\u9762\u5206\u6790\u7684\u5f62\u6001\u3002 \u5982\u679c\u4ece\u6811\u7684\u6784\u9020\u6765\u770b\u7684\u8bdd\uff0c\u5728parser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u4ece\u5b50\u6811\u5f00\u59cb\u6784\u9020\uff0c\u7136\u540e\u5c06\u4e00\u68f5\u4e00\u68f5\u7684\u5b50\u6811\u7ed3\u5408\u8d77\u6765\u6784\u9020\u66f4\u5927\u7684\u6811\u3002\u5728data structure\u4e2d\uff0c\u6811\u662f\u6709\u4e00\u4e2a\u4e00\u4e2a\u7684node\u8fde\u63a5\u800c\u6210\u7684\uff0c\u6240\u4ee5\u8bbf\u95ee\u4e00\u68f5\u6811\u53ea\u9700\u8981\u5f97\u5230\u8fd9\u68f5\u6811\u7684\u6839\u7ed3\u70b9\u5373\u53ef\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528node\u6765\u4ee3\u66ff\u4e00\u68f5\u6811\u3002\u6240\u4ee5\u5728\u6811\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\uff0c\u6240\u64cd\u4f5c\u7684\u662f\u4e00\u4e2a\u4e00\u4e2a\u7684node\uff0c\u6240\u4ee5\u4f7f\u7528\u4f7f\u7528stack\u5c31\u53ef\u4ee5\u5b8c\u6210\u4e00\u68f5\u6811\u7684\u6784\u9020\u3002 \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5bf9\u7406\u8bba\u6a21\u578b\u7684\u5b9e\u73b0\u65f6\u5f80\u5f80\u9009\u62e9\u7684\u662f\u901a\u7528\u7684\uff0c\u7b80\u5355\u7684\u65b9\u5f0f\uff08\u8282\u7ea6\u5185\u5b58\u7b49\uff09\uff0c\"\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\"\uff0c\u6240\u4ee5\u6211\u4eec\u4ec5\u4ec5\u9700\u8981\u7684\u662f\u80fd\u591f\u6ee1\u8db3\u8fd9\u6761\u8def\u5f84\u7684\u7ed3\u6784\u5373\u53ef\u3002\u800c\u6808\u8fd9\u79cd\u7ed3\u6784\u5c31\u6b63\u597d\u7b26\u5408\u8fd9\u4e9b\u8981\u6c42\uff1a \u6808\u662f\u7ebf\u6027\u7684 \u6808\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5b9e\u73b0\u4ece\u7ec8\u70b9\u56de\u5230\u8d77\u70b9 \u518d\u56de\u5230\u7406\u8bba\u5206\u6790\u5c42\u9762\uff0c\u5b9e\u9645\u6267\u884c\u8fc7\u7a0b\u548c\u7406\u8bba\u5c42\u9762\u7684\u6a21\u578b\u4e4b\u95f4\u662f\u600e\u6837\u7684\u5173\u8054\u5462\uff1f\u5b9e\u9645\u6267\u884c\u6d41\u7a0b\u5bf9\u5e94\u7684\u662f\u5bf9\u6811\u6267\u884c\u6df1\u5ea6\u4f18\u5148\u540e\u5e8f\u904d\u5386\uff1b 4.6.5 Viable Prefixes # Why can LR(0) automata be used to make shift-reduce decisions? The LR(0) automaton for a grammar characterizes the strings of grammar symbols that can appear on the stack of a shift-reduce parser for the grammar. The stack contents must be a prefix of a right-sentential form . If the stack holds $\\alpha$ and the rest of the input is $x$, then a sequence of reductions will take $\\alpha x$ to $S$ . In terms of derivations, $S \\xrightarrow[rm]{*} \\alpha x$. Not all prefixes of right-sentential forms can appear on the stack, however, since the parser must not shift past the handle . For example, suppose $E \\xrightarrow[rm]{ \\ast } F \\ast id \\xrightarrow[rm]{} (E) \\ast id$ Then, at various times during the parse, the stack will hold $($; $(E$ , and $(E )$, but it must not hold $(E ) \\ast$, since $(E )$ is a handle, which the parser must reduce to $F$ before shifting $\\ast$. The prefixes of right sentential forms that can appear on the stack of a shift-reduce parser are called viable prefixes . They are defined as follows: a viable prefix is a prefix of a right-sentential form that does not continue past the right end of the rightmost handle of that sentential form . By this definition, it is always possible to add terminal symbols to the end of a viable prefix to obtain a right-sentential form. NOTE: In simple terms, a prefix cannot contain a handle. Once it does, it should be reduced. SLR parsing is based on the fact that LR(0) automata recognize viable prefixes . We say item $A \\to \\beta _1 \\beta _2$ is valid for a viable prefix $\\alpha \\beta _1$ if there is a derivation $S \\xrightarrow [rm] { } \\alpha Aw \\xrightarrow [rm]{ } \\alpha \\beta _1 \\beta _2 w$ . In general, an item will be valid for many viable prefixes. NOTE: viable prefix\u548c","title":"4.6-Introduction-to-LR-Parsing-Simple-LR"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#46-introduction-to-lr-parsing-simple-lr","text":"The most prevalent type of bottom-up parser to day is based on a concept called LR(k ) parsing; the \"L\" is for left-to-right scanning of the input, the \"R\" for constructing a rightmost derivation in reverse, and the k for the number of input symbols of lookahead that are used in making parsing decisions. The cases k = 0 or k = 1 are of practical interest, and we shall only consider LR parsers with $k \\le 1$ here. When (k ) is omitted, k is assumed to be 1. This section introduces the basic concepts of LR parsing and the easiest method for constructing shift-reduce parsers, called \"simple LR\" (or SLR, for short). Some familiarity with the basic concepts is helpful even if the LR parser itself is constructed using an automatic parser generator. We begin with \"items\" and \"parser states;\" the diagnostic output from an LR parser generator typically includes parser states , which can be used to isolate the sources of parsing conflicts. Section 4.7 introduces two, more complex methods canonical-LR and LALR that are used in the majority of LR parsers.","title":"4.6 Introduction to LR Parsing: Simple LR"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#461-why-lr-parsers","text":"LR parsers are table-driven, much like the nonrecursive LL parsers of Section 4.4.4. A grammar for which we can construct a parsing table using one of the methods in this section and the next is said to be an LR grammar . Intuitively, for a grammar to be LR it is sufficient that a left-to-right shift-reduce parser be able to recognize handles of right-sentential forms when they appear on top of the stack.","title":"4.6.1 Why LR Parsers?"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#462-items-and-the-lr0-automaton","text":"How do es a shift-reduce parser know when to shift and when to reduce? For example, with stack contents $T and next input symbol * in Fig. 4.28, how does the parser know that T on the top of the stack is not a handle, so the appropriate action is to shift and not to reduce T to E ? An LR parser makes shift-reduce decisions by maintaining states to keep track of where we are in a parse. States represent sets of \"items.\" An LR(0) item ( item for short) of a grammar G is a production of G with a dot at some position of the body. Thus, production $A \\to XYZ$ yields the four items The production $A \\to \\epsilon$ generates only one item, $A \\to \\bullet$ . Intuitively, an item indicates how much of a production we have seen at a given point in the parsing process. For example, the item $A \\to X Y Z$ indicates that we hope to see a string derivable from $X Y Z$ next on the input. Item $A \\to X \\cdot Y Z$ indicates that we have just seen on the input a string derivable from $X$ and that we hope next to see a string derivable from $Y Z$ . Item $A \\to X Y Z$ indicates that we have seen the body $X Y Z$ and that it may b e time to reduce $X Y Z$ to $A$. One collection of sets of LR(0) items, called the canonical LR(0) collection, provides the basis for constructing a deterministic finite automaton that is used to make parsing decisions. Such an automaton is called an LR(0) automaton . In particular, each state of the LR(0) automaton represents a set of items in the canonical LR(0) collection . The automaton for the expression grammar (4.1), shown in Fig. 4.31, will serve as the running example for discussing the canonical LR(0) collection for a grammar. To construct the canonical LR(0) collection for a grammar, we define an augmented grammar and two functions, CLOSURE and GOTO. If $G$ is a grammar with start symbol $S$ , then $\\acute{G}$ , the augmented grammar for G, is $G$ with a new start symbol $\\acute{S}$ and production $\\acute{S} \\to S $. The purpose of this new starting production is to indicate to the parser when it should stop parsing and announce acceptance of the input. That is, acceptance occurs when and only when the parser is about to reduce by $\\acute{S} \\to S $.","title":"4.6.2 Items and the LR(0) Automaton"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#closure-of-item-sets","text":"","title":"Closure of Item Sets"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#the-function-goto","text":"","title":"The Function GOTO"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#use-of-the-lr0-automaton","text":"The central idea behind \u201cSimple LR,\" or SLR, parsing is the construction from the grammar of the LR(0) automaton. The states of this automaton are the sets of items from the canonical LR(0) collection, and the transitions are given by the GOTO function. The LR(0) automaton for the expression grammar (4.1) appeared earlier in Fig. 4.31. The start state of the LR(0) automaton is $CLOSURE({[ \\acute{S} \\to S]})$, where $\\acute{S}$ is the start symbol of the augmented grammar. All states are accepting states. We say \"state j \" to refer to the state corresponding to the set of items $I_j$.","title":"Use of the LR(0) Automaton"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#lr0","text":"\u4e00\u4e2a\u4ea7\u751f\u5f0f\u53ef\u80fd\u6709\u591a\u4e2a\u72b6\u6001\uff0c\u5b83\u4eec\u88ab\u6210\u4e3a\u9879\u3002\u6839\u636e\u9879\u7684\u5b9a\u4e49\uff0c\u53ef\u4ee5\u770b\u51fa\u5b83\u662f\u4e3areduction\u800c\u751f\u7684\uff0c\u5b83\u8868\u793a\u4e86\u5206\u6790\u5668\u5728\u5206\u6790\u7684\u8fc7\u7a0b\u4e2d\u5df2\u7ecf\u770b\u5230\u4e86 \u4ea7\u751f\u5f0f\u4f53 \u7684\u54ea\u4e9b\u90e8\u5206\uff0c\u4ee5\u53ca\u5b83\u6240\u671f\u671b\u770b\u5230\u7684\u5408\u4e4e\u5b83\u7684\u6587\u6cd5\u7684\u7b26\u53f7\u4e32\u3002\u5df2\u7ecf\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u5de6\u8fb9\uff0c\u671f\u671b\u770b\u5230\u7684\u4f4d\u4e8e\u70b9\u7684\u53f3\u8fb9\u3002\u663e\u7136\uff0c\u5f53\u5df2\u7ecf\u770b\u5230\u4e86\u4e00\u4e2a\u4ea7\u751f\u5f0f\u4f53\u7684\u5168\u90e8\u7b26\u53f7\u540e\uff0c\u5c31\u53ef\u4ee5\u8fdb\u884c\u89c4\u7ea6\u4e86\u3002 \u65e2\u7136\u5b9a\u4e49\u4e86\u72b6\u6001\uff0c\u90a3\u4e48\u80af\u5b9a\u5c31\u4f1a\u6d89\u53ca\u5230\u72b6\u6001\u7684\u8f6c\u6362\uff1a\u72b6\u6001\u7684\u8f6c\u6362\u6216\u8005\u8bf4\u9879\u7684\u8f6c\u6362\u662f\u7531\u5206\u6790\u5668\u5728\u5206\u6790\u8fc7\u7a0b\u4e2d\u770b\u5230\u7684\u6587\u6cd5\u7b26\u53f7\u800c\u89e6\u53d1\u7684\uff0c\u6bcf\u6b21\u770b\u5230\u4e00\u4e2a\u6587\u6cd5\u7b26\u53f7\uff0c\u5b83\u5c31\u53ef\u4ee5\u5c06\u4ea7\u751f\u5f0f\u4f53\u4e2d\u7684$\\bullet$\u5411\u53f3\u79fb\u52a8\u4e00\u6b21\uff0c\u4ece\u800c\u8fdb\u5165\u5230\u4e86\u4e00\u4e2a\u65b0\u7684\u72b6\u6001\u3002 \u90a3\u7ed9\u5b9a\u6587\u6cd5\uff0c\u6211\u4eec\u80fd\u5426\u63d0\u524d\u5c31\u5206\u6790\u51fa\u5b83\u4f1a\u6709\u54ea\u4e9b\u72b6\u6001\uff0c\u54ea\u4e9b\u8f6c\u6362\u5462\uff1f\u7b54\u6848\u662f\u80af\u5b9a\u7684\uff0c\u8fd9\u5c31\u662f\u6211\u4eec\u6784\u9020\u51fa\u7684LR(0)\u81ea\u52a8\u673a\u3002\u663e\u7136\uff0c\u4e00\u4e2aLR(0)\u81ea\u52a8\u673a\u7ed9\u51fa\u4e86\u7ed9\u5b9a\u6587\u6cd5\u4e2d\u7684\u6240\u6709\u7684\u53ef\u80fd\u7684\u6709\u6548\u7684\u8f6c\u6362\u3002","title":"\u6587\u6cd5\u548cLR(0)\u81ea\u52a8\u673a"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#lr0lr","text":"LR(0)\u81ea\u52a8\u673a\u662f \u786e\u5b9a\u6709\u7a77\u72b6\u6001\u673a \uff0c\u5b83\u4ece \u72b6\u60010 \u5f00\u59cb\u5728\u6bcf\u4e2a\u7b26\u53f7\u4e0a\u90fd\u6709\u8f6c\u6362\u3002\u5982\u679c\u4e00\u4e2a\u72b6\u6001\u8868\u793a\u7684\u4ea7\u751f\u5f0f\u7684\u4f53\u5df2\u7ecf\u5168\u90e8\u90fd\u770b\u5230\u4e86\uff0c\u90a3\u4e48\u663e\u7136\u8fd9\u4e2a\u72b6\u6001\u5c31\u4e0d\u4f1a\u518d\u6709\u8f6c\u6362\u4e86\uff0c\u56e0\u6b64\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u53ef\u4ee5\u9009\u62e9\u5bf9\u5b83\u8fdb\u884c \u89c4\u7ea6 \u3002Example 4.43 \u4e2d\u7ed9\u51fa\u7684LR parser\u53ef\u4ee5\u8ba4\u4e3a\u6709\u4e24\u4e2a\u6808: \u5b57\u7b26\u6808 \u72b6\u6001\u6808 \u79fb\u5165\u64cd\u4f5c \u5bf9\u5e94\u7684\u662f\u5728LR(0)\u72b6\u6001\u673a\u8fdb\u884c\u72b6\u6001\u8f6c\u6362\uff0c\u5373\u4ece\u4e00\u4e2a\u72b6\u6001\u8f6c\u79fb\u5230\u53e6\u5916\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u4f1a\u5c06INPUT\u4e2d\u7684symbol\u79fb\u5165\u5230\u5b57\u7b26\u6808\u4e2d\uff0c\u76f8\u5e94\u7684\uff0c\u4e5f\u4f1a\u5c06\u65b0\u5230\u8fbe\u7684\u72b6\u6001\u538b\u5165\u5230\u72b6\u6001\u6808\u4e2d\u3002\u663e\u7136\uff0c\u72b6\u6001\u6808\u8bb0\u5f55\u4e86\u72b6\u6001\u8f6c\u6362\u7684\u8def\u5f84\uff0c\u5373\u6808\u4e2d\u6bcf\u6761\u8bb0\u5f55\u662f\u4ece\u5b83\u540e\u9762\u7684\u4e00\u6761\u8bb0\u5f55\u8f6c\u6362\u800c\u6765\u7684\u3002 \u90a3\u4e48\u89c4\u7ea6\u610f\u5473\u7740\u5bf9\u4e0a\u8ff0\u4e24\u4e2a\u6808\u6267\u884c\u4ec0\u4e48\u64cd\u4f5c\u5462\uff1f\u89c4\u7ea6\u610f\u5473\u7740\u5c06\u5b57\u7b26\u6808\u4e2d \u4ea7\u751f\u5f0f\u7684\u4f53 \u5f39\u51fa\u6808\uff0c\u76f8\u5e94\u7684\u4e5f\u8981\u4ece\u72b6\u6001\u6808\u4e2d\u5f39\u51fa\u76f8\u5e94\u6570\u91cf\u7684\u72b6\u6001\uff0c\u5e76\u4e14\u5c06\u4ea7\u751f\u5f0f\u7684\u5934\u538b\u5165\u6808\u4e2d\uff0c\u90a3\u4e48\u6b64\u65f6\u8981\u538b\u5165\u4ec0\u4e48\u72b6\u6001\u5462\uff1f \u79fb\u5165\u662f\u6cbf\u7740\u81ea\u52a8\u673a\u7684\u67d0\u6761\u8def\u5f84\u8fdb\u884c\u8f6c\u6362\uff0c\u89c4\u7ea6\u5219\u662f\u56de\u5230\u8fd9\u6761\u8def\u5f84\u7684\u8d77\u70b9\uff0c\u663e\u7136\u89c4\u7ea6\u4f1a\u5f97\u5230\u4e00\u4e2anon-terminal\uff08\u5176\u5b9e\u8fd9\u5c31\u76f8\u5f53\u4e8e\u5df2\u7ecf\u6784\u9020\u597d\u4e86\u5b50\u6811\u4e86\uff09\u3002\u7136\u540e\u5728\u5f53\u524d\u6808\u9876\u7684\u72b6\u6001\u57fa\u4e8e\u524d\u9762\u89c4\u7ea6\u7684\u7b26\u53f7\u8fdb\u884c\u8f6c\u6362\uff0c\u7136\u540e\u5c06\u8f6c\u6362\u5f97\u5230\u7684\u65b0\u72b6\u6001\u538b\u5165\u6808\u4e2d\u3002\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u56e0\u4e3aparser\u5df2\u7ecf\u770b\u5230\u4e86\u8fd9\u4e2anon-terminal\u4e86\uff0c\u6240\u4ee5\u5fc5\u7136\u8981\u8fdb\u884c\u72b6\u6001\u7684\u8f6c\u6362\uff1b","title":"\u4eceLR(0)\u81ea\u52a8\u673a\u6765\u770b\u5f85LR\u8bed\u6cd5\u5206\u6790\u7b97\u6cd5"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#_1","text":"\u4eceFigure 4.31: LR(0) automaton for the expression grammar (4.1)\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cLR(0) automaton\u4e5f\u662f\u6811\u5f62\u7684\uff0cLR\u8bed\u6cd5\u5206\u6790\u5668\u5728\u8fd9\u68f5\u6811\u4e0a\u7684\u64cd\u4f5c\u4e5f\u662f\u57fa\u4e8e \u6808 \u7684\u3002\u5176\u5b9e\u5728\u601d\u8003LR(0) parser\u7684\u65f6\u5019\uff0c\u6211\u60f3\u5230\u4e86\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u5982\u679c\u5c06\u7a0b\u5e8f\u6267\u884c\u8fc7\u7a0b\u7cbe\u7b80\u6210\u51fd\u6570\u8c03\u7528\u7684\u8bdd\uff0c\u5176\u5b9e\u6574\u4e2a\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u53ef\u4ee5\u753b\u6210\u6811\u5f62\u7684\uff0c\u5373\u7a0b\u5e8f\u7684 \u51fd\u6570\u8c03\u7528\u6811 \uff1a\u8fd9\u68f5\u6811\u7684 root\u8282\u70b9\u5c31\u662fmain\u51fd\u6570\uff0cmain\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u8fd9\u68f5\u6811\u7684\u7b2c\u4e00\u5c42\uff0c\u7b2c\u4e00\u5c42\u51fd\u6570\u6240\u8c03\u7528\u7684\u51fd\u6570\u5904\u4e8e\u7b2c\u4e8c\u5c42\uff0c\u4f9d\u6b21\u9012\u63a8\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u68f5\u5b8c\u6574\u7684\u6811\u4e86\uff1b\u5e76\u4e14\u548cLR(0) parser\u4e00\u6837\uff0c\u7a0b\u5e8f\u7684\u6267\u884c\u8fc7\u7a0b\u4e5f\u662f\u4f7f\u7528\u7684stack\u3002\u663e\u7136\uff0c\u4e24\u79cd\u60c5\u51b5\u90fd\u6d89\u53ca\u4e86tree\u548cstack\uff1a \u5728LR parser\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u7b26\u53f7\u538b\u6808\u76f4\u81f3\u770b\u5230\u4e86\u5b8c\u6574\u7684\u4ea7\u751f\u5f0f\u4f53\u624d\u5c06\u4ea7\u751f\u5f0f\u4f53\u5f39\u51fa\u6808\u3001\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5f0f\u7684\u5934\u90e8->\u89c4\u7ea6\u5f97\u5230\u4ea7\u751f\u5934\u4f9d\u8d56\u4e8e\u5f97\u5230\u6240\u6709\u7684\u4ea7\u751f\u4f53->\u4e0d\u65ad\u5730\u5c06\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u538b\u6808\uff0c\u76f4\u5230\u89c1\u5230\u5168\u90e8\u7684\u4ea7\u751f\u5f0f\u4f53\u7b26\u53f7\u5c31\u51fa\u6808 \u5728\u51fd\u6570\u8c03\u7528\u4e2d\uff0c\u9700\u8981\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u5165call stack\u4e2d\uff0c\u76f4\u81f3\u6240\u6709\u7684\u5b50\u51fd\u6570\u90fd\u8fd4\u56de\u4e3b\u51fd\u6570\u624d\u5f97\u4ee5\u6267\u884c\u5b8c\u6210 ->\u4e3b\u51fd\u6570\u7684\u503c\u4f9d\u8d56\u4e8e\u6240\u6709\u7684\u5b50\u51fd\u6570->\u4e0d\u65ad\u5730\u5c06\u5b50\u51fd\u6570\u538b\u6808\uff0c\u77e5\u9053\u6700\u5e95\u5c42\u7684\u5b50\u51fd\u6570\u6c42\u89e3\uff0c\u624d\u4f9d\u6b21\u51fa\u6808 \u770b\uff0c\u4e24\u8005\u7684\u6267\u884c\u8fc7\u7a0b\u662f\u591a\u4e48\u5730\u7c7b\u4f3c\uff1b \u4ece\u6811\u7684\u6784\u9020\u7684\u89d2\u5ea6\u6765\u770b\u5f85LR parser\u4e2d\u7684tree\u548c\u51fd\u6570\u8c03\u7528\u4e2d\u7684\u6811\uff1aLR parser\u4e2d\uff0ctree\u7684\u6784\u9020\u662f\u81ea\u5e95\u5411\u4e0a\u7684\uff0c\u800c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u81ea\u5b9a\u5411\u4e0b\u7684\uff0c\u51fd\u6570\u8c03\u7528\u4e2d\u6811\u7684\u6784\u9020\u662f\u6bd4\u8f83\u7c7b\u4f3c\u4e8eLR(0) parser\u4e2d\u6811\u7684\u6784\u9020\u3002 \u65e0\u8bba\u662f LR(0)\u81ea\u52a8\u673a \u4ee5\u53ca \u51fd\u6570\u8c03\u7528\u6811 \uff0c\u5b83\u4eec\u90fd\u662f\u662f\u6211\u4eec\u4ece\u5168\u5c40\u7684\u89d2\u5ea6\uff08\u6574\u4f53\u7684\u89d2\u5ea6\uff0c\u5206\u6790\u7684\u89d2\u5ea6\uff09\u6765\u5206\u6790\u8fd9\u4e2a\u95ee\u9898\uff0c\u5b83\u4eec\u662f\u7406\u8bba\u5c42\u9762\u7684\u5206\u6790\uff0c\u800c\u4e0d\u662f\u5b9e\u9645\u7684\u5b9e\u73b0\uff0c\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4e2d\uff0c\u538b\u6839\u5c31\u4e0d\u9700\u8981\u663e\u5f0f\u5730\u6784\u9020\u51fa\u8fd9\u6837\u7684\u4e00\u68f5\u6811\uff0c\u5e76\u4e14\u538b\u6839\u5c31\u65e0\u9700\u77e5\u9053\u6574\u4e2a\u6811\u662f\u600e\u6837\u7684\u3002\u6bd4\u5982\u5728LR parser\u4e2d\uff0cparser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u51fd\u6570\u7684\u6267\u884c\u662f\u987a\u5e8f\u6267\u884c\u7684\uff0c\u4e00\u6b21\u53ea\u4f1a\u6267\u884c\u4e00\u4e2a\u51fd\u6570\uff1b\u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u5462\uff1f\u6211\u89c9\u5f97\u8fd9\u662f\u7531\u8ba1\u7b97\u673a\u7684\u4f53\u7cfb\u7ed3\u6784\u6240\u51b3\u5b9a\u7684\uff0c\u6b63\u5982\u5404\u79cdautomaton\u6a21\u578b\u6240\u5c55\u793a\u7684\u90a3\u6837\uff0c\u8ba1\u7b97\u673a\u5c31\u662f\u8fd9\u6837\u7684\u89c4\u5219\uff0c\u5c31\u662f\u8fd9\u6837\u7684\u987a\u5e8f\uff0c\u6240\u4ee5\u6211\u4eec\u7684\u7b97\u6cd5\u8bbe\u8ba1\u4e5f\u662f\u9700\u8981\u5bfb\u627e\u89c4\u5219\uff0c\u987a\u5e8f\uff0c\u8fd9\u662f\u4e00\u79cd\u8ba1\u7b97\u601d\u7ef4\uff1b \u6240\u4ee5\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\uff08\u6709\u8d77\u70b9\uff0c\u6709\u7ec8\u70b9\uff09\uff0c\u663e\u7136\u8fd9\u6761\u8def\u5f84\u662f \u7ebf\u6027\u7684 \uff0c\u662f \u8fde\u7eed\u7684 \uff08\u80fd\u591f\u4ece\u7ec8\u70b9\u518d\u8fd4\u56de\u5230\u8d77\u70b9\uff09\u3002\u5982\u679c\u6211\u4eec\u5c06\u6267\u884c\u7684\u8def\u5f84\u8fde\u63a5\u8d77\u6765\uff08\u56e0\u4e3a\u8fd9\u4e9b\u8def\u5f84\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5c06\u5b83\u4eec\u8fde\u63a5\u8d77\u6765\uff09\uff0c\u4ee5\u9002\u5f53\u7684\u65b9\u5f0f\u753b\u51fa\u4e86\uff08\u6b63\u5982Fig. 4.31\uff09\uff0c\u90a3\u4e48\u5b83\u5c31\u80fd\u591f\u5c55\u73b0\u51fa\u6211\u4eec\u7684\u5728\u7406\u8bba\u5c42\u9762\u5206\u6790\u7684\u5f62\u6001\u3002 \u5982\u679c\u4ece\u6811\u7684\u6784\u9020\u6765\u770b\u7684\u8bdd\uff0c\u5728parser\u662f\u4ece\u5de6\u81f3\u53f3\u5bf9\u8f93\u5165\u4e32\u8fdb\u884c\u5206\u6790\uff0c\u4e00\u6b21\u53ea\u4f1a\u53d6\u4e00\u4e2a\u7b26\u53f7\uff0c\u4ece\u5b50\u6811\u5f00\u59cb\u6784\u9020\uff0c\u7136\u540e\u5c06\u4e00\u68f5\u4e00\u68f5\u7684\u5b50\u6811\u7ed3\u5408\u8d77\u6765\u6784\u9020\u66f4\u5927\u7684\u6811\u3002\u5728data structure\u4e2d\uff0c\u6811\u662f\u6709\u4e00\u4e2a\u4e00\u4e2a\u7684node\u8fde\u63a5\u800c\u6210\u7684\uff0c\u6240\u4ee5\u8bbf\u95ee\u4e00\u68f5\u6811\u53ea\u9700\u8981\u5f97\u5230\u8fd9\u68f5\u6811\u7684\u6839\u7ed3\u70b9\u5373\u53ef\uff0c\u6240\u4ee5\u53ef\u4ee5\u4f7f\u7528node\u6765\u4ee3\u66ff\u4e00\u68f5\u6811\u3002\u6240\u4ee5\u5728\u6811\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\uff0c\u6240\u64cd\u4f5c\u7684\u662f\u4e00\u4e2a\u4e00\u4e2a\u7684node\uff0c\u6240\u4ee5\u4f7f\u7528\u4f7f\u7528stack\u5c31\u53ef\u4ee5\u5b8c\u6210\u4e00\u68f5\u6811\u7684\u6784\u9020\u3002 \u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u4e2d\uff0c\u5bf9\u7406\u8bba\u6a21\u578b\u7684\u5b9e\u73b0\u65f6\u5f80\u5f80\u9009\u62e9\u7684\u662f\u901a\u7528\u7684\uff0c\u7b80\u5355\u7684\u65b9\u5f0f\uff08\u8282\u7ea6\u5185\u5b58\u7b49\uff09\uff0c\"\u5b9e\u9645\u7684\u6267\u884c\u8fc7\u7a0b\u4ec5\u4ec5\u5bf9\u5e94\u7684\u662f\u6811\u4e2d\u7684\u4e00\u6761\u8def\u5f84\"\uff0c\u6240\u4ee5\u6211\u4eec\u4ec5\u4ec5\u9700\u8981\u7684\u662f\u80fd\u591f\u6ee1\u8db3\u8fd9\u6761\u8def\u5f84\u7684\u7ed3\u6784\u5373\u53ef\u3002\u800c\u6808\u8fd9\u79cd\u7ed3\u6784\u5c31\u6b63\u597d\u7b26\u5408\u8fd9\u4e9b\u8981\u6c42\uff1a \u6808\u662f\u7ebf\u6027\u7684 \u6808\u662f\u8fde\u7eed\u7684\uff0c\u6240\u4ee5\u80fd\u591f\u5b9e\u73b0\u4ece\u7ec8\u70b9\u56de\u5230\u8d77\u70b9 \u518d\u56de\u5230\u7406\u8bba\u5206\u6790\u5c42\u9762\uff0c\u5b9e\u9645\u6267\u884c\u8fc7\u7a0b\u548c\u7406\u8bba\u5c42\u9762\u7684\u6a21\u578b\u4e4b\u95f4\u662f\u600e\u6837\u7684\u5173\u8054\u5462\uff1f\u5b9e\u9645\u6267\u884c\u6d41\u7a0b\u5bf9\u5e94\u7684\u662f\u5bf9\u6811\u6267\u884c\u6df1\u5ea6\u4f18\u5148\u540e\u5e8f\u904d\u5386\uff1b","title":"\u6811\u4e0e\u6808"},{"location":"Chapter-4-Syntax-Analysis/4.6-Introduction-to-LR-Parsing-Simple-LR/#465-viable-prefixes","text":"Why can LR(0) automata be used to make shift-reduce decisions? The LR(0) automaton for a grammar characterizes the strings of grammar symbols that can appear on the stack of a shift-reduce parser for the grammar. The stack contents must be a prefix of a right-sentential form . If the stack holds $\\alpha$ and the rest of the input is $x$, then a sequence of reductions will take $\\alpha x$ to $S$ . In terms of derivations, $S \\xrightarrow[rm]{*} \\alpha x$. Not all prefixes of right-sentential forms can appear on the stack, however, since the parser must not shift past the handle . For example, suppose $E \\xrightarrow[rm]{ \\ast } F \\ast id \\xrightarrow[rm]{} (E) \\ast id$ Then, at various times during the parse, the stack will hold $($; $(E$ , and $(E )$, but it must not hold $(E ) \\ast$, since $(E )$ is a handle, which the parser must reduce to $F$ before shifting $\\ast$. The prefixes of right sentential forms that can appear on the stack of a shift-reduce parser are called viable prefixes . They are defined as follows: a viable prefix is a prefix of a right-sentential form that does not continue past the right end of the rightmost handle of that sentential form . By this definition, it is always possible to add terminal symbols to the end of a viable prefix to obtain a right-sentential form. NOTE: In simple terms, a prefix cannot contain a handle. Once it does, it should be reduced. SLR parsing is based on the fact that LR(0) automata recognize viable prefixes . We say item $A \\to \\beta _1 \\beta _2$ is valid for a viable prefix $\\alpha \\beta _1$ if there is a derivation $S \\xrightarrow [rm] { } \\alpha Aw \\xrightarrow [rm]{ } \\alpha \\beta _1 \\beta _2 w$ . In general, an item will be valid for many viable prefixes. NOTE: viable prefix\u548c","title":"4.6.5 Viable Prefixes"},{"location":"Chapter-4-Syntax-Analysis/4.7-More-Powerful-LR-Parsers/","text":"4.7 More Powerful LR Parsers 4.7.4 Constructing LALR Parsing Tables 4.7 More Powerful LR Parsers # In this section, we shall extend the previous LR parsing techniques to use one symbol of lookahead on the input. There are two different methods: The \"canonical-LR\" or just \"LR\" method The \"lookahead-LR\" or \"LALR\" method After introducing both these methods, we conclude with a discussion of how to compact LR parsing tables for environments with limited memory. 4.7.4 Constructing LALR Parsing Tables # We now introduce our last parser construction method, the LALR (lookahead-LR) technique. This method is often used in practice, because the tables obtained by it are considerably smaller than the canonical LR tables, yet most common syntactic constructs of programming languages can be expressed conveniently by an LALR grammar. The same is almost true for SLR grammars, but there are a few constructs that cannot be conveniently handled by SLR techniques (see Example 4.48, for example). For a comparison of parser size, the SLR and LALR tables for a grammar always have the same number of states, and this number is typically several hundred states for a language like C. The canonical LR table would typically have several thousand states for the same-size language. Thus, it is much easier and more economical to construct SLR and LALR tables than the canonical LR tables.","title":"4.7-More-Powerful-LR-Parsers"},{"location":"Chapter-4-Syntax-Analysis/4.7-More-Powerful-LR-Parsers/#47-more-powerful-lr-parsers","text":"In this section, we shall extend the previous LR parsing techniques to use one symbol of lookahead on the input. There are two different methods: The \"canonical-LR\" or just \"LR\" method The \"lookahead-LR\" or \"LALR\" method After introducing both these methods, we conclude with a discussion of how to compact LR parsing tables for environments with limited memory.","title":"4.7 More Powerful LR Parsers"},{"location":"Chapter-4-Syntax-Analysis/4.7-More-Powerful-LR-Parsers/#474-constructing-lalr-parsing-tables","text":"We now introduce our last parser construction method, the LALR (lookahead-LR) technique. This method is often used in practice, because the tables obtained by it are considerably smaller than the canonical LR tables, yet most common syntactic constructs of programming languages can be expressed conveniently by an LALR grammar. The same is almost true for SLR grammars, but there are a few constructs that cannot be conveniently handled by SLR techniques (see Example 4.48, for example). For a comparison of parser size, the SLR and LALR tables for a grammar always have the same number of states, and this number is typically several hundred states for a language like C. The canonical LR table would typically have several thousand states for the same-size language. Thus, it is much easier and more economical to construct SLR and LALR tables than the canonical LR tables.","title":"4.7.4 Constructing LALR Parsing Tables"},{"location":"Chapter-4-Syntax-Analysis/VS-LL-parser-VS-LR-parser/","text":"What is the difference between LL and LR parsing? A A \u9884\u6d4b\u5206\u6790\u8868 VS LR\u8bed\u6cd5\u5206\u6790\u8868 \u4eceLR(0)\u81ea\u52a8\u673a\u5230LR\u8bed\u6cd5\u5206\u6790\u8868 LR\u8bed\u6cd5\u5206\u6790\u5668\u7684\u683c\u5c40configuration VS LR(0)\u81ea\u52a8\u673a\u7684\u72b6\u6001 What is the difference between LL and LR parsing? # A # At a high level, the difference between LL parsing and LR parsing is that LL parsers begin at the start symbol and try to apply productions to arrive at the target string , whereas LR parsers begin at the target string and try to arrive back at the start symbol . An LL parse is a left-to-right, leftmost derivation. That is, we consider the input symbols from the left to the right and attempt to construct a leftmost derivation. This is done by beginning at the start symbol and repeatedly expanding out the leftmost nonterminal until we arrive at the target string . An LR parse is a left-to-right, rightmost derivation, meaning that we scan from the left to right and attempt to construct a rightmost derivation. The parser continuously picks a substring of the input and attempts to reverse it back to a nonterminal. During an LL parse, the parser continuously chooses between two actions: Predict : Based on the leftmost nonterminal and some number of lookahead tokens, choose which production ought to be applied to get closer to the input string. Match : Match the leftmost guessed terminal symbol with the leftmost unconsumed symbol of input. As an example, given this grammar: S \u2192 E E \u2192 T + E E \u2192 T T \u2192 int Then given the string int + int + int , an LL(2) parser (which uses two tokens of lookahead) would parse the string as follows: Production Input Action --------------------------------------------------------- S int + int + int Predict S -> E E int + int + int Predict E -> T + E T + E int + int + int Predict T -> int int + E int + int + int Match int + E + int + int Match + E int + int Predict E -> T + E T + E int + int Predict T -> int int + E int + int Match int + E + int Match + E int Predict E -> T T int Predict T -> int int int Match int Accept Notice that in each step we look at the leftmost symbol in our production. If it's a terminal, we match it, and if it's a nonterminal, we predict what it's going to be by choosing one of the rules. In an LR parser, there are two actions: Shift : Add the next token of input to a buffer for consideration. Reduce : Reduce a collection of terminals and nonterminals in this buffer back to some nonterminal by reversing a production. As an example, an LR(1) parser (with one token of lookahead) might parse that same string as follows: Workspace Input Action --------------------------------------------------------- int + int + int Shift int + int + int Reduce T -> int T + int + int Shift T + int + int Shift T + int + int Reduce T -> int T + T + int Shift T + T + int Shift T + T + int Reduce T -> int T + T + T Reduce E -> T T + T + E Reduce E -> T + E T + E Reduce E -> T + E E Reduce S -> E S Accept The two parsing algorithms you mentioned (LL and LR) are known to have different characteristics. LL parsers tend to be easier to write by hand, but they are less powerful than LR parsers and accept a much smaller set of grammars than LR parsers do. LR parsers come in many flavors (LR(0), SLR(1), LALR(1), LR(1), IELR(1), GLR(0), etc.) and are far more powerful. They also tend to have much more complex and are almost always generated by tools like yacc or bison . LL parsers also come in many flavors (including LL(*), which is used by the ANTLR tool), though in practice LL(1) is the most-widely used. As a shameless plug, if you'd like to learn more about LL and LR parsing, I just finished teaching a compilers course and have some handouts and lecture slides on parsing on the course website. I'd be glad to elaborate on any of them if you think it would be useful. A # Josh Haberman in his article LL and LR Parsing Demystified claims that LL parsing directly corresponds with the Polish Notation , whereas LR corresponds to Reverse Polish Notation . The difference between PN and RPN is the order of traversing the binary tree of the equation: + 1 * 2 3 // Polish (prefix) expression; pre-order traversal. 1 2 3 * + // Reverse Polish (postfix) expression; post-order traversal. According to Haberman, this illustrates the main difference between LL and LR parsers: The primary difference between how LL and LR parsers operate is that an LL parser outputs a pre-order traversal of the parse tree and an LR parser outputs a post-order traversal. For the in-depth explanation, examples and conclusions check out Haberman's article . \u9884\u6d4b\u5206\u6790\u8868 VS LR\u8bed\u6cd5\u5206\u6790\u8868 # LL\u548cLR\u90fd\u662f\u8868\u9a71\u52a8\u7684\uff0c\u8fd9\u4e24\u4e2a\u8868\u5c31\u662f\u5206\u522b\u9a71\u52a8\u4e24\u8005\u7684\u8868\u3002 \u8bed\u6cd5\u5206\u6790\u7684\u8fc7\u7a0b\u662f\u4e0d\u65ad\u6839\u636e\u4ea7\u751f\u5f0f\u8fdb\u884c\u8f6c\u6362\u7684\u8fc7\u7a0b\u3002 \u4e24\u8005\u7684\u6784\u9020\u90fd\u662f\u57fa\u4e8e\u5bf9grammar\u7684\u5206\u6790\uff0c\u4e24\u8005\u7684\u6784\u9020\u8fc7\u7a0b\u90fd\u662f\u6cbf\u7740\u4ea7\u751f\u5f0f\u8fdb\u884cderive\uff0c\u6240\u4e0d\u540c\u7684\u662f\uff0c\u9884\u6d4b\u5206\u6790\u8868\u4e00\u76f4derive\u5230\u4e86terminal\uff1b\u800cLR\u8bed\u6cd5\u5206\u6790\u8868\u5219\u662f\u5c06\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u5168\u90e8\u5305\u542b\u4e86\uff1a \u9884\u6d4b\u5206\u6790\u8868\u7684\u6784\u9020\u4f7f\u7528\u662fnon-terminal symbol\u7684FIRST\u548cFOLLOW\uff0cFIRST\u548cFOLLOW\u6240\u5305\u542b\u7684\u90fd\u662fterminal\uff0c\u5176\u5b9e\u5b83\u7684\u76ee\u7684\u662f\u77e5\u9053\u5f53\u9047\u5230\u67d0\u4e2aterminal\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u54ea\u4e2aproduction\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2aterminal\u3002\u5373\u5b83\u4fa7\u91cd\u7684\u662f\u5bf9\u4e8e\u4e00\u4e2anon-terminal\u7684production\uff0c\u5b83\u80fd\u591f\u63a8\u5bfc\u51fa\u4ec0\u4e48\uff0c\u8fd9\u6837\u5b83\u5c31\u80fd\u591f\u636e\u6b64\u6765\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u8fd9\u4e2aproduction\u3002 LR\u8bed\u6cd5\u5206\u6790\u8868\u6b63\u5982\u5176\u540d\u79f0\u6240\u63ed\u793a\u5730\uff0c\u5b83\u5176\u5b9e\u662f\u5bf9\u8bed\u6cd5\u7684\u5206\u6790\uff0c\u5bf9\u8bed\u6cd5\u4e2d\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u8fdb\u884c\u5206\u6790\uff0c\u6784\u9020\u51fa\u6765\u5b83\u7684\u8f6c\u6362\u6240\u5bf9\u5e94\u7684automaton\u3002\u663e\u7136\uff0c\u56e0\u4e3a\u5b83\u7684\u8f6c\u6362\u90fd\u662f\u57fa\u4e8egrammar\u6240\u6784\u9020\u51fa\u6765\u7684\uff0c\u56e0\u6b64\uff0c\u5b83\u7684\u6240\u6709\u7684\u8f6c\u6362\u90fd\u662f\u6709\u6548\u7684\u8f6c\u6362\uff0c\u56e0\u6b64\u53ea\u8981\u5f85\u5206\u6790\u7684\u4e32\u4e0d\u7b26\u5408\u8fd9\u4e2aautomaton\u7684\u8f6c\u6362\uff0c\u90a3\u4e48\u5b83\u5c31\u662f\u65e0\u6548\u7684\u4e86\u3002\u56e0\u6b64\u6211\u4eec\u7684\u5f85\u5206\u6790\u7684\u4e32\u4e00\u5b9a\u662f\u5bf9\u5e94\u4e86automaton\u4e2d\u7684\u4e00\u6761\u8def\u5f84\u3002\u4f7f\u7528\u6211\u4eec\u7684\u6587\u6cd5\uff0c\u6309\u7167\u67d0\u79cd\u63a8\u5bfc\u65b9\u5f0f\u662f\u4e00\u5b9a\u80fd\u591fderive\u8fd9\u4e2a\u4e32\u7684\u3002\u5728\u5f85\u5206\u6790\u7684\u4e32\u4e2d\uff0c\u4ec5\u4ec5\u5305\u542b\u7684\u662fterminal\uff0c\u800cgrammar\u4e2d\uff0c\u662f\u4e24\u8005\u90fd\u5305\u542b\u7684\uff0c\u6240\u4ee5\u5728\u8fdb\u884c\u5206\u6790\u7684\u65f6\u5019\uff0c\u4e00\u5b9a\u8981\u5c06terminal\u89c4\u7ea6\u4e3anon-terminal\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u4f7f\u5206\u6790\u7ee7\u7eed\u4e0b\u53bb\u3002LR\u8bed\u6cd5\u5206\u6790\u662f\u63a8\u5bfc\u7684\u9006\u8fc7\u7a0b\uff0c\u663e\u7136\u6211\u4eec\u662f\u8981\u6cbf\u7740\u63a8\u5bfc\u65b9\u5411\u9006\u6d41\u800c\u4e0a\uff0c\u56e0\u4e3a\u63a8\u5bfc\u7684\u8fc7\u7a0b\u7684\u4e2d\u95f4\u8fc7\u7a0b\u80af\u5b9a\u662f\u4f1a\u5b58\u5728non-terminal\u7684\uff0c\u800c\u6211\u4eec\u662f\u9006\u5411\u8fdb\u884c\u63a8\u5bfc\uff0c\u6240\u4ee5\u80af\u5b9a\u9700\u8981\u5c06\u4e00\u4e9bterminal\u89c4\u7ea6\u4e3anon-terminal\u3002 \u4e3a\u4ec0\u4e48LR\u662fright-most derivation\uff1f \u9884\u6d4b\u5206\u6790\u8868\u5176\u5b9e\u4e5f\u662f\u4e00\u4e2a\u8f6c\u6362\u51fd\u6570\uff0c\u8981\u4f7f\u7528\u54ea\u4e2a\u4ea7\u751f\u5f0f\u8fdb\u884cderivate LR\u662f\u4e00\u4e2aautomaton\uff0c\u72b6\u6001\uff0c\u8f6c\u6362 \u8bed\u6cd5\u5206\u6790\u8868\u7531\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210\uff1a action goto \u4eceLR(0)\u81ea\u52a8\u673a\u5230LR\u8bed\u6cd5\u5206\u6790\u8868 # LR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u662f\u57fa\u4e8eLR(0)\u81ea\u52a8\u673a\u7684 \u672c\u8d28\u8fd8\u662f\u8f6c\u6362\uff0c\u65e0\u8bba\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\u8fd8\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362 action\u4e3b\u8981\u5b9a\u4e49\u7684\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\uff0c\u800cgoto\u5219\u5b9a\u4e49\u7684\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362\u3002 SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u8fc7\u7a0b\u4e3b\u8981\u8ba9\u6211\u611f\u5230\u56f0\u60d1\u7684\u662f\u5b83\u5c06action\u5b9a\u4e49\u6210\u4e86 \u72b6\u6001\u548cterminal\u7684\u51fd\u6570\uff1f SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684ACTION\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\u6709\u8fd9\u6837\u7684\u89c4\u5219\uff1a LR\u8bed\u6cd5\u5206\u6790\u5668\u7684\u683c\u5c40configuration VS LR(0)\u81ea\u52a8\u673a\u7684\u72b6\u6001 #","title":"VS-LL-parser-VS-LR-parser"},{"location":"Chapter-4-Syntax-Analysis/VS-LL-parser-VS-LR-parser/#what-is-the-difference-between-ll-and-lr-parsing","text":"","title":"What is the difference between LL and LR parsing?"},{"location":"Chapter-4-Syntax-Analysis/VS-LL-parser-VS-LR-parser/#a","text":"At a high level, the difference between LL parsing and LR parsing is that LL parsers begin at the start symbol and try to apply productions to arrive at the target string , whereas LR parsers begin at the target string and try to arrive back at the start symbol . An LL parse is a left-to-right, leftmost derivation. That is, we consider the input symbols from the left to the right and attempt to construct a leftmost derivation. This is done by beginning at the start symbol and repeatedly expanding out the leftmost nonterminal until we arrive at the target string . An LR parse is a left-to-right, rightmost derivation, meaning that we scan from the left to right and attempt to construct a rightmost derivation. The parser continuously picks a substring of the input and attempts to reverse it back to a nonterminal. During an LL parse, the parser continuously chooses between two actions: Predict : Based on the leftmost nonterminal and some number of lookahead tokens, choose which production ought to be applied to get closer to the input string. Match : Match the leftmost guessed terminal symbol with the leftmost unconsumed symbol of input. As an example, given this grammar: S \u2192 E E \u2192 T + E E \u2192 T T \u2192 int Then given the string int + int + int , an LL(2) parser (which uses two tokens of lookahead) would parse the string as follows: Production Input Action --------------------------------------------------------- S int + int + int Predict S -> E E int + int + int Predict E -> T + E T + E int + int + int Predict T -> int int + E int + int + int Match int + E + int + int Match + E int + int Predict E -> T + E T + E int + int Predict T -> int int + E int + int Match int + E + int Match + E int Predict E -> T T int Predict T -> int int int Match int Accept Notice that in each step we look at the leftmost symbol in our production. If it's a terminal, we match it, and if it's a nonterminal, we predict what it's going to be by choosing one of the rules. In an LR parser, there are two actions: Shift : Add the next token of input to a buffer for consideration. Reduce : Reduce a collection of terminals and nonterminals in this buffer back to some nonterminal by reversing a production. As an example, an LR(1) parser (with one token of lookahead) might parse that same string as follows: Workspace Input Action --------------------------------------------------------- int + int + int Shift int + int + int Reduce T -> int T + int + int Shift T + int + int Shift T + int + int Reduce T -> int T + T + int Shift T + T + int Shift T + T + int Reduce T -> int T + T + T Reduce E -> T T + T + E Reduce E -> T + E T + E Reduce E -> T + E E Reduce S -> E S Accept The two parsing algorithms you mentioned (LL and LR) are known to have different characteristics. LL parsers tend to be easier to write by hand, but they are less powerful than LR parsers and accept a much smaller set of grammars than LR parsers do. LR parsers come in many flavors (LR(0), SLR(1), LALR(1), LR(1), IELR(1), GLR(0), etc.) and are far more powerful. They also tend to have much more complex and are almost always generated by tools like yacc or bison . LL parsers also come in many flavors (including LL(*), which is used by the ANTLR tool), though in practice LL(1) is the most-widely used. As a shameless plug, if you'd like to learn more about LL and LR parsing, I just finished teaching a compilers course and have some handouts and lecture slides on parsing on the course website. I'd be glad to elaborate on any of them if you think it would be useful.","title":"A"},{"location":"Chapter-4-Syntax-Analysis/VS-LL-parser-VS-LR-parser/#a_1","text":"Josh Haberman in his article LL and LR Parsing Demystified claims that LL parsing directly corresponds with the Polish Notation , whereas LR corresponds to Reverse Polish Notation . The difference between PN and RPN is the order of traversing the binary tree of the equation: + 1 * 2 3 // Polish (prefix) expression; pre-order traversal. 1 2 3 * + // Reverse Polish (postfix) expression; post-order traversal. According to Haberman, this illustrates the main difference between LL and LR parsers: The primary difference between how LL and LR parsers operate is that an LL parser outputs a pre-order traversal of the parse tree and an LR parser outputs a post-order traversal. For the in-depth explanation, examples and conclusions check out Haberman's article .","title":"A"},{"location":"Chapter-4-Syntax-Analysis/VS-LL-parser-VS-LR-parser/#vs-lr","text":"LL\u548cLR\u90fd\u662f\u8868\u9a71\u52a8\u7684\uff0c\u8fd9\u4e24\u4e2a\u8868\u5c31\u662f\u5206\u522b\u9a71\u52a8\u4e24\u8005\u7684\u8868\u3002 \u8bed\u6cd5\u5206\u6790\u7684\u8fc7\u7a0b\u662f\u4e0d\u65ad\u6839\u636e\u4ea7\u751f\u5f0f\u8fdb\u884c\u8f6c\u6362\u7684\u8fc7\u7a0b\u3002 \u4e24\u8005\u7684\u6784\u9020\u90fd\u662f\u57fa\u4e8e\u5bf9grammar\u7684\u5206\u6790\uff0c\u4e24\u8005\u7684\u6784\u9020\u8fc7\u7a0b\u90fd\u662f\u6cbf\u7740\u4ea7\u751f\u5f0f\u8fdb\u884cderive\uff0c\u6240\u4e0d\u540c\u7684\u662f\uff0c\u9884\u6d4b\u5206\u6790\u8868\u4e00\u76f4derive\u5230\u4e86terminal\uff1b\u800cLR\u8bed\u6cd5\u5206\u6790\u8868\u5219\u662f\u5c06\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u5168\u90e8\u5305\u542b\u4e86\uff1a \u9884\u6d4b\u5206\u6790\u8868\u7684\u6784\u9020\u4f7f\u7528\u662fnon-terminal symbol\u7684FIRST\u548cFOLLOW\uff0cFIRST\u548cFOLLOW\u6240\u5305\u542b\u7684\u90fd\u662fterminal\uff0c\u5176\u5b9e\u5b83\u7684\u76ee\u7684\u662f\u77e5\u9053\u5f53\u9047\u5230\u67d0\u4e2aterminal\u7684\u65f6\u5019\uff0c\u4f7f\u7528\u54ea\u4e2aproduction\u53ef\u4ee5\u5f97\u5230\u8fd9\u4e2aterminal\u3002\u5373\u5b83\u4fa7\u91cd\u7684\u662f\u5bf9\u4e8e\u4e00\u4e2anon-terminal\u7684production\uff0c\u5b83\u80fd\u591f\u63a8\u5bfc\u51fa\u4ec0\u4e48\uff0c\u8fd9\u6837\u5b83\u5c31\u80fd\u591f\u636e\u6b64\u6765\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u8fd9\u4e2aproduction\u3002 LR\u8bed\u6cd5\u5206\u6790\u8868\u6b63\u5982\u5176\u540d\u79f0\u6240\u63ed\u793a\u5730\uff0c\u5b83\u5176\u5b9e\u662f\u5bf9\u8bed\u6cd5\u7684\u5206\u6790\uff0c\u5bf9\u8bed\u6cd5\u4e2d\u6240\u6709\u7684\u53ef\u80fd\u7684\u8f6c\u6362\u8fdb\u884c\u5206\u6790\uff0c\u6784\u9020\u51fa\u6765\u5b83\u7684\u8f6c\u6362\u6240\u5bf9\u5e94\u7684automaton\u3002\u663e\u7136\uff0c\u56e0\u4e3a\u5b83\u7684\u8f6c\u6362\u90fd\u662f\u57fa\u4e8egrammar\u6240\u6784\u9020\u51fa\u6765\u7684\uff0c\u56e0\u6b64\uff0c\u5b83\u7684\u6240\u6709\u7684\u8f6c\u6362\u90fd\u662f\u6709\u6548\u7684\u8f6c\u6362\uff0c\u56e0\u6b64\u53ea\u8981\u5f85\u5206\u6790\u7684\u4e32\u4e0d\u7b26\u5408\u8fd9\u4e2aautomaton\u7684\u8f6c\u6362\uff0c\u90a3\u4e48\u5b83\u5c31\u662f\u65e0\u6548\u7684\u4e86\u3002\u56e0\u6b64\u6211\u4eec\u7684\u5f85\u5206\u6790\u7684\u4e32\u4e00\u5b9a\u662f\u5bf9\u5e94\u4e86automaton\u4e2d\u7684\u4e00\u6761\u8def\u5f84\u3002\u4f7f\u7528\u6211\u4eec\u7684\u6587\u6cd5\uff0c\u6309\u7167\u67d0\u79cd\u63a8\u5bfc\u65b9\u5f0f\u662f\u4e00\u5b9a\u80fd\u591fderive\u8fd9\u4e2a\u4e32\u7684\u3002\u5728\u5f85\u5206\u6790\u7684\u4e32\u4e2d\uff0c\u4ec5\u4ec5\u5305\u542b\u7684\u662fterminal\uff0c\u800cgrammar\u4e2d\uff0c\u662f\u4e24\u8005\u90fd\u5305\u542b\u7684\uff0c\u6240\u4ee5\u5728\u8fdb\u884c\u5206\u6790\u7684\u65f6\u5019\uff0c\u4e00\u5b9a\u8981\u5c06terminal\u89c4\u7ea6\u4e3anon-terminal\uff0c\u8fd9\u6837\u624d\u80fd\u591f\u4f7f\u5206\u6790\u7ee7\u7eed\u4e0b\u53bb\u3002LR\u8bed\u6cd5\u5206\u6790\u662f\u63a8\u5bfc\u7684\u9006\u8fc7\u7a0b\uff0c\u663e\u7136\u6211\u4eec\u662f\u8981\u6cbf\u7740\u63a8\u5bfc\u65b9\u5411\u9006\u6d41\u800c\u4e0a\uff0c\u56e0\u4e3a\u63a8\u5bfc\u7684\u8fc7\u7a0b\u7684\u4e2d\u95f4\u8fc7\u7a0b\u80af\u5b9a\u662f\u4f1a\u5b58\u5728non-terminal\u7684\uff0c\u800c\u6211\u4eec\u662f\u9006\u5411\u8fdb\u884c\u63a8\u5bfc\uff0c\u6240\u4ee5\u80af\u5b9a\u9700\u8981\u5c06\u4e00\u4e9bterminal\u89c4\u7ea6\u4e3anon-terminal\u3002 \u4e3a\u4ec0\u4e48LR\u662fright-most derivation\uff1f \u9884\u6d4b\u5206\u6790\u8868\u5176\u5b9e\u4e5f\u662f\u4e00\u4e2a\u8f6c\u6362\u51fd\u6570\uff0c\u8981\u4f7f\u7528\u54ea\u4e2a\u4ea7\u751f\u5f0f\u8fdb\u884cderivate LR\u662f\u4e00\u4e2aautomaton\uff0c\u72b6\u6001\uff0c\u8f6c\u6362 \u8bed\u6cd5\u5206\u6790\u8868\u7531\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210\uff1a action goto","title":"\u9884\u6d4b\u5206\u6790\u8868 VS LR\u8bed\u6cd5\u5206\u6790\u8868"},{"location":"Chapter-4-Syntax-Analysis/VS-LL-parser-VS-LR-parser/#lr0lr","text":"LR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u662f\u57fa\u4e8eLR(0)\u81ea\u52a8\u673a\u7684 \u672c\u8d28\u8fd8\u662f\u8f6c\u6362\uff0c\u65e0\u8bba\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\u8fd8\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362 action\u4e3b\u8981\u5b9a\u4e49\u7684\u662f\u5728terminal\u4e0a\u7684\u8f6c\u6362\uff0c\u800cgoto\u5219\u5b9a\u4e49\u7684\u662f\u5728non-terminal\u4e0a\u7684\u8f6c\u6362\u3002 SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684\u6784\u9020\u8fc7\u7a0b\u4e3b\u8981\u8ba9\u6211\u611f\u5230\u56f0\u60d1\u7684\u662f\u5b83\u5c06action\u5b9a\u4e49\u6210\u4e86 \u72b6\u6001\u548cterminal\u7684\u51fd\u6570\uff1f SLR\u8bed\u6cd5\u5206\u6790\u8868\u7684ACTION\u7684\u6784\u9020\u8fc7\u7a0b\u4e2d\u6709\u8fd9\u6837\u7684\u89c4\u5219\uff1a","title":"\u4eceLR(0)\u81ea\u52a8\u673a\u5230LR\u8bed\u6cd5\u5206\u6790\u8868"},{"location":"Chapter-4-Syntax-Analysis/VS-LL-parser-VS-LR-parser/#lrconfiguration-vs-lr0","text":"","title":"LR\u8bed\u6cd5\u5206\u6790\u5668\u7684\u683c\u5c40configuration VS LR(0)\u81ea\u52a8\u673a\u7684\u72b6\u6001"},{"location":"Chapter-5-Syntax-Directed-Translation/","text":"Chapter 5 Syntax-Directed Translation # This chapter develops the theme of Section 2.3: the translation of languages guided by context-free grammars . The translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation . The techniques are also useful for implementing little languages for specialized tasks; this chapter includes an example from typesetting. We associate information with a language construct by attaching attributes to the grammar symbol(s) representing the construct, as discussed in Section 2.3.2. A syntax-directed definition specifies the values of attributes by associating semantic rules with the grammar productions. For example, an infix-to-postfix translator might have a production and rule From Section 2.3.5, a syntax-directed translation scheme embeds program fragments called semantic actions within production bodies, as in By convention, semantic actions are enclosed within curly braces. Between the two notations, syntax-directed definitions can be more readable, and hence more useful for specifications. However, translation schemes can be more efficient, and hence more useful for implementations. syntax-directed definition SDD semantic rule syntax-directed translation scheme SDT semantic action The most general approach to syntax-directed translation is to construct a parse tree or a syntax tree , and then to compute the values of attributes at the nodes of the tree by visiting the nodes of the tree. In many cases, translation can be done during parsing, without building an explicit tree. We shall therefore study a class of syntax-directed translations called \"L-attributed translations\" (L for left-to-right), which encompass virtually all translations that can be performed during parsing. We also study a smaller class, called \"S-attributed translations\" (S for synthesized), which can be performed easily in connection with a bottom-up parse. NOTE: \u672c\u4e66\u5bf9Syntax-directed translation\u7684\u529f\u80fd\u6ca1\u6709\u8fdb\u884c\u76f4\u63a5\u7684\u63cf\u8ff0\uff0c\u662f\u975e\u5e38\u4efb\u610f\u8ba9\u4eba\u7cca\u6d82\u7684\uff0cwikipedia\u7684 Syntax-directed translation \u975e\u5e38\u76f4\u63a5\u7b80\u660e\u7684\u63cf\u8ff0\u4e86Syntax-directed translation\u7684\u529f\u80fd: Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar . Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax . \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5df2\u7ecf\u5c06SDT\u548c semantics \u5173\u8054\u5230\u4e00\u8d77\u4e86\uff0c\u5176\u5b9eSDT\u662f Semantic analysis \u7684\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u6b63\u5982wikipedia\u7684 compiler \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a A compiler is likely to perform many or all of the following operations: preprocessing , lexical analysis , parsing , semantic analysis ( syntax-directed translation ), conversion of input programs to an intermediate representation , code optimization and code generation .","title":5},{"location":"Chapter-5-Syntax-Directed-Translation/#chapter-5-syntax-directed-translation","text":"This chapter develops the theme of Section 2.3: the translation of languages guided by context-free grammars . The translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation . The techniques are also useful for implementing little languages for specialized tasks; this chapter includes an example from typesetting. We associate information with a language construct by attaching attributes to the grammar symbol(s) representing the construct, as discussed in Section 2.3.2. A syntax-directed definition specifies the values of attributes by associating semantic rules with the grammar productions. For example, an infix-to-postfix translator might have a production and rule From Section 2.3.5, a syntax-directed translation scheme embeds program fragments called semantic actions within production bodies, as in By convention, semantic actions are enclosed within curly braces. Between the two notations, syntax-directed definitions can be more readable, and hence more useful for specifications. However, translation schemes can be more efficient, and hence more useful for implementations. syntax-directed definition SDD semantic rule syntax-directed translation scheme SDT semantic action The most general approach to syntax-directed translation is to construct a parse tree or a syntax tree , and then to compute the values of attributes at the nodes of the tree by visiting the nodes of the tree. In many cases, translation can be done during parsing, without building an explicit tree. We shall therefore study a class of syntax-directed translations called \"L-attributed translations\" (L for left-to-right), which encompass virtually all translations that can be performed during parsing. We also study a smaller class, called \"S-attributed translations\" (S for synthesized), which can be performed easily in connection with a bottom-up parse. NOTE: \u672c\u4e66\u5bf9Syntax-directed translation\u7684\u529f\u80fd\u6ca1\u6709\u8fdb\u884c\u76f4\u63a5\u7684\u63cf\u8ff0\uff0c\u662f\u975e\u5e38\u4efb\u610f\u8ba9\u4eba\u7cca\u6d82\u7684\uff0cwikipedia\u7684 Syntax-directed translation \u975e\u5e38\u76f4\u63a5\u7b80\u660e\u7684\u63cf\u8ff0\u4e86Syntax-directed translation\u7684\u529f\u80fd: Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar . Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax . \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u5df2\u7ecf\u5c06SDT\u548c semantics \u5173\u8054\u5230\u4e00\u8d77\u4e86\uff0c\u5176\u5b9eSDT\u662f Semantic analysis \u7684\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff0c\u6b63\u5982wikipedia\u7684 compiler \u4e2d\u6240\u63cf\u8ff0\u7684\uff1a A compiler is likely to perform many or all of the following operations: preprocessing , lexical analysis , parsing , semantic analysis ( syntax-directed translation ), conversion of input programs to an intermediate representation , code optimization and code generation .","title":"Chapter 5 Syntax-Directed Translation"},{"location":"Chapter-5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/","text":"5.1 Syntax-Directed Definitions # A syntax-directed definition (SDD) is a context-free grammar together with attributes and rules . Attributes are associated with grammar symbols and rules are associated with productions. If X is a symbol and a is one of its attributes, then we write X.a to denote the value of a at a particular parse-tree node labeled X . If we implement the nodes of the parse tree by records or objects, then the attributes of X can b e implemented by data fields in the records that represent the nodes for X . Attributes may be of any kind: numbers, types, table references, or strings, for instance. The strings may even be long sequences of code, say code in the intermediate language used by a compiler. NOTE: SDD consist of two element: attribute and rule, So SDD is used in the future, you need to be clear it means attribute and rule. 5.1.1 Inherited and Synthesized Attributes # We shall deal with two kinds of attributes for nonterminals : A synthesized attribute for a nonterminal A at a parse-tree node N is defined by a semantic rule associated with the production at N . Note that the production must have A as its head . A synthesized attribute at node N is defined only in terms of attribute values at the children of N and at N itself. An inherited attribute for a nonterminal B at a parse-tree node N is defined by a semantic rule associated with the production at the parent of N . Note that the production must have B as a symbol in its body. An inherited attribute at node N is defined only in terms of attribute values at N 's parent, N itself, and N 's siblings. NOTE: The above classification method is based on how to calculate the attribute value. It is obvious that the direction of computation of synthesized attribute is contrast to inherited attribute 's. More precisely, synthesized attribute is suitable to bottom-up parsing while inherited attribute is suitable to top-down parsing . Example 5.2 show how synthesized attribute is calculated while example 5.3 show how inherited attribute is calculated. The computation of attribute will be discussed in later chapter. NOTE: A SDD can has inherited attribute and inherited attribute at the same time, which is introduced in chapter 5.1.2. While we do not allow an inherited attribute at node N to be defined in terms of attribute values at the children of node N , we do allow a synthesized attribute at node N to be defined in terms of inherited attribute values at node N itself. NOTE: Inherited attribute , the name has implied that the attribute is inherited from parent, so it is natural that inherited attribute at node N can not be defined in terms of attribute values at the children of node N or it will be self-contradictory. Terminals can have synthesized attributes , but not inherited attributes . Attributes for terminals have lexical values that are supplied by the lexical analyzer; there are no semantic rules in the SDD itself for computing the value of an attribute for a terminal. NOTE: How about a start symbol? It is obvious that a start symbol can not has inherited attribute because it is the ancestor and it has no parent. Example 5.1 : skipped An SDD that involves only synthesized attributes is called S-attributed ; the SDD in Fig. 5.1 has this property. In an S-attributed SDD , each rule computes an attribute for the nonterminal at the head of a production from attributes taken from the body of the production. For simplicity, the examples in this section have semantic rules without side effects. In practice, it is convenient to allow SDD's to have limited side effects, such as printing the result computed by a desk calculator or interacting with a symbol table. Once the order of evaluation of attributes is discussed in Section 5.2, we shall allow semantic rules to compute arbitrary functions, possibly involving side effects. An S-attributed SDD can be implemented naturally in conjunction with an LR parser . An SDD without side effects is sometimes called an attribute grammar . The rules in an attribute grammar define the value of an attribute purely in terms of the values of other attributes and constants. 5.1.2 Evaluating an SDD at the Nodes of a Parse Tree # To visualize the translation specified by an SDD, it helps to work with parse trees, even though a translator need not actually build a parse tree. Imagine therefore that the rules of an SDD are applied by first constructing a parse tree and then using the rules to evaluate all of the attributes at each of the nodes of the parse tree . A parse tree , showing the value(s) of its attribute(s) is called an annotated parse tree . How do we construct an annotated parse tree ? In what order do we evaluate attributes? Before we can evaluate an attribute at a node of a parse tree, we must evaluate all the attributes up on which its value depends. For example, if all attributes are synthesized , as in Example 5.1, then we must evaluate the val attributes at all of the children of a node before we can evaluate the val attribute at the node itself. With synthesized attributes , we can evaluate attributes in any bottom-up order, such as that of a postorder traversal of the parse tree; the evaluation of S-attributed definitions is discussed in Section 5.2.3. For SDD's with both inherited and synthesized attributes, there is no guarantee that there is even one order in which to evaluate attributes at nodes. For instance, consider nonterminals A and B , with synthesized and inherited attributes A.s and B.i , respectively, along with the production and rules PRODUCTION SEMANTIC RULES $A \\to B$ A.s = B.i; B.i = A.s + 1 These rules are circular; it is impossible to evaluate either A.s at a node N or B.i at the child of N without first evaluating the other. The circular dependency of A.s and B.i at some pair of nodes in a parse tree is suggested by Fig. 5.2. It is computationally difficult to determine whether or not there exist any circularities in any of the parse trees that a given SDD could have to translate. Fortunately, there are useful sub classes of SDD's that are sufficient to guarantee that an order of evaluation exists, as we shall see in Section 5.2. NOTE: Below is the explanation if why determining whether or not there exist any circularities in any of the parse trees of a given SDD is computationally difficult: Without going into details, while the problem is decidable, it cannot be solved by a polynomial-time algorithm, even if P = N P , since it has exponential time complexity. In fact, this is an algorithm problem to find cycle in graph . Example 5.2 : skipped Example 5.3 : The SDD in Fig. 5.4 computes terms like 3 * 5 and 3 * 5 * 7 . The top-down parse of input 3 * 5 begins with the production $T \\to F T'$. Here, F generates the digit 3, but the operator * is generated by T' . Thus, the left operand 3 appears in a different subtree of the parse tree from * . An inherited attribute will therefore be used to pass the operand to the operator. The grammar in this example is an excerpt from a non-left-recursive version of the familiar expression grammar; we used such a grammar as a running example to illustrate top-down parsing in Section 4.4. PRODUCTION SEMANTIC RULES $T \\to F T'$ $T'.inh = F.val \\ T.val = T'.syn$ $T' \\to * F T_1'$ $T_1'.inh = T'.inh * F.val \\ T'.syn= T_1'.syn$ $T' \\to \\epsilon$ $T'.syn = T'.inh$ $F \\to digit$ $F.val = digit.lexval$ Figure 5.4: An SDD based on a grammar suitable for top-down parsing Each of the nonterminals T and F has a synthesized attribute val ; the terminal digit has a synthesized attribute lexval . The nonterminal T' has two attributes: an inherited attribute inh and a synthesized attribute syn . The semantic rules are based on the idea that the left operand of the operator * is inherited. More precisely, the head T' of the production $T' \\to * F T_1'$ inherits the left operand of * in the production body. Given a term x * y * z , the root of the subtree for $* y * z$ inherits x . Then, the root of the subtree for * z inherits the value of * x * y , and so on, if there are more factors in the term. Once all the factors have been accumulated, the result is passed back up the tree using synthesized attributes . To see how the semantic rules are used, consider the annotated parse tree for 3 * 5 in Fig. 5.5. The leftmost leaf in the parse tree, labeled digit , has attribute value lexval = 3 , where the 3 is supplied by the lexical analyzer . Its parent is for production 4, $F \\to digit$. The only semantic rule associated with this production defines $F.val = digit.lexval$ , which equals 3. At the second child of the root, the inherited attribute T'.inh is defined by the semantic rule T'.inh = F.val associated with production 1. Thus, the left operand, 3, for the * operator is passed from left to right across the children of the root. The production at the node for T' is $T' \\to * F T_1'$. (We retain the subscript 1 in the annotated parse tree to distinguish between the two nodes for T' .) The inherited attribute $T_1'.inh $ is defined by the semantic rule $T_1'.inh = T'.inh * F.val$ associated with production 2. With $T'.inh = 3$ and $F.val = 5$, we get $T_1'.inh = 15$. At the lower node for $T_1'$, the production is $T' \\to \\epsilon$. The semantic rule $T'.syn = T'.inh$ defines $T_1'.syn = 15$. The syn attributes at the nodes for $T'$pass the value 15 up the tree to the node for T , where T.val = 15 .","title":"5.1-Syntax-Directed-Definitions"},{"location":"Chapter-5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/#51-syntax-directed-definitions","text":"A syntax-directed definition (SDD) is a context-free grammar together with attributes and rules . Attributes are associated with grammar symbols and rules are associated with productions. If X is a symbol and a is one of its attributes, then we write X.a to denote the value of a at a particular parse-tree node labeled X . If we implement the nodes of the parse tree by records or objects, then the attributes of X can b e implemented by data fields in the records that represent the nodes for X . Attributes may be of any kind: numbers, types, table references, or strings, for instance. The strings may even be long sequences of code, say code in the intermediate language used by a compiler. NOTE: SDD consist of two element: attribute and rule, So SDD is used in the future, you need to be clear it means attribute and rule.","title":"5.1 Syntax-Directed Definitions"},{"location":"Chapter-5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/#511-inherited-and-synthesized-attributes","text":"We shall deal with two kinds of attributes for nonterminals : A synthesized attribute for a nonterminal A at a parse-tree node N is defined by a semantic rule associated with the production at N . Note that the production must have A as its head . A synthesized attribute at node N is defined only in terms of attribute values at the children of N and at N itself. An inherited attribute for a nonterminal B at a parse-tree node N is defined by a semantic rule associated with the production at the parent of N . Note that the production must have B as a symbol in its body. An inherited attribute at node N is defined only in terms of attribute values at N 's parent, N itself, and N 's siblings. NOTE: The above classification method is based on how to calculate the attribute value. It is obvious that the direction of computation of synthesized attribute is contrast to inherited attribute 's. More precisely, synthesized attribute is suitable to bottom-up parsing while inherited attribute is suitable to top-down parsing . Example 5.2 show how synthesized attribute is calculated while example 5.3 show how inherited attribute is calculated. The computation of attribute will be discussed in later chapter. NOTE: A SDD can has inherited attribute and inherited attribute at the same time, which is introduced in chapter 5.1.2. While we do not allow an inherited attribute at node N to be defined in terms of attribute values at the children of node N , we do allow a synthesized attribute at node N to be defined in terms of inherited attribute values at node N itself. NOTE: Inherited attribute , the name has implied that the attribute is inherited from parent, so it is natural that inherited attribute at node N can not be defined in terms of attribute values at the children of node N or it will be self-contradictory. Terminals can have synthesized attributes , but not inherited attributes . Attributes for terminals have lexical values that are supplied by the lexical analyzer; there are no semantic rules in the SDD itself for computing the value of an attribute for a terminal. NOTE: How about a start symbol? It is obvious that a start symbol can not has inherited attribute because it is the ancestor and it has no parent. Example 5.1 : skipped An SDD that involves only synthesized attributes is called S-attributed ; the SDD in Fig. 5.1 has this property. In an S-attributed SDD , each rule computes an attribute for the nonterminal at the head of a production from attributes taken from the body of the production. For simplicity, the examples in this section have semantic rules without side effects. In practice, it is convenient to allow SDD's to have limited side effects, such as printing the result computed by a desk calculator or interacting with a symbol table. Once the order of evaluation of attributes is discussed in Section 5.2, we shall allow semantic rules to compute arbitrary functions, possibly involving side effects. An S-attributed SDD can be implemented naturally in conjunction with an LR parser . An SDD without side effects is sometimes called an attribute grammar . The rules in an attribute grammar define the value of an attribute purely in terms of the values of other attributes and constants.","title":"5.1.1 Inherited and Synthesized Attributes"},{"location":"Chapter-5-Syntax-Directed-Translation/5.1-Syntax-Directed-Definitions/#512-evaluating-an-sdd-at-the-nodes-of-a-parse-tree","text":"To visualize the translation specified by an SDD, it helps to work with parse trees, even though a translator need not actually build a parse tree. Imagine therefore that the rules of an SDD are applied by first constructing a parse tree and then using the rules to evaluate all of the attributes at each of the nodes of the parse tree . A parse tree , showing the value(s) of its attribute(s) is called an annotated parse tree . How do we construct an annotated parse tree ? In what order do we evaluate attributes? Before we can evaluate an attribute at a node of a parse tree, we must evaluate all the attributes up on which its value depends. For example, if all attributes are synthesized , as in Example 5.1, then we must evaluate the val attributes at all of the children of a node before we can evaluate the val attribute at the node itself. With synthesized attributes , we can evaluate attributes in any bottom-up order, such as that of a postorder traversal of the parse tree; the evaluation of S-attributed definitions is discussed in Section 5.2.3. For SDD's with both inherited and synthesized attributes, there is no guarantee that there is even one order in which to evaluate attributes at nodes. For instance, consider nonterminals A and B , with synthesized and inherited attributes A.s and B.i , respectively, along with the production and rules PRODUCTION SEMANTIC RULES $A \\to B$ A.s = B.i; B.i = A.s + 1 These rules are circular; it is impossible to evaluate either A.s at a node N or B.i at the child of N without first evaluating the other. The circular dependency of A.s and B.i at some pair of nodes in a parse tree is suggested by Fig. 5.2. It is computationally difficult to determine whether or not there exist any circularities in any of the parse trees that a given SDD could have to translate. Fortunately, there are useful sub classes of SDD's that are sufficient to guarantee that an order of evaluation exists, as we shall see in Section 5.2. NOTE: Below is the explanation if why determining whether or not there exist any circularities in any of the parse trees of a given SDD is computationally difficult: Without going into details, while the problem is decidable, it cannot be solved by a polynomial-time algorithm, even if P = N P , since it has exponential time complexity. In fact, this is an algorithm problem to find cycle in graph . Example 5.2 : skipped Example 5.3 : The SDD in Fig. 5.4 computes terms like 3 * 5 and 3 * 5 * 7 . The top-down parse of input 3 * 5 begins with the production $T \\to F T'$. Here, F generates the digit 3, but the operator * is generated by T' . Thus, the left operand 3 appears in a different subtree of the parse tree from * . An inherited attribute will therefore be used to pass the operand to the operator. The grammar in this example is an excerpt from a non-left-recursive version of the familiar expression grammar; we used such a grammar as a running example to illustrate top-down parsing in Section 4.4. PRODUCTION SEMANTIC RULES $T \\to F T'$ $T'.inh = F.val \\ T.val = T'.syn$ $T' \\to * F T_1'$ $T_1'.inh = T'.inh * F.val \\ T'.syn= T_1'.syn$ $T' \\to \\epsilon$ $T'.syn = T'.inh$ $F \\to digit$ $F.val = digit.lexval$ Figure 5.4: An SDD based on a grammar suitable for top-down parsing Each of the nonterminals T and F has a synthesized attribute val ; the terminal digit has a synthesized attribute lexval . The nonterminal T' has two attributes: an inherited attribute inh and a synthesized attribute syn . The semantic rules are based on the idea that the left operand of the operator * is inherited. More precisely, the head T' of the production $T' \\to * F T_1'$ inherits the left operand of * in the production body. Given a term x * y * z , the root of the subtree for $* y * z$ inherits x . Then, the root of the subtree for * z inherits the value of * x * y , and so on, if there are more factors in the term. Once all the factors have been accumulated, the result is passed back up the tree using synthesized attributes . To see how the semantic rules are used, consider the annotated parse tree for 3 * 5 in Fig. 5.5. The leftmost leaf in the parse tree, labeled digit , has attribute value lexval = 3 , where the 3 is supplied by the lexical analyzer . Its parent is for production 4, $F \\to digit$. The only semantic rule associated with this production defines $F.val = digit.lexval$ , which equals 3. At the second child of the root, the inherited attribute T'.inh is defined by the semantic rule T'.inh = F.val associated with production 1. Thus, the left operand, 3, for the * operator is passed from left to right across the children of the root. The production at the node for T' is $T' \\to * F T_1'$. (We retain the subscript 1 in the annotated parse tree to distinguish between the two nodes for T' .) The inherited attribute $T_1'.inh $ is defined by the semantic rule $T_1'.inh = T'.inh * F.val$ associated with production 2. With $T'.inh = 3$ and $F.val = 5$, we get $T_1'.inh = 15$. At the lower node for $T_1'$, the production is $T' \\to \\epsilon$. The semantic rule $T'.syn = T'.inh$ defines $T_1'.syn = 15$. The syn attributes at the nodes for $T'$pass the value 15 up the tree to the node for T , where T.val = 15 .","title":"5.1.2 Evaluating an SDD at the Nodes of a Parse Tree"},{"location":"Chapter-5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD's/","text":"5.2 Evaluation Orders for SDD's # Dependency graphs\" are a useful tool for determining an evaluation order for the attribute instances in a given parse tree . While an annotated parse tree shows the values of attributes, a dependency graph helps us determine how those values can be computed. In this section, in addition to dependency graphs, we define two important classes of SDD's: the \"S-attributed\" and the more general \"L-attributed\" SDD's. The translations specified by these two classes fit well with the parsing methods we have studied, and most translations encountered in practice can be written to conform to the requirements of at least one of these classes. 5.2.1 Dependency Graphs # A dependency graph depicts the flow of information among the attribute instances in a particular parse tree ; an edge from one attribute instance to another means that the value of the first is needed to compute the second. Edges express constraints implied by the semantic rules . In more detail: For each parse-tree node, say a node labeled by grammar symbol X , the dependency graph has a node for each attribute associated with X . Suppose that a semantic rule associated with a production p defines the value of synthesized attribute A.b in terms of the value of X.c (the rule may define A.b in terms of other attributes in addition to X.c ). Then, the dependency graph has an edge from X.c to A.b . More precisely, at every node N labeled A where production p is applied, create an edge to attribute b at N , from the attribute c at the child of N corresponding to this instance of the symbol X in the body of the production. Since a node N can have several children labeled X , we again assume that subscripts distinguish among uses of the same symbol at different places in the production. Suppose that a semantic rule associated with a production p defines the value of inherited attribute B.c in terms of the value of X.a . Then, the dependency graph has an edge from X.a to B.c . For each node N labeled B that corresponds to an occurrence of this B in the body of production p , create an edge to attribute c at N from the attribute a at the node M that corresponds to this occurrence of X . Note that M could be either the parent or a sibling of N . NOTE: What is described above is an algorithm for constructing a dependency graph. 5.2.2 Ordering the Evaluation of Attributes # The dependency graph characterizes the possible orders in which we can evaluate the attributes at the various nodes of a parse tree . If the dependency graph has an edge from node M to node N , then the attribute corresponding to M must be evaluated before the attribute of N . Thus, the only allowable orders of evaluation are those sequences of nodes $N_1, N_2,\\dots , N_k$ such that if there is an edge of the dependency graph from $N_i$ to $N_j$, then i < j . Such an ordering embeds a directed graph into a linear order, and is called a topological sort of the graph. If there is any cycle in the graph, then there are no topological sorts ; that is, there is no way to evaluate the SDD on this parse tree . If there are no cycles , however, then there is always at least one topological sort. To see why, since there are no cycles, we can surely find a node with no edge entering. For if there were no such node, we could proceed from predecessor to predecessor until we came back to some node we had already seen, yielding a cycle. Make this node the first in the topological order, remove it from the dependency graph, and repeat the process on the remaining nodes. 5.2.3 S-Attributed Definitions # As mentioned earlier, given an SDD, it is very hard to tell whether there exist any parse trees whose dependency graphs have cycles. In practice, translations can be implemented using classes of SDD's that guarantee an evaluation order, since they do not permit dependency graphs with cycles. Moreover, the two classes introduced in this section can be implemented efficiently in connection with top-down or bottom-up parsing. The first class is defined as follows: An SDD is S-attributed if every attribute is synthesized. When an SDD is S-attributed , we can evaluate its attributes in any bottom-up order of the nodes of the parse tree . It is often especially simple to evaluate the attributes by performing a postorder traversal of the parse tree and evaluating the attributes at a node N when the traversal leaves N for the last time. That is, we apply the function postorder , defined below, to the root of the parse tree (see also the box \"Preorder and Postorder Traversals\" in Section 2.3.4): S-attributed definitions can be implemented during bottom-up parsing, since a bottom-up parse corresponds to a postorder traversal . Specifically, postorder corresponds exactly to the order in which an LR parser reduces a production body to its head. This fact will b e used in Section 5.4.2 to evaluate synthesized attributes and store them on the stack during LR parsing , without creating the tree nodes explicitly. 5.2.4 L-Attributed Definitions # The second class of SDD's is called L-attributed definitions . The idea behind this class is that, between the attributes associated with a production body, dependency-graph edges can go from left to right , but not from right to left (hence \"L-attributed\"). More precisely, each attribute must be either Synthesized, or Inherited, but with the rules limited as follows. Suppose that there is a production $A \\to X_1, X_2, \\dots, X_n$, and that there is an inherited attribute $X_i.a$ computed by a rule associated with this production. Then the rule may use only: Inherited attributes associated with the head A . Either inherited or synthesized attributes associated with the occurrences of symbols $X_1, X_2, \\dots, X_{i-1}$ located to the left of $X_i$. Inherited or synthesized attributes associated with this occurrence of $X_i$ itself, but only in such a way that there are no cycles in a dependency graph formed by the attributes of this $X_i$. Example 5.8 : The SDD in Fig. 5.4 is L-attributed . To see why, consider the semantic rules for inherited attributes, which are repeated here for convenience: The first of these rules defines the inherited attribute T'.inh using only F.val , and F appears to the left of T' in the production body, as required. The second rule defines $T_1'.inh$ using the inherited attribute T'inh associated with the head , and F.val , where F appears to the left of $T_1'$ in the production body. In each of these cases, the rules use information \"from above or from the left,\" as required by the class. The remaining attributes are synthesized. Hence, the SDD is L-attributed. Example 5.9 : Any SDD containing the following production and rules cannot be L-attributed: The second rule defines an inherited attribute B.i , so the entire SDD cannot be S-attributed. Further, although the rule is legal, the SDD cannot be L-attributed, because the attribute C.c is used to help define B.i , and C is to the right of B in the production body. 5.2.5 Semantic Rules with Controlled Side Effects # In practice, translations involve side effects: a desk calculator might print a result; a code generator might enter the type of an identifier into a symbol table. With SDD's, we strike a balance between attribute grammars and translation schemes . Attribute grammars have no side effects and allow any evaluation order consistent with the dependency graph. Translation schemes impose left-to-right evaluation and allow semantic actions to contain any program fragment; translation schemes are discussed in Section 5.4. We shall control side effcts in SDD's in one of the following ways: Permit incidental side effects that do not constrain attribute evaluation. In other words, permit side effects when attribute evaluation based on any topological sort of the dependency graph produces a \"correct\" translation, where \"correct\" depends on the application. Constrain the allowable evaluation orders, so that the same translation is produced for any allowable order. The constraints can be thought of as implicit edges added to the dependency graph.","title":"5.2-Evaluation-Orders-for-SDD's"},{"location":"Chapter-5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD's/#52-evaluation-orders-for-sdds","text":"Dependency graphs\" are a useful tool for determining an evaluation order for the attribute instances in a given parse tree . While an annotated parse tree shows the values of attributes, a dependency graph helps us determine how those values can be computed. In this section, in addition to dependency graphs, we define two important classes of SDD's: the \"S-attributed\" and the more general \"L-attributed\" SDD's. The translations specified by these two classes fit well with the parsing methods we have studied, and most translations encountered in practice can be written to conform to the requirements of at least one of these classes.","title":"5.2 Evaluation Orders for SDD's"},{"location":"Chapter-5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD's/#521-dependency-graphs","text":"A dependency graph depicts the flow of information among the attribute instances in a particular parse tree ; an edge from one attribute instance to another means that the value of the first is needed to compute the second. Edges express constraints implied by the semantic rules . In more detail: For each parse-tree node, say a node labeled by grammar symbol X , the dependency graph has a node for each attribute associated with X . Suppose that a semantic rule associated with a production p defines the value of synthesized attribute A.b in terms of the value of X.c (the rule may define A.b in terms of other attributes in addition to X.c ). Then, the dependency graph has an edge from X.c to A.b . More precisely, at every node N labeled A where production p is applied, create an edge to attribute b at N , from the attribute c at the child of N corresponding to this instance of the symbol X in the body of the production. Since a node N can have several children labeled X , we again assume that subscripts distinguish among uses of the same symbol at different places in the production. Suppose that a semantic rule associated with a production p defines the value of inherited attribute B.c in terms of the value of X.a . Then, the dependency graph has an edge from X.a to B.c . For each node N labeled B that corresponds to an occurrence of this B in the body of production p , create an edge to attribute c at N from the attribute a at the node M that corresponds to this occurrence of X . Note that M could be either the parent or a sibling of N . NOTE: What is described above is an algorithm for constructing a dependency graph.","title":"5.2.1 Dependency Graphs"},{"location":"Chapter-5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD's/#522-ordering-the-evaluation-of-attributes","text":"The dependency graph characterizes the possible orders in which we can evaluate the attributes at the various nodes of a parse tree . If the dependency graph has an edge from node M to node N , then the attribute corresponding to M must be evaluated before the attribute of N . Thus, the only allowable orders of evaluation are those sequences of nodes $N_1, N_2,\\dots , N_k$ such that if there is an edge of the dependency graph from $N_i$ to $N_j$, then i < j . Such an ordering embeds a directed graph into a linear order, and is called a topological sort of the graph. If there is any cycle in the graph, then there are no topological sorts ; that is, there is no way to evaluate the SDD on this parse tree . If there are no cycles , however, then there is always at least one topological sort. To see why, since there are no cycles, we can surely find a node with no edge entering. For if there were no such node, we could proceed from predecessor to predecessor until we came back to some node we had already seen, yielding a cycle. Make this node the first in the topological order, remove it from the dependency graph, and repeat the process on the remaining nodes.","title":"5.2.2 Ordering the Evaluation of Attributes"},{"location":"Chapter-5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD's/#523-s-attributed-definitions","text":"As mentioned earlier, given an SDD, it is very hard to tell whether there exist any parse trees whose dependency graphs have cycles. In practice, translations can be implemented using classes of SDD's that guarantee an evaluation order, since they do not permit dependency graphs with cycles. Moreover, the two classes introduced in this section can be implemented efficiently in connection with top-down or bottom-up parsing. The first class is defined as follows: An SDD is S-attributed if every attribute is synthesized. When an SDD is S-attributed , we can evaluate its attributes in any bottom-up order of the nodes of the parse tree . It is often especially simple to evaluate the attributes by performing a postorder traversal of the parse tree and evaluating the attributes at a node N when the traversal leaves N for the last time. That is, we apply the function postorder , defined below, to the root of the parse tree (see also the box \"Preorder and Postorder Traversals\" in Section 2.3.4): S-attributed definitions can be implemented during bottom-up parsing, since a bottom-up parse corresponds to a postorder traversal . Specifically, postorder corresponds exactly to the order in which an LR parser reduces a production body to its head. This fact will b e used in Section 5.4.2 to evaluate synthesized attributes and store them on the stack during LR parsing , without creating the tree nodes explicitly.","title":"5.2.3 S-Attributed Definitions"},{"location":"Chapter-5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD's/#524-l-attributed-definitions","text":"The second class of SDD's is called L-attributed definitions . The idea behind this class is that, between the attributes associated with a production body, dependency-graph edges can go from left to right , but not from right to left (hence \"L-attributed\"). More precisely, each attribute must be either Synthesized, or Inherited, but with the rules limited as follows. Suppose that there is a production $A \\to X_1, X_2, \\dots, X_n$, and that there is an inherited attribute $X_i.a$ computed by a rule associated with this production. Then the rule may use only: Inherited attributes associated with the head A . Either inherited or synthesized attributes associated with the occurrences of symbols $X_1, X_2, \\dots, X_{i-1}$ located to the left of $X_i$. Inherited or synthesized attributes associated with this occurrence of $X_i$ itself, but only in such a way that there are no cycles in a dependency graph formed by the attributes of this $X_i$. Example 5.8 : The SDD in Fig. 5.4 is L-attributed . To see why, consider the semantic rules for inherited attributes, which are repeated here for convenience: The first of these rules defines the inherited attribute T'.inh using only F.val , and F appears to the left of T' in the production body, as required. The second rule defines $T_1'.inh$ using the inherited attribute T'inh associated with the head , and F.val , where F appears to the left of $T_1'$ in the production body. In each of these cases, the rules use information \"from above or from the left,\" as required by the class. The remaining attributes are synthesized. Hence, the SDD is L-attributed. Example 5.9 : Any SDD containing the following production and rules cannot be L-attributed: The second rule defines an inherited attribute B.i , so the entire SDD cannot be S-attributed. Further, although the rule is legal, the SDD cannot be L-attributed, because the attribute C.c is used to help define B.i , and C is to the right of B in the production body.","title":"5.2.4 L-Attributed Definitions"},{"location":"Chapter-5-Syntax-Directed-Translation/5.2-Evaluation-Orders-for-SDD's/#525-semantic-rules-with-controlled-side-effects","text":"In practice, translations involve side effects: a desk calculator might print a result; a code generator might enter the type of an identifier into a symbol table. With SDD's, we strike a balance between attribute grammars and translation schemes . Attribute grammars have no side effects and allow any evaluation order consistent with the dependency graph. Translation schemes impose left-to-right evaluation and allow semantic actions to contain any program fragment; translation schemes are discussed in Section 5.4. We shall control side effcts in SDD's in one of the following ways: Permit incidental side effects that do not constrain attribute evaluation. In other words, permit side effects when attribute evaluation based on any topological sort of the dependency graph produces a \"correct\" translation, where \"correct\" depends on the application. Constrain the allowable evaluation orders, so that the same translation is produced for any allowable order. The constraints can be thought of as implicit edges added to the dependency graph.","title":"5.2.5 Semantic Rules with Controlled Side Effects"},{"location":"Chapter-5-Syntax-Directed-Translation/5.3-Applications-of-Syntax-Directed-Translation/","text":"5.3 Applications of Syntax-Directed Translation # The syntax-directed translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation. Here, we consider selected examples to illustrate some representative SDD's. The main application in this section is the construction of syntax trees . Since some compilers use syntax trees as an intermediate representation , a common form of SDD turns its input string into a tree. To complete the translation to intermediate code , the compiler may then walk the syntax tree , using another set of rules that are in effect an SDD on the syntax tree rather than the parse tree. (Chapter 6 also discusses approaches to intermediate-code generation that apply an SDD without ever constructing a tree explicitly.) We consider two SDD's for constructing syntax trees for expressions. The first, an S-attributed definition, is suitable for use during bottom-up parsing. The second, L-attributed, is suitable for use during top-down parsing. The final example of this section is an L-attributed definition that deals with basic and array types. 5.3.1 Construction of Syntax Trees # As discussed in Section 2.8.2, each node in a syntax tree represents a construct; the children of the node represent the meaningful components of the construct. A syntax-tree node representing an expression $E_1 + E_2$ has label + and two children representing the sub expressions $E_1$ and $E_2$. We shall implement the nodes of a syntax tree by objects with a suitable number of fields. Each object will have an op field that is the label of the node. The objects will have additional fields as follows: If the node is a leaf, an additional field holds the lexical value for the leaf. A constructor function Leaf (op, val ) creates a leaf object. Alternatively, if nodes are viewed as records, then Leaf returns a pointer to a new record for a leaf. If the node is an interior node , there are as many additional fields as the node has children in the syntax tree . A constructor function Node takes two or more arguments: $Node(op, c_1, c_2, c_3, \\dots, c_k)$ creates an object with first field op and k additional fields for the k children $c_1, c_2, c_3, \\dots, c_k$. Example 5.11 : The S-attributed definition in Fig. 5.10 constructs syntax trees for a simple expression grammar involving only the binary operators + and - . As usual, these operators are at the same precedence level and are jointly left associative. All nonterminals have one synthesized attribute node , which represents a node of the syntax tree. Every time the first production $E \\to E_1 + T$ is used, its rule creates a node with + for op and two children, $E_1.node$ and $T.node$, for the sub expressions. The second production has a similar rule. For production 3, $E \\to T$ , no node is created, since $E.node$ is the same as $T.node$. Similarly, no node is created for production 4, $T \\to (E)$. The value of T.node is the same as E.node , since parentheses are used only for grouping; they influence the structure of the parse tree and the syntax tree , but once their job is done, there is no further need to retain them in the syntax tree .","title":"5.3--Applications-of-Syntax-Directed-Translation"},{"location":"Chapter-5-Syntax-Directed-Translation/5.3-Applications-of-Syntax-Directed-Translation/#53-applications-of-syntax-directed-translation","text":"The syntax-directed translation techniques in this chapter will be applied in Chapter 6 to type checking and intermediate-code generation. Here, we consider selected examples to illustrate some representative SDD's. The main application in this section is the construction of syntax trees . Since some compilers use syntax trees as an intermediate representation , a common form of SDD turns its input string into a tree. To complete the translation to intermediate code , the compiler may then walk the syntax tree , using another set of rules that are in effect an SDD on the syntax tree rather than the parse tree. (Chapter 6 also discusses approaches to intermediate-code generation that apply an SDD without ever constructing a tree explicitly.) We consider two SDD's for constructing syntax trees for expressions. The first, an S-attributed definition, is suitable for use during bottom-up parsing. The second, L-attributed, is suitable for use during top-down parsing. The final example of this section is an L-attributed definition that deals with basic and array types.","title":"5.3 Applications of Syntax-Directed Translation"},{"location":"Chapter-5-Syntax-Directed-Translation/5.3-Applications-of-Syntax-Directed-Translation/#531-construction-of-syntax-trees","text":"As discussed in Section 2.8.2, each node in a syntax tree represents a construct; the children of the node represent the meaningful components of the construct. A syntax-tree node representing an expression $E_1 + E_2$ has label + and two children representing the sub expressions $E_1$ and $E_2$. We shall implement the nodes of a syntax tree by objects with a suitable number of fields. Each object will have an op field that is the label of the node. The objects will have additional fields as follows: If the node is a leaf, an additional field holds the lexical value for the leaf. A constructor function Leaf (op, val ) creates a leaf object. Alternatively, if nodes are viewed as records, then Leaf returns a pointer to a new record for a leaf. If the node is an interior node , there are as many additional fields as the node has children in the syntax tree . A constructor function Node takes two or more arguments: $Node(op, c_1, c_2, c_3, \\dots, c_k)$ creates an object with first field op and k additional fields for the k children $c_1, c_2, c_3, \\dots, c_k$. Example 5.11 : The S-attributed definition in Fig. 5.10 constructs syntax trees for a simple expression grammar involving only the binary operators + and - . As usual, these operators are at the same precedence level and are jointly left associative. All nonterminals have one synthesized attribute node , which represents a node of the syntax tree. Every time the first production $E \\to E_1 + T$ is used, its rule creates a node with + for op and two children, $E_1.node$ and $T.node$, for the sub expressions. The second production has a similar rule. For production 3, $E \\to T$ , no node is created, since $E.node$ is the same as $T.node$. Similarly, no node is created for production 4, $T \\to (E)$. The value of T.node is the same as E.node , since parentheses are used only for grouping; they influence the structure of the parse tree and the syntax tree , but once their job is done, there is no further need to retain them in the syntax tree .","title":"5.3.1 Construction of Syntax Trees"},{"location":"Chapter-5-Syntax-Directed-Translation/wikipedia-Syntax-directed-translation/","text":"Syntax-directed translation Overview Syntax-directed translation # Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar .[ 1] Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax . Overview # Syntax-directed translation fundamentally works by adding actions to the productions in a context-free grammar , resulting in a Syntax-Directed Definition (SDD).[ 2] Actions are steps or procedures that will be carried out when that production is used in a derivation. A grammar specification embedded with actions to be performed is called a syntax-directed translation scheme [ 1] (sometimes simply called a 'translation scheme'.) Each symbol in the grammar can have an attribute , which is a value that is to be associated with the symbol. Common attributes could include a variable type, the value of an expression, etc. Given a symbol X , with an attribute t , that attribute is referred to as X . t Thus, given actions and attributes, the grammar can be used for translating strings from its language by applying the actions and carrying information through each symbol's attribute.","title":"wikipedia Syntax directed translation"},{"location":"Chapter-5-Syntax-Directed-Translation/wikipedia-Syntax-directed-translation/#syntax-directed-translation","text":"Syntax-directed translation refers to a method of compiler implementation where the source language translation is completely driven by the parser . A common method of syntax-directed translation is translating a string into a sequence of actions by attaching one such action to each rule of a grammar .[ 1] Thus, parsing a string of the grammar produces a sequence of rule applications. SDT provides a simple way to attach semantics to any such syntax .","title":"Syntax-directed translation"},{"location":"Chapter-5-Syntax-Directed-Translation/wikipedia-Syntax-directed-translation/#overview","text":"Syntax-directed translation fundamentally works by adding actions to the productions in a context-free grammar , resulting in a Syntax-Directed Definition (SDD).[ 2] Actions are steps or procedures that will be carried out when that production is used in a derivation. A grammar specification embedded with actions to be performed is called a syntax-directed translation scheme [ 1] (sometimes simply called a 'translation scheme'.) Each symbol in the grammar can have an attribute , which is a value that is to be associated with the symbol. Common attributes could include a variable type, the value of an expression, etc. Given a symbol X , with an attribute t , that attribute is referred to as X . t Thus, given actions and attributes, the grammar can be used for translating strings from its language by applying the actions and carrying information through each symbol's attribute.","title":"Overview"},{"location":"Chapter-6-Intermediate-Code-Generation/","text":"Chapter 6 Intermediate-Code Generation #","title":6},{"location":"Chapter-6-Intermediate-Code-Generation/#chapter-6-intermediate-code-generation","text":"","title":"Chapter 6 Intermediate-Code Generation"},{"location":"Chapter-6-Intermediate-Code-Generation/6.3-Types-and-Declarations/","text":"6.3 Types and Declarations 6.3.1 Type Expressions 6.3.4 Storage Layout for Local Names 6.3 Types and Declarations # The applications of types can be grouped under checking and translation : Type checking uses logical rules to reason(\u63a8\u7406) about the behavior of a program at run time. Specifically(\u66f4\u52a0\u786e\u5207\u5730\u8bf4), it ensures that the types of the operands match the type expected by an operator. For example, the && operator in Java expects its two operands to be booleans; the result is also of type boolean. Translation Applications . From the type of a name, a compiler can determine the storage that will be needed for that name at run time. Type information is also needed to calculate the address denoted by an array reference, to insert explicit type conversions, and to choose the right version of an arithmetic operator, among other things. In this section, we examine types and storage layout for names declared within a procedure or a class . The actual storage for a procedure call or an object is allocated at run time , when the procedure is called or the object is created. As we examine(\u68c0\u67e5) local declarations at compile time , we can, however,layout relative addresses , where the relative address of a name or a component of a data structure is an offset from the start of a data area. 6.3.1 Type Expressions # Types have structure, which we shall represent using type expressions : a type expression is either a basic type or is formed by applying an operator called a type constructor (\u7c7b\u578b\u6784\u9020\u7b97\u5b50) to a type expression. The sets of basic types and constructors depend on the language to be checked. Example 6.8 : The array type int[2][3] can be read as array of 2 arrays of 3 integers each and written as a type expression array (2; array (3; integer)) .This type is represented by the tree in Fig. 6.14. The operator array takes two parameters, a number and a type(\u5982\u4e0b\u6240\u793a\uff0carray\u8282\u70b9\u6709\u4e24\u4e2a\u5b50\u8282\u70b9). We shall use the following definition of type expressions : A basic type is a type expression . Typical basic types for a language include boolean, char, integer, float, and void ; the latter denotes the absence of a value. A type name is a type expression . A type expression can be formed by applying the array type constructor to a number and a type expression(\u5b9a\u4e49\u6570\u7ec4\u7c7b\u578b). A record is a data structure with named fields. A type expression can be formed by applying the record type constructor to the field names and their types. Record types will be implemented in Section 6.3.6 by applying the constructor record to a symbol table containing entries for the fields. A type expression can be formed by using the type constructor-> for function types . We write s->t for function from type s to type t . Function types will be useful when type checking is discussed in Section 6.5. If s and t are type expressions, then their Cartesian product s * t is a type expression. Products are introduced for completeness(\u5b8c\u6574\u6027); they can be used to represent a list or tuple of types (e.g., for function parameters).We assume that associates to the left and that it has higher precedence than -> . Type expressions may contain variables whose values are type expressions .Compiler-generated type variables will be used in Section 6.5.4. A convenient way to represent a type expression is to use a graph. The value-number method of Section 6.1.2, can b e adapted to construct a DAG for a type expression, with interior nodes for type constructors and leaves for basic types, type names, and type variables; for example, see the tree in Fig. 6.14. Type Names and Recursive Types Once a class is defined, its name can be used as a type name in C++ or Java; for example, consider Node in the program fragment java public class Node { ... } ... public Node n; Names can be used to define recursive types, which are needed for data structures such as linked lists. The pseudo code for a list element java class Cell { int info; Cell next; ... } defines the recursive type Cell as a class that contains a field info and a field next of type Cell . Similar recursive types can be defined in C using records and pointers. The techniques in this chapter carry over to recursive types. 6.3.4 Storage Layout for Local Names # From the type of a name, we can determine the amount of storage that will be needed for the name at run time. At compile time, we can use these amounts to assign each name a relative address . The type and relative address are saved in the symbol-table entry for the name. Data of varying length, such as strings, or data whose size cannot be determined until run time, such as dynamic arrays, is handled by reserving a known fixed amount of storage for a pointer to the data. Run-time storage management is discussed in Chapter 7. Address Alignment The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. For example, instructions to add integers may expect integers to be aligned, that is, placed at certain positions in memory such as an address divisible by 4. Although an array of ten characters needs only enough bytes to hold ten characters, a compiler may therefore allocate 12 bytes,the next multiple of 4 ,leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium(\u5b9d\u8d35), a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. Suppose that storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory. Typically, a byte is eight bits, and some number of bytes form a machine word. Multibyte objects are stored in consecutive bytes and given the address of the first byte. The width of a type is the number of storage units needed for objects of that type. A basic type, such as a character, integer, or float, requires an integral number of bytes. For easy access, storage for aggregates such as arrays and classes is allocated in one contiguous block of bytes. The translation scheme (SDT) (\u7ffb\u8bd1\u65b9\u6848) in Fig. 6.15 computes types and their widths for basic and array types; record types will be discussed in Section 6.3.6. The SDT uses synthesized attributes type and width for each nonterminal and two variables t and w to pass type and width information from a B node in a parse tree to the node for the production C . In a syntax-directed definition, t and w would be inherited attributes for C . T -> B f t = B :type; w = B :width ; g C f T :type = C :type ; T :width = C :width ; g B -> int f B :type = integer; B :width = 4; g B -> ?oat f B :type = ?oat; B :width = 8; g C -> ? f C :type = t; C :width = w ; g C -> [ num ] C 1 f C :type = array (num:value ; C 1 :type); C :width = num:value ? C 1 :width ; g","title":"6.3-Types-and-Declarations"},{"location":"Chapter-6-Intermediate-Code-Generation/6.3-Types-and-Declarations/#63-types-and-declarations","text":"The applications of types can be grouped under checking and translation : Type checking uses logical rules to reason(\u63a8\u7406) about the behavior of a program at run time. Specifically(\u66f4\u52a0\u786e\u5207\u5730\u8bf4), it ensures that the types of the operands match the type expected by an operator. For example, the && operator in Java expects its two operands to be booleans; the result is also of type boolean. Translation Applications . From the type of a name, a compiler can determine the storage that will be needed for that name at run time. Type information is also needed to calculate the address denoted by an array reference, to insert explicit type conversions, and to choose the right version of an arithmetic operator, among other things. In this section, we examine types and storage layout for names declared within a procedure or a class . The actual storage for a procedure call or an object is allocated at run time , when the procedure is called or the object is created. As we examine(\u68c0\u67e5) local declarations at compile time , we can, however,layout relative addresses , where the relative address of a name or a component of a data structure is an offset from the start of a data area.","title":"6.3 Types and Declarations"},{"location":"Chapter-6-Intermediate-Code-Generation/6.3-Types-and-Declarations/#631-type-expressions","text":"Types have structure, which we shall represent using type expressions : a type expression is either a basic type or is formed by applying an operator called a type constructor (\u7c7b\u578b\u6784\u9020\u7b97\u5b50) to a type expression. The sets of basic types and constructors depend on the language to be checked. Example 6.8 : The array type int[2][3] can be read as array of 2 arrays of 3 integers each and written as a type expression array (2; array (3; integer)) .This type is represented by the tree in Fig. 6.14. The operator array takes two parameters, a number and a type(\u5982\u4e0b\u6240\u793a\uff0carray\u8282\u70b9\u6709\u4e24\u4e2a\u5b50\u8282\u70b9). We shall use the following definition of type expressions : A basic type is a type expression . Typical basic types for a language include boolean, char, integer, float, and void ; the latter denotes the absence of a value. A type name is a type expression . A type expression can be formed by applying the array type constructor to a number and a type expression(\u5b9a\u4e49\u6570\u7ec4\u7c7b\u578b). A record is a data structure with named fields. A type expression can be formed by applying the record type constructor to the field names and their types. Record types will be implemented in Section 6.3.6 by applying the constructor record to a symbol table containing entries for the fields. A type expression can be formed by using the type constructor-> for function types . We write s->t for function from type s to type t . Function types will be useful when type checking is discussed in Section 6.5. If s and t are type expressions, then their Cartesian product s * t is a type expression. Products are introduced for completeness(\u5b8c\u6574\u6027); they can be used to represent a list or tuple of types (e.g., for function parameters).We assume that associates to the left and that it has higher precedence than -> . Type expressions may contain variables whose values are type expressions .Compiler-generated type variables will be used in Section 6.5.4. A convenient way to represent a type expression is to use a graph. The value-number method of Section 6.1.2, can b e adapted to construct a DAG for a type expression, with interior nodes for type constructors and leaves for basic types, type names, and type variables; for example, see the tree in Fig. 6.14. Type Names and Recursive Types Once a class is defined, its name can be used as a type name in C++ or Java; for example, consider Node in the program fragment java public class Node { ... } ... public Node n; Names can be used to define recursive types, which are needed for data structures such as linked lists. The pseudo code for a list element java class Cell { int info; Cell next; ... } defines the recursive type Cell as a class that contains a field info and a field next of type Cell . Similar recursive types can be defined in C using records and pointers. The techniques in this chapter carry over to recursive types.","title":"6.3.1 Type Expressions"},{"location":"Chapter-6-Intermediate-Code-Generation/6.3-Types-and-Declarations/#634-storage-layout-for-local-names","text":"From the type of a name, we can determine the amount of storage that will be needed for the name at run time. At compile time, we can use these amounts to assign each name a relative address . The type and relative address are saved in the symbol-table entry for the name. Data of varying length, such as strings, or data whose size cannot be determined until run time, such as dynamic arrays, is handled by reserving a known fixed amount of storage for a pointer to the data. Run-time storage management is discussed in Chapter 7. Address Alignment The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. For example, instructions to add integers may expect integers to be aligned, that is, placed at certain positions in memory such as an address divisible by 4. Although an array of ten characters needs only enough bytes to hold ten characters, a compiler may therefore allocate 12 bytes,the next multiple of 4 ,leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium(\u5b9d\u8d35), a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. Suppose that storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory. Typically, a byte is eight bits, and some number of bytes form a machine word. Multibyte objects are stored in consecutive bytes and given the address of the first byte. The width of a type is the number of storage units needed for objects of that type. A basic type, such as a character, integer, or float, requires an integral number of bytes. For easy access, storage for aggregates such as arrays and classes is allocated in one contiguous block of bytes. The translation scheme (SDT) (\u7ffb\u8bd1\u65b9\u6848) in Fig. 6.15 computes types and their widths for basic and array types; record types will be discussed in Section 6.3.6. The SDT uses synthesized attributes type and width for each nonterminal and two variables t and w to pass type and width information from a B node in a parse tree to the node for the production C . In a syntax-directed definition, t and w would be inherited attributes for C . T -> B f t = B :type; w = B :width ; g C f T :type = C :type ; T :width = C :width ; g B -> int f B :type = integer; B :width = 4; g B -> ?oat f B :type = ?oat; B :width = 8; g C -> ? f C :type = t; C :width = w ; g C -> [ num ] C 1 f C :type = array (num:value ; C 1 :type); C :width = num:value ? C 1 :width ; g","title":"6.3.4 Storage Layout for Local Names"},{"location":"Chapter-7-Run-Time-Environments/","text":"Chapter 7 Run-Time Environments # A compiler must accurately implement the abstractions embodied in the source-language definition. These abstractions typically include the concepts we discussed in Section 1.6 such as names, scopes, bindings, data types, operators,procedures, parameters, and flow-of-control constructs. The compiler must cooperate with the operating system and other systems software to support these abstractions on the target machine. To do so, the compiler creates and manages a run-time environment in which it assumes its target programs are being executed. This environment deals with a variety of issues such as the layout and allocation of storage locations for the objects named in the source program, the mechanisms used by the target program to access variables , the linkages between procedures, the mechanisms for passing parameters, and the interfaces to the operating system, input/output devices, and other programs. The two themes in this chapter are the allocation of storage locations and access to variables and data . We shall discuss memory management in some detail, including stack allocation , heap management , and garbage collection . In the next chapter, we present techniques for generating target code for many common language constructs. NOTE: \u672c\u7ae0\u7684\u5185\u5bb9\u548c Application binary interface \u76ee\u524d\u76f8\u5173\uff0c\u5728\u672c\u7ae0\u989d\u5916\u6dfb\u52a0\u4e86ABI\u7ae0\u8282\u6765\u8bf4\u660eABI\u76f8\u5173\u77e5\u8bc6\u3002","title":7},{"location":"Chapter-7-Run-Time-Environments/#chapter-7-run-time-environments","text":"A compiler must accurately implement the abstractions embodied in the source-language definition. These abstractions typically include the concepts we discussed in Section 1.6 such as names, scopes, bindings, data types, operators,procedures, parameters, and flow-of-control constructs. The compiler must cooperate with the operating system and other systems software to support these abstractions on the target machine. To do so, the compiler creates and manages a run-time environment in which it assumes its target programs are being executed. This environment deals with a variety of issues such as the layout and allocation of storage locations for the objects named in the source program, the mechanisms used by the target program to access variables , the linkages between procedures, the mechanisms for passing parameters, and the interfaces to the operating system, input/output devices, and other programs. The two themes in this chapter are the allocation of storage locations and access to variables and data . We shall discuss memory management in some detail, including stack allocation , heap management , and garbage collection . In the next chapter, we present techniques for generating target code for many common language constructs. NOTE: \u672c\u7ae0\u7684\u5185\u5bb9\u548c Application binary interface \u76ee\u524d\u76f8\u5173\uff0c\u5728\u672c\u7ae0\u989d\u5916\u6dfb\u52a0\u4e86ABI\u7ae0\u8282\u6765\u8bf4\u660eABI\u76f8\u5173\u77e5\u8bc6\u3002","title":"Chapter 7 Run-Time Environments"},{"location":"Chapter-7-Run-Time-Environments/7.1-Storage-Organization/","text":"7.1 Storage Organization 7.1.1 Static Versus Dynamic Storage Allocation 7.1 Storage Organization # From the perspective of the compiler writer, the executing target program runs in its own logical address space in which each program value has a location . The management and organization of this logical address space is shared between the compiler, operating system, and target machine. The operating system maps the logical addresses into physical addresses , which are usually spread throughout memory. The run-time representation of an object program in the logical address space consists of data and program areas as shown in Fig. 7.1. A compiler for a language like C++ on an operating system like Linux might sub divide memory in this way. Throughout this book, we assume the run-time storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory(\u5b57\u8282\u662f\u5185\u5b58\u7684\u6700\u5c0f\u7f16\u5740\u5355\u5143). A byte is eight bits and four bytes form a machine word. Multibyte objects are stored in consecutive bytes and given the address of the first byte(\u591a\u5b57\u8282\u6570\u636e\u5bf9\u8c61\u603b\u6570\u5b58\u50a8\u5728\u4e00\u6bb5\u8fde\u7eed\u7684\u5b57\u8282\u4e2d\uff0c\u5e76\u4e14\u5c06\u7b2c\u4e00\u4e2a\u5b57\u8282\u4f5c\u4e3a\u5730\u5740). As discussed in Chapter 6, the amount of storage needed for a name is determined from its type. An elementary data type, such as a character, integer,or float, can be stored in an integral number of bytes. Storage for an aggregate type, such as an array or structure, must be large enough to hold all its components. The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. On many machines, instructions to add integers may expect integers to be aligned, that is, placed at an address divisible by 4. Although a character array (as in C) of length 10 needs only enough bytes to hold ten characters, a compiler may allocate 12 bytes to get the proper alignment, leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium, a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. The size of the generated target code is fixed at compile time, so the compiler can place the executable target code in a statically determined area Code ,usually in the low end of memory. Similarly, the size of some program data objects , such as global constants , and data generated by the compiler , such as information to support garbage collection, may be known at compile time , and these data objects can be placed in another statically determined area called Static . One reason for statically allocating as many data objects as possible is that the addresses of these objects can be compiled into the target code. In early versions of Fortran, all data objects could be allocated statically. To maximize the utilization of space at run time , the other two areas, Stack and Heap , are at the opposite ends of the remainder of the address space . These areas are dynamic ; their size can change as the program executes. These areas grow towards each other as needed. The stack is used to store data structures called activation records that get generated during procedure calls. In practice, the stack grows towards lower addresses, the heap towards higher. However, throughout this chapter and the next we shall assume that the stack grows towards higher addresses so that we can use positive offsets for notational convenience in all our examples. As we shall see in the next section, an activation record is used to store information about the status of the machine, such as the value of the program counter and machine registers , when a procedure call occurs. When control returns from the call, the activation of the calling procedure can be restarted after restoring the values of relevant registers and setting the program counter to the point immediately after the call. Data objects whose lifetimes are contained in that of an activation can be allocated on the stack along with other information associated with the activation. Many programming languages allow the programmer to allocate and deallocate data under program control. For example, C has the functions malloc and free that can be used to obtain and give back arbitrary chunks of storage. The heap is used to manage this kind of long-lived data. Section 7.4 will discuss various memory-management algorithms that can be used to maintain the heap 7.1.1 Static Versus Dynamic Storage Allocation # The layout and allocation of data to memory locations in the run-time environment are key issues in storage management . These issues are tricky because the same name in a program text can refer to multiple locations at run time. The two adjectives static and dynamic distinguish between compile time and run time , respectively. We say that a storage-allocation decision is static , if it can be made by the compiler looking only at the text of the program, not at what the program does when it executes. Conversely, a decision is dynamic if it can be decided only while the program is running. Many compilers use some combination of the following two strategies for dynamic storage allocation: Stack storage . Names local to a procedure are allocated space on a stack .We discuss the run-time stack starting in Section 7.2. The stack supports the normal call/return policy for procedures Heap storage . Data that may outlive the call to the procedure that created it is usually allocated on a heap of reusable storage. We discuss heap management starting in Section 7.4. The heap is an area of virtual memory that allows objects or other data elements to obtain storage when they are created and to return that storage when they are invalidated To support heap management , garbage collection enables the run-time system to detect useless data elements and reuse their storage, even if the programmer does not return their space explicitly. Automatic garbage collection is an essential feature of many modern languages, despite it being a difficult operation to do efficiently; it may not even be possible for some languages","title":"7.1-Storage-Organization"},{"location":"Chapter-7-Run-Time-Environments/7.1-Storage-Organization/#71-storage-organization","text":"From the perspective of the compiler writer, the executing target program runs in its own logical address space in which each program value has a location . The management and organization of this logical address space is shared between the compiler, operating system, and target machine. The operating system maps the logical addresses into physical addresses , which are usually spread throughout memory. The run-time representation of an object program in the logical address space consists of data and program areas as shown in Fig. 7.1. A compiler for a language like C++ on an operating system like Linux might sub divide memory in this way. Throughout this book, we assume the run-time storage comes in blocks of contiguous bytes, where a byte is the smallest unit of addressable memory(\u5b57\u8282\u662f\u5185\u5b58\u7684\u6700\u5c0f\u7f16\u5740\u5355\u5143). A byte is eight bits and four bytes form a machine word. Multibyte objects are stored in consecutive bytes and given the address of the first byte(\u591a\u5b57\u8282\u6570\u636e\u5bf9\u8c61\u603b\u6570\u5b58\u50a8\u5728\u4e00\u6bb5\u8fde\u7eed\u7684\u5b57\u8282\u4e2d\uff0c\u5e76\u4e14\u5c06\u7b2c\u4e00\u4e2a\u5b57\u8282\u4f5c\u4e3a\u5730\u5740). As discussed in Chapter 6, the amount of storage needed for a name is determined from its type. An elementary data type, such as a character, integer,or float, can be stored in an integral number of bytes. Storage for an aggregate type, such as an array or structure, must be large enough to hold all its components. The storage layout for data objects is strongly influenced by the addressing constraints of the target machine. On many machines, instructions to add integers may expect integers to be aligned, that is, placed at an address divisible by 4. Although a character array (as in C) of length 10 needs only enough bytes to hold ten characters, a compiler may allocate 12 bytes to get the proper alignment, leaving 2 bytes unused. Space left unused due to alignment considerations is referred to as padding . When space is at a premium, a compiler may pack data so that no padding is left; additional instructions may then need to be executed at run time to position packed data so that it can be operated on as if it were properly aligned. The size of the generated target code is fixed at compile time, so the compiler can place the executable target code in a statically determined area Code ,usually in the low end of memory. Similarly, the size of some program data objects , such as global constants , and data generated by the compiler , such as information to support garbage collection, may be known at compile time , and these data objects can be placed in another statically determined area called Static . One reason for statically allocating as many data objects as possible is that the addresses of these objects can be compiled into the target code. In early versions of Fortran, all data objects could be allocated statically. To maximize the utilization of space at run time , the other two areas, Stack and Heap , are at the opposite ends of the remainder of the address space . These areas are dynamic ; their size can change as the program executes. These areas grow towards each other as needed. The stack is used to store data structures called activation records that get generated during procedure calls. In practice, the stack grows towards lower addresses, the heap towards higher. However, throughout this chapter and the next we shall assume that the stack grows towards higher addresses so that we can use positive offsets for notational convenience in all our examples. As we shall see in the next section, an activation record is used to store information about the status of the machine, such as the value of the program counter and machine registers , when a procedure call occurs. When control returns from the call, the activation of the calling procedure can be restarted after restoring the values of relevant registers and setting the program counter to the point immediately after the call. Data objects whose lifetimes are contained in that of an activation can be allocated on the stack along with other information associated with the activation. Many programming languages allow the programmer to allocate and deallocate data under program control. For example, C has the functions malloc and free that can be used to obtain and give back arbitrary chunks of storage. The heap is used to manage this kind of long-lived data. Section 7.4 will discuss various memory-management algorithms that can be used to maintain the heap","title":"7.1 Storage Organization"},{"location":"Chapter-7-Run-Time-Environments/7.1-Storage-Organization/#711-static-versus-dynamic-storage-allocation","text":"The layout and allocation of data to memory locations in the run-time environment are key issues in storage management . These issues are tricky because the same name in a program text can refer to multiple locations at run time. The two adjectives static and dynamic distinguish between compile time and run time , respectively. We say that a storage-allocation decision is static , if it can be made by the compiler looking only at the text of the program, not at what the program does when it executes. Conversely, a decision is dynamic if it can be decided only while the program is running. Many compilers use some combination of the following two strategies for dynamic storage allocation: Stack storage . Names local to a procedure are allocated space on a stack .We discuss the run-time stack starting in Section 7.2. The stack supports the normal call/return policy for procedures Heap storage . Data that may outlive the call to the procedure that created it is usually allocated on a heap of reusable storage. We discuss heap management starting in Section 7.4. The heap is an area of virtual memory that allows objects or other data elements to obtain storage when they are created and to return that storage when they are invalidated To support heap management , garbage collection enables the run-time system to detect useless data elements and reuse their storage, even if the programmer does not return their space explicitly. Automatic garbage collection is an essential feature of many modern languages, despite it being a difficult operation to do efficiently; it may not even be possible for some languages","title":"7.1.1 Static Versus Dynamic Storage Allocation"},{"location":"Chapter-7-Run-Time-Environments/7.2-Stack-Allocation-of-Space/","text":"7.2 Stack Allocation of Space # Almost all compilers for languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack . Each time a procedure is called, space for its local variables is pushed onto a stack , and when the procedure terminates, that space is popped off the stack. As we shall see, this arrangement not only allows space to be shared by procedure calls whose durations do not overlap in time, but it allows us to compile code for a procedure in such a way that the relative addresses of its nonlocal variables are always the same, regardless of the sequence of procedure calls. 7.2.1 Activation Trees # Stack allocation would not be feasible(\u53ef\u884c\u7684) if procedure calls, or activations of procedures, did not nest in time. The following example illustrates nesting of procedure calls. Example 7.1 : Figure 7.2 contains a sketch of a program that reads nine integers into an array a and sorts them using the recursive quicksort algorithm . int a[11]; void readArray() { /* Reads 9 integers into a[1]; :::; a[9]. */ int i; ... } int partition(int m, int n) { /* Picks a separator value v , and partitions a[m .. n] so that a[m , p - 1] are less than v , a[p] = v , and a[p + 1 , n] are equal to or greater than v . Returns p. */ ... } void quicksort(int m, int n) { int i; if (n > m) { i = partition(m, n); quicksort(m, i-1); quicksort(i+1, n); } } main() { readArray(); a[0] = -9999; a[10] = 9999; quicksort(1,9); } The main function has three tasks. It calls readArray , sets the sentinels, and then calls quicksort on the entire data array. Figure 7.3 suggests a sequence of calls that might result from an execution of the program. In this execution, the call to partition (1, 9) returns 4, so a[1] through a[3] hold elements less than its chosen separator value v , while the larger elements are in a[5] through a[9] .","title":"7.2-Stack-Allocation-of-Space"},{"location":"Chapter-7-Run-Time-Environments/7.2-Stack-Allocation-of-Space/#72-stack-allocation-of-space","text":"Almost all compilers for languages that use procedures, functions, or methods as units of user-defined actions manage at least part of their run-time memory as a stack . Each time a procedure is called, space for its local variables is pushed onto a stack , and when the procedure terminates, that space is popped off the stack. As we shall see, this arrangement not only allows space to be shared by procedure calls whose durations do not overlap in time, but it allows us to compile code for a procedure in such a way that the relative addresses of its nonlocal variables are always the same, regardless of the sequence of procedure calls.","title":"7.2 Stack Allocation of Space"},{"location":"Chapter-7-Run-Time-Environments/7.2-Stack-Allocation-of-Space/#721-activation-trees","text":"Stack allocation would not be feasible(\u53ef\u884c\u7684) if procedure calls, or activations of procedures, did not nest in time. The following example illustrates nesting of procedure calls. Example 7.1 : Figure 7.2 contains a sketch of a program that reads nine integers into an array a and sorts them using the recursive quicksort algorithm . int a[11]; void readArray() { /* Reads 9 integers into a[1]; :::; a[9]. */ int i; ... } int partition(int m, int n) { /* Picks a separator value v , and partitions a[m .. n] so that a[m , p - 1] are less than v , a[p] = v , and a[p + 1 , n] are equal to or greater than v . Returns p. */ ... } void quicksort(int m, int n) { int i; if (n > m) { i = partition(m, n); quicksort(m, i-1); quicksort(i+1, n); } } main() { readArray(); a[0] = -9999; a[10] = 9999; quicksort(1,9); } The main function has three tasks. It calls readArray , sets the sentinels, and then calls quicksort on the entire data array. Figure 7.3 suggests a sequence of calls that might result from an execution of the program. In this execution, the call to partition (1, 9) returns 4, so a[1] through a[3] hold elements less than its chosen separator value v , while the larger elements are in a[5] through a[9] .","title":"7.2.1 Activation Trees"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/","text":"Application binary interface # In computer software , an application binary interface ( ABI ) is an interface between two binary program modules ; often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user. An ABI defines how data structures or computational routines are accessed in machine code , which is a low-level, hardware-dependent format; in contrast, an API defines this access in source code , which is a relatively high-level, hardware-independent, often human-readable format. A common aspect of an ABI is the calling convention , which determines how data is provided as input to or read as output from computational routines; examples are the x86 calling conventions . Adhering to an ABI (which may or may not be officially standardized) is usually the job of a compiler , operating system, or library author; however, an application programmer may have to deal with an ABI directly when writing a program in a mix of programming languages, which can be achieved by using foreign function calls . Description # ABIs cover details such as: a processor instruction set (with details like register file structure, stack organization, memory access types, ...) the sizes, layouts, and alignments of basic data types that the processor can directly access the calling convention , which controls how functions ' arguments are passed and return values are retrieved; for example, whether all parameters are passed on the stack or some are passed in registers , which registers are used for which function parameters, and whether the first function parameter passed on the stack is pushed first or last onto the stack how an application should make system calls to the operating system and, if the ABI specifies direct system calls rather than procedure calls to system call stubs, the system call numbers and in the case of a complete operating system ABI, the binary format of object files , program libraries and so on. Complete ABIs # A complete ABI, such as the Intel Binary Compatibility Standard (iBCS),[ 1] allows a program from one operating system supporting that ABI to run without modifications on any other such system, provided that necessary shared libraries are present, and similar prerequisites are fulfilled. Other[ which? ] ABIs standardize details such as the C++ name mangling ,[ 2] exception propagation,[ 3] and calling convention between compilers on the same platform, but do not require cross-platform compatibility. Embedded ABIs # An embedded-application binary interface (EABI) specifies standard conventions for file formats , data types, register usage, stack frame organization, and function parameter passing of an embedded software program, for use with an embedded operating system . Compilers that support the EABI create object code that is compatible with code generated by other such compilers, allowing developers to link libraries generated with one compiler with object code generated with another compiler. Developers writing their own assembly language code may also interface with assembly generated by a compliant compiler. EABIs are designed to optimize for performance within the limited resources of an embedded system. Therefore, EABIs omit most abstractions that are made between kernel and user code in complex operating systems. For example, dynamic linking is avoided to allow smaller executables and faster loading, fixed register usage allows more compact stacks and kernel calls, and running the application in privileged mode allows direct access to custom hardware operation without the indirection of calling a device driver. [ 4] The choice of EABI can affect performance.[ 5] [ 6] Widely used EABIs include PowerPC ,[ 4] ARM EABI2[ 7] and MIPS EABI.[ 8] Difference between API and ABI # Q: I am new to linux system programming and I came across API and ABI while reading Linux System Programming . Definition of API : An API defines the interfaces by which one piece of software communicates with another at the source level. Definition of ABI : Whereas an API defines a source interface, an ABI defines the low-level binary interface between two or more pieces of software on a particular architecture. It defines how an application interacts with itself, how an application interacts with the kernel , and how an application interacts with libraries . How can a program communicate at a source level ? What is a source level ? Is it related to source code in anyway? Or the source of the library gets included in the main program ? The only difference I know is API is mostly used by programmers and ABI is mostly used by compiler. A: by source level they mean something like include file to expose function definitions \u2013 Anycorn A : API: Application Program Interface # This is the set of public types/variables/functions that you expose from your application/library. In C/C++ this is what you expose in the header files that you ship with the application. ABI: Application Binary Interface # This is how the compiler builds an application. It defines things (but is not limited to): How parameters are passed to functions (registers/stack). Who cleans parameters from the stack (caller/callee). Where the return value is placed for return. How exceptions propagate. What is an application binary interface (ABI)? # Q: I never clearly understood what an ABI is. Please don't point me to a Wikipedia article. If I could understand it, I wouldn't be here posting such a lengthy post. This is my mindset about different interfaces: A TV remote(\u9065\u63a7\u5668) is an interface between the user and the TV. It is an existing entity, but useless (doesn't provide any functionality) by itself. All the functionality for each of those buttons on the remote is implemented in the television set. Interface: It is an \"existing entity\" layer between the functionality and consumer of that functionality. An interface by itself is doesn't do anything. It just invokes the functionality lying behind. Now depending on who the user is there are different type of interfaces. Command Line Interface (CLI) commands are the existing entities, the consumer is the user and functionality lies behind. functionality: my software functionality which solves some purpose to which we are describing this interface. existing entities: commands consumer: user Graphical User Interface(GUI) window, buttons, etc. are the existing entities, and again the consumer is the user and functionality lies behind. functionality: my software functionality which solves some purpose to which we are describing this interface. existing entities: window,buttons etc.. consumer: user Application Programming Interface(API) functions or to be more correct, interfaces (in interfaced based programming) are the existing entities, consumer here is another program not a user, and again functionality lies behind this layer. functionality: my software functionality which solves some purpose to which we are describing this interface. existing entities: functions, Interfaces(array of functions). consumer: another program/application. Application Binary Interface (ABI) Here is where my problem starts. functionality: ??? existing entities: ??? consumer: ??? I've written software in different languages and provided different kind of interfaces (CLI, GUI, and API), but I'm not sure, if I ever, provided any ABI. Wikipedia says: ABIs cover details such as data type, size, and alignment; the calling convention, which controls how functions' arguments are passed and return values retrieved; the system call numbers and how an application should make system calls to the operating system; Other ABIs standardize details such as the C++ name mangling, exception propagation, and calling convention between compilers on the same platform, but do not require cross-platform compatibility. Who needs these details? Please don't say the OS. I know assembly programming. I know how linking & loading works. I know what exactly happens inside. Why did C++ name mangling come in? I thought we are talking at the binary level. Why do languages come in? Anyway, I've downloaded the [ PDF] System V Application Binary Interface Edition 4.1 (1997-03-18) to see what exactly it contains. Well, most of it didn't make any sense. Why does it contain two chapters (4th & 5th) to describe the ELF file format? In fact, these are the only two significant chapters of that specification. The rest of the chapters are \"processor specific\". Anyway, I thought that it is a completely different topic. Please don't say that ELF file format specifications are the ABI. It doesn't qualify to be an interface according to the definition. I know, since we are talking at such a low level it must be very specific. But I'm not sure how is it \"instruction set architecture (ISA)\" specific? Where can I find Microsoft Windows' ABI? So, these are the major queries that are bugging me. A One easy way to understand \"ABI\" is to compare it to \"API\". You are already familiar with the concept of an API. If you want to use the features of, say, some library or your OS, you will use an API. The API consists of data types/structures, constants, functions, etc that you can use in your code to access the functionality of that external component. An ABI is very similar. Think of it as the compiled version of an API (or as an API on the machine-language level). When you write source code, you access the library through an API . Once the code is compiled, your application accesses the binary data in the library through the ABI . The ABI defines the structures and methods that your compiled application will use to access the external library (just like the API did), only on a lower level. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6982\u62ec\u5730\u975e\u5e38\u597d\uff1awhen you code\uff0cuse API to access library;once the code is compiled,the compiled application access the binary data in the library throuth an ABI. ABIs are important when it comes to applications that use external libraries . If a program is built to use a particular library and that library is later updated, you don't want to have to re-compile that application (and from the end-user's standpoint, you may not have the source). If the updated library uses the same ABI , then your program will not need to change. The interface to the library (which is all your program really cares about) is the same even though the internal workings may have changed. Two versions of a library that have the same ABI are sometimes called \" binary-compatible \" since they have the same low-level interface (you should be able to replace the old version with the new one and not have any major problems). SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6d89\u53ca\u5230\u4e86 calling convention Sometimes, ABI changes are unavoidable. When this happens, any programs that use that library will not work unless they are re-compiled to use the new version of the library. If the ABI changes but the API does not(\u6bd4\u5982\u67d0\u4e2a\u6210\u5458\u53d8\u91cf\u7684\u957f\u5ea6\u6539\u53d8\u4e86), then the old and new library versions are sometimes called \" source compatible \". This implies that while a program compiled for one library version will not work with the other, source code written for one will work for the other if re-compiled. For this reason, library writers tend to try to keep their ABI stable (to minimize disruption). Keeping an ABI stable means not changing function interfaces ( return type and number, types, and order of arguments )(\u53c2\u89c1 calling convention \uff09, definitions of data types or data structures, defined constants, etc. New functions and data types can be added, but existing ones must stay the same. If you expand, say, a 16-bit data structure field into a 32-bit field, then already-compiled code that uses that data structure will not be accessing that field (or any following it) correctly. Accessing data structure members gets converted into memory addresses and offsets during compilation and if the data structure changes, then these offsets will not point to what the code is expecting them to point to and the results are unpredictable at best. An ABI isn't necessarily something you will explicitly(\u660e\u786e\u7684) provide unless you are expecting people to interface with your code using assembly. It isn't language-specific either, since (for example) a C application and a Pascal application will use the same ABI after they are compiled. Edit: Regarding your question about the chapters regarding the ELF file format in the SysV ABI docs: The reason this information is included is because the ELF format defines the interface between operating system and application . When you tell the OS to run a program, it expects the program to be formatted in a certain way and (for example) expects the first section of the binary to be an ELF header containing certain information at specific memory offsets. This is how the application communicates important information about itself to the operating system. If you build a program in a non-ELF binary format (such as a.out or PE), then an OS that expects ELF-formatted applications will not be able to interpret the binary file or run the application. This is one big reason why Windows apps cannot be run directly on a Linux machine (or vice versa) without being either re-compiled or run inside some type of emulation layer that can translate from one binary format to another. IIRC, Windows currently uses the Portable Executable (or, PE) format. There are links in the \"external links\" section of that Wikipedia page with more information about the PE format. Also, regarding your note about C++ name mangling: The ABI can define a \"standardized\" way for a C++ compiler to do name mangling for the purpose of compatibility. That is, if I create a library and you develop a program that uses the library, you should be able to use a different compiler than I did and not have to worry about the resulting binaries being incompatible due to different name mangling schemes . This is really only of use if you are defining a new binary file format or writing a compiler or linker. A If you know assembly and how things work at the OS-level, you are conforming to a certain ABI. The ABI govern things like how parameters are passed, where return values are placed. For many platforms there is only one ABI to choose from, and in those cases the ABI is just \"how things work\". However, the ABI also govern things like how classes/objects are laid out in C++. This is necessary if you want to be able to pass object references across module boundaries or if you want to mix code compiled with different compilers. Also, if you have an 64-bit OS which can execute 32-bit binaries, you will have different ABIs for 32- and 64-bit code. In general, any code you link into the same executable must conform to the same ABI. If you want to communicate between code using different ABIs, you must use some form of RPC or serialization protocols. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981 I think you are trying too hard to squeeze in different types of interfaces into a fixed set of characteristics. For example, an interface doesn't necessarily have to be split into consumers and producers. An interface is just a convention by which two entities interact. ABIs can be (partially) ISA-agnostic. Some aspects (such as calling conventions) depend on the ISA , while other aspects (such as C++ class layout) do not. A well defined ABI is very important for people writing compilers. Without a well defined ABI, it would be impossible to generate interoperable code. EDIT: Some notes to clarify: \"Binary\" in ABI does not exclude the use of strings or text. If you want to link a DLL exporting a C++ class, somewhere in it the methods and type signatures must be encoded. That's where C++ name-mangling comes in. The reason why you never provided an ABI is that the vast majority of programmers will never do it. ABIs are provided by the same people designing the platform (i.e. operating system), and very few programmers will ever have the privilege to design a widely-used ABI.","title":"[Application binary interface](https://en.wikipedia.org/wiki/Application_binary_interface)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#application-binary-interface","text":"In computer software , an application binary interface ( ABI ) is an interface between two binary program modules ; often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user. An ABI defines how data structures or computational routines are accessed in machine code , which is a low-level, hardware-dependent format; in contrast, an API defines this access in source code , which is a relatively high-level, hardware-independent, often human-readable format. A common aspect of an ABI is the calling convention , which determines how data is provided as input to or read as output from computational routines; examples are the x86 calling conventions . Adhering to an ABI (which may or may not be officially standardized) is usually the job of a compiler , operating system, or library author; however, an application programmer may have to deal with an ABI directly when writing a program in a mix of programming languages, which can be achieved by using foreign function calls .","title":"Application binary interface"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#description","text":"ABIs cover details such as: a processor instruction set (with details like register file structure, stack organization, memory access types, ...) the sizes, layouts, and alignments of basic data types that the processor can directly access the calling convention , which controls how functions ' arguments are passed and return values are retrieved; for example, whether all parameters are passed on the stack or some are passed in registers , which registers are used for which function parameters, and whether the first function parameter passed on the stack is pushed first or last onto the stack how an application should make system calls to the operating system and, if the ABI specifies direct system calls rather than procedure calls to system call stubs, the system call numbers and in the case of a complete operating system ABI, the binary format of object files , program libraries and so on.","title":"Description"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#complete-abis","text":"A complete ABI, such as the Intel Binary Compatibility Standard (iBCS),[ 1] allows a program from one operating system supporting that ABI to run without modifications on any other such system, provided that necessary shared libraries are present, and similar prerequisites are fulfilled. Other[ which? ] ABIs standardize details such as the C++ name mangling ,[ 2] exception propagation,[ 3] and calling convention between compilers on the same platform, but do not require cross-platform compatibility.","title":"Complete ABIs"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#embedded-abis","text":"An embedded-application binary interface (EABI) specifies standard conventions for file formats , data types, register usage, stack frame organization, and function parameter passing of an embedded software program, for use with an embedded operating system . Compilers that support the EABI create object code that is compatible with code generated by other such compilers, allowing developers to link libraries generated with one compiler with object code generated with another compiler. Developers writing their own assembly language code may also interface with assembly generated by a compliant compiler. EABIs are designed to optimize for performance within the limited resources of an embedded system. Therefore, EABIs omit most abstractions that are made between kernel and user code in complex operating systems. For example, dynamic linking is avoided to allow smaller executables and faster loading, fixed register usage allows more compact stacks and kernel calls, and running the application in privileged mode allows direct access to custom hardware operation without the indirection of calling a device driver. [ 4] The choice of EABI can affect performance.[ 5] [ 6] Widely used EABIs include PowerPC ,[ 4] ARM EABI2[ 7] and MIPS EABI.[ 8]","title":"Embedded ABIs"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#difference-between-api-and-abi","text":"Q: I am new to linux system programming and I came across API and ABI while reading Linux System Programming . Definition of API : An API defines the interfaces by which one piece of software communicates with another at the source level. Definition of ABI : Whereas an API defines a source interface, an ABI defines the low-level binary interface between two or more pieces of software on a particular architecture. It defines how an application interacts with itself, how an application interacts with the kernel , and how an application interacts with libraries . How can a program communicate at a source level ? What is a source level ? Is it related to source code in anyway? Or the source of the library gets included in the main program ? The only difference I know is API is mostly used by programmers and ABI is mostly used by compiler. A: by source level they mean something like include file to expose function definitions \u2013 Anycorn A :","title":"Difference between API and ABI"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#api-application-program-interface","text":"This is the set of public types/variables/functions that you expose from your application/library. In C/C++ this is what you expose in the header files that you ship with the application.","title":"API: Application Program Interface"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#abi-application-binary-interface","text":"This is how the compiler builds an application. It defines things (but is not limited to): How parameters are passed to functions (registers/stack). Who cleans parameters from the stack (caller/callee). Where the return value is placed for return. How exceptions propagate.","title":"ABI: Application Binary Interface"},{"location":"Chapter-7-Run-Time-Environments/ABI/Application-binary-interface/#what-is-an-application-binary-interface-abi","text":"Q: I never clearly understood what an ABI is. Please don't point me to a Wikipedia article. If I could understand it, I wouldn't be here posting such a lengthy post. This is my mindset about different interfaces: A TV remote(\u9065\u63a7\u5668) is an interface between the user and the TV. It is an existing entity, but useless (doesn't provide any functionality) by itself. All the functionality for each of those buttons on the remote is implemented in the television set. Interface: It is an \"existing entity\" layer between the functionality and consumer of that functionality. An interface by itself is doesn't do anything. It just invokes the functionality lying behind. Now depending on who the user is there are different type of interfaces. Command Line Interface (CLI) commands are the existing entities, the consumer is the user and functionality lies behind. functionality: my software functionality which solves some purpose to which we are describing this interface. existing entities: commands consumer: user Graphical User Interface(GUI) window, buttons, etc. are the existing entities, and again the consumer is the user and functionality lies behind. functionality: my software functionality which solves some purpose to which we are describing this interface. existing entities: window,buttons etc.. consumer: user Application Programming Interface(API) functions or to be more correct, interfaces (in interfaced based programming) are the existing entities, consumer here is another program not a user, and again functionality lies behind this layer. functionality: my software functionality which solves some purpose to which we are describing this interface. existing entities: functions, Interfaces(array of functions). consumer: another program/application. Application Binary Interface (ABI) Here is where my problem starts. functionality: ??? existing entities: ??? consumer: ??? I've written software in different languages and provided different kind of interfaces (CLI, GUI, and API), but I'm not sure, if I ever, provided any ABI. Wikipedia says: ABIs cover details such as data type, size, and alignment; the calling convention, which controls how functions' arguments are passed and return values retrieved; the system call numbers and how an application should make system calls to the operating system; Other ABIs standardize details such as the C++ name mangling, exception propagation, and calling convention between compilers on the same platform, but do not require cross-platform compatibility. Who needs these details? Please don't say the OS. I know assembly programming. I know how linking & loading works. I know what exactly happens inside. Why did C++ name mangling come in? I thought we are talking at the binary level. Why do languages come in? Anyway, I've downloaded the [ PDF] System V Application Binary Interface Edition 4.1 (1997-03-18) to see what exactly it contains. Well, most of it didn't make any sense. Why does it contain two chapters (4th & 5th) to describe the ELF file format? In fact, these are the only two significant chapters of that specification. The rest of the chapters are \"processor specific\". Anyway, I thought that it is a completely different topic. Please don't say that ELF file format specifications are the ABI. It doesn't qualify to be an interface according to the definition. I know, since we are talking at such a low level it must be very specific. But I'm not sure how is it \"instruction set architecture (ISA)\" specific? Where can I find Microsoft Windows' ABI? So, these are the major queries that are bugging me. A One easy way to understand \"ABI\" is to compare it to \"API\". You are already familiar with the concept of an API. If you want to use the features of, say, some library or your OS, you will use an API. The API consists of data types/structures, constants, functions, etc that you can use in your code to access the functionality of that external component. An ABI is very similar. Think of it as the compiled version of an API (or as an API on the machine-language level). When you write source code, you access the library through an API . Once the code is compiled, your application accesses the binary data in the library through the ABI . The ABI defines the structures and methods that your compiled application will use to access the external library (just like the API did), only on a lower level. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6982\u62ec\u5730\u975e\u5e38\u597d\uff1awhen you code\uff0cuse API to access library;once the code is compiled,the compiled application access the binary data in the library throuth an ABI. ABIs are important when it comes to applications that use external libraries . If a program is built to use a particular library and that library is later updated, you don't want to have to re-compile that application (and from the end-user's standpoint, you may not have the source). If the updated library uses the same ABI , then your program will not need to change. The interface to the library (which is all your program really cares about) is the same even though the internal workings may have changed. Two versions of a library that have the same ABI are sometimes called \" binary-compatible \" since they have the same low-level interface (you should be able to replace the old version with the new one and not have any major problems). SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u6d89\u53ca\u5230\u4e86 calling convention Sometimes, ABI changes are unavoidable. When this happens, any programs that use that library will not work unless they are re-compiled to use the new version of the library. If the ABI changes but the API does not(\u6bd4\u5982\u67d0\u4e2a\u6210\u5458\u53d8\u91cf\u7684\u957f\u5ea6\u6539\u53d8\u4e86), then the old and new library versions are sometimes called \" source compatible \". This implies that while a program compiled for one library version will not work with the other, source code written for one will work for the other if re-compiled. For this reason, library writers tend to try to keep their ABI stable (to minimize disruption). Keeping an ABI stable means not changing function interfaces ( return type and number, types, and order of arguments )(\u53c2\u89c1 calling convention \uff09, definitions of data types or data structures, defined constants, etc. New functions and data types can be added, but existing ones must stay the same. If you expand, say, a 16-bit data structure field into a 32-bit field, then already-compiled code that uses that data structure will not be accessing that field (or any following it) correctly. Accessing data structure members gets converted into memory addresses and offsets during compilation and if the data structure changes, then these offsets will not point to what the code is expecting them to point to and the results are unpredictable at best. An ABI isn't necessarily something you will explicitly(\u660e\u786e\u7684) provide unless you are expecting people to interface with your code using assembly. It isn't language-specific either, since (for example) a C application and a Pascal application will use the same ABI after they are compiled. Edit: Regarding your question about the chapters regarding the ELF file format in the SysV ABI docs: The reason this information is included is because the ELF format defines the interface between operating system and application . When you tell the OS to run a program, it expects the program to be formatted in a certain way and (for example) expects the first section of the binary to be an ELF header containing certain information at specific memory offsets. This is how the application communicates important information about itself to the operating system. If you build a program in a non-ELF binary format (such as a.out or PE), then an OS that expects ELF-formatted applications will not be able to interpret the binary file or run the application. This is one big reason why Windows apps cannot be run directly on a Linux machine (or vice versa) without being either re-compiled or run inside some type of emulation layer that can translate from one binary format to another. IIRC, Windows currently uses the Portable Executable (or, PE) format. There are links in the \"external links\" section of that Wikipedia page with more information about the PE format. Also, regarding your note about C++ name mangling: The ABI can define a \"standardized\" way for a C++ compiler to do name mangling for the purpose of compatibility. That is, if I create a library and you develop a program that uses the library, you should be able to use a different compiler than I did and not have to worry about the resulting binaries being incompatible due to different name mangling schemes . This is really only of use if you are defining a new binary file format or writing a compiler or linker. A If you know assembly and how things work at the OS-level, you are conforming to a certain ABI. The ABI govern things like how parameters are passed, where return values are placed. For many platforms there is only one ABI to choose from, and in those cases the ABI is just \"how things work\". However, the ABI also govern things like how classes/objects are laid out in C++. This is necessary if you want to be able to pass object references across module boundaries or if you want to mix code compiled with different compilers. Also, if you have an 64-bit OS which can execute 32-bit binaries, you will have different ABIs for 32- and 64-bit code. In general, any code you link into the same executable must conform to the same ABI. If you want to communicate between code using different ABIs, you must use some form of RPC or serialization protocols. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u975e\u5e38\u91cd\u8981 I think you are trying too hard to squeeze in different types of interfaces into a fixed set of characteristics. For example, an interface doesn't necessarily have to be split into consumers and producers. An interface is just a convention by which two entities interact. ABIs can be (partially) ISA-agnostic. Some aspects (such as calling conventions) depend on the ISA , while other aspects (such as C++ class layout) do not. A well defined ABI is very important for people writing compilers. Without a well defined ABI, it would be impossible to generate interoperable code. EDIT: Some notes to clarify: \"Binary\" in ABI does not exclude the use of strings or text. If you want to link a DLL exporting a C++ class, somewhere in it the methods and type signatures must be encoded. That's where C++ name-mangling comes in. The reason why you never provided an ABI is that the vast majority of programmers will never do it. ABIs are provided by the same people designing the platform (i.e. operating system), and very few programmers will ever have the privilege to design a widely-used ABI.","title":"What is an application binary interface (ABI)?"},{"location":"Chapter-7-Run-Time-Environments/ABI/Book-x86-Disassembly/","text":"\u5173\u4e8e\u672c\u4e66 # x86 calling conventions","title":"\u5173\u4e8e\u672c\u4e66"},{"location":"Chapter-7-Run-Time-Environments/ABI/Book-x86-Disassembly/#_1","text":"x86 calling conventions","title":"\u5173\u4e8e\u672c\u4e66"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/","text":"Calling Conventions Demystified Introduction C calling convention (__cdecl) Standard calling convention (__stdcall) Fast calling convention (__fastcall) Thiscall Conclusion Calling Conventions Demystified # Introduction # During the long, hard, but yet beautiful process of learning C++ programming for Windows, you have probably been curious about the strange specifiers that sometime appear in front of function declarations, like __cdecl , __stdcall , __fastcall , WINAPI , etc. After looking through MSDN, or some other reference, you probably found out that these specifiers specify the calling conventions for functions. In this article, I will try to explain different calling conventions used by Visual C++ (and probably other Windows C/C++ compilers). I emphasize that above mentioned specifiers are Microsoft-specific, and that you should not use them if you want to write portable code. So, what are the calling conventions? When a function is called, the arguments are typically passed to it, and the return value is retrieved. A calling convention describes how the arguments are passed and values returned by functions. It also specifies how the function names are decorated. Is it really necessary to understand the calling conventions to write good C/C++ programs? Not at all. However, it may be helpful with debugging. Also, it is necessary for linking C/C++ with assembly code. To understand this article, you will need to have some very basic knowledge of assembly programming. No matter which calling convention is used, the following things will happen: All arguments are widened to 4 bytes (on Win32, of course), and put into appropriate memory locations. These locations are typically on the stack, but may also be in registers; this is specified by calling conventions . Program execution jumps to the address of the called function . Inside the function, registers ESI , EDI , EBX , and EBP are saved on the stack. The part of code that performs these operations is called function prolog (\u51fd\u6570\u5f00\u573a) and usually is generated by the compiler. The function-specific code is executed, and the return value is placed into the EAX register. Registers ESI, EDI, EBX, and EBP are restored from the stack. The piece of code that does this is called function epilog (\u51fd\u6570\u6536\u573a), and as with the function prolog, in most cases the compiler generates it. Arguments are removed from the stack. This operation is called stack cleanup and may be performed either inside the called function or by the caller, depending on the calling convention used. As an example for the calling conventions (except for this ), we are going to use a simple function: Hide Copy Code int sumExample (int a, int b) { return a + b; } The call to this function will look like this: Hide Copy Code int c = sum (2, 3); For __cdecl , __stdcall , and __fastcall calling conventions, I compiled the example code as C (not C++). The function name decorations , mentioned later in the article, apply to the C decoration schema. C++ name decorations are beyond the scope of this article. C calling convention ( __cdecl ) # This convention is the default for C/C++ programs (compiler option /Gd). If a project is set to use some other calling convention, we can still declare a function to use __cdecl : Hide Copy Code int __cdecl sumExample (int a, int b); The main characteristics of __cdecl calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the caller. Function name is decorated by prefixing it with an underscore character '_' . Now, take a look at an example of a __cdecl call: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample ; // cleanup the stack by adding the size of the arguments to ESP register add esp,8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The called function is shown below: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0C0h push ebx push esi push edi lea edi,[ebp-0C0h] mov ecx,30h mov eax,0CCCCCCCCh rep stos dword ptr [edi] ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret Standard calling convention ( __stdcall ) # This convention is usually used to call Win32 API functions. In fact, WINAPI is nothing but another name for __stdcall : Hide Copy Code #define WINAPI __stdcall We can explicitly declare a function to use the __stdcall convention: Hide Copy Code int __stdcall sumExample (int a, int b); Also, we can use the compiler option /Gz to specify __stdcall for all functions not explicitly declared with some other calling convention. The main characteristics of __stdcall calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the called function. Function name is decorated by prepending an underscore character and appending a '@' character and the number of bytes of stack space required. The example follows: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The function code is shown below: Hide Copy Code ; // function prolog goes here (the same code as in the __cdecl example) ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog goes here (the same code as in the __cdecl example) ; // cleanup the stack and return ret 8 Because the stack is cleaned by the called function, the __stdcall calling convention creates smaller executables than __cdecl , in which the code for stack cleanup must be generated for each function call. On the other hand, functions with the variable number of arguments (like printf() ) must use __cdecl , because only the caller knows the number of arguments in each function call; therefore only the caller can perform the stack cleanup. Fast calling convention (__fastcall) # Fast calling convention indicates that the arguments should be placed in registers, rather than on the stack, whenever possible. This reduces the cost of a function call, because operations with registers are faster than with the stack. We can explicitly declare a function to use the __fastcall convention as shown: Hide Copy Code int __fastcall sumExample (int a, int b); We can also use the compiler option /Gr to specify __fastcall for all functions not explicitly declared with some other calling convention. The main characteristics of __fastcall calling convention are: The first two function arguments that require 32 bits or less are placed into registers ECX and EDX. The rest of them are pushed on the stack from right to left. Arguments are popped from the stack by the called function. Function name is decorated by by prepending a '@' character and appending a '@' and the number of bytes (decimal) of space required by the arguments. Note: Microsoft have reserved the right to change the registers for passing the arguments in future compiler versions. Here goes an example: Hide Copy Code ; // put the arguments in the registers EDX and ECX mov edx,3 mov ecx,2 ; // call the function call @fastcallSum@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax Function code: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0D8h push ebx push esi push edi push ecx lea edi,[ebp-0D8h] mov ecx,36h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-14h],edx mov dword ptr [ebp-8],ecx ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ;// function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret How fast is this calling convention, comparing to __cdecl and __stdcall ? Find out for yourselves. Set the compiler option /Gr , and compare the execution time. I didn't find __fastcall to be any faster than other calling conventons, but you may come to different conclusions. Thiscall # Thiscall is the default calling convention for calling member functions of C++ classes (except for those with a variable number of arguments). The main characteristics of thiscall calling convention are: Arguments are passed from right to left, and placed on the stack. this is placed in ECX . Stack cleanup is performed by the called function. The example for this calling convention had to be a little different. First, the code is compiled as C++, and not C. Second, we have a struct with a member function, instead of a global function. Hide Copy Code struct CSum { int sum ( int a, int b) {return a+b;} }; The assembly code for the function call looks like this: Hide Copy Code push 3 push 2 lea ecx,[sumObj] call ?sum@CSum@@QAEHHH@Z ; CSum::sum mov dword ptr [s4],eax The function itself is given below: Hide Copy Code push ebp mov ebp,esp sub esp,0CCh push ebx push esi push edi push ecx lea edi,[ebp-0CCh] mov ecx,33h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-8],ecx mov eax,dword ptr [a] add eax,dword ptr [b] pop edi pop esi pop ebx mov esp,ebp pop ebp ret 8 Now, what happens if we have a member function with a variable number of arguments? In that case, __cdecl is used, and this is pushed onto the stack last. Conclusion # To cut a long story short, we'll outline the main differences between the calling conventions: __cdecl is the default calling convention for C and C++ programs. The advantage of this calling convetion is that it allows functions with a variable number of arguments to be used. The disadvantage is that it creates larger executables. __stdcall is used to call Win32 API functions. It does not allow functions to have a variable number of arguments. __fastcall attempts to put arguments in registers, rather than on the stack, thus making function calls faster. Thiscall calling convention is the default calling convention used by C++ member functions that do not use variable arguments. In most cases, this is all you'll ever need to know about the calling conventions.","title":"Calling Conventions Demystified"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#calling-conventions-demystified","text":"","title":"Calling Conventions Demystified"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#introduction","text":"During the long, hard, but yet beautiful process of learning C++ programming for Windows, you have probably been curious about the strange specifiers that sometime appear in front of function declarations, like __cdecl , __stdcall , __fastcall , WINAPI , etc. After looking through MSDN, or some other reference, you probably found out that these specifiers specify the calling conventions for functions. In this article, I will try to explain different calling conventions used by Visual C++ (and probably other Windows C/C++ compilers). I emphasize that above mentioned specifiers are Microsoft-specific, and that you should not use them if you want to write portable code. So, what are the calling conventions? When a function is called, the arguments are typically passed to it, and the return value is retrieved. A calling convention describes how the arguments are passed and values returned by functions. It also specifies how the function names are decorated. Is it really necessary to understand the calling conventions to write good C/C++ programs? Not at all. However, it may be helpful with debugging. Also, it is necessary for linking C/C++ with assembly code. To understand this article, you will need to have some very basic knowledge of assembly programming. No matter which calling convention is used, the following things will happen: All arguments are widened to 4 bytes (on Win32, of course), and put into appropriate memory locations. These locations are typically on the stack, but may also be in registers; this is specified by calling conventions . Program execution jumps to the address of the called function . Inside the function, registers ESI , EDI , EBX , and EBP are saved on the stack. The part of code that performs these operations is called function prolog (\u51fd\u6570\u5f00\u573a) and usually is generated by the compiler. The function-specific code is executed, and the return value is placed into the EAX register. Registers ESI, EDI, EBX, and EBP are restored from the stack. The piece of code that does this is called function epilog (\u51fd\u6570\u6536\u573a), and as with the function prolog, in most cases the compiler generates it. Arguments are removed from the stack. This operation is called stack cleanup and may be performed either inside the called function or by the caller, depending on the calling convention used. As an example for the calling conventions (except for this ), we are going to use a simple function: Hide Copy Code int sumExample (int a, int b) { return a + b; } The call to this function will look like this: Hide Copy Code int c = sum (2, 3); For __cdecl , __stdcall , and __fastcall calling conventions, I compiled the example code as C (not C++). The function name decorations , mentioned later in the article, apply to the C decoration schema. C++ name decorations are beyond the scope of this article.","title":"Introduction"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#c-calling-convention-__cdecl","text":"This convention is the default for C/C++ programs (compiler option /Gd). If a project is set to use some other calling convention, we can still declare a function to use __cdecl : Hide Copy Code int __cdecl sumExample (int a, int b); The main characteristics of __cdecl calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the caller. Function name is decorated by prefixing it with an underscore character '_' . Now, take a look at an example of a __cdecl call: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample ; // cleanup the stack by adding the size of the arguments to ESP register add esp,8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The called function is shown below: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0C0h push ebx push esi push edi lea edi,[ebp-0C0h] mov ecx,30h mov eax,0CCCCCCCCh rep stos dword ptr [edi] ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret","title":"C calling convention (__cdecl)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#standard-calling-convention-__stdcall","text":"This convention is usually used to call Win32 API functions. In fact, WINAPI is nothing but another name for __stdcall : Hide Copy Code #define WINAPI __stdcall We can explicitly declare a function to use the __stdcall convention: Hide Copy Code int __stdcall sumExample (int a, int b); Also, we can use the compiler option /Gz to specify __stdcall for all functions not explicitly declared with some other calling convention. The main characteristics of __stdcall calling convention are: Arguments are passed from right to left, and placed on the stack. Stack cleanup is performed by the called function. Function name is decorated by prepending an underscore character and appending a '@' character and the number of bytes of stack space required. The example follows: Hide Copy Code ; // push arguments to the stack, from right to left push 3 push 2 ; // call the function call _sumExample@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax The function code is shown below: Hide Copy Code ; // function prolog goes here (the same code as in the __cdecl example) ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ; // function epilog goes here (the same code as in the __cdecl example) ; // cleanup the stack and return ret 8 Because the stack is cleaned by the called function, the __stdcall calling convention creates smaller executables than __cdecl , in which the code for stack cleanup must be generated for each function call. On the other hand, functions with the variable number of arguments (like printf() ) must use __cdecl , because only the caller knows the number of arguments in each function call; therefore only the caller can perform the stack cleanup.","title":"Standard calling convention (__stdcall)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#fast-calling-convention-__fastcall","text":"Fast calling convention indicates that the arguments should be placed in registers, rather than on the stack, whenever possible. This reduces the cost of a function call, because operations with registers are faster than with the stack. We can explicitly declare a function to use the __fastcall convention as shown: Hide Copy Code int __fastcall sumExample (int a, int b); We can also use the compiler option /Gr to specify __fastcall for all functions not explicitly declared with some other calling convention. The main characteristics of __fastcall calling convention are: The first two function arguments that require 32 bits or less are placed into registers ECX and EDX. The rest of them are pushed on the stack from right to left. Arguments are popped from the stack by the called function. Function name is decorated by by prepending a '@' character and appending a '@' and the number of bytes (decimal) of space required by the arguments. Note: Microsoft have reserved the right to change the registers for passing the arguments in future compiler versions. Here goes an example: Hide Copy Code ; // put the arguments in the registers EDX and ECX mov edx,3 mov ecx,2 ; // call the function call @fastcallSum@8 ; // copy the return value from EAX to a local variable (int c) mov dword ptr [c],eax Function code: Hide Copy Code ; // function prolog push ebp mov ebp,esp sub esp,0D8h push ebx push esi push edi push ecx lea edi,[ebp-0D8h] mov ecx,36h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-14h],edx mov dword ptr [ebp-8],ecx ; // return a + b; mov eax,dword ptr [a] add eax,dword ptr [b] ;// function epilog pop edi pop esi pop ebx mov esp,ebp pop ebp ret How fast is this calling convention, comparing to __cdecl and __stdcall ? Find out for yourselves. Set the compiler option /Gr , and compare the execution time. I didn't find __fastcall to be any faster than other calling conventons, but you may come to different conclusions.","title":"Fast calling convention (__fastcall)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#thiscall","text":"Thiscall is the default calling convention for calling member functions of C++ classes (except for those with a variable number of arguments). The main characteristics of thiscall calling convention are: Arguments are passed from right to left, and placed on the stack. this is placed in ECX . Stack cleanup is performed by the called function. The example for this calling convention had to be a little different. First, the code is compiled as C++, and not C. Second, we have a struct with a member function, instead of a global function. Hide Copy Code struct CSum { int sum ( int a, int b) {return a+b;} }; The assembly code for the function call looks like this: Hide Copy Code push 3 push 2 lea ecx,[sumObj] call ?sum@CSum@@QAEHHH@Z ; CSum::sum mov dword ptr [s4],eax The function itself is given below: Hide Copy Code push ebp mov ebp,esp sub esp,0CCh push ebx push esi push edi push ecx lea edi,[ebp-0CCh] mov ecx,33h mov eax,0CCCCCCCCh rep stos dword ptr [edi] pop ecx mov dword ptr [ebp-8],ecx mov eax,dword ptr [a] add eax,dword ptr [b] pop edi pop esi pop ebx mov esp,ebp pop ebp ret 8 Now, what happens if we have a member function with a variable number of arguments? In that case, __cdecl is used, and this is pushed onto the stack last.","title":"Thiscall"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-Conventions-Demystified/#conclusion","text":"To cut a long story short, we'll outline the main differences between the calling conventions: __cdecl is the default calling convention for C and C++ programs. The advantage of this calling convetion is that it allows functions with a variable number of arguments to be used. The disadvantage is that it creates larger executables. __stdcall is used to call Win32 API functions. It does not allow functions to have a variable number of arguments. __fastcall attempts to put arguments in registers, rather than on the stack, thus making function calls faster. Thiscall calling convention is the default calling convention used by C++ member functions that do not use variable arguments. In most cases, this is all you'll ever need to know about the calling conventions.","title":"Conclusion"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/","text":"Calling convention # In computer science , a calling convention is an implementation-level (low-level) scheme for how subroutines receive parameters from their caller and how they return a result. Differences in various implementations include where parameters, return values , return addresses and scope links are placed, and how the tasks of preparing for a function call and restoring the environment afterward are divided between the caller and the callee(\u4ee5\u53ca\u5982\u4f55\u5728\u8c03\u7528\u8005\u548c\u88ab\u8c03\u7528\u8005\u4e4b\u95f4\u5212\u5206\u51c6\u5907\u51fd\u6570\u8c03\u7528\u548c\u6062\u590d\u73af\u5883\u7684\u4efb\u52a1). Calling conventions may be related to a particular programming language's evaluation strategy but most often are not considered part of it (or vice versa), as the evaluation strategy is usually defined on a higher abstraction level and seen as a part of the language rather than as a low-level implementation detail of a particular language's compiler . Variations # Calling conventions may differ in: Where parameters, return values and return addresses are placed (in registers , on the call stack , a mix of both, or in other memory structures) The order in which actual arguments for formal parameters are passed (or the parts of a large or complex argument) How a (possibly long or complex) return value is delivered from the callee back to the caller (on the stack, in a register, or within the heap) How the task of setting up for and cleaning up after a function call is divided between the caller and the callee Whether and how metadata describing the arguments is passed Where the previous value of the frame pointer is stored, which is used to restore the frame pointer when the routine ends (in the stack frame, or in some register) Where any static scope links for the routine's non-local data access are placed (typically at one or more positions in the stack frame, but sometimes in a general register, or, for some architectures, in special-purpose registers) How local variables are allocated can sometimes also be part of the calling convention (when the caller allocates for the callee) In some cases, differences also include the following: Conventions on which registers may be directly used by the callee, without being preserved (otherwise regarded as an ABI detail) Which registers are considered to be volatile and, if volatile, need not be restored by the callee (often regarded as an ABI detail) Compiler variation # Although some [ which? ] languages actually may specify this partially in the programming language specification (or in some pivotal implementation), different implementations of such languages (i.e. different compilers ) may typically still use various calling conventions , often selectable. Reasons for this are performance, frequent adaptation to the conventions of other popular languages (with or without technical reasons), and restrictions or conventions imposed by various \"platforms\" (combinations of CPU architectures and operating systems ). Architecture variation # CPU architectures always have more than one possible calling convention[ why? ]. With many general-purpose registers and other features, the potential number of calling conventions is large, although some[ which? ] architectures are formally specified to use only one calling convention, supplied by the architect. x86 (32-bit) # Main article: x86 calling conventions The x86 architecture is used with many different calling conventions. Due to the small number of architectural registers, the x86 calling conventions mostly pass arguments on the stack, while the return value (or a pointer to it) is passed in a register. Some conventions use registers for the first few parameters, which may improve performance for short and simple leaf-routines very frequently invoked (i.e. routines that do not call other routines and do not have to be reentrant ). Example call: push EAX ; pass some register result push byte[EBP+20] ; pass some memory variable (FASM/TASM syntax) push 3 ; pass some constant call calc ; the returned result is now in EAX Typical callee structure: ( some or all (except ret) of the instructions below may be optimized away in simple procedures ) calc: push EBP ; save old frame pointer mov EBP,ESP ; get new frame pointer sub ESP,localsize ; reserve place for locals . . ; perform calculations, leave result in EAX . mov ESP,EBP ; free space for locals pop EBP ; restore old frame pointer ret paramsize ; free parameter space and return ARM (A32) # The standard 32-bit ARM calling convention allocates the 15 general-purpose registers as: r14 is the link register. (The BL instruction, used in a subroutine call, stores the return address in this register). r13 is the stack pointer. (The Push/Pop instructions in \"Thumb\" operating mode use this register only). r12 is the Intra-Procedure-call scratch register. r4 to r11: used to hold local variables. r0 to r3: used to hold argument values passed to a subroutine, and also hold results returned from a subroutine. The 16th register, r15, is the program counter. If the type of value returned is too large to fit in r0 to r3, or whose size cannot be determined statically at compile time, then the caller must allocate space for that value at run time, and pass a pointer to that space in r0. Subroutines must preserve the contents of r4 to r11 and the stack pointer. (Perhaps by saving them to the stack in the function prologue, then using them as scratch space, then restoring them from the stack in the function epilogue). In particular, subroutines that call other subroutines must save the return address in the link register r14 to the stack before calling those other subroutines. However, such subroutines do not need to return that value to r14\u2014they merely need to load that value into r15, the program counter, to return. The ARM calling convention mandates using a full-descending stack.[ 1] This calling convention causes a \"typical\" ARM subroutine to In the prologue, push r4 to r11 to the stack, and push the return address in r14, to the stack. (This can be done with a single STM instruction). copy any passed arguments (in r0 to r3) to the local scratch registers (r4 to r11). allocate other local variables to the remaining local scratch registers (r4 to r11). do calculations and call other subroutines as necessary using BL, assuming r0 to r3, r12 and r14 will not be preserved. put the result in r0 In the epilogue, pull r4 to r11 from the stack, and pull the return address to the program counter r15. (This can be done with a single LDM instruction). See also # Calling Conventions #","title":"[Calling convention](https://en.wikipedia.org/wiki/Calling_convention)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#calling-convention","text":"In computer science , a calling convention is an implementation-level (low-level) scheme for how subroutines receive parameters from their caller and how they return a result. Differences in various implementations include where parameters, return values , return addresses and scope links are placed, and how the tasks of preparing for a function call and restoring the environment afterward are divided between the caller and the callee(\u4ee5\u53ca\u5982\u4f55\u5728\u8c03\u7528\u8005\u548c\u88ab\u8c03\u7528\u8005\u4e4b\u95f4\u5212\u5206\u51c6\u5907\u51fd\u6570\u8c03\u7528\u548c\u6062\u590d\u73af\u5883\u7684\u4efb\u52a1). Calling conventions may be related to a particular programming language's evaluation strategy but most often are not considered part of it (or vice versa), as the evaluation strategy is usually defined on a higher abstraction level and seen as a part of the language rather than as a low-level implementation detail of a particular language's compiler .","title":"Calling convention"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#variations","text":"Calling conventions may differ in: Where parameters, return values and return addresses are placed (in registers , on the call stack , a mix of both, or in other memory structures) The order in which actual arguments for formal parameters are passed (or the parts of a large or complex argument) How a (possibly long or complex) return value is delivered from the callee back to the caller (on the stack, in a register, or within the heap) How the task of setting up for and cleaning up after a function call is divided between the caller and the callee Whether and how metadata describing the arguments is passed Where the previous value of the frame pointer is stored, which is used to restore the frame pointer when the routine ends (in the stack frame, or in some register) Where any static scope links for the routine's non-local data access are placed (typically at one or more positions in the stack frame, but sometimes in a general register, or, for some architectures, in special-purpose registers) How local variables are allocated can sometimes also be part of the calling convention (when the caller allocates for the callee) In some cases, differences also include the following: Conventions on which registers may be directly used by the callee, without being preserved (otherwise regarded as an ABI detail) Which registers are considered to be volatile and, if volatile, need not be restored by the callee (often regarded as an ABI detail)","title":"Variations"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#compiler-variation","text":"Although some [ which? ] languages actually may specify this partially in the programming language specification (or in some pivotal implementation), different implementations of such languages (i.e. different compilers ) may typically still use various calling conventions , often selectable. Reasons for this are performance, frequent adaptation to the conventions of other popular languages (with or without technical reasons), and restrictions or conventions imposed by various \"platforms\" (combinations of CPU architectures and operating systems ).","title":"Compiler variation"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#architecture-variation","text":"CPU architectures always have more than one possible calling convention[ why? ]. With many general-purpose registers and other features, the potential number of calling conventions is large, although some[ which? ] architectures are formally specified to use only one calling convention, supplied by the architect.","title":"Architecture variation"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#x86-32-bit","text":"Main article: x86 calling conventions The x86 architecture is used with many different calling conventions. Due to the small number of architectural registers, the x86 calling conventions mostly pass arguments on the stack, while the return value (or a pointer to it) is passed in a register. Some conventions use registers for the first few parameters, which may improve performance for short and simple leaf-routines very frequently invoked (i.e. routines that do not call other routines and do not have to be reentrant ). Example call: push EAX ; pass some register result push byte[EBP+20] ; pass some memory variable (FASM/TASM syntax) push 3 ; pass some constant call calc ; the returned result is now in EAX Typical callee structure: ( some or all (except ret) of the instructions below may be optimized away in simple procedures ) calc: push EBP ; save old frame pointer mov EBP,ESP ; get new frame pointer sub ESP,localsize ; reserve place for locals . . ; perform calculations, leave result in EAX . mov ESP,EBP ; free space for locals pop EBP ; restore old frame pointer ret paramsize ; free parameter space and return","title":"x86 (32-bit)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#arm-a32","text":"The standard 32-bit ARM calling convention allocates the 15 general-purpose registers as: r14 is the link register. (The BL instruction, used in a subroutine call, stores the return address in this register). r13 is the stack pointer. (The Push/Pop instructions in \"Thumb\" operating mode use this register only). r12 is the Intra-Procedure-call scratch register. r4 to r11: used to hold local variables. r0 to r3: used to hold argument values passed to a subroutine, and also hold results returned from a subroutine. The 16th register, r15, is the program counter. If the type of value returned is too large to fit in r0 to r3, or whose size cannot be determined statically at compile time, then the caller must allocate space for that value at run time, and pass a pointer to that space in r0. Subroutines must preserve the contents of r4 to r11 and the stack pointer. (Perhaps by saving them to the stack in the function prologue, then using them as scratch space, then restoring them from the stack in the function epilogue). In particular, subroutines that call other subroutines must save the return address in the link register r14 to the stack before calling those other subroutines. However, such subroutines do not need to return that value to r14\u2014they merely need to load that value into r15, the program counter, to return. The ARM calling convention mandates using a full-descending stack.[ 1] This calling convention causes a \"typical\" ARM subroutine to In the prologue, push r4 to r11 to the stack, and push the return address in r14, to the stack. (This can be done with a single STM instruction). copy any passed arguments (in r0 to r3) to the local scratch registers (r4 to r11). allocate other local variables to the remaining local scratch registers (r4 to r11). do calculations and call other subroutines as necessary using BL, assuming r0 to r3, r12 and r14 will not be preserved. put the result in r0 In the epilogue, pull r4 to r11 from the stack, and pull the return address to the program counter r15. (This can be done with a single LDM instruction).","title":"ARM (A32)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#see-also","text":"","title":"See also"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-convention/Calling-convention/#calling-conventions","text":"","title":"Calling Conventions"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-stack/Call-stack-summary/","text":"call stack of recursion function how to using user stack to replace the call stack of recursion function call stack of recursion function # \u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u6808\u53ef\u4ee5\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u53c2\u89c1\u300a recursion-analysis-and-representation.md \u300b how to using user stack to replace the call stack of recursion function # \u5982\u4f55\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6808\u6765\u66ff\u4ee3call stack\uff0c\u53c2\u89c1\u300a recursion-to-iteration.md \u300b","title":"Call stack summary"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-stack/Call-stack-summary/#call-stack-of-recursion-function","text":"\u9012\u5f52\u51fd\u6570\u7684\u8c03\u7528\u6808\u53ef\u4ee5\u4f7f\u7528\u9012\u5f52\u8c03\u7528\u6811\u6765\u8fdb\u884c\u63cf\u8ff0\uff0c\u53c2\u89c1\u300a recursion-analysis-and-representation.md \u300b","title":"call stack of recursion function"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-stack/Call-stack-summary/#how-to-using-user-stack-to-replace-the-call-stack-of-recursion-function","text":"\u5982\u4f55\u4f7f\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u6808\u6765\u66ff\u4ee3call stack\uff0c\u53c2\u89c1\u300a recursion-to-iteration.md \u300b","title":"how to using user stack to replace the call stack of recursion function"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-stack/Call-stack/","text":"Call stack Description Functions of the call stack Call stack # In computer science , a call stack is a stack data structure that stores information about the active subroutines of a computer program . This kind of stack is also known as an execution stack , program stack , control stack , run-time stack , or machine stack , and is often shortened to just \"the stack\". Although maintenance of the call stack is important for the proper functioning of most software , the details are normally hidden and automatic in high-level programming languages . Many computer instruction sets provide special instructions for manipulating stacks. THINKING : \u54ea\u4e9binstruction set\u662f\u539f\u6765manipulation stack\u7684\uff1f SUMMARY : \u663e\u7136operating system\u4e3a\u4e86\u652f\u6301multiple thread\uff0c\u5c31\u4e0d\u9700\u8981\u8981\u8ba9\u6bcf\u4e2athread\u6709\u4e00\u4e2a\u81ea\u5df1\u7684call stack\uff1b\u5728Wikipedia\u7684 Thread control block \u4e2d\u5c31\u8c08\u53ca\u6bcf\u4e2athread\u90fd\u6709\u4e00\u4e2a Stack pointer \uff0c\u800c Process control block \u4e2d\uff0c\u53ef\u80fd\u5c31\u4e0d\u9700\u8981 Stack pointer \u4e86\uff1b A call stack is used for several related purposes, but the main reason for having one is to keep track of the point to which each active subroutine should return control when it finishes executing(\u53c2\u89c1 call trace ). An active subroutine is one that has been called but is yet to complete execution after which control should be handed back to the point of call. Such activations of subroutines may be nested to any level (recursive as a special case), hence the stack structure. If, for example, a subroutine DrawSquare calls a subroutine DrawLine from four different places, DrawLine must know where to return when its execution completes. To accomplish this, the address following the call instruction , the return address , is pushed onto the call stack with each call(\u53c2\u89c1procedure's prologue and epilogue \uff09. Description # Since the call stack is organized as a stack , the caller pushes the return address onto the stack, and the called subroutine, when it finishes, pulls or pops the return address off the call stack and transfers control to that address. If a called subroutine calls on yet another subroutine, it will push another return address onto the call stack, and so on, with the information stacking up and unstacking as the program dictates. If the pushing consumes all of the space allocated for the call stack, an error called a stack overflow occurs, generally causing the program to crash . Adding a subroutine's entry to the call stack is sometimes called \"winding\"; conversely, removing entries is \"unwinding\". There is usually exactly one call stack associated with a running program (or more accurately, with each task or thread of a process ), although additional stacks may be created for signal handling or cooperative multitasking (as with setcontext ). Since there is only one in this important context, it can be referred to as the stack (implicitly, \"of the task\"); however, in the Forth programming language the data stack or parameter stack is accessed more explicitly than the call stack and is commonly referred to as the stack (see below). In high-level programming languages , the specifics of the call stack are usually hidden from the programmer. They are given access only to a set of functions, and not the memory on the stack itself. This is an example of abstraction . Most assembly languages , on the other hand, require programmers to be involved with manipulating the stack. The actual details of the stack in a programming language depend upon the compiler , operating system , and the available instruction set . Functions of the call stack #","title":"Call stack"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-stack/Call-stack/#call-stack","text":"In computer science , a call stack is a stack data structure that stores information about the active subroutines of a computer program . This kind of stack is also known as an execution stack , program stack , control stack , run-time stack , or machine stack , and is often shortened to just \"the stack\". Although maintenance of the call stack is important for the proper functioning of most software , the details are normally hidden and automatic in high-level programming languages . Many computer instruction sets provide special instructions for manipulating stacks. THINKING : \u54ea\u4e9binstruction set\u662f\u539f\u6765manipulation stack\u7684\uff1f SUMMARY : \u663e\u7136operating system\u4e3a\u4e86\u652f\u6301multiple thread\uff0c\u5c31\u4e0d\u9700\u8981\u8981\u8ba9\u6bcf\u4e2athread\u6709\u4e00\u4e2a\u81ea\u5df1\u7684call stack\uff1b\u5728Wikipedia\u7684 Thread control block \u4e2d\u5c31\u8c08\u53ca\u6bcf\u4e2athread\u90fd\u6709\u4e00\u4e2a Stack pointer \uff0c\u800c Process control block \u4e2d\uff0c\u53ef\u80fd\u5c31\u4e0d\u9700\u8981 Stack pointer \u4e86\uff1b A call stack is used for several related purposes, but the main reason for having one is to keep track of the point to which each active subroutine should return control when it finishes executing(\u53c2\u89c1 call trace ). An active subroutine is one that has been called but is yet to complete execution after which control should be handed back to the point of call. Such activations of subroutines may be nested to any level (recursive as a special case), hence the stack structure. If, for example, a subroutine DrawSquare calls a subroutine DrawLine from four different places, DrawLine must know where to return when its execution completes. To accomplish this, the address following the call instruction , the return address , is pushed onto the call stack with each call(\u53c2\u89c1procedure's prologue and epilogue \uff09.","title":"Call stack"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-stack/Call-stack/#description","text":"Since the call stack is organized as a stack , the caller pushes the return address onto the stack, and the called subroutine, when it finishes, pulls or pops the return address off the call stack and transfers control to that address. If a called subroutine calls on yet another subroutine, it will push another return address onto the call stack, and so on, with the information stacking up and unstacking as the program dictates. If the pushing consumes all of the space allocated for the call stack, an error called a stack overflow occurs, generally causing the program to crash . Adding a subroutine's entry to the call stack is sometimes called \"winding\"; conversely, removing entries is \"unwinding\". There is usually exactly one call stack associated with a running program (or more accurately, with each task or thread of a process ), although additional stacks may be created for signal handling or cooperative multitasking (as with setcontext ). Since there is only one in this important context, it can be referred to as the stack (implicitly, \"of the task\"); however, in the Forth programming language the data stack or parameter stack is accessed more explicitly than the call stack and is commonly referred to as the stack (see below). In high-level programming languages , the specifics of the call stack are usually hidden from the programmer. They are given access only to a set of functions, and not the memory on the stack itself. This is an example of abstraction . Most assembly languages , on the other hand, require programmers to be involved with manipulating the stack. The actual details of the stack in a programming language depend upon the compiler , operating system , and the available instruction set .","title":"Description"},{"location":"Chapter-7-Run-Time-Environments/ABI/Function-Call/Call-stack/Call-stack/#functions-of-the-call-stack","text":"","title":"Functions of the call stack"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Library(computing)/","text":"Library (computing) # Illustration of an application which uses libvorbisfile to play an Ogg Vorbis file In computer science , a library is a collection of non-volatile resources used by computer programs , often for software development . These may include configuration data, documentation, help data, message templates, pre-written code and subroutines , classes , values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets . A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system. Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program. This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface and not the internal details of the library. The value of a library lies in the reuse of the behavior. When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion, and ease the distribution of the code. The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases . If the code of the library is accessed during the build of the invoking program, then the library is called a static library .[ 1] An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation. The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution. In this case the library is called a dynamic library (loaded at run time ). A dynamic library can be loaded and linked when preparing a program for execution, by the linker . Alternatively, in the middle of execution, an application may explicitly request that a module be loaded . Most compiled languages have a standard library although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have commoditized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries. Linking # Main articles: Link time and Linker (computing) Libraries are important in the program linking or binding process, which resolves references known as links or symbols to library modules. The linking process is usually automatically done by a linker or binder program that searches a set of libraries and other modules in a given order. Usually it is not considered an error if a link target can be found multiple times in a given set of libraries. Linking may be done when an executable file is created, or whenever the program is used at run time . The references being resolved may be addresses(\u5730\u5740) for jumps and other routine calls. They may be in the main program, or in one module depending upon another. They are resolved into fixed or relocatable addresses (from a common base) by allocating runtime memory for the memory segments of each module referenced. Some programming languages may use a feature called smart linking whereby the linker is aware of or integrated with the compiler, such that the linker knows how external references are used, and code in a library that is never actually used , even though internally referenced, can be discarded from the compiled application. For example, a program that only uses integers for arithmetic, or does no arithmetic operations at all, can exclude floating-point library routines. This smart-linking feature can lead to smaller application file sizes and reduced memory usage. Relocation # Main article: Relocation (computer science) Some references in a program or library module are stored in a relative or symbolic form which cannot be resolved until all code and libraries are assigned final static addresses . Relocation is the process of adjusting these references, and is done either by the linker or the loader . In general, relocation cannot be done to individual libraries themselves because the addresses in memory may vary depending on the program using them and other libraries they are combined with. Position-independent code avoids references to absolute addresses and therefore does not require relocation. See also # Program Library HOWTO https://stackoverflow.com/questions/480764/linux-error-while-loading-shared-libraries-cannot-open-shared-object-file-no-s","title":"[Library (computing)](https://en.wikipedia.org/wiki/Library_(computing)#Shared_libraries)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Library(computing)/#library-computing","text":"Illustration of an application which uses libvorbisfile to play an Ogg Vorbis file In computer science , a library is a collection of non-volatile resources used by computer programs , often for software development . These may include configuration data, documentation, help data, message templates, pre-written code and subroutines , classes , values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets . A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system. Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program. This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface and not the internal details of the library. The value of a library lies in the reuse of the behavior. When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion, and ease the distribution of the code. The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases . If the code of the library is accessed during the build of the invoking program, then the library is called a static library .[ 1] An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation. The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution. In this case the library is called a dynamic library (loaded at run time ). A dynamic library can be loaded and linked when preparing a program for execution, by the linker . Alternatively, in the middle of execution, an application may explicitly request that a module be loaded . Most compiled languages have a standard library although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have commoditized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries.","title":"Library (computing)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Library(computing)/#linking","text":"Main articles: Link time and Linker (computing) Libraries are important in the program linking or binding process, which resolves references known as links or symbols to library modules. The linking process is usually automatically done by a linker or binder program that searches a set of libraries and other modules in a given order. Usually it is not considered an error if a link target can be found multiple times in a given set of libraries. Linking may be done when an executable file is created, or whenever the program is used at run time . The references being resolved may be addresses(\u5730\u5740) for jumps and other routine calls. They may be in the main program, or in one module depending upon another. They are resolved into fixed or relocatable addresses (from a common base) by allocating runtime memory for the memory segments of each module referenced. Some programming languages may use a feature called smart linking whereby the linker is aware of or integrated with the compiler, such that the linker knows how external references are used, and code in a library that is never actually used , even though internally referenced, can be discarded from the compiled application. For example, a program that only uses integers for arithmetic, or does no arithmetic operations at all, can exclude floating-point library routines. This smart-linking feature can lead to smaller application file sizes and reduced memory usage.","title":"Linking"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Library(computing)/#relocation","text":"Main article: Relocation (computer science) Some references in a program or library module are stored in a relative or symbolic form which cannot be resolved until all code and libraries are assigned final static addresses . Relocation is the process of adjusting these references, and is done either by the linker or the loader . In general, relocation cannot be done to individual libraries themselves because the addresses in memory may vary depending on the program using them and other libraries they are combined with. Position-independent code avoids references to absolute addresses and therefore does not require relocation.","title":"Relocation"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Library(computing)/#see-also","text":"Program Library HOWTO https://stackoverflow.com/questions/480764/linux-error-while-loading-shared-libraries-cannot-open-shared-object-file-no-s","title":"See also"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/","text":"Position Independent Code (PIC) in shared libraries Some problems of load-time relocation PIC - introduction Key insight #1 - offset between text and data sections Key insight #2 - making an IP-relative offset work on x86 The Global Offset Table (GOT) PIC with data references through GOT - an example Function calls in PIC The lazy binding optimization The Procedure Linkage Table (PLT) PIC with function calls through PLT and GOT - an example Controlling if and when the resolution is done by the loader The costs of PIC Conclusion Position Independent Code (PIC) in shared libraries # I've described the need for special handling of shared libraries while loading them into the process's address space in a previous article . Briefly, when the linker creates a shared library, it doesn't know in advance where it might be loaded. This creates a problem for the data and code references within the library, which should be somehow made to point to the correct memory locations. There are two main approaches to solve this problem in Linux ELF shared libraries: Load-time relocation Position independent code (PIC) Load-time relocation was already covered . Here, I want to explain the second approach - PIC. I originally planned to focus on both x86 and x64 (a.k.a. x86-64) in this article, but as it grew longer and longer I decided it won't be practical. So, it will explain only how PIC works on x86, picking this older architecture specifically because (unlike x64) it wasn't designed with PIC in mind, so implementing PIC on it is a bit trickier. A future (hopefully much shorter) article will build upon the foundation of this one to explain how PIC is implemented on x64. Some problems of load-time relocation # As we've seen in the previous article, load-time relocation is a fairly straightforward method, and it works. PIC, however, is much more popular nowadays, and is usually the recommended method of building shared libraries. Why is this so? Load-time relocation has a couple of problems: it takes time to perform, and it makes the text section of the library non-shareable. First, the performance problem. If a shared library was linked with load-time relocation entries, it will take some time to actually perform these relocations when the application is loaded. You may think that the cost shouldn't be too large - after all, the loader doesn't have to scan through the whole text section - it should only look at the relocation entries. But if a complex piece of software loads multiple large shared libraries at start-up, and each shared library must first have its load-time relocations applied, these costs can build up and result in a noticeable delay in the start-up time of the application. Second, the non-shareable text section problem, which is somewhat more serious. One of the main points of having shared libraries in the first place, is saving RAM. Some common shared libraries are used by multiple applications. If the text section (where the code is) of the shared library can only be loaded into memory once (and then mapped into the virtual memories of many processes), considerable amounts of RAM can be saved. But this is not possible with load-time relocation, since when using this technique the text section has to be modified at load-time to apply the relocations. Therefore, for each application that loaded this shared library, it will have to be wholly placed in RAM again [ 1] . Different applications won't be able to really share it. Moreover, having a writable text section (it must be kept writable, to allow the dynamic loader to perform the relocations) poses a security risk, making it easier to exploit the application. As we'll see in this article, PIC mostly mitigates these problems. PIC - introduction # The idea behind PIC is simple - add an additional level of indirection to all global data and function references in the code. By cleverly utilizing some artifacts of the linking and loading processes, it's possible to make the text section of the shared library truly position independent , in the sense that it can be easily mapped into different memory addresses without needing to change one bit. In the next few sections I will explain in detail how this feat is achieved. Key insight #1 - offset between text and data sections # One of the key insights on which PIC relies is the offset between the text and data sections, known to the linker at link-time . When the linker combines several object files together, it collects their sections (for example, all text sections get unified into a single large text section). Therefore, the linker knows both about the sizes of the sections and about their relative locations. For example, the text section may be immediately followed by the data section, so the offset from any given instruction in the text section to the beginning of the data section is just the size of the text section minus the offset of the instruction from the beginning of the text section - and both these quantities are known to the linker. In the diagram above, the code section was loaded into some address (unknown at link-time) 0xXXXX0000 (the X-es literally mean \"don't care\"), and the data section right after it at offset 0xXXXXF000. Then, if some instruction at offset 0x80 in the code section wants to reference stuff in the data section, the linker knows the relative offset (0xEF80 in this case) and can encode it in the instruction. Note that it wouldn't matter if another section was placed between the code and data sections, or if the data section preceded the code section. Since the linker knows the sizes of all sections and decides where to place them, the insight holds. Key insight #2 - making an IP-relative offset work on x86 # The above is only useful if we can actually put the relative offset to work. But data references (i.e. in the mov instruction) on x86 require absolute addresses. So, what can we do? If we have a relative address and need an absolute address, what's missing is the value of the instruction pointer (since, by definition, the relative address is relative to the instruction's location). There's no instruction to obtain the value of the instruction pointer on x86, but we can use a simple trick to get it. Here's some assembly pseudo-code that demonstrates it: call TMPLABEL TMPLABEL: pop ebx What happens here is: The CPU executes call TMPLABEL , which causes it to save the address of the next instruction (the pop ebx ) on stack and jump to the label. Since the instruction at the label is pop ebx , it gets executed next. It pops a value from the stack into ebx . But this value is the address of the instruction itself, so ebx now effectively contains the value of the instruction pointer. The Global Offset Table (GOT) # With this at hand, we can finally get to the implementation of position-independent data addressing on x86. It is accomplished by means of a \"global offset table\", or in short GOT. A GOT is simply a table of addresses, residing in the data section. Suppose some instruction in the code section wants to refer to a variable. Instead of referring to it directly by absolute address (which would require a relocation), it refers to an entry in the GOT. Since the GOT is in a known place in the data section, this reference is relative and known to the linker. The GOT entry, in turn, will contain the absolute address of the variable: In pseudo-assembly, we replace an absolute addressing instruction: ; Place the value of the variable in edx mov edx, [ADDR_OF_VAR] With displacement addressing from a register, along with an extra indirection: ; 1. Somehow get the address of the GOT into ebx lea ebx, ADDR_OF_GOT ; 2. Suppose ADDR_OF_VAR is stored at offset 0x10 ; in the GOT. Then this will place ADDR_OF_VAR ; into edx. mov edx, DWORD PTR [ebx + 0x10] ; 3. Finally, access the variable and place its ; value into edx. mov edx, DWORD PTR [edx] So, we've gotten rid of a relocation in the code section by redirecting variable references through the GOT. But we've also created a relocation in the data section. Why? Because the GOT still has to contain the absolute address of the variable for the scheme described above to work. So what have we gained? A lot, it turns out. A relocation in the data section is much less problematic than one in the code section, for two reasons (which directly address the two main problems of load-time relocation of code described in the beginning of the article): Relocations in the code section are required per variable reference , while in the GOT we only need to relocate once per variable . There are likely much more references to variables than variables, so this is more efficient. The data section is writable and not shared between processes anyway, so adding relocations to it does no harm. Moving relocations from the code section, however, allows to make it read-only and share it between processes. PIC with data references through GOT - an example # I will now show a complete example that demonstrates the mechanics of PIC: int myglob = 42; int ml_func(int a, int b) { return myglob + a + b; } This chunk of code will be compiled into a shared library (using the -fpic and -shared flags as appropriate) named libmlpic_dataonly.so . Let's take a look at its disassembly, focusing on the ml_func function: 0000043c <ml_func>: 43c: 55 push ebp 43d: 89 e5 mov ebp,esp 43f: e8 16 00 00 00 call 45a <__i686.get_pc_thunk.cx> 444: 81 c1 b0 1b 00 00 add ecx,0x1bb0 44a: 8b 81 f0 ff ff ff mov eax,DWORD PTR [ecx-0x10] 450: 8b 00 mov eax,DWORD PTR [eax] 452: 03 45 08 add eax,DWORD PTR [ebp+0x8] 455: 03 45 0c add eax,DWORD PTR [ebp+0xc] 458: 5d pop ebp 459: c3 ret 0000045a <__i686.get_pc_thunk.cx>: 45a: 8b 0c 24 mov ecx,DWORD PTR [esp] 45d: c3 ret I'm going to refer to instructions by their addresses (the left-most number in the disassembly). This address is the offset from the load address of the shared library. At 43f , the address of the next instruction is placed into ecx , by means of the technique described in the \"key insight #2\" section above. At 444 , a known constant offset from the instruction to the place where the GOT is located is added to ecx . So ecx now serves as a base pointer to GOT. At 44a , a value is taken from [ecx - 0x10] , which is a GOT entry, and placed into eax . This is the address of myglob . At 450 the indirection is done, and the value of myglob is placed into eax . Later the parameters a and b are added to myglob and the value is returned (by keeping it in eax ). We can also query the shared library with readelf -S to see where the GOT section was placed: Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al <snip> [19] .got PROGBITS 00001fe4 000fe4 000010 04 WA 0 0 4 [20] .got.plt PROGBITS 00001ff4 000ff4 000014 04 WA 0 0 4 <snip> Let's do some math to check the computation done by the compiler to find myglob . As I mentioned above, the call to __i686.get_pc_thunk.cx places the address of the next instruction into ecx . That address is 0x444 [ 2] . The next instruction then adds 0x1bb0 to it, and the result in ecx is going to be 0x1ff4 . Finally, to actually obtain the GOT entry holding the address of myglob , displacement addressing is used - [ecx - 0x10] , so the entry is at 0x1fe4 , which is the first entry in the GOT according to the section header. Why there's another section whose name starts with .got will be explained later in the article [ 3] . Note that the compiler chooses to point ecx to after the GOT and then use negative offsets to obtain entries. This is fine, as long as the math works out. And so far it does. There's something we're still missing, however. How does the address of myglob actually get into the GOT slot at 0x1fe4 ? Recall that I mentioned a relocation, so let's find it: > readelf -r libmlpic_dataonly.so Relocation section '.rel.dyn' at offset 0x2dc contains 5 entries: Offset Info Type Sym.Value Sym. Name 00002008 00000008 R_386_RELATIVE 00001fe4 00000406 R_386_GLOB_DAT 0000200c myglob <snip> Note the relocation section for myglob , pointing to address 0x1fe4 , as expected. The relocation is of type R_386_GLOB_DAT , which simply tells the dynamic loader - \"put the actual value of the symbol (i.e. its address) into that offset\". So everything works out nicely. All that's left is to check how it actually looks when the library is loaded. We can do this by writing a simple \"driver\" executable that links to libmlpic_dataonly.so and calls ml_func , and then running it through GDB. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func [...] (gdb) run Starting program: [...]pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_reloc_dataonly.c:5 5 return myglob + a + b; (gdb) set disassembly-flavor intel (gdb) disas ml_func Dump of assembler code for function ml_func: 0x0013143c <+0>: push ebp 0x0013143d <+1>: mov ebp,esp 0x0013143f <+3>: call 0x13145a <__i686.get_pc_thunk.cx> 0x00131444 <+8>: add ecx,0x1bb0 => 0x0013144a <+14>: mov eax,DWORD PTR [ecx-0x10] 0x00131450 <+20>: mov eax,DWORD PTR [eax] 0x00131452 <+22>: add eax,DWORD PTR [ebp+0x8] 0x00131455 <+25>: add eax,DWORD PTR [ebp+0xc] 0x00131458 <+28>: pop ebp 0x00131459 <+29>: ret End of assembler dump. (gdb) i registers eax 0x1 1 ecx 0x132ff4 1257460 [...] skipping output The debugger has entered ml_func , and stopped at IP 0x0013144a [ 4] . We see that ecx holds the value 0x132ff4 (which is the address of the instruction plus 0x1bb0 , as explained before). Note that at this point, at runtime, these are absolute addresses - the shared library has already been loaded into the address space of the process. So, the GOT entry for myglob is at [ecx - 0x10] . Let's check what's there: (gdb) x 0x132fe4 0x132fe4: 0x0013300c So, we'd expect 0x0013300c to be the address of myglob . Let's verify: (gdb) p &myglob $1 = (int *) 0x13300c Indeed, it is! Function calls in PIC # Alright, so this is how data addressing works in position independent code. But what about function calls? Theoretically, the exact same approach could work for function calls as well. Instead of call actually containing the address of the function to call, let it contain the address of a known GOT entry, and fill in that entry during loading. But this is not how function calls work in PIC. What actually happens is a bit more complicated. Before I explain how it's done, a few words about the motivation for such a mechanism. The lazy binding optimization # When a shared library refers to some function, the real address of that function is not known until load time. Resolving this address is called binding , and it's something the dynamic loader does when it loads the shared library into the process's memory space. This binding process is non-trivial, since the loader has to actually look up the function symbol in special tables [ 5] . So, resolving each function takes time. Not a lot of time, but it adds up since the amount of functions in libraries is typically much larger than the amount of global variables. Moreover, most of these resolutions are done in vain, because in a typical run of a program only a fraction of functions actually get called (think about various functions handling error and special conditions, which typically don't get called at all). So, to speed up this process, a clever lazy binding scheme was devised. \"Lazy\" is a generic name for a family of optimizations in computer programming, where work is delayed until the last moment when it's actually needed, with the intention of avoiding doing this work if its results are never required during a specific run of a program. Good examples of laziness are copy-on-write and lazy evaluation . This lazy binding scheme is attained by adding yet another level of indirection - the PLT. The Procedure Linkage Table (PLT) # The PLT is part of the executable text section, consisting of a set of entries (one for each external function the shared library calls). Each PLT entry is a short chunk of executable code. Instead of calling the function directly, the code calls an entry in the PLT, which then takes care to call the actual function. This arrangement is sometimes called a \" trampoline \". Each PLT entry also has a corresponding entry in the GOT which contains the actual offset to the function, but only when the dynamic loader resolves it. I know this is confusing, but hopefully it will be come clearer once I explain the details in the next few paragraphs and diagrams. As the previous section mentioned, PLTs allow lazy resolution of functions. When the shared library is first loaded, the function calls have not been resolved yet: Explanation: In the code, a function func is called. The compiler translates it to a call to func@plt , which is some N-th entry in the PLT. The PLT consists of a special first entry, followed by a bunch of identically structured entries, one for each function needing resolution. Each PLT entry but the first consists of these parts: A jump to a location which is specified in a corresponding GOT entry Preparation of arguments for a \"resolver\" routine Call to the resolver routine, which resides in the first entry of the PLT The first PLT entry is a call to a resolver routine, which is located in the dynamic loader itself [ 6] . This routine resolves the actual address of the function. More on its action a bit later. Before the function's actual address has been resolved, the Nth GOT entry just points to after the jump. This is why this arrow in the diagram is colored differently - it's not an actual jump, just a pointer. What happens when func is called for the first time is this: PLT[n] is called and jumps to the address pointed to in GOT[n] . This address points into PLT[n] itself, to the preparation of arguments for the resolver. The resolver is then called. The resolver performs resolution of the actual address of func , places its actual address into GOT[n] and calls func . After the first call, the diagram looks a bit differently: Note that GOT[n] now points to the actual func [ 7] instead of back into the PLT. So, when func is called again: PLT[n] is called and jumps to the address pointed to in GOT[n] . GOT[n] points to func , so this just transfers control to func . In other words, now func is being actually called, without going through the resolver, at the cost of one additional jump. That's all there is to it, really. This mechanism allows lazy resolution of functions, and no resolution at all for functions that aren't actually called. It also leaves the code/text section of the library completely position independent, since the only place where an absolute address is used is the GOT, which resides in the data section and will be relocated by the dynamic loader. Even the PLT itself is PIC, so it can live in the read-only text section. I didn't get into much details regarding the resolver, but it's really not important for our purpose here. The resolver is simply a chunk of low-level code in the loader that does symbol resolution. The arguments prepared for it in each PLT entry, along with a suitable relocation entry, help it know about the symbol that needs resolution and about the GOT entry to update. PIC with function calls through PLT and GOT - an example # Once again, to fortify the hard-learned theory with a practical demonstration, here's a complete example showing function call resolution using the mechanism described above. I'll be moving forward a bit faster this time. Here's the code for the shared library: int myglob = 42; int ml_util_func(int a) { return a + 1; } int ml_func(int a, int b) { int c = b + ml_util_func(a); myglob += c; return b + myglob; } This code will be compiled into libmlpic.so , and the focus is going to be on the call to ml_util_func from ml_func . Let's first disassemble ml_func : 00000477 <ml_func>: 477: 55 push ebp 478: 89 e5 mov ebp,esp 47a: 53 push ebx 47b: 83 ec 24 sub esp,0x24 47e: e8 e4 ff ff ff call 467 <__i686.get_pc_thunk.bx> 483: 81 c3 71 1b 00 00 add ebx,0x1b71 489: 8b 45 08 mov eax,DWORD PTR [ebp+0x8] 48c: 89 04 24 mov DWORD PTR [esp],eax 48f: e8 0c ff ff ff call 3a0 <ml_util_func@plt> <... snip more code> The interesting part is the call to ml_util_func@plt . Note also that the address of GOT is in ebx . Here's what ml_util_func@plt looks like (it's in an executable section called .plt ): 000003a0 <ml_util_func@plt>: 3a0: ff a3 14 00 00 00 jmp DWORD PTR [ebx+0x14] 3a6: 68 10 00 00 00 push 0x10 3ab: e9 c0 ff ff ff jmp 370 <_init+0x30> Recall that each PLT entry consists of three parts: A jump to an address specified in GOT (this is the jump to [ebx+0x14] ) Preparation of arguments for the resolver Call to the resolver The resolver (PLT entry 0) resides at address 0x370 , but it's of no interest to us here. What's more interesting is to see what the GOT contains. For that, we first have to do some math. The \"get IP\" trick in ml_func was done on address 0x483 , to which 0x1b71 is added. So the base of the GOT is at 0x1ff4 . We can take a peek at the GOT contents with readelf [ 8] : > readelf -x .got.plt libmlpic.so Hex dump of section '.got.plt': 0x00001ff4 241f0000 00000000 00000000 86030000 $............... 0x00002004 96030000 a6030000 ........ The GOT entry ml_util_func@plt looks at is at offset +0x14 , or 0x2008 . From above, the word at that location is 0x3a6 , which is the address of the push instruction in ml_util_func@plt . To help the dynamic loader do its job, a relocation entry is also added and specifies which place in the GOT to relocate for ml_util_func : > readelf -r libmlpic.so [...] snip output Relocation section '.rel.plt' at offset 0x328 contains 3 entries: Offset Info Type Sym.Value Sym. Name 00002000 00000107 R_386_JUMP_SLOT 00000000 __cxa_finalize 00002004 00000207 R_386_JUMP_SLOT 00000000 __gmon_start__ 00002008 00000707 R_386_JUMP_SLOT 0000046c ml_util_func The last line means that the dynamic loader should place the value (address) of symbol ml_util_func into 0x2008 (which, recall, is the GOT entry for this function). It would be interesting to see this GOT entry modification actually happen after the first call. Let's once again use GDB for the inspection. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func Breakpoint 1 at 0x80483c0 (gdb) run Starting program: /pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_main.c:10 10 int c = b + ml_util_func(a); (gdb) We're now before the first call to ml_util_func . Recall that GOT is pointed to by ebx in this code. Let's see what's in it: (gdb) i registers ebx ebx 0x132ff4 And the offset to the entry we need is at [ebx+0x14] : (gdb) x/w 0x133008 0x133008: 0x001313a6 Yep, the 0x3a6 ending, looks right. Now, let's step until after the call to ml_util_func and check again: (gdb) step ml_util_func (a=1) at ml_main.c:5 5 return a + 1; (gdb) x/w 0x133008 0x133008: 0x0013146c The value at 0x133008 was changed. Hence, 0x0013146c should be the real address of ml_util_func , placed in there by the dynamic loader: (gdb) p &ml_util_func $1 = (int (*)(int)) 0x13146c <ml_util_func> Just as expected. Controlling if and when the resolution is done by the loader # This would be a good place to mention that the process of lazy symbol resolution performed by the dynamic loader can be configured with some environment variables (and corresponding flags to ld when linking the shared library). This is sometimes useful for special performance requirements or debugging. The LD_BIND_NOW env var, when defined, tells the dynamic loader to always perform the resolution for all symbols at start-up time, and not lazily. You can easily verify this in action by setting this env var and re-running the previous sample with GDB. You'll see that the GOT entry for ml_util_func contains its real address even before the first call to the function. Conversely, the LD_BIND_NOT env var tells the dynamic loader not to update the GOT entry at all. Each call to an external function will then go through the dynamic loader and be resolved anew. The dynamic loader is configurable by other flags as well. I encourage you to go over man ld.so - it contains some interesting information. The costs of PIC # This article started by stating the problems of load-time relocation and how the PIC approach fixes them. But PIC is also not without problems. One immediately apparent cost is the extra indirection required for all external references to data and code in PIC. That's an extra memory load for each reference to a global variable, and for each call to a function. How problematic this is in practice depends on the compiler, the CPU architecture and the particular application. Another, less apparent cost, is the increased register usage required to implement PIC. In order to avoid locating the GOT too frequently, it makes sense for the compiler to generate code that keeps its address in a register (usually ebx ). But that ties down a whole register just for the sake of GOT. While not a big problem for RISC architectures that tend to have a lot of general purposes registers, it presents a performance problem for architectures like x86, which has a small amount of registers. PIC means having one general purpose register less, which adds up indirect costs since now more memory references have to be made. Conclusion # This article explained what position independent code is, and how it helps create shared libraries with shareable read-only text sections. There are some tradeoffs when choosing between PIC and its alternative (load-time relocation), and the eventual outcome really depends on a lot of factors, like the CPU architecture on which the program is going to run. That said, PIC is becoming more and more popular. Some non-Intel architectures like SPARC64 force PIC-only code for shared libraries, and many others (for example, ARM) include IP-relative addressing modes to make PIC more efficient. Both are true for the successor of x86, the x64 architecture. I will discuss PIC on x64 in a future article. The focus of this article, however, has not been on performance considerations or architectural decisions. My aim was to explain, given that PIC is used, how it works . If the explanation wasn't clear enough - please let me know in the comments and I will try to provide more information. [ 1] Unless all applications load this library into the exact same virtual memory address. But this usually isn't done on Linux. [ 2] 0x444 (and all other addresses mentioned in this computation) is relative to the load address of the shared library, which is unknown until an executable actually loads it at runtime. Note how it doesn't matter in the code since it only juggles relative addresses. [ 3] The astute reader may wonder why .got is a separate section at all. Didn't I just show in the diagrams that it's located in the data section? In practice, it is. I don't want to get into the distinction between ELF sections and segments here, since that would take use too far away from the point. But briefly, any number of \"data\" sections can be defined for a library and mapped into a read-write segment. This doesn't really matter, as long as the ELF file is organized correctly. Separating the data segment into different logical sections provides modularity and makes the linker's job easier. [ 4] Note that gdb skipped the part where ecx is assigned. That's because it's kind-of considered to be part of the function's prolog (the real reason is in the way gcc structures its debug info, of course). Several references to global data and functions are made inside a function, and a register pointing to GOT can serve all of them. [ 5] Shared library ELF objects actually come with special hash table sections for this purpose. [ 6] The dynamic loader on Linux is just another shared library which gets loaded into the address space of all running processes. [ 7] I placed func in a separate code section, although in theory this could be the same one where the call to func is made (i.e. in the same shared library). The \"extra credit\" section of this article has information about why a call to an external function in the same shared library needs PIC (or relocation) as well. [ 8] Recall that in the data reference example I promised to explain why there are apparently two GOT sections in the object: .got and .got.plt . Now it should become obvious that this is just to conveniently split the GOT entries required for global data from GOT entries required for the PLT. This is also why when the GOT offset is computed in functions, it points to .got.plt , which comes right after .got . This way, negative offsets lead us to .got , while positive offsets lead us to .got.plt . While convenient, such an arrangement is by no means compulsory. Both parts could be placed into a single .got section.","title":"Position Independent Code(PIC) in shared libraries"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#position-independent-code-pic-in-shared-libraries","text":"I've described the need for special handling of shared libraries while loading them into the process's address space in a previous article . Briefly, when the linker creates a shared library, it doesn't know in advance where it might be loaded. This creates a problem for the data and code references within the library, which should be somehow made to point to the correct memory locations. There are two main approaches to solve this problem in Linux ELF shared libraries: Load-time relocation Position independent code (PIC) Load-time relocation was already covered . Here, I want to explain the second approach - PIC. I originally planned to focus on both x86 and x64 (a.k.a. x86-64) in this article, but as it grew longer and longer I decided it won't be practical. So, it will explain only how PIC works on x86, picking this older architecture specifically because (unlike x64) it wasn't designed with PIC in mind, so implementing PIC on it is a bit trickier. A future (hopefully much shorter) article will build upon the foundation of this one to explain how PIC is implemented on x64.","title":"Position Independent Code (PIC) in shared libraries"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#some-problems-of-load-time-relocation","text":"As we've seen in the previous article, load-time relocation is a fairly straightforward method, and it works. PIC, however, is much more popular nowadays, and is usually the recommended method of building shared libraries. Why is this so? Load-time relocation has a couple of problems: it takes time to perform, and it makes the text section of the library non-shareable. First, the performance problem. If a shared library was linked with load-time relocation entries, it will take some time to actually perform these relocations when the application is loaded. You may think that the cost shouldn't be too large - after all, the loader doesn't have to scan through the whole text section - it should only look at the relocation entries. But if a complex piece of software loads multiple large shared libraries at start-up, and each shared library must first have its load-time relocations applied, these costs can build up and result in a noticeable delay in the start-up time of the application. Second, the non-shareable text section problem, which is somewhat more serious. One of the main points of having shared libraries in the first place, is saving RAM. Some common shared libraries are used by multiple applications. If the text section (where the code is) of the shared library can only be loaded into memory once (and then mapped into the virtual memories of many processes), considerable amounts of RAM can be saved. But this is not possible with load-time relocation, since when using this technique the text section has to be modified at load-time to apply the relocations. Therefore, for each application that loaded this shared library, it will have to be wholly placed in RAM again [ 1] . Different applications won't be able to really share it. Moreover, having a writable text section (it must be kept writable, to allow the dynamic loader to perform the relocations) poses a security risk, making it easier to exploit the application. As we'll see in this article, PIC mostly mitigates these problems.","title":"Some problems of load-time relocation"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#pic-introduction","text":"The idea behind PIC is simple - add an additional level of indirection to all global data and function references in the code. By cleverly utilizing some artifacts of the linking and loading processes, it's possible to make the text section of the shared library truly position independent , in the sense that it can be easily mapped into different memory addresses without needing to change one bit. In the next few sections I will explain in detail how this feat is achieved.","title":"PIC - introduction"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#key-insight-1-offset-between-text-and-data-sections","text":"One of the key insights on which PIC relies is the offset between the text and data sections, known to the linker at link-time . When the linker combines several object files together, it collects their sections (for example, all text sections get unified into a single large text section). Therefore, the linker knows both about the sizes of the sections and about their relative locations. For example, the text section may be immediately followed by the data section, so the offset from any given instruction in the text section to the beginning of the data section is just the size of the text section minus the offset of the instruction from the beginning of the text section - and both these quantities are known to the linker. In the diagram above, the code section was loaded into some address (unknown at link-time) 0xXXXX0000 (the X-es literally mean \"don't care\"), and the data section right after it at offset 0xXXXXF000. Then, if some instruction at offset 0x80 in the code section wants to reference stuff in the data section, the linker knows the relative offset (0xEF80 in this case) and can encode it in the instruction. Note that it wouldn't matter if another section was placed between the code and data sections, or if the data section preceded the code section. Since the linker knows the sizes of all sections and decides where to place them, the insight holds.","title":"Key insight #1 - offset between text and data sections"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#key-insight-2-making-an-ip-relative-offset-work-on-x86","text":"The above is only useful if we can actually put the relative offset to work. But data references (i.e. in the mov instruction) on x86 require absolute addresses. So, what can we do? If we have a relative address and need an absolute address, what's missing is the value of the instruction pointer (since, by definition, the relative address is relative to the instruction's location). There's no instruction to obtain the value of the instruction pointer on x86, but we can use a simple trick to get it. Here's some assembly pseudo-code that demonstrates it: call TMPLABEL TMPLABEL: pop ebx What happens here is: The CPU executes call TMPLABEL , which causes it to save the address of the next instruction (the pop ebx ) on stack and jump to the label. Since the instruction at the label is pop ebx , it gets executed next. It pops a value from the stack into ebx . But this value is the address of the instruction itself, so ebx now effectively contains the value of the instruction pointer.","title":"Key insight #2 - making an IP-relative offset work on x86"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-global-offset-table-got","text":"With this at hand, we can finally get to the implementation of position-independent data addressing on x86. It is accomplished by means of a \"global offset table\", or in short GOT. A GOT is simply a table of addresses, residing in the data section. Suppose some instruction in the code section wants to refer to a variable. Instead of referring to it directly by absolute address (which would require a relocation), it refers to an entry in the GOT. Since the GOT is in a known place in the data section, this reference is relative and known to the linker. The GOT entry, in turn, will contain the absolute address of the variable: In pseudo-assembly, we replace an absolute addressing instruction: ; Place the value of the variable in edx mov edx, [ADDR_OF_VAR] With displacement addressing from a register, along with an extra indirection: ; 1. Somehow get the address of the GOT into ebx lea ebx, ADDR_OF_GOT ; 2. Suppose ADDR_OF_VAR is stored at offset 0x10 ; in the GOT. Then this will place ADDR_OF_VAR ; into edx. mov edx, DWORD PTR [ebx + 0x10] ; 3. Finally, access the variable and place its ; value into edx. mov edx, DWORD PTR [edx] So, we've gotten rid of a relocation in the code section by redirecting variable references through the GOT. But we've also created a relocation in the data section. Why? Because the GOT still has to contain the absolute address of the variable for the scheme described above to work. So what have we gained? A lot, it turns out. A relocation in the data section is much less problematic than one in the code section, for two reasons (which directly address the two main problems of load-time relocation of code described in the beginning of the article): Relocations in the code section are required per variable reference , while in the GOT we only need to relocate once per variable . There are likely much more references to variables than variables, so this is more efficient. The data section is writable and not shared between processes anyway, so adding relocations to it does no harm. Moving relocations from the code section, however, allows to make it read-only and share it between processes.","title":"The Global Offset Table (GOT)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#pic-with-data-references-through-got-an-example","text":"I will now show a complete example that demonstrates the mechanics of PIC: int myglob = 42; int ml_func(int a, int b) { return myglob + a + b; } This chunk of code will be compiled into a shared library (using the -fpic and -shared flags as appropriate) named libmlpic_dataonly.so . Let's take a look at its disassembly, focusing on the ml_func function: 0000043c <ml_func>: 43c: 55 push ebp 43d: 89 e5 mov ebp,esp 43f: e8 16 00 00 00 call 45a <__i686.get_pc_thunk.cx> 444: 81 c1 b0 1b 00 00 add ecx,0x1bb0 44a: 8b 81 f0 ff ff ff mov eax,DWORD PTR [ecx-0x10] 450: 8b 00 mov eax,DWORD PTR [eax] 452: 03 45 08 add eax,DWORD PTR [ebp+0x8] 455: 03 45 0c add eax,DWORD PTR [ebp+0xc] 458: 5d pop ebp 459: c3 ret 0000045a <__i686.get_pc_thunk.cx>: 45a: 8b 0c 24 mov ecx,DWORD PTR [esp] 45d: c3 ret I'm going to refer to instructions by their addresses (the left-most number in the disassembly). This address is the offset from the load address of the shared library. At 43f , the address of the next instruction is placed into ecx , by means of the technique described in the \"key insight #2\" section above. At 444 , a known constant offset from the instruction to the place where the GOT is located is added to ecx . So ecx now serves as a base pointer to GOT. At 44a , a value is taken from [ecx - 0x10] , which is a GOT entry, and placed into eax . This is the address of myglob . At 450 the indirection is done, and the value of myglob is placed into eax . Later the parameters a and b are added to myglob and the value is returned (by keeping it in eax ). We can also query the shared library with readelf -S to see where the GOT section was placed: Section Headers: [Nr] Name Type Addr Off Size ES Flg Lk Inf Al <snip> [19] .got PROGBITS 00001fe4 000fe4 000010 04 WA 0 0 4 [20] .got.plt PROGBITS 00001ff4 000ff4 000014 04 WA 0 0 4 <snip> Let's do some math to check the computation done by the compiler to find myglob . As I mentioned above, the call to __i686.get_pc_thunk.cx places the address of the next instruction into ecx . That address is 0x444 [ 2] . The next instruction then adds 0x1bb0 to it, and the result in ecx is going to be 0x1ff4 . Finally, to actually obtain the GOT entry holding the address of myglob , displacement addressing is used - [ecx - 0x10] , so the entry is at 0x1fe4 , which is the first entry in the GOT according to the section header. Why there's another section whose name starts with .got will be explained later in the article [ 3] . Note that the compiler chooses to point ecx to after the GOT and then use negative offsets to obtain entries. This is fine, as long as the math works out. And so far it does. There's something we're still missing, however. How does the address of myglob actually get into the GOT slot at 0x1fe4 ? Recall that I mentioned a relocation, so let's find it: > readelf -r libmlpic_dataonly.so Relocation section '.rel.dyn' at offset 0x2dc contains 5 entries: Offset Info Type Sym.Value Sym. Name 00002008 00000008 R_386_RELATIVE 00001fe4 00000406 R_386_GLOB_DAT 0000200c myglob <snip> Note the relocation section for myglob , pointing to address 0x1fe4 , as expected. The relocation is of type R_386_GLOB_DAT , which simply tells the dynamic loader - \"put the actual value of the symbol (i.e. its address) into that offset\". So everything works out nicely. All that's left is to check how it actually looks when the library is loaded. We can do this by writing a simple \"driver\" executable that links to libmlpic_dataonly.so and calls ml_func , and then running it through GDB. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func [...] (gdb) run Starting program: [...]pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_reloc_dataonly.c:5 5 return myglob + a + b; (gdb) set disassembly-flavor intel (gdb) disas ml_func Dump of assembler code for function ml_func: 0x0013143c <+0>: push ebp 0x0013143d <+1>: mov ebp,esp 0x0013143f <+3>: call 0x13145a <__i686.get_pc_thunk.cx> 0x00131444 <+8>: add ecx,0x1bb0 => 0x0013144a <+14>: mov eax,DWORD PTR [ecx-0x10] 0x00131450 <+20>: mov eax,DWORD PTR [eax] 0x00131452 <+22>: add eax,DWORD PTR [ebp+0x8] 0x00131455 <+25>: add eax,DWORD PTR [ebp+0xc] 0x00131458 <+28>: pop ebp 0x00131459 <+29>: ret End of assembler dump. (gdb) i registers eax 0x1 1 ecx 0x132ff4 1257460 [...] skipping output The debugger has entered ml_func , and stopped at IP 0x0013144a [ 4] . We see that ecx holds the value 0x132ff4 (which is the address of the instruction plus 0x1bb0 , as explained before). Note that at this point, at runtime, these are absolute addresses - the shared library has already been loaded into the address space of the process. So, the GOT entry for myglob is at [ecx - 0x10] . Let's check what's there: (gdb) x 0x132fe4 0x132fe4: 0x0013300c So, we'd expect 0x0013300c to be the address of myglob . Let's verify: (gdb) p &myglob $1 = (int *) 0x13300c Indeed, it is!","title":"PIC with data references through GOT - an example"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#function-calls-in-pic","text":"Alright, so this is how data addressing works in position independent code. But what about function calls? Theoretically, the exact same approach could work for function calls as well. Instead of call actually containing the address of the function to call, let it contain the address of a known GOT entry, and fill in that entry during loading. But this is not how function calls work in PIC. What actually happens is a bit more complicated. Before I explain how it's done, a few words about the motivation for such a mechanism.","title":"Function calls in PIC"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-lazy-binding-optimization","text":"When a shared library refers to some function, the real address of that function is not known until load time. Resolving this address is called binding , and it's something the dynamic loader does when it loads the shared library into the process's memory space. This binding process is non-trivial, since the loader has to actually look up the function symbol in special tables [ 5] . So, resolving each function takes time. Not a lot of time, but it adds up since the amount of functions in libraries is typically much larger than the amount of global variables. Moreover, most of these resolutions are done in vain, because in a typical run of a program only a fraction of functions actually get called (think about various functions handling error and special conditions, which typically don't get called at all). So, to speed up this process, a clever lazy binding scheme was devised. \"Lazy\" is a generic name for a family of optimizations in computer programming, where work is delayed until the last moment when it's actually needed, with the intention of avoiding doing this work if its results are never required during a specific run of a program. Good examples of laziness are copy-on-write and lazy evaluation . This lazy binding scheme is attained by adding yet another level of indirection - the PLT.","title":"The lazy binding optimization"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-procedure-linkage-table-plt","text":"The PLT is part of the executable text section, consisting of a set of entries (one for each external function the shared library calls). Each PLT entry is a short chunk of executable code. Instead of calling the function directly, the code calls an entry in the PLT, which then takes care to call the actual function. This arrangement is sometimes called a \" trampoline \". Each PLT entry also has a corresponding entry in the GOT which contains the actual offset to the function, but only when the dynamic loader resolves it. I know this is confusing, but hopefully it will be come clearer once I explain the details in the next few paragraphs and diagrams. As the previous section mentioned, PLTs allow lazy resolution of functions. When the shared library is first loaded, the function calls have not been resolved yet: Explanation: In the code, a function func is called. The compiler translates it to a call to func@plt , which is some N-th entry in the PLT. The PLT consists of a special first entry, followed by a bunch of identically structured entries, one for each function needing resolution. Each PLT entry but the first consists of these parts: A jump to a location which is specified in a corresponding GOT entry Preparation of arguments for a \"resolver\" routine Call to the resolver routine, which resides in the first entry of the PLT The first PLT entry is a call to a resolver routine, which is located in the dynamic loader itself [ 6] . This routine resolves the actual address of the function. More on its action a bit later. Before the function's actual address has been resolved, the Nth GOT entry just points to after the jump. This is why this arrow in the diagram is colored differently - it's not an actual jump, just a pointer. What happens when func is called for the first time is this: PLT[n] is called and jumps to the address pointed to in GOT[n] . This address points into PLT[n] itself, to the preparation of arguments for the resolver. The resolver is then called. The resolver performs resolution of the actual address of func , places its actual address into GOT[n] and calls func . After the first call, the diagram looks a bit differently: Note that GOT[n] now points to the actual func [ 7] instead of back into the PLT. So, when func is called again: PLT[n] is called and jumps to the address pointed to in GOT[n] . GOT[n] points to func , so this just transfers control to func . In other words, now func is being actually called, without going through the resolver, at the cost of one additional jump. That's all there is to it, really. This mechanism allows lazy resolution of functions, and no resolution at all for functions that aren't actually called. It also leaves the code/text section of the library completely position independent, since the only place where an absolute address is used is the GOT, which resides in the data section and will be relocated by the dynamic loader. Even the PLT itself is PIC, so it can live in the read-only text section. I didn't get into much details regarding the resolver, but it's really not important for our purpose here. The resolver is simply a chunk of low-level code in the loader that does symbol resolution. The arguments prepared for it in each PLT entry, along with a suitable relocation entry, help it know about the symbol that needs resolution and about the GOT entry to update.","title":"The Procedure Linkage Table (PLT)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#pic-with-function-calls-through-plt-and-got-an-example","text":"Once again, to fortify the hard-learned theory with a practical demonstration, here's a complete example showing function call resolution using the mechanism described above. I'll be moving forward a bit faster this time. Here's the code for the shared library: int myglob = 42; int ml_util_func(int a) { return a + 1; } int ml_func(int a, int b) { int c = b + ml_util_func(a); myglob += c; return b + myglob; } This code will be compiled into libmlpic.so , and the focus is going to be on the call to ml_util_func from ml_func . Let's first disassemble ml_func : 00000477 <ml_func>: 477: 55 push ebp 478: 89 e5 mov ebp,esp 47a: 53 push ebx 47b: 83 ec 24 sub esp,0x24 47e: e8 e4 ff ff ff call 467 <__i686.get_pc_thunk.bx> 483: 81 c3 71 1b 00 00 add ebx,0x1b71 489: 8b 45 08 mov eax,DWORD PTR [ebp+0x8] 48c: 89 04 24 mov DWORD PTR [esp],eax 48f: e8 0c ff ff ff call 3a0 <ml_util_func@plt> <... snip more code> The interesting part is the call to ml_util_func@plt . Note also that the address of GOT is in ebx . Here's what ml_util_func@plt looks like (it's in an executable section called .plt ): 000003a0 <ml_util_func@plt>: 3a0: ff a3 14 00 00 00 jmp DWORD PTR [ebx+0x14] 3a6: 68 10 00 00 00 push 0x10 3ab: e9 c0 ff ff ff jmp 370 <_init+0x30> Recall that each PLT entry consists of three parts: A jump to an address specified in GOT (this is the jump to [ebx+0x14] ) Preparation of arguments for the resolver Call to the resolver The resolver (PLT entry 0) resides at address 0x370 , but it's of no interest to us here. What's more interesting is to see what the GOT contains. For that, we first have to do some math. The \"get IP\" trick in ml_func was done on address 0x483 , to which 0x1b71 is added. So the base of the GOT is at 0x1ff4 . We can take a peek at the GOT contents with readelf [ 8] : > readelf -x .got.plt libmlpic.so Hex dump of section '.got.plt': 0x00001ff4 241f0000 00000000 00000000 86030000 $............... 0x00002004 96030000 a6030000 ........ The GOT entry ml_util_func@plt looks at is at offset +0x14 , or 0x2008 . From above, the word at that location is 0x3a6 , which is the address of the push instruction in ml_util_func@plt . To help the dynamic loader do its job, a relocation entry is also added and specifies which place in the GOT to relocate for ml_util_func : > readelf -r libmlpic.so [...] snip output Relocation section '.rel.plt' at offset 0x328 contains 3 entries: Offset Info Type Sym.Value Sym. Name 00002000 00000107 R_386_JUMP_SLOT 00000000 __cxa_finalize 00002004 00000207 R_386_JUMP_SLOT 00000000 __gmon_start__ 00002008 00000707 R_386_JUMP_SLOT 0000046c ml_util_func The last line means that the dynamic loader should place the value (address) of symbol ml_util_func into 0x2008 (which, recall, is the GOT entry for this function). It would be interesting to see this GOT entry modification actually happen after the first call. Let's once again use GDB for the inspection. > gdb driver [...] skipping output (gdb) set environment LD_LIBRARY_PATH=. (gdb) break ml_func Breakpoint 1 at 0x80483c0 (gdb) run Starting program: /pic_tests/driver Breakpoint 1, ml_func (a=1, b=1) at ml_main.c:10 10 int c = b + ml_util_func(a); (gdb) We're now before the first call to ml_util_func . Recall that GOT is pointed to by ebx in this code. Let's see what's in it: (gdb) i registers ebx ebx 0x132ff4 And the offset to the entry we need is at [ebx+0x14] : (gdb) x/w 0x133008 0x133008: 0x001313a6 Yep, the 0x3a6 ending, looks right. Now, let's step until after the call to ml_util_func and check again: (gdb) step ml_util_func (a=1) at ml_main.c:5 5 return a + 1; (gdb) x/w 0x133008 0x133008: 0x0013146c The value at 0x133008 was changed. Hence, 0x0013146c should be the real address of ml_util_func , placed in there by the dynamic loader: (gdb) p &ml_util_func $1 = (int (*)(int)) 0x13146c <ml_util_func> Just as expected.","title":"PIC with function calls through PLT and GOT - an example"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#controlling-if-and-when-the-resolution-is-done-by-the-loader","text":"This would be a good place to mention that the process of lazy symbol resolution performed by the dynamic loader can be configured with some environment variables (and corresponding flags to ld when linking the shared library). This is sometimes useful for special performance requirements or debugging. The LD_BIND_NOW env var, when defined, tells the dynamic loader to always perform the resolution for all symbols at start-up time, and not lazily. You can easily verify this in action by setting this env var and re-running the previous sample with GDB. You'll see that the GOT entry for ml_util_func contains its real address even before the first call to the function. Conversely, the LD_BIND_NOT env var tells the dynamic loader not to update the GOT entry at all. Each call to an external function will then go through the dynamic loader and be resolved anew. The dynamic loader is configurable by other flags as well. I encourage you to go over man ld.so - it contains some interesting information.","title":"Controlling if and when the resolution is done by the loader"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#the-costs-of-pic","text":"This article started by stating the problems of load-time relocation and how the PIC approach fixes them. But PIC is also not without problems. One immediately apparent cost is the extra indirection required for all external references to data and code in PIC. That's an extra memory load for each reference to a global variable, and for each call to a function. How problematic this is in practice depends on the compiler, the CPU architecture and the particular application. Another, less apparent cost, is the increased register usage required to implement PIC. In order to avoid locating the GOT too frequently, it makes sense for the compiler to generate code that keeps its address in a register (usually ebx ). But that ties down a whole register just for the sake of GOT. While not a big problem for RISC architectures that tend to have a lot of general purposes registers, it presents a performance problem for architectures like x86, which has a small amount of registers. PIC means having one general purpose register less, which adds up indirect costs since now more memory references have to be made.","title":"The costs of PIC"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-Independent-Code(PIC)-in-shared-libraries/#conclusion","text":"This article explained what position independent code is, and how it helps create shared libraries with shareable read-only text sections. There are some tradeoffs when choosing between PIC and its alternative (load-time relocation), and the eventual outcome really depends on a lot of factors, like the CPU architecture on which the program is going to run. That said, PIC is becoming more and more popular. Some non-Intel architectures like SPARC64 force PIC-only code for shared libraries, and many others (for example, ARM) include IP-relative addressing modes to make PIC more efficient. Both are true for the successor of x86, the x64 architecture. I will discuss PIC on x64 in a future article. The focus of this article, however, has not been on performance considerations or architectural decisions. My aim was to explain, given that PIC is used, how it works . If the explanation wasn't clear enough - please let me know in the comments and I will try to provide more information. [ 1] Unless all applications load this library into the exact same virtual memory address. But this usually isn't done on Linux. [ 2] 0x444 (and all other addresses mentioned in this computation) is relative to the load address of the shared library, which is unknown until an executable actually loads it at runtime. Note how it doesn't matter in the code since it only juggles relative addresses. [ 3] The astute reader may wonder why .got is a separate section at all. Didn't I just show in the diagrams that it's located in the data section? In practice, it is. I don't want to get into the distinction between ELF sections and segments here, since that would take use too far away from the point. But briefly, any number of \"data\" sections can be defined for a library and mapped into a read-write segment. This doesn't really matter, as long as the ELF file is organized correctly. Separating the data segment into different logical sections provides modularity and makes the linker's job easier. [ 4] Note that gdb skipped the part where ecx is assigned. That's because it's kind-of considered to be part of the function's prolog (the real reason is in the way gcc structures its debug info, of course). Several references to global data and functions are made inside a function, and a register pointing to GOT can serve all of them. [ 5] Shared library ELF objects actually come with special hash table sections for this purpose. [ 6] The dynamic loader on Linux is just another shared library which gets loaded into the address space of all running processes. [ 7] I placed func in a separate code section, although in theory this could be the same one where the call to func is made (i.e. in the same shared library). The \"extra credit\" section of this article has information about why a call to an external function in the same shared library needs PIC (or relocation) as well. [ 8] Recall that in the data reference example I promised to explain why there are apparently two GOT sections in the object: .got and .got.plt . Now it should become obvious that this is just to conveniently split the GOT entries required for global data from GOT entries required for the PLT. This is also why when the GOT offset is computed in functions, it points to .got.plt , which comes right after .got . This way, negative offsets lead us to .got , while positive offsets lead us to .got.plt . While convenient, such an arrangement is by no means compulsory. Both parts could be placed into a single .got section.","title":"Conclusion"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-independent-code/","text":"Position-independent code # In computing , position-independent code [ 1] ( PIC [ 1] ) or position-independent executable ( PIE ) is a body of machine code that, being placed somewhere in the primary memory , executes properly regardless of its absolute address (\u65e0\u8bba\u5176\u7edd\u5bf9\u5730\u5740\u5982\u4f55\uff0c\u5b83\u90fd\u88ab\u653e\u7f6e\u5728\u4e3b\u5b58\u50a8\u5668\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e). PIC is commonly used for shared libraries , so that the same library code can be loaded in a location in each program address space where it will not overlap any other uses of memory (for example, other shared libraries). PIC was also used on older computer systems lacking an MMU ,[ 2] so that the operating system could keep applications away from each other even within the single address space of an MMU-less system. Position-independent code can be executed at any memory address without modification. This differs from absolute code ,[ 1] which must be loaded at a specific location to function correctly,[ 1] and load-time locatable (LTL) code,[ 1] in which a linker or program loader modifies a program before execution so it can be run only from a particular memory location.[ 1] Generating position-independent code is often the default behavior for compilers , but they may place restrictions on the use of some language features, such as disallowing use of absolute addresses (position-independent code has to use relative addressing ). Instructions that refer directly to specific memory addresses sometimes execute faster, and replacing them with equivalent relative-addressing instructions may result in slightly slower execution, although modern processors make the difference practically negligible.[ 3]","title":"[Position-independent code](https://en.wikipedia.org/wiki/Position-independent_code)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Position-independent-code/Position-independent-code/#position-independent-code","text":"In computing , position-independent code [ 1] ( PIC [ 1] ) or position-independent executable ( PIE ) is a body of machine code that, being placed somewhere in the primary memory , executes properly regardless of its absolute address (\u65e0\u8bba\u5176\u7edd\u5bf9\u5730\u5740\u5982\u4f55\uff0c\u5b83\u90fd\u88ab\u653e\u7f6e\u5728\u4e3b\u5b58\u50a8\u5668\u4e2d\u7684\u67d0\u4e2a\u4f4d\u7f6e). PIC is commonly used for shared libraries , so that the same library code can be loaded in a location in each program address space where it will not overlap any other uses of memory (for example, other shared libraries). PIC was also used on older computer systems lacking an MMU ,[ 2] so that the operating system could keep applications away from each other even within the single address space of an MMU-less system. Position-independent code can be executed at any memory address without modification. This differs from absolute code ,[ 1] which must be loaded at a specific location to function correctly,[ 1] and load-time locatable (LTL) code,[ 1] in which a linker or program loader modifies a program before execution so it can be run only from a particular memory location.[ 1] Generating position-independent code is often the default behavior for compilers , but they may place restrictions on the use of some language features, such as disallowing use of absolute addresses (position-independent code has to use relative addressing ). Instructions that refer directly to specific memory addresses sometimes execute faster, and replacing them with equivalent relative-addressing instructions may result in slightly slower execution, although modern processors make the difference practically negligible.[ 3]","title":"Position-independent code"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/","text":"Relocation (computing) Segmentation Relocation table Unix-like systems Relocation procedure Example G53OPS - Operating Systems Relocation and Protection What does 'relocation' mean? Relocation (computing) # Relocation is the process of assigning load addresses for position-dependent code and data of a program and adjusting the code and data to reflect the assigned addresses .[ 1] [ 2] Prior to the advent of multiprocess systems, and still in many embedded systems the addresses for objects were absolute starting at a known location, often zero. Since multiprocessing systems dynamically link and switch between programs it became necessary to be able to relocate objects using position-independent code . A linker usually performs relocation in conjunction with symbol resolution , the process of searching files and libraries to replace symbolic references or names of libraries with actual usable addresses in memory before running a program. Relocation is typically done by the linker at link time , but it can also be done at load time by a relocating loader , or at run time by the running program itself. Some architectures avoid relocation entirely by deferring address assignment to run time; this is known as zero address arithmetic .[ which? ] THINKING : \u7f16\u8bd1\u751f\u6210\u7684executable\uff0c\u5b83\u4eec\u4e5f\u662f\u6709\u5730\u5740\u7a7a\u95f4\u7684 Segmentation # Object files are segmented into various memory segment types. Example segments include code segment(.text) , initialized data segment(.data) , uninitialized data segment(.bss ), or others.[ clarification needed ] Relocation table # The relocation table is a list of pointers created by the translator (a compiler or assembler ) and stored in the object or executable file. Each entry in the table, or \"fixup\", is a pointer to an absolute address in the object code that must be changed when the loader relocates the program so that it will refer to the correct location. Fixups are designed to support relocation of the program as a complete unit. In some cases, each fixup in the table is itself relative to a base address of zero, so the fixups themselves must be changed as the loader moves through the table.[ 3] In some architectures a fixup that crosses certain boundaries (such as a segment boundary) or that is not aligned on a word boundary is illegal and flagged as an error by the linker.[ 4] SUMMARY : \u663e\u7136\uff0c\u6bcf\u4e2aexecutable\u90fd\u5305\u542b\u4e00\u4e2arelocation table\u3002 Unix-like systems # The Executable and Linkable Format (ELF) executable format and shared library format used by most Unix-like systems allows several types of relocation to be defined.[ 5] Relocation procedure # The linker reads segment information and relocation tables in the object files and performs relocation by: merging all segments of common type into a single segment of that type assigning unique run time addresses to each section and each symbol, giving all code (functions) and data (global variables) unique run time addresses referring to the relocation table to modify[ why? ] symbols so that they point to the correct[ clarification needed ] run time addresses. Example # The following example uses Donald Knuth 's MIX architecture and MIXAL assembly language. The principles are the same for any architecture, though the details will change. (A) Program SUBR is compiled to produce object file (B), shown as both machine code and assembler. The compiler may start the compiled code at an arbitrary location, often location zero as shown. Location 13 contains the machine code for the jump instruction to statement ST in location 5. (C) If SUBR is later linked with other code it may be stored at a location other than zero. In this example the linker places it at location 120. The address in the jump instruction, which is now at location 133, must be relocated to point to the new location of the code for statement ST , now 125. [1 61 shown in the instruction is the MIX machine code representation of 125]. (D) When the program is loaded into memory to run it may be loaded at some location other than the one assigned by the linker. This example shows SUBR now at location 300. The address in the jump instruction, now at 313, needs to be relocated again so that it points to the updated location of ST , 305. [4 49 is the MIX machine representation of 305]. G53OPS - Operating Systems # Relocation and Protection # As soon as we introduce multiprogramming we have two problems that we need to address. Relocation : When a program is run it does not know in advance what location it will be loaded at. Therefore, the program cannot simply generate static addresses (e.g. from jump instructions). Instead, they must be made relative to where the program has been loaded. SUMMARY : \u5728\u7f16\u8bd1\u9636\u6bb5\uff0c\u7f16\u8bd1\u5668\u662f\u65e0\u6cd5\u5f97\u77e5\u5176\u751f\u6210\u7684executable\u5728\u8fd0\u884c\u65f6\u7684location\u7684\uff0c\u56e0\u6b64\u7f16\u8bd1\u5668\u751f\u6210\u7684executable\u4e0d\u80fd\u591f\u4f7f\u7528static address\uff0c\u5b83\u53ea\u80fd\u591f\u4f7f\u7528relative address\uff1b\u7f16\u8bd1\u5668\u5b9e\u73b0relative address\u7684\u65b9\u5f0f\u662f\u4f7f\u7528symbol\u3002\u8fd9\u662f\u4f7f\u7528relocation\u7684\u539f\u56e0\u3002 Protection : Once you can have two programs in memory at the same time there is a danger that one program can write to the address space of another program. This is obviously dangerous and should be avoided. In order to cater(\u8fce\u5408) for relocation we could make the loader modify all the relevant addresses as the binary file is loaded. The OS/360 worked in this way but the scheme suffers from the following problems \u00b7 The program cannot be moved, after it has been loaded without going through the same process. \u00b7 Using this scheme does not help the protection problem as the program can still generate illegal addresses (maybe by using absolute addressing). \u00b7 The program needs to have some sort of map that tells the loader which addresses need to be modified. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4ecb\u7ecd\u7684\u5185\u5bb9\u6307\u51fa\u4e86\u662f\u7531 loader \u6765\u6267\u884crelocation\uff0c\u4ee5\u53ca\u6267\u884c\u7684\u65f6\u673a\u3002\u5b83\u8fd8\u6d89\u53ca\u5230\u4e86relocation\u7684\u4e00\u4e9b\u5b9e\u73b0\u65b9\u6cd5\uff0c\u5982\u5f15\u5165relocation table\u3002 A solution, which solves both the relocation and protection problem is to equip(\u914d\u5907) the machine with two registers called the base and limit registers. The base register stores the start address of the partition and the limit register holds the length of the partition. Any address that is generated by the program has the base register added to it. In addition, all addresses are checked to ensure they are within the range of the partition. An additional benefit of this scheme is that if a program is moved within memory, only its base register needs to be amended. This is obviously a lot quicker than having to modify every address reference within the program. The IBM PC uses a scheme similar to this, although it does not have a limit register. What does 'relocation' mean? #","title":"Relocation(computing)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#relocation-computing","text":"Relocation is the process of assigning load addresses for position-dependent code and data of a program and adjusting the code and data to reflect the assigned addresses .[ 1] [ 2] Prior to the advent of multiprocess systems, and still in many embedded systems the addresses for objects were absolute starting at a known location, often zero. Since multiprocessing systems dynamically link and switch between programs it became necessary to be able to relocate objects using position-independent code . A linker usually performs relocation in conjunction with symbol resolution , the process of searching files and libraries to replace symbolic references or names of libraries with actual usable addresses in memory before running a program. Relocation is typically done by the linker at link time , but it can also be done at load time by a relocating loader , or at run time by the running program itself. Some architectures avoid relocation entirely by deferring address assignment to run time; this is known as zero address arithmetic .[ which? ] THINKING : \u7f16\u8bd1\u751f\u6210\u7684executable\uff0c\u5b83\u4eec\u4e5f\u662f\u6709\u5730\u5740\u7a7a\u95f4\u7684","title":"Relocation (computing)"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#segmentation","text":"Object files are segmented into various memory segment types. Example segments include code segment(.text) , initialized data segment(.data) , uninitialized data segment(.bss ), or others.[ clarification needed ]","title":"Segmentation"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#relocation-table","text":"The relocation table is a list of pointers created by the translator (a compiler or assembler ) and stored in the object or executable file. Each entry in the table, or \"fixup\", is a pointer to an absolute address in the object code that must be changed when the loader relocates the program so that it will refer to the correct location. Fixups are designed to support relocation of the program as a complete unit. In some cases, each fixup in the table is itself relative to a base address of zero, so the fixups themselves must be changed as the loader moves through the table.[ 3] In some architectures a fixup that crosses certain boundaries (such as a segment boundary) or that is not aligned on a word boundary is illegal and flagged as an error by the linker.[ 4] SUMMARY : \u663e\u7136\uff0c\u6bcf\u4e2aexecutable\u90fd\u5305\u542b\u4e00\u4e2arelocation table\u3002","title":"Relocation table"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#unix-like-systems","text":"The Executable and Linkable Format (ELF) executable format and shared library format used by most Unix-like systems allows several types of relocation to be defined.[ 5]","title":"Unix-like systems"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#relocation-procedure","text":"The linker reads segment information and relocation tables in the object files and performs relocation by: merging all segments of common type into a single segment of that type assigning unique run time addresses to each section and each symbol, giving all code (functions) and data (global variables) unique run time addresses referring to the relocation table to modify[ why? ] symbols so that they point to the correct[ clarification needed ] run time addresses.","title":"Relocation procedure"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#example","text":"The following example uses Donald Knuth 's MIX architecture and MIXAL assembly language. The principles are the same for any architecture, though the details will change. (A) Program SUBR is compiled to produce object file (B), shown as both machine code and assembler. The compiler may start the compiled code at an arbitrary location, often location zero as shown. Location 13 contains the machine code for the jump instruction to statement ST in location 5. (C) If SUBR is later linked with other code it may be stored at a location other than zero. In this example the linker places it at location 120. The address in the jump instruction, which is now at location 133, must be relocated to point to the new location of the code for statement ST , now 125. [1 61 shown in the instruction is the MIX machine code representation of 125]. (D) When the program is loaded into memory to run it may be loaded at some location other than the one assigned by the linker. This example shows SUBR now at location 300. The address in the jump instruction, now at 313, needs to be relocated again so that it points to the updated location of ST , 305. [4 49 is the MIX machine representation of 305].","title":"Example"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#g53ops-operating-systems","text":"","title":"G53OPS - Operating Systems"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#relocation-and-protection","text":"As soon as we introduce multiprogramming we have two problems that we need to address. Relocation : When a program is run it does not know in advance what location it will be loaded at. Therefore, the program cannot simply generate static addresses (e.g. from jump instructions). Instead, they must be made relative to where the program has been loaded. SUMMARY : \u5728\u7f16\u8bd1\u9636\u6bb5\uff0c\u7f16\u8bd1\u5668\u662f\u65e0\u6cd5\u5f97\u77e5\u5176\u751f\u6210\u7684executable\u5728\u8fd0\u884c\u65f6\u7684location\u7684\uff0c\u56e0\u6b64\u7f16\u8bd1\u5668\u751f\u6210\u7684executable\u4e0d\u80fd\u591f\u4f7f\u7528static address\uff0c\u5b83\u53ea\u80fd\u591f\u4f7f\u7528relative address\uff1b\u7f16\u8bd1\u5668\u5b9e\u73b0relative address\u7684\u65b9\u5f0f\u662f\u4f7f\u7528symbol\u3002\u8fd9\u662f\u4f7f\u7528relocation\u7684\u539f\u56e0\u3002 Protection : Once you can have two programs in memory at the same time there is a danger that one program can write to the address space of another program. This is obviously dangerous and should be avoided. In order to cater(\u8fce\u5408) for relocation we could make the loader modify all the relevant addresses as the binary file is loaded. The OS/360 worked in this way but the scheme suffers from the following problems \u00b7 The program cannot be moved, after it has been loaded without going through the same process. \u00b7 Using this scheme does not help the protection problem as the program can still generate illegal addresses (maybe by using absolute addressing). \u00b7 The program needs to have some sort of map that tells the loader which addresses need to be modified. SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u4ecb\u7ecd\u7684\u5185\u5bb9\u6307\u51fa\u4e86\u662f\u7531 loader \u6765\u6267\u884crelocation\uff0c\u4ee5\u53ca\u6267\u884c\u7684\u65f6\u673a\u3002\u5b83\u8fd8\u6d89\u53ca\u5230\u4e86relocation\u7684\u4e00\u4e9b\u5b9e\u73b0\u65b9\u6cd5\uff0c\u5982\u5f15\u5165relocation table\u3002 A solution, which solves both the relocation and protection problem is to equip(\u914d\u5907) the machine with two registers called the base and limit registers. The base register stores the start address of the partition and the limit register holds the length of the partition. Any address that is generated by the program has the base register added to it. In addition, all addresses are checked to ensure they are within the range of the partition. An additional benefit of this scheme is that if a program is moved within memory, only its base register needs to be amended. This is obviously a lot quicker than having to modify every address reference within the program. The IBM PC uses a scheme similar to this, although it does not have a limit register.","title":"Relocation and Protection"},{"location":"Chapter-7-Run-Time-Environments/ABI/Library/Relocation/Relocation(computing)/#what-does-relocation-mean","text":"","title":"What does 'relocation' mean?"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/","text":"Name mangling Examples C C++ Simple example[edit] Complex example[edit] How different compilers mangle the same functions[edit] Handling of C symbols when linking from C++ Standardised name mangling in C++ Real-world effects of C++ name mangling Demangle via c++filt Demangle via builtin GCC ABI Python Name mangling # In compiler construction, name mangling (also called name decoration ) is a technique used to solve various problems caused by the need to resolve unique names for programming entities in many modern programming languages . It provides a way of encoding additional information in the name of a function , structure , class or another datatype in order to pass more semantic information from the compilers to linkers . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u4f7f\u7528name mangling\u7684\u610f\u56fe The need arises where the language allows different entities to be named with the same identifier as long as they occupy a different namespace (where a namespace is typically defined by a module, class, or explicit namespace directive) or have different signatures (such as function overloading ). Any object code produced by compilers is usually linked with other pieces of object code (produced by the same or another compiler) by a type of program called a linker . The linker needs a great deal of information on each program entity. For example, to correctly link a function it needs its name, the number of arguments and their types, and so on. Examples # C # Although name mangling is not generally required or used by languages that do not support function overloading (such as C and classic Pascal), they use it in some cases to provide additional information about a function. For example, compilers targeted at Microsoft Windows platforms support a variety of calling conventions , which determine the manner in which parameters are sent to subroutines and results returned. Because the different calling conventions are not compatible with one another, compilers mangle symbols with codes detailing which convention should be used to call the specific routine. SUMMARY : encode calling convention in the name The mangling scheme was established by Microsoft, and has been informally followed by other compilers including Digital Mars, Borland, and GNU GCC, when compiling code for the Windows platforms. The scheme even applies to other languages, such as Pascal , D , Delphi , Fortran , and C# . This allows subroutines written in those languages to call, or be called by, existing Windows libraries using a calling convention different from their default. When compiling the following C examples: int _cdecl f (int x) { return 0; } int _stdcall g (int y) { return 0; } int _fastcall h (int z) { return 0; } 32 bit compilers emit, respectively: _f _g@4 @h@4 In the stdcall and fastcall mangling schemes, the function is encoded as _**name**@**X** and @**name**@**X** respectively, where X is the number of bytes, in decimal, of the argument(s) in the parameter list (including those passed in registers, for fastcall). In the case of cdecl , the function name is merely prefixed by an underscore. The 64-bit convention on Windows (Microsoft C) has no leading underscore. This difference may in some rare cases lead to unresolved externals when porting such code to 64 bits. For example, Fortran code can use 'alias' to link against a C method by name as follows: SUBROUTINE f() !DEC$ ATTRIBUTES C, ALIAS:'_f' :: f END SUBROUTINE This will compile and link fine under 32 bits, but generate an unresolved external '_f' under 64 bits. One workaround for this is not to use 'alias' at all (in which the method names typically need to be capitalized in C and Fortran). Another is to use the BIND option: SUBROUTINE f() BIND(C,NAME=\"f\") END SUBROUTINE C++ # C++ compilers are the most widespread users of name mangling . The first C++ compilers were implemented as translators to C source code, which would then be compiled by a C compiler to object code; because of this, symbol names had to conform to C identifier rules. Even later, with the emergence of compilers which produced machine code or assembly directly, the system's linker generally did not support C++ symbols, and mangling was still required. The C++ language does not define a standard decoration scheme, so each compiler uses its own. C++ also has complex language features, such as classes , templates , namespaces , and operator overloading , that alter the meaning of specific symbols based on context or usage. Meta-data about these features can be disambiguated(\u6d88\u9664) by mangling (decorating) the name of a symbol . Because the name-mangling systems for such features are not standardized across compilers, few linkers can link object code that was produced by different compilers. Simple example[ edit ] # A single C++ translation unit might define two functions named f() : int f (void) { return 1; } int f (int) { return 0; } void g (void) { int i = f(), j = f(0); } These are distinct functions, with no relation to each other apart from the name. The C++ compiler therefore will encode the type information in the symbol name, the result being something resembling: int __f_v (void) { return 1; } int __f_i (int) { return 0; } void __g_v (void) { int i = __f_v(), j = __f_i(0); } Even though its name is unique, g() is still mangled: name mangling applies to all symbols. Complex example[ edit ] # The mangled symbols in this example, in the comments below the respective identifier name, are those produced by the GNU GCC 3.x compilers: namespace wikipedia { class article { public: std::string format (void); /* = _ZN9wikipedia7article6formatEv */ bool print_to (std::ostream&); /* = _ZN9wikipedia7article8print_toERSo */ class wikilink { public: wikilink (std::string const& name); /* = _ZN9wikipedia7article8wikilinkC1ERKSs */ }; }; } All mangled symbols begin with _Z (note that an identifier beginning with an underscore followed by a capital is a reserved identifier in C, so conflict with user identifiers is avoided); for nested names (including both namespaces and classes), this is followed by N , then a series of pairs (the length being the length of the next identifier), and finally E . For example, wikipedia::article::format becomes _ZN9Wikipedia7article6formatE For functions, this is then followed by the type information; as format() is a void function, this is simply v ; hence: _ZN9Wikipedia7article6formatEv For print_to , the standard type std::ostream (which is a typedef for std::basic_ostream<char, std::char_traits<char> > ) is used, which has the special alias So ; a reference to this type is therefore RSo , with the complete name for the function being: _ZN9Wikipedia7article8print_toERSo How different compilers mangle the same functions[ edit ] # There isn't a standard scheme by which even trivial C++ identifiers are mangled, and consequently different compilers (or even different versions of the same compiler, or the same compiler on different platforms) mangle public symbols in radically different (and thus totally incompatible) ways. Consider how different C++ compilers mangle the same functions: Compiler void h(int) void h(int, char) void h(void) Intel C++ 8.0 for Linux _Z1hi _Z1hic _Z1hv HP aC++ A.05.55 IA-64 IAR EWARM C++ 5.4 ARM GCC 3. x and higher Clang 1. x and higher[ 1] IAR EWARM C++ 7.4 ARM _Z<number>hi _Z<number>hic _Z<number>hv GCC 2.9 x h__Fi h__Fic h__Fv HP aC++ A.03.45 PA-RISC Microsoft Visual C++ v6-v10 ( mangling details ) ?h@@YAXH@Z ?h@@YAXHD@Z ?h@@YAXXZ Digital Mars C++ Borland C++ v3.1 @h$qi @h$qizc @h$qv OpenVMS C++ V6.5 (ARM mode) H__XI H__XIC H__XV OpenVMS C++ V6.5 (ANSI mode) CXX$__7H__FIC26CDH77 CXX$__7H__FV2CB06E8 OpenVMS C++ X7.1 IA-64 CXX$_Z1HI2DSQ26A CXX$_Z1HIC2NP3LI4 CXX$_Z1HV0BCA19V SunPro CC __1cBh6Fi_v_ __1cBh6Fic_v_ __1cBh6F_v_ Tru64 C++ V6.5 (ARM mode) h__Xi h__Xic h__Xv Tru64 C++ V6.5 (ANSI mode) __7h__Fi __7h__Fic __7h__Fv Watcom C++ 10.6 W?h$n(i)v W?h$n(ia)v W?h$n()v Notes: The Compaq C++ compiler on OpenVMS VAX and Alpha (but not IA-64) and Tru64 has two name mangling schemes. The original, pre-standard scheme is known as ARM model, and is based on the name mangling described in the C++ Annotated Reference Manual (ARM). With the advent of new features in standard C++, particularly templates , the ARM scheme became more and more unsuitable \u2014 it could not encode certain function types, or produced identical mangled names for different functions. It was therefore replaced by the newer \"ANSI\" model, which supported all ANSI template features, but was not backwards compatible. On IA-64, a standard Application Binary Interface (ABI) exists (see external links ), which defines (among other things) a standard name-mangling scheme, and which is used by all the IA-64 compilers. GNU GCC 3. x , in addition, has adopted the name mangling scheme defined in this standard for use on other, non-Intel platforms. The Visual Studio and Windows SDK include the program undname which prints the C-style function prototype for a given mangled name. On Microsoft Windows, the Intel compiler[ 2] and Clang [ 3] uses the Visual C++ name mangling for compatibility. For the IAR EWARM C++ 7.4 ARM compiler the best way to determine the name of a function is to compile with the assembler output turned on and to look at the output in the \".s\" file thus generated. Handling of C symbols when linking from C++ # The job of the common C++ idiom: #ifdef __cplusplus extern \"C\" { #endif /* ... */ #ifdef __cplusplus } #endif is to ensure that the symbols within are \"unmangled\" \u2013 that the compiler emits(\u521b\u9020\uff0c\u751f\u4ea7\u51fa) a binary file with their names undecorated, as a C compiler would do. As C language definitions are unmangled, the C++ compiler needs to avoid mangling references to these identifiers. For example, the standard strings library, <string.h> usually contains something resembling: #ifdef __cplusplus extern \"C\" { #endif void *memset (void *, int, size_t); char *strcat (char *, const char *); int strcmp (const char *, const char *); char *strcpy (char *, const char *); #ifdef __cplusplus } #endif Thus, code such as: if (strcmp(argv[1], \"-x\") == 0) strcpy(a, argv[2]); else memset (a, 0, sizeof(a)); uses the correct, unmangled strcmp and memset . If the extern had not been used, the (SunPro) C++ compiler would produce code equivalent to: if (__1cGstrcmp6Fpkc1_i_(argv[1], \"-x\") == 0) __1cGstrcpy6Fpcpkc_0_(a, argv[2]); else __1cGmemset6FpviI_0_ (a, 0, sizeof(a)); Since those symbols do not exist in the C runtime library ( e.g. libc ), link errors would result. Standardised name mangling in C++ # Though it would seem that standardised name mangling in the C++ language would lead to greater interoperability between compiler implementations, such a standardization by itself would not suffice to guarantee C++ compiler interoperability and it might even create a false impression that interoperability is possible and safe when it isn't. Name mangling is only one of several application binary interface (ABI) details that need to be decided and observed by a C++ implementation. Other ABI aspects like exception handling , virtual table layout, structure and stack frame padding , etc. also cause differing C++ implementations to be incompatible. Further, requiring a particular form of mangling would cause issues for systems where implementation limits (e.g., length of symbols) dictate a particular mangling scheme. A standardised requirement for name mangling would also prevent an implementation where mangling was not required at all \u2014 for example, a linker which understood the C++ language. The C++ standard therefore does not attempt to standardise name mangling. On the contrary, the Annotated C++ Reference Manual (also known as ARM , ISBN 0-201-51459-1 , section 7.2.1c) actively encourages the use of different mangling schemes to prevent linking when other aspects of the ABI, such as exception handling and virtual table layout, are incompatible. Nevertheless, as detailed in the section above, on some platforms[ 4] the full C++ ABI has been standardized, including name mangling. Real-world effects of C++ name mangling # Because C++ symbols are routinely exported from DLL and shared object files, the name mangling scheme is not merely a compiler-internal matter. Different compilers (or different versions of the same compiler, in many cases) produce such binaries under different name decoration schemes, meaning that symbols are frequently unresolved if the compilers used to create the library and the program using it employed different schemes. For example, if a system with multiple C++ compilers installed (e.g., GNU GCC and the OS vendor's compiler) wished to install the Boost C++ Libraries , it would have to be compiled multiple times (once for GCC and once for the vendor compiler). It is good for safety purposes that compilers producing incompatible object codes (codes based on different ABIs, regarding e.g., classes and exceptions) use different name mangling schemes. This guarantees that these incompatibilities are detected at the linking phase, not when executing the software (which could lead to obscure(\u6a21\u7cca\u7684\uff0c\u9690\u6666\u7684) bugs and serious stability issues). For this reason name decoration is an important aspect of any C++-related ABI . Demangle via c++filt # $ c++filt _ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_ Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const Demangle via builtin GCC ABI # #include <stdio.h> #include <stdlib.h> #include <cxxabi.h> int main() { const char *mangled_name = \"_ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_\"; char *demangled_name; int status = -1; demangled_name = abi::__cxa_demangle(mangled_name, NULL, NULL, &status); printf(\"Demangled: %s\\n\", demangled_name); free(demangled_name); return 0; } Output: Demangled: Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const Python # In Python , mangling is used for \"private\" class members which are designated as such by giving them a name with two leading underscores and no more than one trailing underscore. For example, __thing will be mangled, as will ___thing and __thing_ , but __thing__ and __thing___ will not. Python's runtime does not restrict access to such members, the mangling only prevents name collisions if a derived class defines a member with the same name. On encountering name mangled attributes, Python transforms these names by prepending a single underscore and the name of the enclosing class, for example: >>> class Test(object): ... def __mangled_name(self): ... pass ... def normal_name(self): ... pass >>> t = Test() >>> [attr for attr in dir(t) if 'name' in attr] ['_Test__mangled_name', 'normal_name']","title":"Name mangling"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#name-mangling","text":"In compiler construction, name mangling (also called name decoration ) is a technique used to solve various problems caused by the need to resolve unique names for programming entities in many modern programming languages . It provides a way of encoding additional information in the name of a function , structure , class or another datatype in order to pass more semantic information from the compilers to linkers . SUMMARY : \u4e0a\u9762\u8fd9\u6bb5\u8bdd\u63cf\u8ff0\u4e86\u4f7f\u7528name mangling\u7684\u610f\u56fe The need arises where the language allows different entities to be named with the same identifier as long as they occupy a different namespace (where a namespace is typically defined by a module, class, or explicit namespace directive) or have different signatures (such as function overloading ). Any object code produced by compilers is usually linked with other pieces of object code (produced by the same or another compiler) by a type of program called a linker . The linker needs a great deal of information on each program entity. For example, to correctly link a function it needs its name, the number of arguments and their types, and so on.","title":"Name mangling"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#examples","text":"","title":"Examples"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#c","text":"Although name mangling is not generally required or used by languages that do not support function overloading (such as C and classic Pascal), they use it in some cases to provide additional information about a function. For example, compilers targeted at Microsoft Windows platforms support a variety of calling conventions , which determine the manner in which parameters are sent to subroutines and results returned. Because the different calling conventions are not compatible with one another, compilers mangle symbols with codes detailing which convention should be used to call the specific routine. SUMMARY : encode calling convention in the name The mangling scheme was established by Microsoft, and has been informally followed by other compilers including Digital Mars, Borland, and GNU GCC, when compiling code for the Windows platforms. The scheme even applies to other languages, such as Pascal , D , Delphi , Fortran , and C# . This allows subroutines written in those languages to call, or be called by, existing Windows libraries using a calling convention different from their default. When compiling the following C examples: int _cdecl f (int x) { return 0; } int _stdcall g (int y) { return 0; } int _fastcall h (int z) { return 0; } 32 bit compilers emit, respectively: _f _g@4 @h@4 In the stdcall and fastcall mangling schemes, the function is encoded as _**name**@**X** and @**name**@**X** respectively, where X is the number of bytes, in decimal, of the argument(s) in the parameter list (including those passed in registers, for fastcall). In the case of cdecl , the function name is merely prefixed by an underscore. The 64-bit convention on Windows (Microsoft C) has no leading underscore. This difference may in some rare cases lead to unresolved externals when porting such code to 64 bits. For example, Fortran code can use 'alias' to link against a C method by name as follows: SUBROUTINE f() !DEC$ ATTRIBUTES C, ALIAS:'_f' :: f END SUBROUTINE This will compile and link fine under 32 bits, but generate an unresolved external '_f' under 64 bits. One workaround for this is not to use 'alias' at all (in which the method names typically need to be capitalized in C and Fortran). Another is to use the BIND option: SUBROUTINE f() BIND(C,NAME=\"f\") END SUBROUTINE","title":"C"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#c_1","text":"C++ compilers are the most widespread users of name mangling . The first C++ compilers were implemented as translators to C source code, which would then be compiled by a C compiler to object code; because of this, symbol names had to conform to C identifier rules. Even later, with the emergence of compilers which produced machine code or assembly directly, the system's linker generally did not support C++ symbols, and mangling was still required. The C++ language does not define a standard decoration scheme, so each compiler uses its own. C++ also has complex language features, such as classes , templates , namespaces , and operator overloading , that alter the meaning of specific symbols based on context or usage. Meta-data about these features can be disambiguated(\u6d88\u9664) by mangling (decorating) the name of a symbol . Because the name-mangling systems for such features are not standardized across compilers, few linkers can link object code that was produced by different compilers.","title":"C++"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#simple-exampleedit","text":"A single C++ translation unit might define two functions named f() : int f (void) { return 1; } int f (int) { return 0; } void g (void) { int i = f(), j = f(0); } These are distinct functions, with no relation to each other apart from the name. The C++ compiler therefore will encode the type information in the symbol name, the result being something resembling: int __f_v (void) { return 1; } int __f_i (int) { return 0; } void __g_v (void) { int i = __f_v(), j = __f_i(0); } Even though its name is unique, g() is still mangled: name mangling applies to all symbols.","title":"Simple example[edit]"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#complex-exampleedit","text":"The mangled symbols in this example, in the comments below the respective identifier name, are those produced by the GNU GCC 3.x compilers: namespace wikipedia { class article { public: std::string format (void); /* = _ZN9wikipedia7article6formatEv */ bool print_to (std::ostream&); /* = _ZN9wikipedia7article8print_toERSo */ class wikilink { public: wikilink (std::string const& name); /* = _ZN9wikipedia7article8wikilinkC1ERKSs */ }; }; } All mangled symbols begin with _Z (note that an identifier beginning with an underscore followed by a capital is a reserved identifier in C, so conflict with user identifiers is avoided); for nested names (including both namespaces and classes), this is followed by N , then a series of pairs (the length being the length of the next identifier), and finally E . For example, wikipedia::article::format becomes _ZN9Wikipedia7article6formatE For functions, this is then followed by the type information; as format() is a void function, this is simply v ; hence: _ZN9Wikipedia7article6formatEv For print_to , the standard type std::ostream (which is a typedef for std::basic_ostream<char, std::char_traits<char> > ) is used, which has the special alias So ; a reference to this type is therefore RSo , with the complete name for the function being: _ZN9Wikipedia7article8print_toERSo","title":"Complex example[edit]"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#how-different-compilers-mangle-the-same-functionsedit","text":"There isn't a standard scheme by which even trivial C++ identifiers are mangled, and consequently different compilers (or even different versions of the same compiler, or the same compiler on different platforms) mangle public symbols in radically different (and thus totally incompatible) ways. Consider how different C++ compilers mangle the same functions: Compiler void h(int) void h(int, char) void h(void) Intel C++ 8.0 for Linux _Z1hi _Z1hic _Z1hv HP aC++ A.05.55 IA-64 IAR EWARM C++ 5.4 ARM GCC 3. x and higher Clang 1. x and higher[ 1] IAR EWARM C++ 7.4 ARM _Z<number>hi _Z<number>hic _Z<number>hv GCC 2.9 x h__Fi h__Fic h__Fv HP aC++ A.03.45 PA-RISC Microsoft Visual C++ v6-v10 ( mangling details ) ?h@@YAXH@Z ?h@@YAXHD@Z ?h@@YAXXZ Digital Mars C++ Borland C++ v3.1 @h$qi @h$qizc @h$qv OpenVMS C++ V6.5 (ARM mode) H__XI H__XIC H__XV OpenVMS C++ V6.5 (ANSI mode) CXX$__7H__FIC26CDH77 CXX$__7H__FV2CB06E8 OpenVMS C++ X7.1 IA-64 CXX$_Z1HI2DSQ26A CXX$_Z1HIC2NP3LI4 CXX$_Z1HV0BCA19V SunPro CC __1cBh6Fi_v_ __1cBh6Fic_v_ __1cBh6F_v_ Tru64 C++ V6.5 (ARM mode) h__Xi h__Xic h__Xv Tru64 C++ V6.5 (ANSI mode) __7h__Fi __7h__Fic __7h__Fv Watcom C++ 10.6 W?h$n(i)v W?h$n(ia)v W?h$n()v Notes: The Compaq C++ compiler on OpenVMS VAX and Alpha (but not IA-64) and Tru64 has two name mangling schemes. The original, pre-standard scheme is known as ARM model, and is based on the name mangling described in the C++ Annotated Reference Manual (ARM). With the advent of new features in standard C++, particularly templates , the ARM scheme became more and more unsuitable \u2014 it could not encode certain function types, or produced identical mangled names for different functions. It was therefore replaced by the newer \"ANSI\" model, which supported all ANSI template features, but was not backwards compatible. On IA-64, a standard Application Binary Interface (ABI) exists (see external links ), which defines (among other things) a standard name-mangling scheme, and which is used by all the IA-64 compilers. GNU GCC 3. x , in addition, has adopted the name mangling scheme defined in this standard for use on other, non-Intel platforms. The Visual Studio and Windows SDK include the program undname which prints the C-style function prototype for a given mangled name. On Microsoft Windows, the Intel compiler[ 2] and Clang [ 3] uses the Visual C++ name mangling for compatibility. For the IAR EWARM C++ 7.4 ARM compiler the best way to determine the name of a function is to compile with the assembler output turned on and to look at the output in the \".s\" file thus generated.","title":"How different compilers mangle the same functions[edit]"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#handling-of-c-symbols-when-linking-from-c","text":"The job of the common C++ idiom: #ifdef __cplusplus extern \"C\" { #endif /* ... */ #ifdef __cplusplus } #endif is to ensure that the symbols within are \"unmangled\" \u2013 that the compiler emits(\u521b\u9020\uff0c\u751f\u4ea7\u51fa) a binary file with their names undecorated, as a C compiler would do. As C language definitions are unmangled, the C++ compiler needs to avoid mangling references to these identifiers. For example, the standard strings library, <string.h> usually contains something resembling: #ifdef __cplusplus extern \"C\" { #endif void *memset (void *, int, size_t); char *strcat (char *, const char *); int strcmp (const char *, const char *); char *strcpy (char *, const char *); #ifdef __cplusplus } #endif Thus, code such as: if (strcmp(argv[1], \"-x\") == 0) strcpy(a, argv[2]); else memset (a, 0, sizeof(a)); uses the correct, unmangled strcmp and memset . If the extern had not been used, the (SunPro) C++ compiler would produce code equivalent to: if (__1cGstrcmp6Fpkc1_i_(argv[1], \"-x\") == 0) __1cGstrcpy6Fpcpkc_0_(a, argv[2]); else __1cGmemset6FpviI_0_ (a, 0, sizeof(a)); Since those symbols do not exist in the C runtime library ( e.g. libc ), link errors would result.","title":"Handling of C symbols when linking from C++"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#standardised-name-mangling-in-c","text":"Though it would seem that standardised name mangling in the C++ language would lead to greater interoperability between compiler implementations, such a standardization by itself would not suffice to guarantee C++ compiler interoperability and it might even create a false impression that interoperability is possible and safe when it isn't. Name mangling is only one of several application binary interface (ABI) details that need to be decided and observed by a C++ implementation. Other ABI aspects like exception handling , virtual table layout, structure and stack frame padding , etc. also cause differing C++ implementations to be incompatible. Further, requiring a particular form of mangling would cause issues for systems where implementation limits (e.g., length of symbols) dictate a particular mangling scheme. A standardised requirement for name mangling would also prevent an implementation where mangling was not required at all \u2014 for example, a linker which understood the C++ language. The C++ standard therefore does not attempt to standardise name mangling. On the contrary, the Annotated C++ Reference Manual (also known as ARM , ISBN 0-201-51459-1 , section 7.2.1c) actively encourages the use of different mangling schemes to prevent linking when other aspects of the ABI, such as exception handling and virtual table layout, are incompatible. Nevertheless, as detailed in the section above, on some platforms[ 4] the full C++ ABI has been standardized, including name mangling.","title":"Standardised name mangling in C++"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#real-world-effects-of-c-name-mangling","text":"Because C++ symbols are routinely exported from DLL and shared object files, the name mangling scheme is not merely a compiler-internal matter. Different compilers (or different versions of the same compiler, in many cases) produce such binaries under different name decoration schemes, meaning that symbols are frequently unresolved if the compilers used to create the library and the program using it employed different schemes. For example, if a system with multiple C++ compilers installed (e.g., GNU GCC and the OS vendor's compiler) wished to install the Boost C++ Libraries , it would have to be compiled multiple times (once for GCC and once for the vendor compiler). It is good for safety purposes that compilers producing incompatible object codes (codes based on different ABIs, regarding e.g., classes and exceptions) use different name mangling schemes. This guarantees that these incompatibilities are detected at the linking phase, not when executing the software (which could lead to obscure(\u6a21\u7cca\u7684\uff0c\u9690\u6666\u7684) bugs and serious stability issues). For this reason name decoration is an important aspect of any C++-related ABI .","title":"Real-world effects of C++ name mangling"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#demangle-via-cfilt","text":"$ c++filt _ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_ Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const","title":"Demangle via c++filt"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#demangle-via-builtin-gcc-abi","text":"#include <stdio.h> #include <stdlib.h> #include <cxxabi.h> int main() { const char *mangled_name = \"_ZNK3MapI10StringName3RefI8GDScriptE10ComparatorIS0_E16DefaultAllocatorE3hasERKS0_\"; char *demangled_name; int status = -1; demangled_name = abi::__cxa_demangle(mangled_name, NULL, NULL, &status); printf(\"Demangled: %s\\n\", demangled_name); free(demangled_name); return 0; } Output: Demangled: Map<StringName, Ref<GDScript>, Comparator<StringName>, DefaultAllocator>::has(StringName const&) const","title":"Demangle via builtin GCC ABI"},{"location":"Chapter-7-Run-Time-Environments/ABI/Name-mangling/Name-mangling/#python","text":"In Python , mangling is used for \"private\" class members which are designated as such by giving them a name with two leading underscores and no more than one trailing underscore. For example, __thing will be mangled, as will ___thing and __thing_ , but __thing__ and __thing___ will not. Python's runtime does not restrict access to such members, the mangling only prevents name collisions if a derived class defines a member with the same name. On encountering name mangled attributes, Python transforms these names by prepending a single underscore and the name of the enclosing class, for example: >>> class Test(object): ... def __mangled_name(self): ... pass ... def normal_name(self): ... pass >>> t = Test() >>> [attr for attr in dir(t) if 'name' in attr] ['_Test__mangled_name', 'normal_name']","title":"Python"},{"location":"Chapter-9-Machine-Independent-Optimizations/","text":"Chapter 9 Machine-Independent Optimizations #","title":9},{"location":"Chapter-9-Machine-Independent-Optimizations/#chapter-9-machine-independent-optimizations","text":"","title":"Chapter 9 Machine-Independent Optimizations"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/","text":"What is the difference between an Abstract Syntax Tree and a Concrete Syntax Tree? A A A What's the difference between parse tree and AST? - Parse Tree is the result of your grammar with its artifacts (you can write an infinity of grammars for the same language), an AST reduce the Parse Tree the closest possible to the language. Several grammars for the same language will give different parse trees but should result to the same AST. (you can also reduce different scripts (different parse trees from the same grammar) to the same AST) \u2013 Guillaume86 Aug 29 '12 at 14:38 A A A A What would an AST (abstract syntax tree) for an object-oriented programming language look like? A A DMS Software Reengineering Toolkit Abstract vs. Concrete Syntax Trees Compiling an AST back to source code What is the difference between an Abstract Syntax Tree and a Concrete Syntax Tree? # I've been reading a bit about how interpreters/compilers work, and one area where I'm getting confused is the difference between an AST and a CST. My understanding is that the parser makes a CST, hands it to the semantic analyzer which turns it into an AST . However, my understanding is that the semantic analyzer simply ensures that rules are followed. I don't really understand why it would actually make any changes to make it abstract rather than concrete. Is there something that I'm missing about the semantic analyzer , or is the difference between an AST and CST somewhat artificial? A # A concrete syntax tree represents the source text exactly in parsed form. In general, it conforms to the context-free grammar defining the source language. However, the concrete grammar and tree have a lot of things that are necessary to make source text unambiguously parseable , but do not contribute to actual meaning. For example, to implement operator precedence , your CFG usually has several levels of expression components (term, factor, etc.), with the operators connecting them at the different levels (you add terms to get expressions, terms are composed of factors optionally multipled, etc.). To actually interpret or compile the language, however, you don't need this; you just need Expression nodes that have operators and operands. The abstract syntax tree is the result of simplifying the concrete syntax tree down to this things actually needed to represent the meaning of the program. This tree has a much simpler definition and is thus easier to process in the later stages of execution . You usually don't need to actually build a concrete syntax tree. The action routines in your YACC (or Antlr, or Menhir, or whatever...) grammar can directly build the abstract syntax tree , so the concrete syntax tree only exists as a conceptual entity representing the parse structure of your source text. COMMENTS : Supplement: the Python interpreter first builds a CST and then converts to AST. \u2013 cgsdfc Dec 2 '18 at 12:18 A # A concrete syntax tree matches what the grammar rules say is the syntax. The purpose of the abstract syntax tree is have a \"simple\" representation of what's essential in \"the syntax tree\". A real value in the AST IMHO is that it is smaller than the CST, and therefore takes less time to process. (You might say, who cares? But I work with a tool where we have tens of millions of nodes live at once!). Most parser generators that have any support for building syntax trees insist that you personally specify exactly how they get built under the assumption that your tree nodes will be \"simpler\" than the CST (and in that, they are generally right, as programmers are pretty lazy). Arguably it means you have to code fewer tree visitor functions, and that's valuable, too, in that it minimizes engineering energy. When you have 3500 rules (e.g., for COBOL) this matters. And this \"simpler\"ness leads to the good property of \"smallness\". But having such ASTs creates a problem that wasn't there: it doesn't match the grammar, and now you have to mentally track both of them. And when there are 1500 AST nodes for a 3500 rule grammar, this matters a lot. And if the grammar evolves (they always do!), now you have two giant sets of things to keep in synch. Another solution is to let the parser simply build CST nodes for you and just use those. This is a huge advantage when building the grammars: there's no need to invent 1500 special AST nodes to model 3500 grammar rules. Just think about the tree being isomorphic to the grammar. From the point of view of the grammar engineer this is completely brainless, which lets him focus on getting the grammar right and hacking at it to his heart's content. Arguably you have to write more node visitor rules, but that can be managed. More on this later. What we do with the DMS Software Reengineering Toolkit is to automatically build a CST based on the results of a (GLR) parsing process. DMS then automatically constructs an \"compressed\" CST for space efficiency reasons, by eliminating non-value carrying terminals (keywords, punctation), semantically useless unary productions, and forming lists for grammar rule pairs that are list like: L = e ; L = L e ; L2 = e2 ; L2 = L2 ',' e2 ; and a wide variety of variations of such forms. You think in terms of the grammar rules and the virtual CST; the tool operates on the compressed representation. Easy on your brain, faster/smaller at runtime. Remarkably, the compressed CST built this way looks a lot an AST that you might have designed by hand (see link at end to examples). In particular, the compressed CST doesn't carry any nodes that are just concrete syntax. There are minor bits of awkwardness: for example while the concrete nodes for '(' and ')' classically found in expression subgrammars are not in the tree, a \"parentheses node\" does appear in the compressed CST and has to be handled. A true AST would not have this. This seems like a pretty small price to pay for the convenience of not have to specify the AST construction, ever. And the documentation for the tree is always available and correct: the grammar is the documentation. How do we avoid \"extra visitors\"? We don't entirely, but DMS provides an AST library that walks the AST and handles the differences between the CST and the AST transparently. DMS also offers an \"attribute grammar\" evaluator (AGE), which is a method for passing values computed a nodes up and down the tree; the AGE handles all the tree representation issues and so the tool engineer only worries about writing computations effectively directly on the grammar rules themselves. Finally, DMS also provides \"surface-syntax\" patterns, which allows code fragments from the grammar to used to find specific types of subtrees, without knowing most of the node types involved. One of the other answers observes that if you want to build tools that can regenerate source, your AST will have to match the CST. That's not really right, but it is far easier to regenerate the source if you have CST nodes. DMS generates most of the prettyprinter automatically because it has access to both :-} Bottom line: ASTs are good for small, both phyiscal and conceptual. Automated AST construction from the CST provides both, and lets you avoid the problem of tracking two different sets. EDIT March 2015: Link to examples of CST vs. \"AST\" built this way A # This blog post may be helpful. It seems to me that the AST \"throws away\" a lot of intermediate grammatical/structural information that wouldn't contribute to semantics. For example, you don't care that 3 is an atom is a term is a factor is a.... You just care that it's 3 when you're implementing the exponentiation expression or whatever. What's the difference between parse tree and AST? # Are they generated by different phases of a compiling process? Or are they just different names for the same thing? COMMENTS : - Parse Tree is the result of your grammar with its artifacts (you can write an infinity of grammars for the same language), an AST reduce the Parse Tree the closest possible to the language. Several grammars for the same language will give different parse trees but should result to the same AST. (you can also reduce different scripts (different parse trees from the same grammar) to the same AST) \u2013 Guillaume86 Aug 29 '12 at 14:38 # A # This is based on the Expression Evaluator grammar by Terrence Parr. The grammar for this example: grammar Expr002; options { output=AST; ASTLabelType=CommonTree; // type of $stat.tree ref etc... } prog : ( stat )+ ; stat : expr NEWLINE -> expr | ID '=' expr NEWLINE -> ^('=' ID expr) | NEWLINE -> ; expr : multExpr (( '+'^ | '-'^ ) multExpr)* ; multExpr : atom ('*'^ atom)* ; atom : INT | ID | '('! expr ')'! ; ID : ('a'..'z' | 'A'..'Z' )+ ; INT : '0'..'9'+ ; NEWLINE : '\\r'? '\\n' ; WS : ( ' ' | '\\t' )+ { skip(); } ; Input x=1 y=2 3*(x+y) Parse Tree The parse tree is a concrete representation of the input. The parse tree retains all of the information of the input. The empty boxes represent whitespace, i.e. end of line. AST The AST is an abstract representation of the input. Notice that parens are not present in the AST because the associations are derivable from the tree structure. For a more through explanation see Compilers and Compiler Generators pg. 23 or Abstract Syntax Trees on pg. 21 in Syntax and Semantics of Programming Languages COMMENTS How do you derive the AST from the parse tree? What's the method of simplifying a parse tree into an AST? \u2013 CMCDragonkai Feb 15 '15 at 8:54 A # From what I understand, the AST focuses more on the abstract relationships between the components of source code, while the parse tree focuses on the actual implementation of the grammar utilized by the language, including the nitpicky details. They are definitely not the same, since another term for \"parse tree\" is \"concrete syntax tree\". I found this page which attempts to resolve this exact question. A # The DSL book from Martin Fowler explains this nicely. The AST only contains all 'useful' elements that will be used for further processing, while the parse tree contains all the artifacts (spaces, brackets, ...) from the original document you parse A # Take the pascal assignment Age:= 42; The syntax tree would look just like the source code. Below I am putting brackets around the nodes. [Age][:=][42][;] An abstract tree would look like this [=][Age][42] The assignment becomes a node with 2 elements, Age and 42 . The idea is that you can execute the assignment. Also note that the pascal syntax disappears. Thus it is possible to have more than one language generate the same AST. This is useful for cross language script engines. What would an AST (abstract syntax tree) for an object-oriented programming language look like? # I'm reading about AST (abstract syntax trees) but all the samples I see use expressions such as: a + b * c Which could be represented in a lispy like syntax as: (+ a (* b c) ) Which will be the equivalent to: + / \\ a * / \\ b c My question is How an AST for a class in a OOPL would look like? My naive attempt is for this Java code: class Person { String name; int age; public String toString() { return \"name\"; } } Is: ;Hand written (classDeclaration Person (varDeclaration String name) (varDeclaration int age ) (funcDeclaration String toString (return \"name\") ) ) But I'm not quite sure how close or far am I to a real AST representation. Does it depends on the language I choose. How much detail is needed? Are those \"xyzDeclaraction\" needed or could be as: (Person (String name) (int age)) Where can I see a \"real\" representation of an actual programming language to learn more. A # AST is an abstraction of the CST ( concrete syntax tree , or, parse tree). The concrete syntax tree is the tree resulting from the productions (in the grammar) used to parse the file. So your AST is basically derived from your grammar definition, but has for transformed Exp / | \\ / | \\ * Ident BinOp Ident into / \\ / | \\ \"x\" \"y\" / | \\ \"x\" * \"y\" All in all I think the example in your post looks fine. I would probably wrap the variable declarations in a varDeclList and the function declaration in a methDeclList , and the return statement in a stmtList . (See below.) One more or less \"real\" representation of an AST is described by Apple in his book \"Modern Compiler Implementation in Java\". (Resources can be found here .) Using those classes, your program would be represented as follows: Program ClassDeclList ClassDecl Identifier id: Person VarDeclList VarDecl type: String id: name VarDecl type: int id: age MethDeclList MethodDecl modifiers: public returnType: String id: toString Formals (empty) StmtList returnStmt Identifier id: name A # OP: Where can I see a real representation of an actual programming language to learn more? For your source text as a file Person.java: class Person { String name; int age; public String toString() { return \"name\"; } } what follows are both Concrete and Abstract Syntax Tree in an S-expression-style dump of the parser tree from our DMS Software Reengineering Toolkit , using its Java1.6 parser. All the apparant complexity is pretty much caused by the real complexity of the language (e.g., of Java itself). The CST clearly contains more stuff (139 nodes) than the AST (54 nodes). The AST drops everything that can be automatically inferred from the grammar, given the AST. This includes removing non-value-carrying leaves, unary productions, and compressing spines caused by left or right recursive grammar rules into explicit list nodes. A left paren signals a new subtree. Following the left paren is the name of the node type; @Java~Java1_.6 might seem unnecessary until you understand DMS can handle many languages at once, including langauges nested inside one another. The #nnnnnn is the memory address of the node. ^M means \"this node has M parents and is left off when M==1. Things inside [...] are the node value. A { M } means this list node has M list-children. Each node is stamped with position information. This is the Concrete Syntax tree (see further down for AST): (compilation_unit@Java~Java1_6=1#4885d00^0 Line 1 Column 1 File C:/temp/Person.java (type_declarations@Java~Java1_6=15#4885cc0 Line 1 Column 1 File C:/temp/Person.java (type_declarations@Java~Java1_6=16#4884d80 Line 1 Column 1 File C:/temp/Person.java)type_declarations (type_declaration@Java~Java1_6=17#4885ca0 Line 1 Column 1 File C:/temp/Person.java (type_class_modifiers@Java~Java1_6=77#4884dc0 Line 1 Column 1 File C:/temp/Person.java)type_class_modifiers (class_header@Java~Java1_6=89#4884ec0 Line 1 Column 1 File C:/temp/Person.java |('class'@Java~Java1_6=459#4884c60[Keyword:0] Line 1 Column 1 File C:/temp/Person.java)'class' |(IDENTIFIER@Java~Java1_6=447#4884e20[`Person'] Line 1 Column 7 File C:/temp/Person.java)IDENTIFIER |(type_parameters@Java~Java1_6=408#4884e80 Line 1 Column 14 File C:/temp/Person.java)type_parameters )class_header (class_body@Java~Java1_6=94#4885c80 Line 1 Column 14 File C:/temp/Person.java |('{'@Java~Java1_6=448#4884e60[Keyword:0] Line 1 Column 14 File C:/temp/Person.java)'{' |(class_body_declarations@Java~Java1_6=111#4885c60 Line 2 Column 5 File C:/temp/Person.java | (class_body_declarations@Java~Java1_6=111#4885380 Line 2 Column 5 File C:/temp/Person.java | (class_body_declarations@Java~Java1_6=110#4885400 Line 2 Column 5 File C:/temp/Person.java | (class_body_declaration@Java~Java1_6=118#4885360 Line 2 Column 5 File C:/temp/Person.java | |(field_declaration@Java~Java1_6=168#4885440 Line 2 Column 5 File C:/temp/Person.java | | (field_modifiers@Java~Java1_6=170#4884f40 Line 2 Column 5 File C:/temp/Person.java)field_modifiers | | (type@Java~Java1_6=191#48852c0 Line 2 Column 5 File C:/temp/Person.java | | (name@Java~Java1_6=406#48851e0 Line 2 Column 5 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#4884f20[`String'] Line 2 Column 5 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#4885160 Line 2 Column 12 File C:/temp/Person.java)type_arguments | | )name | | (brackets@Java~Java1_6=157#4885260 Line 2 Column 12 File C:/temp/Person.java)brackets | | )type | | (variable_declarator_list@Java~Java1_6=179#4884e00 Line 2 Column 12 File C:/temp/Person.java | | (variable_declarator@Java~Java1_6=181#4885300 Line 2 Column 12 File C:/temp/Person.java | | (variable_declarator_id@Java~Java1_6=167#4885320 Line 2 Column 12 File C:/temp/Person.java | | |(IDENTIFIER@Java~Java1_6=447#4885140[`name'] Line 2 Column 12 File C:/temp/Person.java)IDENTIFIER | | |(brackets@Java~Java1_6=157#4885040 Line 2 Column 16 File C:/temp/Person.java)brackets | | )variable_declarator_id | | )variable_declarator | | )variable_declarator_list | | (';'@Java~Java1_6=440#4885100[Keyword:0] Line 2 Column 16 File C:/temp/Person.java)';' | |)field_declaration | )class_body_declaration | )class_body_declarations | (class_body_declaration@Java~Java1_6=118#48852e0 Line 3 Column 5 File C:/temp/Person.java | (field_declaration@Java~Java1_6=168#4885480 Line 3 Column 5 File C:/temp/Person.java | |(field_modifiers@Java~Java1_6=170#4885340 Line 3 Column 5 File C:/temp/Person.java)field_modifiers | |(type@Java~Java1_6=192#4885220 Line 3 Column 5 File C:/temp/Person.java | | (primitive_type@Java~Java1_6=198#4885420 Line 3 Column 5 File C:/temp/Person.java | | ('int'@Java~Java1_6=479#48853e0[Keyword:0] Line 3 Column 5 File C:/temp/Person.java)'int' | | )primitive_type | | (brackets@Java~Java1_6=157#4885200 Line 3 Column 12 File C:/temp/Person.java)brackets | |)type | |(variable_declarator_list@Java~Java1_6=179#4885540 Line 3 Column 12 File C:/temp/Person.java | | (variable_declarator@Java~Java1_6=181#4885520 Line 3 Column 12 File C:/temp/Person.java | | (variable_declarator_id@Java~Java1_6=167#4885500 Line 3 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#4884fc0[`age'] Line 3 Column 12 File C:/temp/Person.java)IDENTIFIER | | (brackets@Java~Java1_6=157#48854e0 Line 3 Column 15 File C:/temp/Person.java)brackets | | )variable_declarator_id | | )variable_declarator | |)variable_declarator_list | |(';'@Java~Java1_6=440#48854c0[Keyword:0] Line 3 Column 15 File C:/temp/Person.java)';' | )field_declaration | )class_body_declaration | )class_body_declarations | (class_body_declaration@Java~Java1_6=117#4885c40 Line 4 Column 5 File C:/temp/Person.java | (method_declaration@Java~Java1_6=135#4885c00 Line 4 Column 5 File C:/temp/Person.java | (method_modifiers@Java~Java1_6=141#4885700 Line 4 Column 5 File C:/temp/Person.java | |(method_modifiers@Java~Java1_6=142#4884e40 Line 4 Column 5 File C:/temp/Person.java)method_modifiers | |(method_modifier@Java~Java1_6=147#48856a0 Line 4 Column 5 File C:/temp/Person.java | | ('public'@Java~Java1_6=453#48853a0[Keyword:0] Line 4 Column 5 File C:/temp/Person.java)'public' | |)method_modifier | )method_modifiers | (type_parameters@Java~Java1_6=408#4885740 Line 4 Column 12 File C:/temp/Person.java)type_parameters | (type@Java~Java1_6=191#4885900 Line 4 Column 12 File C:/temp/Person.java | |(name@Java~Java1_6=406#48852a0 Line 4 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#4885660[`String'] Line 4 Column 12 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#48851a0 Line 4 Column 19 File C:/temp/Person.java)type_arguments | |)name | |(brackets@Java~Java1_6=157#48858c0 Line 4 Column 19 File C:/temp/Person.java)brackets | )type | (IDENTIFIER@Java~Java1_6=447#48855c0[`toString'] Line 4 Column 19 File C:/temp/Person.java)IDENTIFIER | (parameters@Java~Java1_6=158#48858e0 Line 4 Column 27 File C:/temp/Person.java | |('('@Java~Java1_6=450#4885840[Keyword:0] Line 4 Column 27 File C:/temp/Person.java)'(' | |(')'@Java~Java1_6=451#4885620[Keyword:0] Line 4 Column 28 File C:/temp/Person.java)')' | )parameters | (brackets@Java~Java1_6=157#4885060 Line 5 Column 7 File C:/temp/Person.java)brackets | (block@Java~Java1_6=217#4885be0 Line 5 Column 7 File C:/temp/Person.java | |('{'@Java~Java1_6=448#48851c0[Keyword:0] Line 5 Column 7 File C:/temp/Person.java)'{' | |(statement_sequence@Java~Java1_6=218#4885ba0 Line 5 Column 9 File C:/temp/Person.java | | (statement_sequence_member@Java~Java1_6=223#4885b80 Line 5 Column 9 File C:/temp/Person.java | | (executable_statement@Java~Java1_6=243#4885b60 Line 5 Column 9 File C:/temp/Person.java | | ('return'@Java~Java1_6=491#4884f60[Keyword:0] Line 5 Column 9 File C:/temp/Person.java)'return' | | (expression@Java~Java1_6=332#4885ac0 Line 5 Column 16 File C:/temp/Person.java | | |(conditional_expression@Java~Java1_6=345#4885a60 Line 5 Column 16 File C:/temp/Person.java | | | (conditional_or_expression@Java~Java1_6=347#4885a20 Line 5 Column 16 File C:/temp/Person.java | | | (conditional_and_expression@Java~Java1_6=349#48859e0 Line 5 Column 16 File C:/temp/Person.java | | | (inclusive_or_expression@Java~Java1_6=351#48857e0 Line 5 Column 16 File C:/temp/Person.java | | | |(exclusive_or_expression@Java~Java1_6=353#48855a0 Line 5 Column 16 File C:/temp/Person.java | | | | (and_expression@Java~Java1_6=355#4885940 Line 5 Column 16 File C:/temp/Person.java | | | | (equality_expression@Java~Java1_6=357#4885880 Line 5 Column 16 File C:/temp/Person.java | | | | (relational_expression@Java~Java1_6=360#4885800 Line 5 Column 16 File C:/temp/Person.java | | | | |(shift_expression@Java~Java1_6=366#48856c0 Line 5 Column 16 File C:/temp/Person.java | | | | | (additive_expression@Java~Java1_6=370#4885180 Line 5 Column 16 File C:/temp/Person.java | | | | | (multiplicative_expression@Java~Java1_6=373#4885780 Line 5 Column 16 File C:/temp/Person.java | | | | | (unary_expression@Java~Java1_6=383#4885600 Line 5 Column 16 File C:/temp/Person.java | | | | | |(unary_expression_not_plus_minus@Java~Java1_6=389#4885680 Line 5 Column 16 File C:/temp/Person.java | | | | | | (literal@Java~Java1_6=390#4884f80 Line 5 Column 16 File C:/temp/Person.java | | | | | | (STRING@Java~Java1_6=536#4885120[`name'] Line 5 Column 16 File C:/temp/Person.java)STRING | | | | | | )literal | | | | | |)unary_expression_not_plus_minus | | | | | )unary_expression | | | | | )multiplicative_expression | | | | | )additive_expression | | | | |)shift_expression | | | | )relational_expression | | | | )equality_expression | | | | )and_expression | | | |)exclusive_or_expression | | | )inclusive_or_expression | | | )conditional_and_expression | | | )conditional_or_expression | | |)conditional_expression | | )expression | | (';'@Java~Java1_6=440#48856e0[Keyword:0] Line 5 Column 22 File C:/temp/Person.java)';' | | )executable_statement | | )statement_sequence_member | |)statement_sequence | |('}'@Java~Java1_6=449#4885b40[Keyword:0] Line 5 Column 28 File C:/temp/Person.java)'}' | )block | )method_declaration | )class_body_declaration |)class_body_declarations |('}'@Java~Java1_6=449#4885bc0[Keyword:0] Line 6 Column 1 File C:/temp/Person.java)'}' )class_body )type_declaration )type_declarations (optional_CONTROL_Z@Java~Java1_6=5#4885ce0 Line 7 Column 1 File C:/temp/Person.java)optional_CONTROL_Z )compilation_unit This is the AST (automatically generated by DMS from the CST): (compilation_unit@Java~Java1_6=1#486f900^0 Line 1 Column 1 File C:/temp/Person.java (type_declarations@Java~Java1_6=15#486f4c0 {1} Line 1 Column 1 File C:/temp/Person.java (type_declaration@Java~Java1_6=17#486f5e0 Line 1 Column 1 File C:/temp/Person.java (type_class_modifiers@Java~Java1_6=77#486eda0 Line 1 Column 1 File C:/temp/Person.java)type_class_modifiers (class_header@Java~Java1_6=89#486ee60 Line 1 Column 1 File C:/temp/Person.java |(IDENTIFIER@Java~Java1_6=447#486ede0[`Person'] Line 1 Column 7 File C:/temp/Person.java)IDENTIFIER |(type_parameters@Java~Java1_6=408#486ee20 Line 1 Column 14 File C:/temp/Person.java)type_parameters )class_header (class_body@Java~Java1_6=94#486f040 Line 1 Column 14 File C:/temp/Person.java |(class_body_declarations@Java~Java1_6=111#486ee40 {3} Line 2 Column 5 File C:/temp/Person.java | (class_body_declaration@Java~Java1_6=118#486f300 Line 2 Column 5 File C:/temp/Person.java | (field_declaration@Java~Java1_6=168#486f380 Line 2 Column 5 File C:/temp/Person.java | (field_modifiers@Java~Java1_6=170#486eec0 Line 2 Column 5 File C:/temp/Person.java)field_modifiers | (type@Java~Java1_6=191#486f240 Line 2 Column 5 File C:/temp/Person.java | |(name@Java~Java1_6=406#486f180 Line 2 Column 5 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486eea0[`String'] Line 2 Column 5 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#486f0e0 Line 2 Column 12 File C:/temp/Person.java)type_arguments | |)name | |(brackets@Java~Java1_6=157#486f200 Line 2 Column 12 File C:/temp/Person.java)brackets | )type | (variable_declarator@Java~Java1_6=181#486ef20 Line 2 Column 12 File C:/temp/Person.java | |(variable_declarator_id@Java~Java1_6=167#486efe0 Line 2 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486f0c0[`name'] Line 2 Column 12 File C:/temp/Person.java)IDENTIFIER | | (brackets@Java~Java1_6=157#486f060 Line 2 Column 16 File C:/temp/Person.java)brackets | |)variable_declarator_id | )variable_declarator | )field_declaration | )class_body_declaration | (class_body_declaration@Java~Java1_6=118#486f000 Line 3 Column 5 File C:/temp/Person.java | (field_declaration@Java~Java1_6=168#486f320 Line 3 Column 5 File C:/temp/Person.java | (field_modifiers@Java~Java1_6=170#486f2a0 Line 3 Column 5 File C:/temp/Person.java)field_modifiers | (type@Java~Java1_6=192#486eee0 Line 3 Column 5 File C:/temp/Person.java | |(primitive_type@Java~Java1_6=198#486ef60 Line 3 Column 5 File C:/temp/Person.java)primitive_type | |(brackets@Java~Java1_6=157#486ee00 Line 3 Column 12 File C:/temp/Person.java)brackets | )type | (variable_declarator@Java~Java1_6=181#486f2c0 Line 3 Column 12 File C:/temp/Person.java | |(variable_declarator_id@Java~Java1_6=167#486f3a0 Line 3 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486f120[`age'] Line 3 Column 12 File C:/temp/Person.java)IDENTIFIER | | (brackets@Java~Java1_6=157#486ef00 Line 3 Column 15 File C:/temp/Person.java)brackets | |)variable_declarator_id | )variable_declarator | )field_declaration | )class_body_declaration | (class_body_declaration@Java~Java1_6=117#486f7a0 Line 4 Column 5 File C:/temp/Person.java | (method_declaration@Java~Java1_6=135#486f480 Line 4 Column 5 File C:/temp/Person.java | (method_modifiers@Java~Java1_6=141#486f460 {1} Line 4 Column 5 File C:/temp/Person.java | |(method_modifier@Java~Java1_6=147#486f400 Line 4 Column 5 File C:/temp/Person.java)method_modifier | )method_modifiers | (type_parameters@Java~Java1_6=408#486f540 Line 4 Column 12 File C:/temp/Person.java)type_parameters | (type@Java~Java1_6=191#486f740 Line 4 Column 12 File C:/temp/Person.java | |(name@Java~Java1_6=406#486f620 Line 4 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486f080[`String'] Line 4 Column 12 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#486f640 Line 4 Column 19 File C:/temp/Person.java)type_arguments | |)name | |(brackets@Java~Java1_6=157#486f700 Line 4 Column 19 File C:/temp/Person.java)brackets | )type | (IDENTIFIER@Java~Java1_6=447#486f140[`toString'] Line 4 Column 19 File C:/temp/Person.java)IDENTIFIER | (parameters@Java~Java1_6=158#486f760 Line 4 Column 27 File C:/temp/Person.java)parameters | (brackets@Java~Java1_6=157#486f820 Line 5 Column 7 File C:/temp/Person.java)brackets | (block@Java~Java1_6=217#486f780 Line 5 Column 7 File C:/temp/Person.java | |(statement_sequence@Java~Java1_6=218#486f6e0 Line 5 Column 9 File C:/temp/Person.java | | (statement_sequence_member@Java~Java1_6=223#486f6c0 Line 5 Column 9 File C:/temp/Person.java | | (executable_statement@Java~Java1_6=243#486f6a0 Line 5 Column 9 File C:/temp/Person.java | | (unary_expression_not_plus_minus@Java~Java1_6=389#486f720 Line 5 Column 16 File C:/temp/Person.java | | |(literal@Java~Java1_6=390#486f280 Line 5 Column 16 File C:/temp/Person.java | | | (STRING@Java~Java1_6=536#486f160[`name'] Line 5 Column 16 File C:/temp/Person.java)STRING | | |)literal | | )unary_expression_not_plus_minus | | )executable_statement | | )statement_sequence_member | |)statement_sequence | )block | )method_declaration | )class_body_declaration |)class_body_declarations )class_body )type_declaration )type_declarations (optional_CONTROL_Z@Java~Java1_6=5#486f4e0 Line 7 Column 1 File C:/temp/Person.java)optional_CONTROL_Z )compilation_unit EDIT March 2015: Here's a link to some C++ AST examples Edit May 2015: DMS has long done Java 1.7 and 1.8, too. DMS Software Reengineering Toolkit # Abstract vs. Concrete Syntax Trees # Compiling an AST back to source code #","title":"VS abstract syntax tree VS concrete syntax tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#what-is-the-difference-between-an-abstract-syntax-tree-and-a-concrete-syntax-tree","text":"I've been reading a bit about how interpreters/compilers work, and one area where I'm getting confused is the difference between an AST and a CST. My understanding is that the parser makes a CST, hands it to the semantic analyzer which turns it into an AST . However, my understanding is that the semantic analyzer simply ensures that rules are followed. I don't really understand why it would actually make any changes to make it abstract rather than concrete. Is there something that I'm missing about the semantic analyzer , or is the difference between an AST and CST somewhat artificial?","title":"What is the difference between an Abstract Syntax Tree and a Concrete Syntax Tree?"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a","text":"A concrete syntax tree represents the source text exactly in parsed form. In general, it conforms to the context-free grammar defining the source language. However, the concrete grammar and tree have a lot of things that are necessary to make source text unambiguously parseable , but do not contribute to actual meaning. For example, to implement operator precedence , your CFG usually has several levels of expression components (term, factor, etc.), with the operators connecting them at the different levels (you add terms to get expressions, terms are composed of factors optionally multipled, etc.). To actually interpret or compile the language, however, you don't need this; you just need Expression nodes that have operators and operands. The abstract syntax tree is the result of simplifying the concrete syntax tree down to this things actually needed to represent the meaning of the program. This tree has a much simpler definition and is thus easier to process in the later stages of execution . You usually don't need to actually build a concrete syntax tree. The action routines in your YACC (or Antlr, or Menhir, or whatever...) grammar can directly build the abstract syntax tree , so the concrete syntax tree only exists as a conceptual entity representing the parse structure of your source text. COMMENTS : Supplement: the Python interpreter first builds a CST and then converts to AST. \u2013 cgsdfc Dec 2 '18 at 12:18","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_1","text":"A concrete syntax tree matches what the grammar rules say is the syntax. The purpose of the abstract syntax tree is have a \"simple\" representation of what's essential in \"the syntax tree\". A real value in the AST IMHO is that it is smaller than the CST, and therefore takes less time to process. (You might say, who cares? But I work with a tool where we have tens of millions of nodes live at once!). Most parser generators that have any support for building syntax trees insist that you personally specify exactly how they get built under the assumption that your tree nodes will be \"simpler\" than the CST (and in that, they are generally right, as programmers are pretty lazy). Arguably it means you have to code fewer tree visitor functions, and that's valuable, too, in that it minimizes engineering energy. When you have 3500 rules (e.g., for COBOL) this matters. And this \"simpler\"ness leads to the good property of \"smallness\". But having such ASTs creates a problem that wasn't there: it doesn't match the grammar, and now you have to mentally track both of them. And when there are 1500 AST nodes for a 3500 rule grammar, this matters a lot. And if the grammar evolves (they always do!), now you have two giant sets of things to keep in synch. Another solution is to let the parser simply build CST nodes for you and just use those. This is a huge advantage when building the grammars: there's no need to invent 1500 special AST nodes to model 3500 grammar rules. Just think about the tree being isomorphic to the grammar. From the point of view of the grammar engineer this is completely brainless, which lets him focus on getting the grammar right and hacking at it to his heart's content. Arguably you have to write more node visitor rules, but that can be managed. More on this later. What we do with the DMS Software Reengineering Toolkit is to automatically build a CST based on the results of a (GLR) parsing process. DMS then automatically constructs an \"compressed\" CST for space efficiency reasons, by eliminating non-value carrying terminals (keywords, punctation), semantically useless unary productions, and forming lists for grammar rule pairs that are list like: L = e ; L = L e ; L2 = e2 ; L2 = L2 ',' e2 ; and a wide variety of variations of such forms. You think in terms of the grammar rules and the virtual CST; the tool operates on the compressed representation. Easy on your brain, faster/smaller at runtime. Remarkably, the compressed CST built this way looks a lot an AST that you might have designed by hand (see link at end to examples). In particular, the compressed CST doesn't carry any nodes that are just concrete syntax. There are minor bits of awkwardness: for example while the concrete nodes for '(' and ')' classically found in expression subgrammars are not in the tree, a \"parentheses node\" does appear in the compressed CST and has to be handled. A true AST would not have this. This seems like a pretty small price to pay for the convenience of not have to specify the AST construction, ever. And the documentation for the tree is always available and correct: the grammar is the documentation. How do we avoid \"extra visitors\"? We don't entirely, but DMS provides an AST library that walks the AST and handles the differences between the CST and the AST transparently. DMS also offers an \"attribute grammar\" evaluator (AGE), which is a method for passing values computed a nodes up and down the tree; the AGE handles all the tree representation issues and so the tool engineer only worries about writing computations effectively directly on the grammar rules themselves. Finally, DMS also provides \"surface-syntax\" patterns, which allows code fragments from the grammar to used to find specific types of subtrees, without knowing most of the node types involved. One of the other answers observes that if you want to build tools that can regenerate source, your AST will have to match the CST. That's not really right, but it is far easier to regenerate the source if you have CST nodes. DMS generates most of the prettyprinter automatically because it has access to both :-} Bottom line: ASTs are good for small, both phyiscal and conceptual. Automated AST construction from the CST provides both, and lets you avoid the problem of tracking two different sets. EDIT March 2015: Link to examples of CST vs. \"AST\" built this way","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_2","text":"This blog post may be helpful. It seems to me that the AST \"throws away\" a lot of intermediate grammatical/structural information that wouldn't contribute to semantics. For example, you don't care that 3 is an atom is a term is a factor is a.... You just care that it's 3 when you're implementing the exponentiation expression or whatever.","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#whats-the-difference-between-parse-tree-and-ast","text":"Are they generated by different phases of a compiling process? Or are they just different names for the same thing? COMMENTS :","title":"What's the difference between parse tree and AST?"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#-parse-tree-is-the-result-of-your-grammar-with-its-artifacts-you-can-write-an-infinity-of-grammars-for-the-same-language-an-ast-reduce-the-parse-tree-the-closest-possible-to-the-language-several-grammars-for-the-same-language-will-give-different-parse-trees-but-should-result-to-the-same-ast-you-can-also-reduce-different-scripts-different-parse-trees-from-the-same-grammar-to-the-same-ast-guillaume86-aug-29-12-at-1438","text":"","title":"- Parse Tree is the result of your grammar with its artifacts (you can write an infinity of grammars for the same language), an AST reduce the Parse Tree the closest possible to the language. Several grammars for the same language will give different parse trees but should result to the same AST. (you can also reduce different scripts (different parse trees from the same grammar) to the same AST) \u2013 Guillaume86 Aug 29 '12 at 14:38"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_3","text":"This is based on the Expression Evaluator grammar by Terrence Parr. The grammar for this example: grammar Expr002; options { output=AST; ASTLabelType=CommonTree; // type of $stat.tree ref etc... } prog : ( stat )+ ; stat : expr NEWLINE -> expr | ID '=' expr NEWLINE -> ^('=' ID expr) | NEWLINE -> ; expr : multExpr (( '+'^ | '-'^ ) multExpr)* ; multExpr : atom ('*'^ atom)* ; atom : INT | ID | '('! expr ')'! ; ID : ('a'..'z' | 'A'..'Z' )+ ; INT : '0'..'9'+ ; NEWLINE : '\\r'? '\\n' ; WS : ( ' ' | '\\t' )+ { skip(); } ; Input x=1 y=2 3*(x+y) Parse Tree The parse tree is a concrete representation of the input. The parse tree retains all of the information of the input. The empty boxes represent whitespace, i.e. end of line. AST The AST is an abstract representation of the input. Notice that parens are not present in the AST because the associations are derivable from the tree structure. For a more through explanation see Compilers and Compiler Generators pg. 23 or Abstract Syntax Trees on pg. 21 in Syntax and Semantics of Programming Languages COMMENTS How do you derive the AST from the parse tree? What's the method of simplifying a parse tree into an AST? \u2013 CMCDragonkai Feb 15 '15 at 8:54","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_4","text":"From what I understand, the AST focuses more on the abstract relationships between the components of source code, while the parse tree focuses on the actual implementation of the grammar utilized by the language, including the nitpicky details. They are definitely not the same, since another term for \"parse tree\" is \"concrete syntax tree\". I found this page which attempts to resolve this exact question.","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_5","text":"The DSL book from Martin Fowler explains this nicely. The AST only contains all 'useful' elements that will be used for further processing, while the parse tree contains all the artifacts (spaces, brackets, ...) from the original document you parse","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_6","text":"Take the pascal assignment Age:= 42; The syntax tree would look just like the source code. Below I am putting brackets around the nodes. [Age][:=][42][;] An abstract tree would look like this [=][Age][42] The assignment becomes a node with 2 elements, Age and 42 . The idea is that you can execute the assignment. Also note that the pascal syntax disappears. Thus it is possible to have more than one language generate the same AST. This is useful for cross language script engines.","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#what-would-an-ast-abstract-syntax-tree-for-an-object-oriented-programming-language-look-like","text":"I'm reading about AST (abstract syntax trees) but all the samples I see use expressions such as: a + b * c Which could be represented in a lispy like syntax as: (+ a (* b c) ) Which will be the equivalent to: + / \\ a * / \\ b c My question is How an AST for a class in a OOPL would look like? My naive attempt is for this Java code: class Person { String name; int age; public String toString() { return \"name\"; } } Is: ;Hand written (classDeclaration Person (varDeclaration String name) (varDeclaration int age ) (funcDeclaration String toString (return \"name\") ) ) But I'm not quite sure how close or far am I to a real AST representation. Does it depends on the language I choose. How much detail is needed? Are those \"xyzDeclaraction\" needed or could be as: (Person (String name) (int age)) Where can I see a \"real\" representation of an actual programming language to learn more.","title":"What would an AST (abstract syntax tree) for an object-oriented programming language look like?"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_7","text":"AST is an abstraction of the CST ( concrete syntax tree , or, parse tree). The concrete syntax tree is the tree resulting from the productions (in the grammar) used to parse the file. So your AST is basically derived from your grammar definition, but has for transformed Exp / | \\ / | \\ * Ident BinOp Ident into / \\ / | \\ \"x\" \"y\" / | \\ \"x\" * \"y\" All in all I think the example in your post looks fine. I would probably wrap the variable declarations in a varDeclList and the function declaration in a methDeclList , and the return statement in a stmtList . (See below.) One more or less \"real\" representation of an AST is described by Apple in his book \"Modern Compiler Implementation in Java\". (Resources can be found here .) Using those classes, your program would be represented as follows: Program ClassDeclList ClassDecl Identifier id: Person VarDeclList VarDecl type: String id: name VarDecl type: int id: age MethDeclList MethodDecl modifiers: public returnType: String id: toString Formals (empty) StmtList returnStmt Identifier id: name","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#a_8","text":"OP: Where can I see a real representation of an actual programming language to learn more? For your source text as a file Person.java: class Person { String name; int age; public String toString() { return \"name\"; } } what follows are both Concrete and Abstract Syntax Tree in an S-expression-style dump of the parser tree from our DMS Software Reengineering Toolkit , using its Java1.6 parser. All the apparant complexity is pretty much caused by the real complexity of the language (e.g., of Java itself). The CST clearly contains more stuff (139 nodes) than the AST (54 nodes). The AST drops everything that can be automatically inferred from the grammar, given the AST. This includes removing non-value-carrying leaves, unary productions, and compressing spines caused by left or right recursive grammar rules into explicit list nodes. A left paren signals a new subtree. Following the left paren is the name of the node type; @Java~Java1_.6 might seem unnecessary until you understand DMS can handle many languages at once, including langauges nested inside one another. The #nnnnnn is the memory address of the node. ^M means \"this node has M parents and is left off when M==1. Things inside [...] are the node value. A { M } means this list node has M list-children. Each node is stamped with position information. This is the Concrete Syntax tree (see further down for AST): (compilation_unit@Java~Java1_6=1#4885d00^0 Line 1 Column 1 File C:/temp/Person.java (type_declarations@Java~Java1_6=15#4885cc0 Line 1 Column 1 File C:/temp/Person.java (type_declarations@Java~Java1_6=16#4884d80 Line 1 Column 1 File C:/temp/Person.java)type_declarations (type_declaration@Java~Java1_6=17#4885ca0 Line 1 Column 1 File C:/temp/Person.java (type_class_modifiers@Java~Java1_6=77#4884dc0 Line 1 Column 1 File C:/temp/Person.java)type_class_modifiers (class_header@Java~Java1_6=89#4884ec0 Line 1 Column 1 File C:/temp/Person.java |('class'@Java~Java1_6=459#4884c60[Keyword:0] Line 1 Column 1 File C:/temp/Person.java)'class' |(IDENTIFIER@Java~Java1_6=447#4884e20[`Person'] Line 1 Column 7 File C:/temp/Person.java)IDENTIFIER |(type_parameters@Java~Java1_6=408#4884e80 Line 1 Column 14 File C:/temp/Person.java)type_parameters )class_header (class_body@Java~Java1_6=94#4885c80 Line 1 Column 14 File C:/temp/Person.java |('{'@Java~Java1_6=448#4884e60[Keyword:0] Line 1 Column 14 File C:/temp/Person.java)'{' |(class_body_declarations@Java~Java1_6=111#4885c60 Line 2 Column 5 File C:/temp/Person.java | (class_body_declarations@Java~Java1_6=111#4885380 Line 2 Column 5 File C:/temp/Person.java | (class_body_declarations@Java~Java1_6=110#4885400 Line 2 Column 5 File C:/temp/Person.java | (class_body_declaration@Java~Java1_6=118#4885360 Line 2 Column 5 File C:/temp/Person.java | |(field_declaration@Java~Java1_6=168#4885440 Line 2 Column 5 File C:/temp/Person.java | | (field_modifiers@Java~Java1_6=170#4884f40 Line 2 Column 5 File C:/temp/Person.java)field_modifiers | | (type@Java~Java1_6=191#48852c0 Line 2 Column 5 File C:/temp/Person.java | | (name@Java~Java1_6=406#48851e0 Line 2 Column 5 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#4884f20[`String'] Line 2 Column 5 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#4885160 Line 2 Column 12 File C:/temp/Person.java)type_arguments | | )name | | (brackets@Java~Java1_6=157#4885260 Line 2 Column 12 File C:/temp/Person.java)brackets | | )type | | (variable_declarator_list@Java~Java1_6=179#4884e00 Line 2 Column 12 File C:/temp/Person.java | | (variable_declarator@Java~Java1_6=181#4885300 Line 2 Column 12 File C:/temp/Person.java | | (variable_declarator_id@Java~Java1_6=167#4885320 Line 2 Column 12 File C:/temp/Person.java | | |(IDENTIFIER@Java~Java1_6=447#4885140[`name'] Line 2 Column 12 File C:/temp/Person.java)IDENTIFIER | | |(brackets@Java~Java1_6=157#4885040 Line 2 Column 16 File C:/temp/Person.java)brackets | | )variable_declarator_id | | )variable_declarator | | )variable_declarator_list | | (';'@Java~Java1_6=440#4885100[Keyword:0] Line 2 Column 16 File C:/temp/Person.java)';' | |)field_declaration | )class_body_declaration | )class_body_declarations | (class_body_declaration@Java~Java1_6=118#48852e0 Line 3 Column 5 File C:/temp/Person.java | (field_declaration@Java~Java1_6=168#4885480 Line 3 Column 5 File C:/temp/Person.java | |(field_modifiers@Java~Java1_6=170#4885340 Line 3 Column 5 File C:/temp/Person.java)field_modifiers | |(type@Java~Java1_6=192#4885220 Line 3 Column 5 File C:/temp/Person.java | | (primitive_type@Java~Java1_6=198#4885420 Line 3 Column 5 File C:/temp/Person.java | | ('int'@Java~Java1_6=479#48853e0[Keyword:0] Line 3 Column 5 File C:/temp/Person.java)'int' | | )primitive_type | | (brackets@Java~Java1_6=157#4885200 Line 3 Column 12 File C:/temp/Person.java)brackets | |)type | |(variable_declarator_list@Java~Java1_6=179#4885540 Line 3 Column 12 File C:/temp/Person.java | | (variable_declarator@Java~Java1_6=181#4885520 Line 3 Column 12 File C:/temp/Person.java | | (variable_declarator_id@Java~Java1_6=167#4885500 Line 3 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#4884fc0[`age'] Line 3 Column 12 File C:/temp/Person.java)IDENTIFIER | | (brackets@Java~Java1_6=157#48854e0 Line 3 Column 15 File C:/temp/Person.java)brackets | | )variable_declarator_id | | )variable_declarator | |)variable_declarator_list | |(';'@Java~Java1_6=440#48854c0[Keyword:0] Line 3 Column 15 File C:/temp/Person.java)';' | )field_declaration | )class_body_declaration | )class_body_declarations | (class_body_declaration@Java~Java1_6=117#4885c40 Line 4 Column 5 File C:/temp/Person.java | (method_declaration@Java~Java1_6=135#4885c00 Line 4 Column 5 File C:/temp/Person.java | (method_modifiers@Java~Java1_6=141#4885700 Line 4 Column 5 File C:/temp/Person.java | |(method_modifiers@Java~Java1_6=142#4884e40 Line 4 Column 5 File C:/temp/Person.java)method_modifiers | |(method_modifier@Java~Java1_6=147#48856a0 Line 4 Column 5 File C:/temp/Person.java | | ('public'@Java~Java1_6=453#48853a0[Keyword:0] Line 4 Column 5 File C:/temp/Person.java)'public' | |)method_modifier | )method_modifiers | (type_parameters@Java~Java1_6=408#4885740 Line 4 Column 12 File C:/temp/Person.java)type_parameters | (type@Java~Java1_6=191#4885900 Line 4 Column 12 File C:/temp/Person.java | |(name@Java~Java1_6=406#48852a0 Line 4 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#4885660[`String'] Line 4 Column 12 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#48851a0 Line 4 Column 19 File C:/temp/Person.java)type_arguments | |)name | |(brackets@Java~Java1_6=157#48858c0 Line 4 Column 19 File C:/temp/Person.java)brackets | )type | (IDENTIFIER@Java~Java1_6=447#48855c0[`toString'] Line 4 Column 19 File C:/temp/Person.java)IDENTIFIER | (parameters@Java~Java1_6=158#48858e0 Line 4 Column 27 File C:/temp/Person.java | |('('@Java~Java1_6=450#4885840[Keyword:0] Line 4 Column 27 File C:/temp/Person.java)'(' | |(')'@Java~Java1_6=451#4885620[Keyword:0] Line 4 Column 28 File C:/temp/Person.java)')' | )parameters | (brackets@Java~Java1_6=157#4885060 Line 5 Column 7 File C:/temp/Person.java)brackets | (block@Java~Java1_6=217#4885be0 Line 5 Column 7 File C:/temp/Person.java | |('{'@Java~Java1_6=448#48851c0[Keyword:0] Line 5 Column 7 File C:/temp/Person.java)'{' | |(statement_sequence@Java~Java1_6=218#4885ba0 Line 5 Column 9 File C:/temp/Person.java | | (statement_sequence_member@Java~Java1_6=223#4885b80 Line 5 Column 9 File C:/temp/Person.java | | (executable_statement@Java~Java1_6=243#4885b60 Line 5 Column 9 File C:/temp/Person.java | | ('return'@Java~Java1_6=491#4884f60[Keyword:0] Line 5 Column 9 File C:/temp/Person.java)'return' | | (expression@Java~Java1_6=332#4885ac0 Line 5 Column 16 File C:/temp/Person.java | | |(conditional_expression@Java~Java1_6=345#4885a60 Line 5 Column 16 File C:/temp/Person.java | | | (conditional_or_expression@Java~Java1_6=347#4885a20 Line 5 Column 16 File C:/temp/Person.java | | | (conditional_and_expression@Java~Java1_6=349#48859e0 Line 5 Column 16 File C:/temp/Person.java | | | (inclusive_or_expression@Java~Java1_6=351#48857e0 Line 5 Column 16 File C:/temp/Person.java | | | |(exclusive_or_expression@Java~Java1_6=353#48855a0 Line 5 Column 16 File C:/temp/Person.java | | | | (and_expression@Java~Java1_6=355#4885940 Line 5 Column 16 File C:/temp/Person.java | | | | (equality_expression@Java~Java1_6=357#4885880 Line 5 Column 16 File C:/temp/Person.java | | | | (relational_expression@Java~Java1_6=360#4885800 Line 5 Column 16 File C:/temp/Person.java | | | | |(shift_expression@Java~Java1_6=366#48856c0 Line 5 Column 16 File C:/temp/Person.java | | | | | (additive_expression@Java~Java1_6=370#4885180 Line 5 Column 16 File C:/temp/Person.java | | | | | (multiplicative_expression@Java~Java1_6=373#4885780 Line 5 Column 16 File C:/temp/Person.java | | | | | (unary_expression@Java~Java1_6=383#4885600 Line 5 Column 16 File C:/temp/Person.java | | | | | |(unary_expression_not_plus_minus@Java~Java1_6=389#4885680 Line 5 Column 16 File C:/temp/Person.java | | | | | | (literal@Java~Java1_6=390#4884f80 Line 5 Column 16 File C:/temp/Person.java | | | | | | (STRING@Java~Java1_6=536#4885120[`name'] Line 5 Column 16 File C:/temp/Person.java)STRING | | | | | | )literal | | | | | |)unary_expression_not_plus_minus | | | | | )unary_expression | | | | | )multiplicative_expression | | | | | )additive_expression | | | | |)shift_expression | | | | )relational_expression | | | | )equality_expression | | | | )and_expression | | | |)exclusive_or_expression | | | )inclusive_or_expression | | | )conditional_and_expression | | | )conditional_or_expression | | |)conditional_expression | | )expression | | (';'@Java~Java1_6=440#48856e0[Keyword:0] Line 5 Column 22 File C:/temp/Person.java)';' | | )executable_statement | | )statement_sequence_member | |)statement_sequence | |('}'@Java~Java1_6=449#4885b40[Keyword:0] Line 5 Column 28 File C:/temp/Person.java)'}' | )block | )method_declaration | )class_body_declaration |)class_body_declarations |('}'@Java~Java1_6=449#4885bc0[Keyword:0] Line 6 Column 1 File C:/temp/Person.java)'}' )class_body )type_declaration )type_declarations (optional_CONTROL_Z@Java~Java1_6=5#4885ce0 Line 7 Column 1 File C:/temp/Person.java)optional_CONTROL_Z )compilation_unit This is the AST (automatically generated by DMS from the CST): (compilation_unit@Java~Java1_6=1#486f900^0 Line 1 Column 1 File C:/temp/Person.java (type_declarations@Java~Java1_6=15#486f4c0 {1} Line 1 Column 1 File C:/temp/Person.java (type_declaration@Java~Java1_6=17#486f5e0 Line 1 Column 1 File C:/temp/Person.java (type_class_modifiers@Java~Java1_6=77#486eda0 Line 1 Column 1 File C:/temp/Person.java)type_class_modifiers (class_header@Java~Java1_6=89#486ee60 Line 1 Column 1 File C:/temp/Person.java |(IDENTIFIER@Java~Java1_6=447#486ede0[`Person'] Line 1 Column 7 File C:/temp/Person.java)IDENTIFIER |(type_parameters@Java~Java1_6=408#486ee20 Line 1 Column 14 File C:/temp/Person.java)type_parameters )class_header (class_body@Java~Java1_6=94#486f040 Line 1 Column 14 File C:/temp/Person.java |(class_body_declarations@Java~Java1_6=111#486ee40 {3} Line 2 Column 5 File C:/temp/Person.java | (class_body_declaration@Java~Java1_6=118#486f300 Line 2 Column 5 File C:/temp/Person.java | (field_declaration@Java~Java1_6=168#486f380 Line 2 Column 5 File C:/temp/Person.java | (field_modifiers@Java~Java1_6=170#486eec0 Line 2 Column 5 File C:/temp/Person.java)field_modifiers | (type@Java~Java1_6=191#486f240 Line 2 Column 5 File C:/temp/Person.java | |(name@Java~Java1_6=406#486f180 Line 2 Column 5 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486eea0[`String'] Line 2 Column 5 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#486f0e0 Line 2 Column 12 File C:/temp/Person.java)type_arguments | |)name | |(brackets@Java~Java1_6=157#486f200 Line 2 Column 12 File C:/temp/Person.java)brackets | )type | (variable_declarator@Java~Java1_6=181#486ef20 Line 2 Column 12 File C:/temp/Person.java | |(variable_declarator_id@Java~Java1_6=167#486efe0 Line 2 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486f0c0[`name'] Line 2 Column 12 File C:/temp/Person.java)IDENTIFIER | | (brackets@Java~Java1_6=157#486f060 Line 2 Column 16 File C:/temp/Person.java)brackets | |)variable_declarator_id | )variable_declarator | )field_declaration | )class_body_declaration | (class_body_declaration@Java~Java1_6=118#486f000 Line 3 Column 5 File C:/temp/Person.java | (field_declaration@Java~Java1_6=168#486f320 Line 3 Column 5 File C:/temp/Person.java | (field_modifiers@Java~Java1_6=170#486f2a0 Line 3 Column 5 File C:/temp/Person.java)field_modifiers | (type@Java~Java1_6=192#486eee0 Line 3 Column 5 File C:/temp/Person.java | |(primitive_type@Java~Java1_6=198#486ef60 Line 3 Column 5 File C:/temp/Person.java)primitive_type | |(brackets@Java~Java1_6=157#486ee00 Line 3 Column 12 File C:/temp/Person.java)brackets | )type | (variable_declarator@Java~Java1_6=181#486f2c0 Line 3 Column 12 File C:/temp/Person.java | |(variable_declarator_id@Java~Java1_6=167#486f3a0 Line 3 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486f120[`age'] Line 3 Column 12 File C:/temp/Person.java)IDENTIFIER | | (brackets@Java~Java1_6=157#486ef00 Line 3 Column 15 File C:/temp/Person.java)brackets | |)variable_declarator_id | )variable_declarator | )field_declaration | )class_body_declaration | (class_body_declaration@Java~Java1_6=117#486f7a0 Line 4 Column 5 File C:/temp/Person.java | (method_declaration@Java~Java1_6=135#486f480 Line 4 Column 5 File C:/temp/Person.java | (method_modifiers@Java~Java1_6=141#486f460 {1} Line 4 Column 5 File C:/temp/Person.java | |(method_modifier@Java~Java1_6=147#486f400 Line 4 Column 5 File C:/temp/Person.java)method_modifier | )method_modifiers | (type_parameters@Java~Java1_6=408#486f540 Line 4 Column 12 File C:/temp/Person.java)type_parameters | (type@Java~Java1_6=191#486f740 Line 4 Column 12 File C:/temp/Person.java | |(name@Java~Java1_6=406#486f620 Line 4 Column 12 File C:/temp/Person.java | | (IDENTIFIER@Java~Java1_6=447#486f080[`String'] Line 4 Column 12 File C:/temp/Person.java)IDENTIFIER | | (type_arguments@Java~Java1_6=407#486f640 Line 4 Column 19 File C:/temp/Person.java)type_arguments | |)name | |(brackets@Java~Java1_6=157#486f700 Line 4 Column 19 File C:/temp/Person.java)brackets | )type | (IDENTIFIER@Java~Java1_6=447#486f140[`toString'] Line 4 Column 19 File C:/temp/Person.java)IDENTIFIER | (parameters@Java~Java1_6=158#486f760 Line 4 Column 27 File C:/temp/Person.java)parameters | (brackets@Java~Java1_6=157#486f820 Line 5 Column 7 File C:/temp/Person.java)brackets | (block@Java~Java1_6=217#486f780 Line 5 Column 7 File C:/temp/Person.java | |(statement_sequence@Java~Java1_6=218#486f6e0 Line 5 Column 9 File C:/temp/Person.java | | (statement_sequence_member@Java~Java1_6=223#486f6c0 Line 5 Column 9 File C:/temp/Person.java | | (executable_statement@Java~Java1_6=243#486f6a0 Line 5 Column 9 File C:/temp/Person.java | | (unary_expression_not_plus_minus@Java~Java1_6=389#486f720 Line 5 Column 16 File C:/temp/Person.java | | |(literal@Java~Java1_6=390#486f280 Line 5 Column 16 File C:/temp/Person.java | | | (STRING@Java~Java1_6=536#486f160[`name'] Line 5 Column 16 File C:/temp/Person.java)STRING | | |)literal | | )unary_expression_not_plus_minus | | )executable_statement | | )statement_sequence_member | |)statement_sequence | )block | )method_declaration | )class_body_declaration |)class_body_declarations )class_body )type_declaration )type_declarations (optional_CONTROL_Z@Java~Java1_6=5#486f4e0 Line 7 Column 1 File C:/temp/Person.java)optional_CONTROL_Z )compilation_unit EDIT March 2015: Here's a link to some C++ AST examples Edit May 2015: DMS has long done Java 1.7 and 1.8, too.","title":"A"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#dms-software-reengineering-toolkit","text":"","title":"DMS Software Reengineering Toolkit"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#abstract-vs-concrete-syntax-trees","text":"","title":"Abstract vs. Concrete Syntax Trees"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-abstract-syntax-tree-VS-concrete-syntax-tree/#compiling-an-ast-back-to-source-code","text":"","title":"Compiling an AST back to source code"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/VS-expression-tree-VS-parse-tree/","text":"\u5728\u300aCompilers Principles, Techniques, & Tools Second Edition\u300b4.2.4 Parse Trees and Derivations\u8282\u4e2d\u6709\u8fd9\u6837\u7684\u63cf\u8ff0\uff1a A parse tree is a graphical representation of a derivation that filters out the order in which productions are applied to replace nonterminals. Each interior node of a parse tree represents the application of a production. The interior node is labeled with the nonterminal A in the head of the production; the children of the node are labeled, from left to right, by the symbols in the body of the production by which this A was replaced during the derivation. expression tree\u7684interior node\u662foperator\u3002 \u4e24\u8005\u4e4b\u95f4\u65e2\u5b58\u5728\u7740\u76f8\u540c\u70b9\u4e5f\u5b58\u5728\u7740\u4e0d\u540c\u70b9\u3002","title":"VS expression tree VS parse tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/parse-tree-TO-AST/","text":"Building a compiler for your own language: from the parse tree to the Abstract Syntax Tree #","title":"[Building a compiler for your own language: from the parse tree to the Abstract Syntax Tree](https://tomassetti.me/parse-tree-abstract-syntax-tree/)"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/parse-tree-TO-AST/#building-a-compiler-for-your-own-language-from-the-parse-tree-to-the-abstract-syntax-tree","text":"","title":"Building a compiler for your own language: from the parse tree to the Abstract Syntax Tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/parse-tree-VS-DFA/","text":"parse tree VS DFA # \u5728cpython\u7684 pgen \u4e2d\uff0c\u4f7f\u7528DFA\u6765\u8868\u793aproduction\u7684body\uff0c\u8fd9\u662f\u56e0\u4e3apython\u7684grammar\u6240\u4f7f\u7528\u7684\u662fEBNF\uff0c\u5176\u4e2d\u6269\u5c55\u4e86\u5bf9regular expression\u7684\u652f\u6301\u3002 \u5728 Natural Language Processing with Python \u7684 8. Analyzing Sentence Structure \u4e2d\u7ed9\u6211\u4eec\u6f14\u793a\u4e86\u5b9a\u4e49CFG\uff0c\u5e76\u6309\u7167\u8fd9\u4e2aCFG\u6765\u5bf9\u6587\u672c\u8fdb\u884c\u89e3\u6790\u5f97\u5230parse tree\u3002\u90a3\u5728nltk\u4e2d\u662f\u5982\u4f55\u6765\u8868\u793a\u5b83\u7684grammar\u7684\u5462\uff1fnltk\u7684parser\u662f\u5982\u4f55\u8fd0\u7528\u5b83\u7684grammar\u7684\u6765\u5b9e\u73b0parse\u7684\u5462\uff1f","title":"parse tree VS DFA"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/parse-tree-VS-DFA/#parse-tree-vs-dfa","text":"\u5728cpython\u7684 pgen \u4e2d\uff0c\u4f7f\u7528DFA\u6765\u8868\u793aproduction\u7684body\uff0c\u8fd9\u662f\u56e0\u4e3apython\u7684grammar\u6240\u4f7f\u7528\u7684\u662fEBNF\uff0c\u5176\u4e2d\u6269\u5c55\u4e86\u5bf9regular expression\u7684\u652f\u6301\u3002 \u5728 Natural Language Processing with Python \u7684 8. Analyzing Sentence Structure \u4e2d\u7ed9\u6211\u4eec\u6f14\u793a\u4e86\u5b9a\u4e49CFG\uff0c\u5e76\u6309\u7167\u8fd9\u4e2aCFG\u6765\u5bf9\u6587\u672c\u8fdb\u884c\u89e3\u6790\u5f97\u5230parse tree\u3002\u90a3\u5728nltk\u4e2d\u662f\u5982\u4f55\u6765\u8868\u793a\u5b83\u7684grammar\u7684\u5462\uff1fnltk\u7684parser\u662f\u5982\u4f55\u8fd0\u7528\u5b83\u7684grammar\u7684\u6765\u5b9e\u73b0parse\u7684\u5462\uff1f","title":"parse tree VS DFA"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/parse-tree-VS-syntax-tree/","text":"parse tree\u548csyntax tree\u5728\u672c\u4e66\u4e2d\u591a\u6b21\u51fa\u73b0\uff0c\u6709\u5fc5\u8981\u5bf9\u5b83\u4eec\u8fdb\u884c\u5bf9\u6bd4\u4e00\u4e0b\u3002 \u4ecb\u7ecdparse tree\u7684\u4e3b\u8981\u7ae0\u8282\u6709\uff1a 2.2.3 Parse Trees 2.4 Parsing 2.9 Summary of Chapter 2 Chapter 4 Syntax Analysis Chapter 5 Syntax-Directed Translation \u672c\u4e66\u4e2d\u533a\u5206parse tree\u548csyntax tree\u7684\u7ae0\u8282\u6709\uff1a 2.5.1 Abstract and Concrete Syntax Abstract syntax trees, or simply syntax trees , resemble parse trees to an extent. However, in the syntax tree , interior nodes represent programming constructs while in the parse tree , the interior nodes represent nonterminals . Many nonterminals of a grammar represent programming constructs, but others are \"helpers\" of one sort of another, such as those representing terms, factors, or other variations of expressions. In the syntax tree, these helpers typically are not needed and are hence dropped. To emphasize the contrast, a parse tree is sometimes called a concrete syntax tree , and the underlying grammar is called a concrete syntax for the language. \u4ecb\u7ecdsyntax tree\u7684\u7ae0\u8282\u6709\uff1a 2.1 Introduction 2.8.2 Construction of Syntax Trees 2.8.4 Three-Address Code 5.3.1 Construction of Syntax Trees 6.1 Variants of Syntax Trees","title":"parse tree VS syntax tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Abstract-syntax-tree/","text":"Abstract syntax tree Application in compilers Motivation Design Design patterns Usage Abstract syntax tree # In computer science , an abstract syntax tree ( AST ), or just syntax tree , is a tree representation of the abstract syntactic structure of source code written in a programming language . Each node of the tree denotes a construct occurring in the source code. NOTE : \u8868\u793a\u6e90\u4ee3\u7801\u7684\u62bd\u8c61\u8bed\u6cd5\u7ed3\u6784\uff0c\u6811\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u8868\u793a\u6e90\u4ee3\u7801\u4e2d\u7684\u4e00\u4e2aconstruct\u3002 The syntax is \"abstract\" in the sense that it does not represent every detail appearing in the real syntax , but rather just the structural or content-related details . For instance, grouping parentheses are implicit in the tree structure , so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. This distinguishes abstract syntax trees from concrete syntax trees , traditionally designated parse trees . Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis . SUMMARY : parse tree\u548cabstract syntax tree\u7684\u6784\u5efa Abstract syntax trees are also used in program analysis and program transformation systems. An abstract syntax tree for the following code for the Euclidean algorithm while b \u2260 0 if a > b a := a \u2212 b else b := b \u2212 a return a Application in compilers # Abstract syntax trees are data structures widely used in compilers to represent the structure of program code. An AST is usually the result of the syntax analysis phase of a compiler. It often serves as an intermediate representation of the program through several stages that the compiler requires, and has a strong impact on the final output of the compiler. Motivation # An AST has several properties that aid the further steps of the compilation process: An AST can be edited and enhanced with information such as properties and annotations for every element it contains. Such editing and annotation is impossible with the source code of a program, since it would imply changing it. Compared to the source code , an AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). An AST usually contains extra information about the program, due to the consecutive stages of analysis by the compiler. For example, it may store the position of each element in the source code, allowing the compiler to print useful error messages . ASTs are needed because of the inherent nature of programming languages and their documentation. Languages are often ambiguous by nature. In order to avoid this ambiguity, programming languages are often specified as a context-free grammar (CFG). However, there are often aspects of programming languages that a CFG can't express, but are part of the language and are documented in its specification. These are details that require a context to determine their validity and behaviour. For example, if a language allows new types to be declared, a CFG cannot predict the names of such types nor the way in which they should be used. Even if a language has a predefined set of types, enforcing proper usage usually requires some context. Another example is duck typing , where the type of an element can change depending on context. Operator overloading is yet another case where correct usage and final function are determined based on the context. Java provides an excellent example, where the '+' operator is both numerical addition and concatenation of strings. Although there are other data structures involved in the inner workings of a compiler, the AST performs a unique function. During the first stage, the syntax analysis stage, a compiler produces a parse tree. This parse tree can be used to perform almost all functions of a compiler by means of s yntax-directed translation . Although this method can lead to a more efficient compiler, it goes against the software engineering principles of writing and maintaining programs[ citation needed ]. Another advantage that the AST has over a parse tree is the size, particularly the smaller height of the AST and the smaller number of elements. Design # The design of an AST is often closely linked with the design of a compiler and its expected features. Core requirements include the following: Variable types must be preserved, as well as the location of each declaration in source code. The order of executable statements must be explicitly represented and well defined. Left and right components of binary operations must be stored and correctly identified. Identifiers and their assigned values must be stored for assignment statements. These requirements can be used to design the data structure for the AST. Some operations will always require two elements, such as the two terms for addition. However, some language constructs require an arbitrarily large number of children, such as argument lists passed to programs from the command shell . As a result, an AST used to represent code written in such a language has to also be flexible enough to allow for quick addition of an unknown quantity of children. To support compiler verification it should be possible to unparse an AST into source code form. The source code produced should be sufficiently similar to the original in appearance and identical in execution, upon recompilation. Design patterns # Due to the complexity of the requirements for an AST and the overall complexity of a compiler, it is beneficial to apply sound software development principles. One of these is to use proven design patterns to enhance modularity and ease of development. Different operations don't necessarily have different types, so it is important to have a sound node class hierarchy. This is crucial in the creation and the modification of the AST as the compiler progresses. Because the compiler traverses the tree several times to determine syntactic correctness, it is important to make traversing the tree a simple operation. The compiler executes a specific set of operations, depending on the type of each node, upon reaching it, so it often makes sense to use the visitor pattern . Usage # The AST is used intensively during semantic analysis , where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program. After verifying correctness, the AST serves as the base for code generation. The AST is often used to generate an intermediate representation (IR), sometimes called an intermediate language , for the code generation.","title":"wikipedia Abstract syntax tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Abstract-syntax-tree/#abstract-syntax-tree","text":"In computer science , an abstract syntax tree ( AST ), or just syntax tree , is a tree representation of the abstract syntactic structure of source code written in a programming language . Each node of the tree denotes a construct occurring in the source code. NOTE : \u8868\u793a\u6e90\u4ee3\u7801\u7684\u62bd\u8c61\u8bed\u6cd5\u7ed3\u6784\uff0c\u6811\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u8868\u793a\u6e90\u4ee3\u7801\u4e2d\u7684\u4e00\u4e2aconstruct\u3002 The syntax is \"abstract\" in the sense that it does not represent every detail appearing in the real syntax , but rather just the structural or content-related details . For instance, grouping parentheses are implicit in the tree structure , so these do not have to be represented as separate nodes. Likewise, a syntactic construct like an if-condition-then expression may be denoted by means of a single node with three branches. This distinguishes abstract syntax trees from concrete syntax trees , traditionally designated parse trees . Parse trees are typically built by a parser during the source code translation and compiling process. Once built, additional information is added to the AST by means of subsequent processing, e.g., contextual analysis . SUMMARY : parse tree\u548cabstract syntax tree\u7684\u6784\u5efa Abstract syntax trees are also used in program analysis and program transformation systems. An abstract syntax tree for the following code for the Euclidean algorithm while b \u2260 0 if a > b a := a \u2212 b else b := b \u2212 a return a","title":"Abstract syntax tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Abstract-syntax-tree/#application-in-compilers","text":"Abstract syntax trees are data structures widely used in compilers to represent the structure of program code. An AST is usually the result of the syntax analysis phase of a compiler. It often serves as an intermediate representation of the program through several stages that the compiler requires, and has a strong impact on the final output of the compiler.","title":"Application in compilers"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Abstract-syntax-tree/#motivation","text":"An AST has several properties that aid the further steps of the compilation process: An AST can be edited and enhanced with information such as properties and annotations for every element it contains. Such editing and annotation is impossible with the source code of a program, since it would imply changing it. Compared to the source code , an AST does not include inessential punctuation and delimiters (braces, semicolons, parentheses, etc.). An AST usually contains extra information about the program, due to the consecutive stages of analysis by the compiler. For example, it may store the position of each element in the source code, allowing the compiler to print useful error messages . ASTs are needed because of the inherent nature of programming languages and their documentation. Languages are often ambiguous by nature. In order to avoid this ambiguity, programming languages are often specified as a context-free grammar (CFG). However, there are often aspects of programming languages that a CFG can't express, but are part of the language and are documented in its specification. These are details that require a context to determine their validity and behaviour. For example, if a language allows new types to be declared, a CFG cannot predict the names of such types nor the way in which they should be used. Even if a language has a predefined set of types, enforcing proper usage usually requires some context. Another example is duck typing , where the type of an element can change depending on context. Operator overloading is yet another case where correct usage and final function are determined based on the context. Java provides an excellent example, where the '+' operator is both numerical addition and concatenation of strings. Although there are other data structures involved in the inner workings of a compiler, the AST performs a unique function. During the first stage, the syntax analysis stage, a compiler produces a parse tree. This parse tree can be used to perform almost all functions of a compiler by means of s yntax-directed translation . Although this method can lead to a more efficient compiler, it goes against the software engineering principles of writing and maintaining programs[ citation needed ]. Another advantage that the AST has over a parse tree is the size, particularly the smaller height of the AST and the smaller number of elements.","title":"Motivation"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Abstract-syntax-tree/#design","text":"The design of an AST is often closely linked with the design of a compiler and its expected features. Core requirements include the following: Variable types must be preserved, as well as the location of each declaration in source code. The order of executable statements must be explicitly represented and well defined. Left and right components of binary operations must be stored and correctly identified. Identifiers and their assigned values must be stored for assignment statements. These requirements can be used to design the data structure for the AST. Some operations will always require two elements, such as the two terms for addition. However, some language constructs require an arbitrarily large number of children, such as argument lists passed to programs from the command shell . As a result, an AST used to represent code written in such a language has to also be flexible enough to allow for quick addition of an unknown quantity of children. To support compiler verification it should be possible to unparse an AST into source code form. The source code produced should be sufficiently similar to the original in appearance and identical in execution, upon recompilation.","title":"Design"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Abstract-syntax-tree/#design-patterns","text":"Due to the complexity of the requirements for an AST and the overall complexity of a compiler, it is beneficial to apply sound software development principles. One of these is to use proven design patterns to enhance modularity and ease of development. Different operations don't necessarily have different types, so it is important to have a sound node class hierarchy. This is crucial in the creation and the modification of the AST as the compiler progresses. Because the compiler traverses the tree several times to determine syntactic correctness, it is important to make traversing the tree a simple operation. The compiler executes a specific set of operations, depending on the type of each node, upon reaching it, so it often makes sense to use the visitor pattern .","title":"Design patterns"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Abstract-syntax-tree/#usage","text":"The AST is used intensively during semantic analysis , where the compiler checks for correct usage of the elements of the program and the language. The compiler also generates symbol tables based on the AST during semantic analysis. A complete traversal of the tree allows verification of the correctness of the program. After verifying correctness, the AST serves as the base for code generation. The AST is often used to generate an intermediate representation (IR), sometimes called an intermediate language , for the code generation.","title":"Usage"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Parse-tree/","text":"Parse tree Constituency-based parse trees Parse tree # A parse tree or parsing tree [ 1] or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar . The term parse tree itself is used primarily in computational linguistics ; in theoretical syntax, the term syntax tree is more common. Parse trees concretely[ clarification needed ] reflect the syntax of the input language, making them distinct from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, parse trees do not use distinct symbol shapes for different types of constituents \uff08\u6210\u5206\uff09. Parse trees are usually constructed based on either the constituency relation of constituency grammars ( phrase structure grammars ) or the dependency relation of dependency grammars . Parse trees may be generated for sentences in natural languages (see natural language processing ), as well as during processing of computer languages, such as programming languages .[ citation needed ] A related concept is that of phrase marker or P-marker , as used in transformational generative grammar . A phrase marker is a linguistic expression marked as to its phrase structure. This may be presented in the form of a tree, or as a bracketed expression. Phrase markers are generated by applying phrase structure rules , and themselves are subject to further transformational rules.[ 2] A set of possible parse trees for a syntactically ambiguous sentence is called a \"parse forest.\"[ 3] Constituency-based parse trees #","title":"wikipedia Parse tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Parse-tree/#parse-tree","text":"A parse tree or parsing tree [ 1] or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar . The term parse tree itself is used primarily in computational linguistics ; in theoretical syntax, the term syntax tree is more common. Parse trees concretely[ clarification needed ] reflect the syntax of the input language, making them distinct from the abstract syntax trees used in computer programming. Unlike Reed-Kellogg sentence diagrams used for teaching grammar, parse trees do not use distinct symbol shapes for different types of constituents \uff08\u6210\u5206\uff09. Parse trees are usually constructed based on either the constituency relation of constituency grammars ( phrase structure grammars ) or the dependency relation of dependency grammars . Parse trees may be generated for sentences in natural languages (see natural language processing ), as well as during processing of computer languages, such as programming languages .[ citation needed ] A related concept is that of phrase marker or P-marker , as used in transformational generative grammar . A phrase marker is a linguistic expression marked as to its phrase structure. This may be presented in the form of a tree, or as a bracketed expression. Phrase markers are generated by applying phrase structure rules , and themselves are subject to further transformational rules.[ 2] A set of possible parse trees for a syntactically ambiguous sentence is called a \"parse forest.\"[ 3]","title":"Parse tree"},{"location":"parse-tree-VS-syntax-tree-VS-DFA/wikipedia-Parse-tree/#constituency-based-parse-trees","text":"","title":"Constituency-based parse trees"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/","text":"Basic block Definition Basic Blocks and Flow Graphs | Examples Basic Blocks Example Of Basic Block Example Of Not A Basic Block Partitioning Intermediate Code Into Basic Blocks Rule-01: Determining Leaders Rule-02: Determining Basic Blocks Flow Graphs PRACTICE PROBLEMS BASED ON BASIC BLOCKS & FLOW GRAPHS Problem-01: Solution Problem-02: Solution Basic block # In compiler construction , a basic block is a straight-line code sequence with no branches in except to the entry and no branches out except at the exit.[ 1] [ 2] This restricted form makes a basic block highly amenable to analysis.[ 3] Compilers usually decompose programs into their basic blocks as a first step in the analysis process. Basic blocks form the vertices or nodes in a control flow graph . Definition # The code in a basic block has: One entry point , meaning no code within it is the destination of a jump instruction anywhere in the program. One exit point, meaning only the last instruction can cause the program to begin executing code in a different basic block. Under these circumstances, whenever the first instruction in a basic block is executed, the rest of the instructions are necessarily executed exactly once, in order.[ 4] [ 5] The code may be source code , assembly code or some other sequence of instructions. More formally, a sequence of instructions forms a basic block if: The instruction in each position dominates , or always executes before, all those in later positions. No other instruction executes between two instructions in the sequence. This definition is more general than the intuitive one in some ways. For example, it allows unconditional jumps to labels not targeted by other jumps. This definition embodies the properties that make basic blocks easy to work with when constructing an algorithm. The blocks to which control may transfer after reaching the end of a block are called that block's successors , while the blocks from which control may have come when entering a block are called that block's predecessors . The start of a basic block may be jumped to from more than one location. Basic Blocks and Flow Graphs | Examples # Basic Blocks # Basic block is a set of statements that always executes in a sequence one after the other. The characteristics of basic blocks are They do not contain any kind of jump statements in them. There is no possibility of branching or getting halt in the middle. All the statements execute in the same order they appear. They do not lose lose the flow control of the program. NOTE: \u6709\u70b9\u539f\u5b50\u7684\u542b\u4e49 Example Of Basic Block # Three Address Code for the expression a = b + c + d is- Here, All the statements execute in a sequence one after the other. Thus, they form a basic block. Example Of Not A Basic Block # Three Address Code for the expression If A<B then 1 else 0 is- Here, The statements do not execute in a sequence one after the other. Thus, they do not form a basic block. Partitioning Intermediate Code Into Basic Blocks # Any given code can be partitioned into basic blocks using the following rules Rule-01: Determining Leaders # Following statements of the code are called as Leaders First statement of the code. Statement that is a target of the conditional or unconditional goto statement. Statement that appears immediately after a goto statement. Rule-02: Determining Basic Blocks # All the statements that follow the leader (including the leader ) till the next leader appears form one basic block. The first statement of the code is called as the first leader . The block containing the first leader is called as Initial block . Flow Graphs # A flow graph is a directed graph with flow control information added to the basic blocks. The basic blocks serve as nodes of the flow graph. There is a directed edge from block B1 to block B2 if B2 appears immediately after B1 in the code. PRACTICE PROBLEMS BASED ON BASIC BLOCKS & FLOW GRAPHS # Problem-01: # Compute the basic blocks for the given three address statements (1) PROD = 0 (2) I = 1 (3) T2 = addr(A) \u2013 4 (4) T4 = addr(B) \u2013 4 (5) T1 = 4 x I (6) T3 = T2[T1] (7) T5 = T4[T1] (8) T6 = T3 x T5 (9) PROD = PROD + T6 (10) I = I + 1 (11) IF I <=20 GOTO (5) Solution # We have- PROD = 0 is a leader since first statement of the code is a leader. T1 = 4 x I is a leader since target of the conditional goto statement is a leader. Now, the given code can be partitioned into two basic blocks as- Problem-02: # Draw a flow graph for the three address statements given in problem-01. Solution # Firstly, we compute the basic blocks (already done above). Secondly, we assign the flow control information. The required flow graph is-","title":"wikipedia Basic block"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#basic-block","text":"In compiler construction , a basic block is a straight-line code sequence with no branches in except to the entry and no branches out except at the exit.[ 1] [ 2] This restricted form makes a basic block highly amenable to analysis.[ 3] Compilers usually decompose programs into their basic blocks as a first step in the analysis process. Basic blocks form the vertices or nodes in a control flow graph .","title":"Basic block"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#definition","text":"The code in a basic block has: One entry point , meaning no code within it is the destination of a jump instruction anywhere in the program. One exit point, meaning only the last instruction can cause the program to begin executing code in a different basic block. Under these circumstances, whenever the first instruction in a basic block is executed, the rest of the instructions are necessarily executed exactly once, in order.[ 4] [ 5] The code may be source code , assembly code or some other sequence of instructions. More formally, a sequence of instructions forms a basic block if: The instruction in each position dominates , or always executes before, all those in later positions. No other instruction executes between two instructions in the sequence. This definition is more general than the intuitive one in some ways. For example, it allows unconditional jumps to labels not targeted by other jumps. This definition embodies the properties that make basic blocks easy to work with when constructing an algorithm. The blocks to which control may transfer after reaching the end of a block are called that block's successors , while the blocks from which control may have come when entering a block are called that block's predecessors . The start of a basic block may be jumped to from more than one location.","title":"Definition"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#basic-blocks-and-flow-graphs-examples","text":"","title":"Basic Blocks and Flow Graphs | Examples"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#basic-blocks","text":"Basic block is a set of statements that always executes in a sequence one after the other. The characteristics of basic blocks are They do not contain any kind of jump statements in them. There is no possibility of branching or getting halt in the middle. All the statements execute in the same order they appear. They do not lose lose the flow control of the program. NOTE: \u6709\u70b9\u539f\u5b50\u7684\u542b\u4e49","title":"Basic Blocks"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#example-of-basic-block","text":"Three Address Code for the expression a = b + c + d is- Here, All the statements execute in a sequence one after the other. Thus, they form a basic block.","title":"Example Of Basic Block"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#example-of-not-a-basic-block","text":"Three Address Code for the expression If A<B then 1 else 0 is- Here, The statements do not execute in a sequence one after the other. Thus, they do not form a basic block.","title":"Example Of Not A Basic Block"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#partitioning-intermediate-code-into-basic-blocks","text":"Any given code can be partitioned into basic blocks using the following rules","title":"Partitioning Intermediate Code Into Basic Blocks"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#rule-01-determining-leaders","text":"Following statements of the code are called as Leaders First statement of the code. Statement that is a target of the conditional or unconditional goto statement. Statement that appears immediately after a goto statement.","title":"Rule-01: Determining Leaders"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#rule-02-determining-basic-blocks","text":"All the statements that follow the leader (including the leader ) till the next leader appears form one basic block. The first statement of the code is called as the first leader . The block containing the first leader is called as Initial block .","title":"Rule-02: Determining Basic Blocks"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#flow-graphs","text":"A flow graph is a directed graph with flow control information added to the basic blocks. The basic blocks serve as nodes of the flow graph. There is a directed edge from block B1 to block B2 if B2 appears immediately after B1 in the code.","title":"Flow Graphs"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#practice-problems-based-on-basic-blocks-flow-graphs","text":"","title":"PRACTICE PROBLEMS BASED ON BASIC BLOCKS &amp; FLOW GRAPHS"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#problem-01","text":"Compute the basic blocks for the given three address statements (1) PROD = 0 (2) I = 1 (3) T2 = addr(A) \u2013 4 (4) T4 = addr(B) \u2013 4 (5) T1 = 4 x I (6) T3 = T2[T1] (7) T5 = T4[T1] (8) T6 = T3 x T5 (9) PROD = PROD + T6 (10) I = I + 1 (11) IF I <=20 GOTO (5)","title":"Problem-01:"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#solution","text":"We have- PROD = 0 is a leader since first statement of the code is a leader. T1 = 4 x I is a leader since target of the conditional goto statement is a leader. Now, the given code can be partitioned into two basic blocks as-","title":"Solution"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#problem-02","text":"Draw a flow graph for the three address statements given in problem-01.","title":"Problem-02:"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Basic-block/#solution_1","text":"Firstly, we compute the basic blocks (already done above). Secondly, we assign the flow control information. The required flow graph is-","title":"Solution"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-analysis/","text":"Control flow analysis Control flow analysis #","title":"wikipedia Control flow analysis"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-analysis/#control-flow-analysis","text":"","title":"Control flow analysis"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/","text":"Control-flow graph Definition Reachability Domination relationship Special edges Loop management Reducibility Loop connectedness Control-flow graph # In computer science , a control-flow graph ( CFG ) is a representation , using graph notation, of all paths that might be traversed through a program during its execution . The control-flow graph is due to Frances E. Allen ,[ 1] who notes that Reese T. Prosser used boolean connectivity matrices for flow analysis before.[ 2] The CFG is essential to many compiler optimizations and static-analysis tools. Definition # In a control-flow graph each node in the graph represents a basic block , i.e. a straight-line piece of code without any jumps or jump targets ; jump targets start a block, and jumps end a block. Directed edges are used to represent jumps in the control flow . There are, in most presentations, two specially designated blocks: the entry block , through which control enters into the flow graph, and the exit block , through which all control flow leaves.[ 3] NOTE: \u8868\u793ajump\u7684\u5173\u952e\u5b57\uff1a break goto Because of its construction procedure, in a CFG, every edge A\u2192B has the property that: outdegree (A) > 1 or indegree(B) > 1 (or both). The CFG can thus be obtained, at least conceptually, by starting from the program's (full) flow graph\u2014i.e. the graph in which every node represents an individual instruction\u2014and performing an edge contraction for every edge that falsifies the predicate above, i.e. contracting every edge whose source has a single exit and whose destination has a single entry. This contraction-based algorithm is of no practical importance, except as a visualization aid for understanding the CFG construction, because the CFG can be more efficiently constructed directly from the program by scanning it for basic blocks . Reachability # Main article: Reachability Reachability is a graph property useful in optimization. If a subgraph is not connected from the subgraph containing the entry block, that subgraph is unreachable during any execution, and so is unreachable code ; under normal conditions it can be safely removed. If the exit block is unreachable from the entry block, an infinite loop may exist. Not all infinite loops are detectable, see Halting problem . A halting order may also exist there. Unreachable code and infinite loops are possible even if the programmer does not explicitly code them: optimizations like constant propagation and constant folding followed by jump threading can collapse multiple basic blocks into one, cause edges to be removed from a CFG, etc., thus possibly disconnecting parts of the graph. Domination relationship # Main article: Dominator (graph theory) A block M dominates a block N if every path from the entry that reaches block N has to pass through block M. The entry block dominates all blocks. In the reverse direction, block M postdominates block N if every path from N to the exit has to pass through block M. The exit block postdominates all blocks. It is said that a block M immediately dominates block N if M dominates N, and there is no intervening block P such that M dominates P and P dominates N. In other words, M is the last dominator on all paths from entry to N. Each block has a unique immediate dominator. Similarly, there is a notion of immediate postdominator , analogous to immediate dominator . The dominator tree is an ancillary data structure depicting the dominator relationships. There is an arc from Block M to Block N if M is an immediate dominator of N. This graph is a tree, since each block has a unique immediate dominator. This tree is rooted at the entry block. The dominator tree can be calculated efficiently using Lengauer\u2013Tarjan's algorithm . A postdominator tree is analogous to the dominator tree . This tree is rooted at the exit block. Special edges # A back edge is an edge that points to a block that has already been met during a depth-first ( DFS ) traversal of the graph. Back edges are typical of loops. A critical edge is an edge which is neither the only edge leaving its source block, nor the only edge entering its destination block. These edges must be split : a new block must be created in the middle of the edge, in order to insert computations on the edge without affecting any other edges. An abnormal edge is an edge whose destination is unknown. Exception handling constructs can produce them. These edges tend to inhibit optimization. An impossible edge (also known as a fake edge ) is an edge which has been added to the graph solely to preserve the property that the exit block postdominates all blocks. It cannot ever be traversed. Loop management # A loop header (sometimes called the entry point of the loop) is a dominator that is the target of a loop-forming back edge. The loop header dominates all blocks in the loop body. A block may be a loop header for more than one loop. A loop may have multiple entry points, in which case it has no \"loop header\". Suppose block M is a dominator with several incoming edges, some of them being back edges (so M is a loop header). It is advantageous to several optimization passes to break M up into two blocks Mpre and Mloop. The contents of M and back edges are moved to Mloop, the rest of the edges are moved to point into Mpre, and a new edge from Mpre to Mloop is inserted (so that Mpre is the immediate dominator of Mloop). In the beginning, Mpre would be empty, but passes like loop-invariant code motion could populate it. Mpre is called the loop pre-header , and Mloop would be the loop header. Reducibility # A reducible CFG is one with edges that can be partitioned into two disjoint sets: forward edges, and back edges, such that:[ 5] Forward edges form a directed acyclic graph with all nodes reachable from the entry node. For all back edges (A, B), node B dominates node A. Structured programming languages are often designed such that all CFGs they produce are reducible, and common structured programming statements such as IF, FOR, WHILE, BREAK, and CONTINUE produce reducible graphs. To produce irreducible graphs, statements such as GOTO are needed. Irreducible graphs may also be produced by some compiler optimizations. Loop connectedness # The loop connectedness of a CFG is defined with respect to a given depth-first search tree (DFST) of the CFG. This DFST should be rooted at the start node and cover every node of the CFG. Edges in the CFG which run from a node to one of its DFST ancestors (including itself) are called back edges. The loop connectedness is the largest number of back edges found in any cycle-free path of the CFG. In a reducible CFG, the loop connectedness is independent of the DFST chosen.[ 6] [ 7] Loop connectedness has been used to reason about the time complexity of data-flow analysis .[ 6]","title":"wikipedia Control flow graph"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#control-flow-graph","text":"In computer science , a control-flow graph ( CFG ) is a representation , using graph notation, of all paths that might be traversed through a program during its execution . The control-flow graph is due to Frances E. Allen ,[ 1] who notes that Reese T. Prosser used boolean connectivity matrices for flow analysis before.[ 2] The CFG is essential to many compiler optimizations and static-analysis tools.","title":"Control-flow graph"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#definition","text":"In a control-flow graph each node in the graph represents a basic block , i.e. a straight-line piece of code without any jumps or jump targets ; jump targets start a block, and jumps end a block. Directed edges are used to represent jumps in the control flow . There are, in most presentations, two specially designated blocks: the entry block , through which control enters into the flow graph, and the exit block , through which all control flow leaves.[ 3] NOTE: \u8868\u793ajump\u7684\u5173\u952e\u5b57\uff1a break goto Because of its construction procedure, in a CFG, every edge A\u2192B has the property that: outdegree (A) > 1 or indegree(B) > 1 (or both). The CFG can thus be obtained, at least conceptually, by starting from the program's (full) flow graph\u2014i.e. the graph in which every node represents an individual instruction\u2014and performing an edge contraction for every edge that falsifies the predicate above, i.e. contracting every edge whose source has a single exit and whose destination has a single entry. This contraction-based algorithm is of no practical importance, except as a visualization aid for understanding the CFG construction, because the CFG can be more efficiently constructed directly from the program by scanning it for basic blocks .","title":"Definition"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#reachability","text":"Main article: Reachability Reachability is a graph property useful in optimization. If a subgraph is not connected from the subgraph containing the entry block, that subgraph is unreachable during any execution, and so is unreachable code ; under normal conditions it can be safely removed. If the exit block is unreachable from the entry block, an infinite loop may exist. Not all infinite loops are detectable, see Halting problem . A halting order may also exist there. Unreachable code and infinite loops are possible even if the programmer does not explicitly code them: optimizations like constant propagation and constant folding followed by jump threading can collapse multiple basic blocks into one, cause edges to be removed from a CFG, etc., thus possibly disconnecting parts of the graph.","title":"Reachability"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#domination-relationship","text":"Main article: Dominator (graph theory) A block M dominates a block N if every path from the entry that reaches block N has to pass through block M. The entry block dominates all blocks. In the reverse direction, block M postdominates block N if every path from N to the exit has to pass through block M. The exit block postdominates all blocks. It is said that a block M immediately dominates block N if M dominates N, and there is no intervening block P such that M dominates P and P dominates N. In other words, M is the last dominator on all paths from entry to N. Each block has a unique immediate dominator. Similarly, there is a notion of immediate postdominator , analogous to immediate dominator . The dominator tree is an ancillary data structure depicting the dominator relationships. There is an arc from Block M to Block N if M is an immediate dominator of N. This graph is a tree, since each block has a unique immediate dominator. This tree is rooted at the entry block. The dominator tree can be calculated efficiently using Lengauer\u2013Tarjan's algorithm . A postdominator tree is analogous to the dominator tree . This tree is rooted at the exit block.","title":"Domination relationship"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#special-edges","text":"A back edge is an edge that points to a block that has already been met during a depth-first ( DFS ) traversal of the graph. Back edges are typical of loops. A critical edge is an edge which is neither the only edge leaving its source block, nor the only edge entering its destination block. These edges must be split : a new block must be created in the middle of the edge, in order to insert computations on the edge without affecting any other edges. An abnormal edge is an edge whose destination is unknown. Exception handling constructs can produce them. These edges tend to inhibit optimization. An impossible edge (also known as a fake edge ) is an edge which has been added to the graph solely to preserve the property that the exit block postdominates all blocks. It cannot ever be traversed.","title":"Special edges"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#loop-management","text":"A loop header (sometimes called the entry point of the loop) is a dominator that is the target of a loop-forming back edge. The loop header dominates all blocks in the loop body. A block may be a loop header for more than one loop. A loop may have multiple entry points, in which case it has no \"loop header\". Suppose block M is a dominator with several incoming edges, some of them being back edges (so M is a loop header). It is advantageous to several optimization passes to break M up into two blocks Mpre and Mloop. The contents of M and back edges are moved to Mloop, the rest of the edges are moved to point into Mpre, and a new edge from Mpre to Mloop is inserted (so that Mpre is the immediate dominator of Mloop). In the beginning, Mpre would be empty, but passes like loop-invariant code motion could populate it. Mpre is called the loop pre-header , and Mloop would be the loop header.","title":"Loop management"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#reducibility","text":"A reducible CFG is one with edges that can be partitioned into two disjoint sets: forward edges, and back edges, such that:[ 5] Forward edges form a directed acyclic graph with all nodes reachable from the entry node. For all back edges (A, B), node B dominates node A. Structured programming languages are often designed such that all CFGs they produce are reducible, and common structured programming statements such as IF, FOR, WHILE, BREAK, and CONTINUE produce reducible graphs. To produce irreducible graphs, statements such as GOTO are needed. Irreducible graphs may also be produced by some compiler optimizations.","title":"Reducibility"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Control-flow-graph/#loop-connectedness","text":"The loop connectedness of a CFG is defined with respect to a given depth-first search tree (DFST) of the CFG. This DFST should be rooted at the start node and cover every node of the CFG. Edges in the CFG which run from a node to one of its DFST ancestors (including itself) are called back edges. The loop connectedness is the largest number of back edges found in any cycle-free path of the CFG. In a reducible CFG, the loop connectedness is independent of the DFST chosen.[ 6] [ 7] Loop connectedness has been used to reason about the time complexity of data-flow analysis .[ 6]","title":"Loop connectedness"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Data-flow-analysis/","text":"Data-flow analysis #","title":"[Data-flow analysis](https://en.wikipedia.org/wiki/Data-flow_analysis)"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Data-flow-analysis/#data-flow-analysis","text":"","title":"Data-flow analysis"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Optimizing-compiler/","text":"Optimizing compiler Optimizing compiler #","title":"wikipedia Optimizing compiler"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Optimizing-compiler/#optimizing-compiler","text":"","title":"Optimizing compiler"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Static-single-assignment-form/","text":"Static single assignment form #","title":"[Static single assignment form](https://en.wikipedia.org/wiki/Static_single_assignment_form)"},{"location":"wikipedia-Optimizing-compiler/wikipedia-Static-single-assignment-form/#static-single-assignment-form","text":"","title":"Static single assignment form"}]}