{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"compiler-principle \uf0c1 Compilers Principles, Techniques and Tools Second Edition is really an amazing book, as mentioned in Chapter 1 Introduction This book is is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages , machine architecture , language theory , algorithms , and software engineering . As mentioned in the paragraph above, the study of compiler is an interdiscipline invoving computer science ( programming languages , machine architecture , algorithms , and software engineering ) and language theory , so the book may be hard to understand if lacking experience in programming, grasp of common data structure(especially tree ) and algorithms(especially recursion ). The language theory in the paragraph above mainly refers to the formal language theory , I have create a repository Automata-and-formal-language to record my knowledge of formal language theory and automata theory . A basic understanding of these theories will help understand the book. The data structure mentioned in the book can be found in my repository data-structure . The algorithm mentioned in the book can be found in my repository algorithm .","title":"Home"},{"location":"#compiler-principle","text":"Compilers Principles, Techniques and Tools Second Edition is really an amazing book, as mentioned in Chapter 1 Introduction This book is is about how to design and implement compilers. We shall discover that a few basic ideas can be used to construct translators for a wide variety of languages and machines. Besides compilers, the principles and techniques for compiler design are applicable to so many other domains that they are likely to be reused many times in the career of a computer scientist. The study of compiler writing touches up on programming languages , machine architecture , language theory , algorithms , and software engineering . As mentioned in the paragraph above, the study of compiler is an interdiscipline invoving computer science ( programming languages , machine architecture , algorithms , and software engineering ) and language theory , so the book may be hard to understand if lacking experience in programming, grasp of common data structure(especially tree ) and algorithms(especially recursion ). The language theory in the paragraph above mainly refers to the formal language theory , I have create a repository Automata-and-formal-language to record my knowledge of formal language theory and automata theory . A basic understanding of these theories will help understand the book. The data structure mentioned in the book can be found in my repository data-structure . The algorithm mentioned in the book can be found in my repository algorithm .","title":"compiler-principle"},{"location":"Chapter1-Introduction/1.2The-Structure-of-a-Compiler/","text":"","title":"1.2The-Structure-of-a-Compiler"},{"location":"Chapter1-Introduction/introduction/","text":"Introduction From formal language From the perspectives of a compiler front end Introduction \uf0c1 How long have you been programming? Have you ever wondered what a programming language is? There are many ways to answer this question, here I will try to answer this question from some different perspectives. From formal language \uf0c1 A programming language is a formal language , so it is equipped with alphabet words and lexical grammar to defining the syntax of tokens . Syntax and formal grammar to describe its syntax semantics From the perspectives of a compiler front end \uf0c1 alphabet Lexical grammar Lexical analysis regular expressions Finite-state machine Scanner generators, for example Lex (software) Syntax (programming languages) syntax analysis Context-free grammar parse tree , abstract syntax tree Parser generators, for example Yacc Semantics (computer science) Semantic analysis (compilers) Lexical grammar , syntax and semantics define a programming language,.Only when specifying lexical grammar , syntax and semantics can the compiler understand what programmer have written and translate the language correctly.","title":"introduction"},{"location":"Chapter1-Introduction/introduction/#introduction","text":"How long have you been programming? Have you ever wondered what a programming language is? There are many ways to answer this question, here I will try to answer this question from some different perspectives.","title":"Introduction"},{"location":"Chapter1-Introduction/introduction/#from-formal-language","text":"A programming language is a formal language , so it is equipped with alphabet words and lexical grammar to defining the syntax of tokens . Syntax and formal grammar to describe its syntax semantics","title":"From  formal language"},{"location":"Chapter1-Introduction/introduction/#from-the-perspectives-of-a-compiler-front-end","text":"alphabet Lexical grammar Lexical analysis regular expressions Finite-state machine Scanner generators, for example Lex (software) Syntax (programming languages) syntax analysis Context-free grammar parse tree , abstract syntax tree Parser generators, for example Yacc Semantics (computer science) Semantic analysis (compilers) Lexical grammar , syntax and semantics define a programming language,.Only when specifying lexical grammar , syntax and semantics can the compiler understand what programmer have written and translate the language correctly.","title":"From the perspectives of a compiler front end"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.1Introduction/","text":"2.1 Introduction 2.1 Introduction \uf0c1 For compiler, what can context-free grammar do? Specify syntax as described in Section 2.2 Help guide the translation of programs, a grammar-oriented compiling technique known as syntax-directed translation introduced in Section 2.4 Parsing or syntax analysis is introduced in Section 2.4.","title":"2.1Introduction"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.1Introduction/#21-introduction","text":"For compiler, what can context-free grammar do? Specify syntax as described in Section 2.2 Help guide the translation of programs, a grammar-oriented compiling technique known as syntax-directed translation introduced in Section 2.4 Parsing or syntax analysis is introduced in Section 2.4.","title":"2.1 Introduction"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/","text":"2.2 Syntax Definition 2.2.1 Definition of Grammars 2.2.2 Derivations 2.2.3 Parse Trees 2.2.4 Ambiguity 2.2.5 Associativity of Operators 2.2.6 Precedence of Operators Generalizing the Expression Grammar of Example 2.6 2.2 Syntax Definition \uf0c1 In this section, we introduce a notation \u2014 the \" context-free grammar ,\" or \"grammar\" for short \u2014 that is used to specify the syntax of a language. Grammars will be used throughout this book to organize compiler front ends . TIPS: Wikipedia has a good explanation of context-free grammar TIPS: It is a convention that there is a section called language reference or language specification to describe grammar of the programming language in the official document of the programming language , such as The Python Language Reference use extended BNF notation to describe syntax of Python language. The Java\u00ae Language Specification 2.2.1 Definition of Grammars \uf0c1 A context-free grammar has four components: 1. A set of terminal symbols , sometimes referred to as \"tokens.\" The terminals are the elementary symbols of the language defined by the grammar. 2. A set of nonterminals , sometimes called \"syntactic variables.\" Each nonterminal represents a set of strings of terminals, in a manner we shall describe. 3. A set of productions , where each production consists of a nonterminal, called the head or left side of the production, an arrow, and a sequence of terminals and/or nonterminals, called the body or right side of the production. 4. A designation of one of the nonterminals as the start symbol. Example 2.1 : $$ \\begin{align} list \\to list + digit \\tag {2.1}\\ list \\to list - digit \\tag {2.2}\\ list \\to digit \\tag {2.3}\\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\tag {2.4} \\end{align} $$ TIPS: Programmer should be sensitive of recursion , it is obvious that Context-free grammar above has the property of recursion , a grammar is informally called a recursive grammar if it contains production rules that are recursive . 2.2.2 Derivations \uf0c1 TIPS: In addition to using the word derivation /derive, word expand is also used a lot. I think word expand can visually reflect the process of expanding an inner nodes of the parse tree describing in 2.2.3. A grammar derives strings by beginning with the start symbol and repeatedly replacing a nonterminal by the body of a production for that nonterminal. The terminal strings that can be derived from the start symbol form the language defined by the grammar. TIPS: Here is the definition of language , please recall my question in the first introduction that what a programming language is. Parsing is the problem of taking a string of terminals and figuring out how to derive it from the start symbol of the grammar, and if it cannot be derived from the start symbol of the grammar, then reporting syntax errors within the string. TIPS: The process of parsing is the inverse of the process of deriving. The process of deriving is to generate terminal strings in the direction of generation while the process of parsing is to find productions in the opposite direction of production. They are like recursion and corecursion . TIPS: Implementation of parsing is described in chapter 2.4. 2.2.3 Parse Trees \uf0c1 TIPS: Parse tree reveal us of the power of tree structure, and it is an typical application of tree . Tree data structures figure prominently in compiling. TIPS: Parse tree is the software implementation of grammar. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0c so do tree structure. This is one of reason why tree structure is selected as implementation of grammar. TIPS: This chapter gives the algorithm of build a parse tree according to the grammar. 2.2.4 Ambiguity \uf0c1 TIPS: Programming language should be accurate and the designer should be careful to design grammar to avoid ambiguity. Source of ambiguity includes precedence of operators associativity of operators 2.2.5 Associativity of Operators \uf0c1 TIPS: We can define associativity by grammar, this reveals us of the power if context free grammar. By convention, $9+5+2$ is equivalent to $(9+5)+2$ and $9-5-2$ is equivalent to $(9-5)-2$. When an operand like 5 has operators to its left and right, conventions are needed for deciding which operator applies to that operand. We say that the operator + associates to the left, because an operand with plus signs on both sides of it belongs to the operator to its left. In most programming languages the four arithmetic operators, addition, subtraction, multiplication, and division are left-associative. Some common operators such as exponentiation are right-associative. As another example, the assignment operator in C and its descendants is right-associative; that is, the expression a=b=c is treated in the same way as the expression a=(b=c) . For clarity, sample grammar for left-associative operator and sample grammar for right-associative operator are listed below along their parse tree. Strings like a=b=c with a right-associative operator are generated by the following grammar: $$ \\begin{align} right \\to letter=right|letter \\ letter \\to a|b| \\dots |z \\end{align} $$ Strings like 1+2 , 1-2 with a left-associative operator are generated by the following grammar: $$ \\begin{align} list \\to list + digit \\ list \\to list - digit \\ list \\to digit \\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\end{align} $$ The contrast between a parse tree for a left-associative operator like - and a parse tree for a right-associative operator like = is shown by Fig. 2.7. Note that the parse tree for 9-5-2 grows down towards the left, whereas the parse tree for a=b=c grows down towards the right. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0ca grammar is informally called a recursive grammar if it contains production rules that are recursive . Here we can further subdivide, a grammar for a context-free language is left recursive if there exists a non-terminal symbol A that can be put through the production rules to produce a string with A (as the leftmost symbol), so grammars for left-associative operator are left recursive while grammars for right-associative operator are right recursive . For comparison, here are the ambiguous grammars and its parse tree. $$ list \\to list + digit | list - digit | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 $$ 2.2.6 Precedence of Operators \uf0c1 We say that * has higher precedence than + if * takes its operands before + does. In ordinary arithmetic, multiplication and division have higher precedence than addition and subtraction. Therefore, 5 is taken by * in b oth 9+5*2 and 9*5+2 ; i.e., the expressions are equivalent to 9+(5*2) and (9*5)+2 , respectively. Example 2.6 : A grammar for arithmetic expressions can be constructed from a table showing the associativity and precedence of operators. We start with the four common arithmetic operators and a precedence table, showing the operators in order of increasing precedence. Operators on the same line have the same associativity and precedence: left-associative + - left-associative: * / We create two nonterminals expr and term for the two levels of precedence( expr for + - and term for * / ), and an extra nonterminal factor for generating basic units in expressions. The basic units in expressions are presently digits and parenthesized expressions. $$ factor \\to digit | ( expr ) $$ Now consider the binary operators, * and / , that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\ | term / factor \\ | factor $$ Similarly, expr generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\ | expr - term \\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\ term \\to term * factor | term / factor | factor \\ factor \\to digit | ( expr ) $$ TIPS: The priorities decrease from bottom to top Generalizing the Expression Grammar of Example 2.6 \uf0c1 We can think of a factor as an expression that cannot b e \"torn apart\" by any operator. By \"torn apart,\" we mean that placing an operator next to any factor, on either side, does not cause any piece of the factor , other than the whole , to become an operand of that operator. If the factor is a parenthesized expression , the parentheses protect against such \"tearing,\" while if the factor is a single operand, it cannot be torn apart. A term (that is not also a factor) is an expression that can be torn apart by operators of the highest precedence: * and / , but not by the lower-precedence operators. An expression (that is not a term or factor) can be torn apart by any operator. We can generalize this idea to any number n of precedence levels. We need n + 1 nonterminals. The first, like factor in Example 2.6, can never be torn apart. Typically, the production bodies for this nonterminal are only single operands and parenthesized expressions . Then, for each precedence level, there is one nonterminal representing expressions that can be torn apart only by operators at that level or higher. Typically, the productions for this nonterminal have bodies representing uses of the operators at that level, plus one body that is just the nonterminal for the next higher level. TIPS: The reason one more is needed is that it is for factor TIPS: When generating a parse tree from a grammar for expression, the higher the priority of operator, the earlier it is built, and the deeper it is","title":"2.2Syntax-Definition"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#22-syntax-definition","text":"In this section, we introduce a notation \u2014 the \" context-free grammar ,\" or \"grammar\" for short \u2014 that is used to specify the syntax of a language. Grammars will be used throughout this book to organize compiler front ends . TIPS: Wikipedia has a good explanation of context-free grammar TIPS: It is a convention that there is a section called language reference or language specification to describe grammar of the programming language in the official document of the programming language , such as The Python Language Reference use extended BNF notation to describe syntax of Python language. The Java\u00ae Language Specification","title":"2.2 Syntax Definition"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#221-definition-of-grammars","text":"A context-free grammar has four components: 1. A set of terminal symbols , sometimes referred to as \"tokens.\" The terminals are the elementary symbols of the language defined by the grammar. 2. A set of nonterminals , sometimes called \"syntactic variables.\" Each nonterminal represents a set of strings of terminals, in a manner we shall describe. 3. A set of productions , where each production consists of a nonterminal, called the head or left side of the production, an arrow, and a sequence of terminals and/or nonterminals, called the body or right side of the production. 4. A designation of one of the nonterminals as the start symbol. Example 2.1 : $$ \\begin{align} list \\to list + digit \\tag {2.1}\\ list \\to list - digit \\tag {2.2}\\ list \\to digit \\tag {2.3}\\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\tag {2.4} \\end{align} $$ TIPS: Programmer should be sensitive of recursion , it is obvious that Context-free grammar above has the property of recursion , a grammar is informally called a recursive grammar if it contains production rules that are recursive .","title":"2.2.1 Definition of Grammars"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#222-derivations","text":"TIPS: In addition to using the word derivation /derive, word expand is also used a lot. I think word expand can visually reflect the process of expanding an inner nodes of the parse tree describing in 2.2.3. A grammar derives strings by beginning with the start symbol and repeatedly replacing a nonterminal by the body of a production for that nonterminal. The terminal strings that can be derived from the start symbol form the language defined by the grammar. TIPS: Here is the definition of language , please recall my question in the first introduction that what a programming language is. Parsing is the problem of taking a string of terminals and figuring out how to derive it from the start symbol of the grammar, and if it cannot be derived from the start symbol of the grammar, then reporting syntax errors within the string. TIPS: The process of parsing is the inverse of the process of deriving. The process of deriving is to generate terminal strings in the direction of generation while the process of parsing is to find productions in the opposite direction of production. They are like recursion and corecursion . TIPS: Implementation of parsing is described in chapter 2.4.","title":"2.2.2 Derivations"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#223-parse-trees","text":"TIPS: Parse tree reveal us of the power of tree structure, and it is an typical application of tree . Tree data structures figure prominently in compiling. TIPS: Parse tree is the software implementation of grammar. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0c so do tree structure. This is one of reason why tree structure is selected as implementation of grammar. TIPS: This chapter gives the algorithm of build a parse tree according to the grammar.","title":"2.2.3 Parse Trees"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#224-ambiguity","text":"TIPS: Programming language should be accurate and the designer should be careful to design grammar to avoid ambiguity. Source of ambiguity includes precedence of operators associativity of operators","title":"2.2.4 Ambiguity"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#225-associativity-of-operators","text":"TIPS: We can define associativity by grammar, this reveals us of the power if context free grammar. By convention, $9+5+2$ is equivalent to $(9+5)+2$ and $9-5-2$ is equivalent to $(9-5)-2$. When an operand like 5 has operators to its left and right, conventions are needed for deciding which operator applies to that operand. We say that the operator + associates to the left, because an operand with plus signs on both sides of it belongs to the operator to its left. In most programming languages the four arithmetic operators, addition, subtraction, multiplication, and division are left-associative. Some common operators such as exponentiation are right-associative. As another example, the assignment operator in C and its descendants is right-associative; that is, the expression a=b=c is treated in the same way as the expression a=(b=c) . For clarity, sample grammar for left-associative operator and sample grammar for right-associative operator are listed below along their parse tree. Strings like a=b=c with a right-associative operator are generated by the following grammar: $$ \\begin{align} right \\to letter=right|letter \\ letter \\to a|b| \\dots |z \\end{align} $$ Strings like 1+2 , 1-2 with a left-associative operator are generated by the following grammar: $$ \\begin{align} list \\to list + digit \\ list \\to list - digit \\ list \\to digit \\ digit \\to 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \\end{align} $$ The contrast between a parse tree for a left-associative operator like - and a parse tree for a right-associative operator like = is shown by Fig. 2.7. Note that the parse tree for 9-5-2 grows down towards the left, whereas the parse tree for a=b=c grows down towards the right. TIPS: Recall what concluded in 2.2.1 Definition of Grammars is that Context-free grammar has the property of recursion \uff0ca grammar is informally called a recursive grammar if it contains production rules that are recursive . Here we can further subdivide, a grammar for a context-free language is left recursive if there exists a non-terminal symbol A that can be put through the production rules to produce a string with A (as the leftmost symbol), so grammars for left-associative operator are left recursive while grammars for right-associative operator are right recursive . For comparison, here are the ambiguous grammars and its parse tree. $$ list \\to list + digit | list - digit | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 $$","title":"2.2.5 Associativity of Operators"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#226-precedence-of-operators","text":"We say that * has higher precedence than + if * takes its operands before + does. In ordinary arithmetic, multiplication and division have higher precedence than addition and subtraction. Therefore, 5 is taken by * in b oth 9+5*2 and 9*5+2 ; i.e., the expressions are equivalent to 9+(5*2) and (9*5)+2 , respectively. Example 2.6 : A grammar for arithmetic expressions can be constructed from a table showing the associativity and precedence of operators. We start with the four common arithmetic operators and a precedence table, showing the operators in order of increasing precedence. Operators on the same line have the same associativity and precedence: left-associative + - left-associative: * / We create two nonterminals expr and term for the two levels of precedence( expr for + - and term for * / ), and an extra nonterminal factor for generating basic units in expressions. The basic units in expressions are presently digits and parenthesized expressions. $$ factor \\to digit | ( expr ) $$ Now consider the binary operators, * and / , that have the highest precedence. Since these operators associate to the left, the productions are similar to those for lists that associate to the left. $$ term \\to term * factor \\ | term / factor \\ | factor $$ Similarly, expr generates lists of terms separated by the additive operators. $$ expr \\to expr + term \\ | expr - term \\ | term $$ The resulting grammar is therefore $$ expr \\to expr + term | expr - term | term \\ term \\to term * factor | term / factor | factor \\ factor \\to digit | ( expr ) $$ TIPS: The priorities decrease from bottom to top","title":"2.2.6 Precedence of Operators"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.2Syntax-Definition/#generalizing-the-expression-grammar-of-example-26","text":"We can think of a factor as an expression that cannot b e \"torn apart\" by any operator. By \"torn apart,\" we mean that placing an operator next to any factor, on either side, does not cause any piece of the factor , other than the whole , to become an operand of that operator. If the factor is a parenthesized expression , the parentheses protect against such \"tearing,\" while if the factor is a single operand, it cannot be torn apart. A term (that is not also a factor) is an expression that can be torn apart by operators of the highest precedence: * and / , but not by the lower-precedence operators. An expression (that is not a term or factor) can be torn apart by any operator. We can generalize this idea to any number n of precedence levels. We need n + 1 nonterminals. The first, like factor in Example 2.6, can never be torn apart. Typically, the production bodies for this nonterminal are only single operands and parenthesized expressions . Then, for each precedence level, there is one nonterminal representing expressions that can be torn apart only by operators at that level or higher. Typically, the productions for this nonterminal have bodies representing uses of the operators at that level, plus one body that is just the nonterminal for the next higher level. TIPS: The reason one more is needed is that it is for factor TIPS: When generating a parse tree from a grammar for expression, the higher the priority of operator, the earlier it is built, and the deeper it is","title":"Generalizing the Expression Grammar of Example 2.6"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.3Syntax-Directed-Translation/","text":"2.3 Syntax-Directed Translation 2.3 Syntax-Directed Translation \uf0c1","title":"2.3Syntax-Directed-Translation"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.3Syntax-Directed-Translation/#23-syntax-directed-translation","text":"","title":"2.3 Syntax-Directed Translation"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.4Parsing/","text":"2.4 Parsing 2.4.1 Top-Down Parsing 2.4.2 Predictive Parsing 2.4.3 When to Use $\\epsilon$-Productions 2.4 Parsing \uf0c1 Parsing is the process of determining how a string of terminals can be generated by a grammar . In discussing this problem, it is helpful to think of a parse tree being constructed, even though a compiler may not construct one, in practice. However, a parser must be capable of constructing the tree in principle, or else the translation cannot be guaranteed correct. NOTE: The concept of parsing is introduced in chapter 2.2.2 Derivations This section introduces a parsing method called \" recursive descent ,\" which can be used both to parse and to implement syntax-directed translators. A complete Java program, implementing the translation scheme of Fig. 2.15, appears in the next section. A viable alternative is to use a software tool to generate a translator directly from a translation scheme. Section 4.9 describes such a tool Yacc; it can implement the translation scheme of Fig. 2.15 without modification. For any context-free grammar there is a parser that takes at most $O (n^3)$ time to parse a string of n terminals. But cubic time is generally too expensive. Fortunately, for real programming languages, we can generally design a grammar that can be parsed quickly. Linear-time algorithms suffice to parse essentially all languages that arise in practice. Programming-language parsers almost always make a single left-to-right scan over the input, looking ahead one terminal at a time, and constructing pieces of the parse tree as they go. Most parsing methods fall into one of two classes, called the top-down and bottom-up methods. These terms refer to the order in which nodes in the parse tree are constructed. In top-down parsers, construction starts at the root and proceeds towards the leaves, while in bottom-up parsers, construction starts at the leaves and proceeds towards the root. The popularity of top-down parsers is due to the fact that efficient parsers can be constructed more easily by hand using top-down methods. Bottom-up parsing, however, can handle a larger class of grammars and translation schemes, so software tools for generating parsers directly from grammars often use bottom-up methods. TIPS: Wikipedia has a special section on parsing algorithms Parsing algorithms 2.4.1 Top-Down Parsing \uf0c1 We introduce top-down parsing by considering a grammar that is well-suited for this class of methods. Later in this section, we consider the construction of top-down parsers in general. The grammar in Fig. 2.16 generates a subset of the statements of C or Java. We use the boldface terminals if and for for the keywords \"if\" and \"for\", respectively, to emphasize that these character sequences are treated as units, i.e., as single terminal symbols. Further, the terminal expr represents expressions; a more complete grammar would use a nonterminal expr and have productions for nonterminal expr . Similarly, other is a terminal representing other statement constructs. The top-down construction of a parse tree like the one in Fig. 2.17, is done by starting with the root, labeled with the starting nonterminal stmt , and repeatedly performing the following two steps. At node N , labeled with nonterminal A , select one of the productions for A and construct children at N for the symbols in the production body. Find the next node at which a subtree is to be constructed, typically the leftmost unexpanded nonterminal of the tree. For some grammars, the above steps can be implemented during a single left-to-right scan of the input string. The current terminal being scanned in the input is frequently referred to as the lookahead symbol . Initially, the lookahead symbol is the first, i.e., leftmost, terminal of the input string. Figure 2.18 illustrates the construction of the parse tree in Fig. 2.17 for the input string for ( ; expr ; expr ) other Initially, the terminal for is the lookahead symbol , and the known part of the parse tree consists of the root, labeled with the starting nonterminal stmt in Fig. 2.18(a). The objective is to construct the remainder of the parse tree in such a way that the string generated by the parse tree matches the input string. For a match to occur, the nonterminal stmt in Fig. 2.18(a) must derive a string that starts with the lookahead symbol for . In the grammar of Fig. 2.16, there is just one production for stmt that can derive such a string, so we select it, and construct the children of the root labeled with the symbols in the production body. This expansion of the parse tree is shown in Fig. 2.18(b). Each of the three snapshots in Fig. 2.18 has arrows marking the lookahead symbol in the input and the node in the parse tree that is being considered. Once children are constructed at a node, we next consider the leftmost child. In Fig. 2.18(b), children have just been constructed at the root, and the leftmost child labeled with for is being considered. At the nonterminal node labeled optexpr , we repeat the process of selecting a production for a nonterminal. Productions with $\\epsilon$ as the body (\" $\\epsilon$ -productions\") require special treatment. For the moment, we use them as a default when no other production can be used; we return to them in Section 2.4.3. With nonterminal optexpr and lookahead ; , the $\\epsilon$ -productions is used, since ; does not match the only other production for optexpr , which has terminal expr as its body. In general, the selection of a production for a nonterminal may involve trial-and-error ; that is, we may have to try a production and backtrack to try another production if the first is found to be unsuitable. A production is unsuitable if, after using the production, we cannot complete the tree to match the input string. Backtracking is not needed, however, in an important special case called predictive parsing, which we discuss next. NOTE: There is a good illustration of Top-Down Parser in this blog . 2.4.2 Predictive Parsing \uf0c1 Recursive-descent parsing is a top-down method of syntax analysis in which a set of recursive procedures is used to process the input. One procedure is associated with each nonterminal of a grammar. Here, we consider a simple form of recursive-descent parsing , called predictive parsing , in which the lookahead symbol unambiguously determines the flow of control through the procedure body for each nonterminal . The sequence of procedure calls during the analysis of an input string implicitly defines a parse tree for the input, and can be used to build an explicit parse tree , if desired. The predictive parser in Fig. 2.19 consists of procedures for the nonterminals stmt and optexpr of the grammar in Fig. 2.16 and an additional procedure match, used to simplify the code for stmt and optexpr . Procedure match(t) compares its argument t with the lookahead symbol and advances to the next input terminal if they match. Thus match changes the value of variable lookahead, a global variable that holds the currently scanned input terminal. void stmt () { switch ( lookahead ) { case expr: match (expr); match (';'); break; case 'if': match ('if'); match ('('); match (expr); match (')'); stmt (); break; case 'for': match ('for'); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); break; case other; match (other); break; default: report (\"syntax error\"); } void optexpr () { if ( lookahead == expr ) match (expr); } void match (terminal t) { if ( lookahead == t ) lookahead = nextTerminal; else report (\"syntax error\"); } Figure 2.19: Pseudo code for a predictive parser NOTE: A predictive parsing approach is one that hardcodes the grammar into the program Parsing begins with a call of the procedure for the starting nonterminal stmt . With the same input as in Fig. 2.18, lookahead is initially the first terminal for . Procedure stmt executes code corresponding to the production $$ stmt \\to for ( optexpr ; optexpr ; optexpr ) stmt $$ In the code for the production body -- that is, the for case of procedure stmt --each terminal is matched with the lookahead symbol, and each nonterminal leads to a call of its procedure, in the following sequence of calls: match (for); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); Predictive parsing relies on information about the first symbols that can be generated by a production body. More precisely, let $\\alpha$ be a string of grammar symbols (terminals and/or nonterminals). We define $FIRST (\\alpha)$ to be the set of terminals that appear as the first symbols of one or more strings of terminals generated from $\\alpha$. If $\\alpha$ is $\\epsilon$ or can generate $\\epsilon$ , then $\\epsilon$ is also in $FIRST (\\alpha)$. NOTE: Only when $FIRST (\\alpha)$ is known, can flow of control determine. The details of how one computes $FIRST (\\alpha)$ are in Section 4.4.2. Here, we shall just use ad hoc reasoning to deduce the symbols in $FIRST (\\alpha)$; typically, $\\alpha$ will either begin with a terminal, which is therefore the only symbol in $FIRST (\\alpha)$, or $\\alpha$ will begin with a nonterminal whose production bodies begin with terminals, in which case these terminals are the only members of $FIRST (\\alpha)$. For example, with respect to the grammar of Fig. 2.16, the following are correct calculations of $FIRST$. FIRST (stmt) = {expr; if; for; other} FIRST(expr ;) = {expr} NOTE: FIRST (stmt) corresponds to the case of void stmt () . The FIRST sets must be considered if there are two pro ductions $A \\to \\alpha$ and $A \\to \\beta$ . Ignoring $\\epsilon$-productions for the moment, predictive parsing requires $FIRST (\\alpha)$ and $FIRST(\\beta )$ to b e disjoint. The lookahead symbol can then be used to decide which production to use; if the lookahead symbol is in $FIRST (\\alpha)$, then $\\alpha$ is used. Otherwise, if the lookahead symbol is in $FIRST (\\beta)$, then $\\beta$ is used. 2.4.3 When to Use $\\epsilon$-Productions \uf0c1","title":"2.4Parsing"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.4Parsing/#24-parsing","text":"Parsing is the process of determining how a string of terminals can be generated by a grammar . In discussing this problem, it is helpful to think of a parse tree being constructed, even though a compiler may not construct one, in practice. However, a parser must be capable of constructing the tree in principle, or else the translation cannot be guaranteed correct. NOTE: The concept of parsing is introduced in chapter 2.2.2 Derivations This section introduces a parsing method called \" recursive descent ,\" which can be used both to parse and to implement syntax-directed translators. A complete Java program, implementing the translation scheme of Fig. 2.15, appears in the next section. A viable alternative is to use a software tool to generate a translator directly from a translation scheme. Section 4.9 describes such a tool Yacc; it can implement the translation scheme of Fig. 2.15 without modification. For any context-free grammar there is a parser that takes at most $O (n^3)$ time to parse a string of n terminals. But cubic time is generally too expensive. Fortunately, for real programming languages, we can generally design a grammar that can be parsed quickly. Linear-time algorithms suffice to parse essentially all languages that arise in practice. Programming-language parsers almost always make a single left-to-right scan over the input, looking ahead one terminal at a time, and constructing pieces of the parse tree as they go. Most parsing methods fall into one of two classes, called the top-down and bottom-up methods. These terms refer to the order in which nodes in the parse tree are constructed. In top-down parsers, construction starts at the root and proceeds towards the leaves, while in bottom-up parsers, construction starts at the leaves and proceeds towards the root. The popularity of top-down parsers is due to the fact that efficient parsers can be constructed more easily by hand using top-down methods. Bottom-up parsing, however, can handle a larger class of grammars and translation schemes, so software tools for generating parsers directly from grammars often use bottom-up methods. TIPS: Wikipedia has a special section on parsing algorithms Parsing algorithms","title":"2.4 Parsing"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.4Parsing/#241-top-down-parsing","text":"We introduce top-down parsing by considering a grammar that is well-suited for this class of methods. Later in this section, we consider the construction of top-down parsers in general. The grammar in Fig. 2.16 generates a subset of the statements of C or Java. We use the boldface terminals if and for for the keywords \"if\" and \"for\", respectively, to emphasize that these character sequences are treated as units, i.e., as single terminal symbols. Further, the terminal expr represents expressions; a more complete grammar would use a nonterminal expr and have productions for nonterminal expr . Similarly, other is a terminal representing other statement constructs. The top-down construction of a parse tree like the one in Fig. 2.17, is done by starting with the root, labeled with the starting nonterminal stmt , and repeatedly performing the following two steps. At node N , labeled with nonterminal A , select one of the productions for A and construct children at N for the symbols in the production body. Find the next node at which a subtree is to be constructed, typically the leftmost unexpanded nonterminal of the tree. For some grammars, the above steps can be implemented during a single left-to-right scan of the input string. The current terminal being scanned in the input is frequently referred to as the lookahead symbol . Initially, the lookahead symbol is the first, i.e., leftmost, terminal of the input string. Figure 2.18 illustrates the construction of the parse tree in Fig. 2.17 for the input string for ( ; expr ; expr ) other Initially, the terminal for is the lookahead symbol , and the known part of the parse tree consists of the root, labeled with the starting nonterminal stmt in Fig. 2.18(a). The objective is to construct the remainder of the parse tree in such a way that the string generated by the parse tree matches the input string. For a match to occur, the nonterminal stmt in Fig. 2.18(a) must derive a string that starts with the lookahead symbol for . In the grammar of Fig. 2.16, there is just one production for stmt that can derive such a string, so we select it, and construct the children of the root labeled with the symbols in the production body. This expansion of the parse tree is shown in Fig. 2.18(b). Each of the three snapshots in Fig. 2.18 has arrows marking the lookahead symbol in the input and the node in the parse tree that is being considered. Once children are constructed at a node, we next consider the leftmost child. In Fig. 2.18(b), children have just been constructed at the root, and the leftmost child labeled with for is being considered. At the nonterminal node labeled optexpr , we repeat the process of selecting a production for a nonterminal. Productions with $\\epsilon$ as the body (\" $\\epsilon$ -productions\") require special treatment. For the moment, we use them as a default when no other production can be used; we return to them in Section 2.4.3. With nonterminal optexpr and lookahead ; , the $\\epsilon$ -productions is used, since ; does not match the only other production for optexpr , which has terminal expr as its body. In general, the selection of a production for a nonterminal may involve trial-and-error ; that is, we may have to try a production and backtrack to try another production if the first is found to be unsuitable. A production is unsuitable if, after using the production, we cannot complete the tree to match the input string. Backtracking is not needed, however, in an important special case called predictive parsing, which we discuss next. NOTE: There is a good illustration of Top-Down Parser in this blog .","title":"2.4.1 Top-Down Parsing"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.4Parsing/#242-predictive-parsing","text":"Recursive-descent parsing is a top-down method of syntax analysis in which a set of recursive procedures is used to process the input. One procedure is associated with each nonterminal of a grammar. Here, we consider a simple form of recursive-descent parsing , called predictive parsing , in which the lookahead symbol unambiguously determines the flow of control through the procedure body for each nonterminal . The sequence of procedure calls during the analysis of an input string implicitly defines a parse tree for the input, and can be used to build an explicit parse tree , if desired. The predictive parser in Fig. 2.19 consists of procedures for the nonterminals stmt and optexpr of the grammar in Fig. 2.16 and an additional procedure match, used to simplify the code for stmt and optexpr . Procedure match(t) compares its argument t with the lookahead symbol and advances to the next input terminal if they match. Thus match changes the value of variable lookahead, a global variable that holds the currently scanned input terminal. void stmt () { switch ( lookahead ) { case expr: match (expr); match (';'); break; case 'if': match ('if'); match ('('); match (expr); match (')'); stmt (); break; case 'for': match ('for'); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); break; case other; match (other); break; default: report (\"syntax error\"); } void optexpr () { if ( lookahead == expr ) match (expr); } void match (terminal t) { if ( lookahead == t ) lookahead = nextTerminal; else report (\"syntax error\"); } Figure 2.19: Pseudo code for a predictive parser NOTE: A predictive parsing approach is one that hardcodes the grammar into the program Parsing begins with a call of the procedure for the starting nonterminal stmt . With the same input as in Fig. 2.18, lookahead is initially the first terminal for . Procedure stmt executes code corresponding to the production $$ stmt \\to for ( optexpr ; optexpr ; optexpr ) stmt $$ In the code for the production body -- that is, the for case of procedure stmt --each terminal is matched with the lookahead symbol, and each nonterminal leads to a call of its procedure, in the following sequence of calls: match (for); match ('('); optexpr (); match (';'); optexpr (); match (';'); optexpr (); match (')'); stmt (); Predictive parsing relies on information about the first symbols that can be generated by a production body. More precisely, let $\\alpha$ be a string of grammar symbols (terminals and/or nonterminals). We define $FIRST (\\alpha)$ to be the set of terminals that appear as the first symbols of one or more strings of terminals generated from $\\alpha$. If $\\alpha$ is $\\epsilon$ or can generate $\\epsilon$ , then $\\epsilon$ is also in $FIRST (\\alpha)$. NOTE: Only when $FIRST (\\alpha)$ is known, can flow of control determine. The details of how one computes $FIRST (\\alpha)$ are in Section 4.4.2. Here, we shall just use ad hoc reasoning to deduce the symbols in $FIRST (\\alpha)$; typically, $\\alpha$ will either begin with a terminal, which is therefore the only symbol in $FIRST (\\alpha)$, or $\\alpha$ will begin with a nonterminal whose production bodies begin with terminals, in which case these terminals are the only members of $FIRST (\\alpha)$. For example, with respect to the grammar of Fig. 2.16, the following are correct calculations of $FIRST$. FIRST (stmt) = {expr; if; for; other} FIRST(expr ;) = {expr} NOTE: FIRST (stmt) corresponds to the case of void stmt () . The FIRST sets must be considered if there are two pro ductions $A \\to \\alpha$ and $A \\to \\beta$ . Ignoring $\\epsilon$-productions for the moment, predictive parsing requires $FIRST (\\alpha)$ and $FIRST(\\beta )$ to b e disjoint. The lookahead symbol can then be used to decide which production to use; if the lookahead symbol is in $FIRST (\\alpha)$, then $\\alpha$ is used. Otherwise, if the lookahead symbol is in $FIRST (\\beta)$, then $\\beta$ is used.","title":"2.4.2 Predictive Parsing"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/2.4Parsing/#243-when-to-use-epsilon-productions","text":"","title":"2.4.3 When to Use $\\epsilon$-Productions"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/A-Simple-Syntax-Directed-Translator/","text":"Introduction Introduction \uf0c1 This chapter is an introduction to the compiling techniques in Chapters 3 through 6 of this book. It illustrates the techniques by developing a working Java program that translates representative programming language statements into three-address code , an intermediate representation. We start small by creating a syntax-directed translator that maps infix arithmetic expressions into postfix expressions. We then extend this translator to map code fragments as shown in Fig. 2.1 into three-address code of the form in Fig. 2.2. TIPS: It is not easy to implement mapping infix arithmetic expressions into postfix expressions, there are some algorithms Shunting-yard algorithm { int i; int j; float[100] a; float v; float x; while ( true ) { do i = i+1; while ( a[i] < v ); do j = j-1; while ( a[j] > v ); if ( i >= j ) break; x = a[i]; a[i] = a[j]; a[j] = x; } } Figure 2.1: A code fragment to be translated | | | | :--: | :---------------------: | | 1: | i = i + 1 | | 2: | t1 = a [ i ] | | 3: | if t1 < v goto 1 | | 4: | j = j - 1 | | 5: | t2 = a [ j ] | | 6: | if t2 > v goto 4 | | 7: | ifFalse i >= j goto 9 | | 8: | goto 14 | | 9: | x = a [ i ] | | 10: | t3 = a [ j ] | | 11: | a [ i ] = t3 | | 12: | a [ j ] = x | | 13: | goto 1 | | 14: | | Figure 2.2: Simplified intermediate code for the program fragment in Fig. 2.1","title":"A-Simple-Syntax-Directed-Translator"},{"location":"Chapter2-A-Simple-Syntax-Directed-Translator/A-Simple-Syntax-Directed-Translator/#introduction","text":"This chapter is an introduction to the compiling techniques in Chapters 3 through 6 of this book. It illustrates the techniques by developing a working Java program that translates representative programming language statements into three-address code , an intermediate representation. We start small by creating a syntax-directed translator that maps infix arithmetic expressions into postfix expressions. We then extend this translator to map code fragments as shown in Fig. 2.1 into three-address code of the form in Fig. 2.2. TIPS: It is not easy to implement mapping infix arithmetic expressions into postfix expressions, there are some algorithms Shunting-yard algorithm { int i; int j; float[100] a; float v; float x; while ( true ) { do i = i+1; while ( a[i] < v ); do j = j-1; while ( a[j] > v ); if ( i >= j ) break; x = a[i]; a[i] = a[j]; a[j] = x; } } Figure 2.1: A code fragment to be translated | | | | :--: | :---------------------: | | 1: | i = i + 1 | | 2: | t1 = a [ i ] | | 3: | if t1 < v goto 1 | | 4: | j = j - 1 | | 5: | t2 = a [ j ] | | 6: | if t2 > v goto 4 | | 7: | ifFalse i >= j goto 9 | | 8: | goto 14 | | 9: | x = a [ i ] | | 10: | t3 = a [ j ] | | 11: | a [ i ] = t3 | | 12: | a [ j ] = x | | 13: | goto 1 | | 14: | | Figure 2.2: Simplified intermediate code for the program fragment in Fig. 2.1","title":"Introduction"},{"location":"Chapter3-Lexical-Analysis/3.1The-Role-of-the-Lexical-Analyzer/","text":"3.1 The Role of the Lexical Analyzer \uf0c1 As the first phase of a compiler, the main task of the lexical analyzer is to read the input characters of the source program, group them into lexemes, and produce as output a sequence of tokens for each lexeme in the source program. The stream of tokens is sent to the parser for syntax analysis. It is common for the lexical analyzer to interact with the symbol table as well. When the lexical analyzer discovers a lexeme constituting an identifier, it needs to enter that lexeme into the symbol table. In some cases, information regarding the kind of identifier may be read from the symbol table by the lexical analyzer to assist it in determining the proper token it must pass to the parser.","title":"3.1The-Role-of-the-Lexical-Analyzer"},{"location":"Chapter3-Lexical-Analysis/3.1The-Role-of-the-Lexical-Analyzer/#31-the-role-of-the-lexical-analyzer","text":"As the first phase of a compiler, the main task of the lexical analyzer is to read the input characters of the source program, group them into lexemes, and produce as output a sequence of tokens for each lexeme in the source program. The stream of tokens is sent to the parser for syntax analysis. It is common for the lexical analyzer to interact with the symbol table as well. When the lexical analyzer discovers a lexeme constituting an identifier, it needs to enter that lexeme into the symbol table. In some cases, information regarding the kind of identifier may be read from the symbol table by the lexical analyzer to assist it in determining the proper token it must pass to the parser.","title":"3.1 The Role of the Lexical Analyzer"},{"location":"Chapter3-Lexical-Analysis/3.4Recognition-of-Tokens/","text":"3.4 Recognition of Tokens Aho-Corasick algorithm 3.4 Recognition of Tokens \uf0c1 NOTE: This chapter describes conversion from regular-expression patterns to transition diagrams by hand, which is intended to help reader to understand the finite automaton (finite state machine) widely used in engineering described in section 3.5. In essence, a transition diagram is a visual representation of a finite state machine. In the previous section we learned how to express patterns using regular expressions. Now, we must study how to take the patterns for all the needed tokens and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns. Our discussion will make use of the following running example. $$ \\begin{align} stmt \\to if \\; expr \\; then \\; stmt \\ | if \\; expr \\; then \\; stmt \\; else \\; stmt \\ | ? \\ expr \\to term \\; relop \\; term \\ | term \\ term \\to id \\ | number \\end{align} $$ Figure 3.10: A grammar for branching statements Aho-Corasick algorithm \uf0c1 NOTE: The main content of this chapter is passed, but in the exercise, the author introduce Aho-Corasick algorithm and KMP algorithm which are interesting. The following exercises, up to Exercise 3.4.12, introduce the Aho-Corasick algorithm for recognizing a collection of keywords in a text string in time proportional to the length of the text and the sum of the length of the keywords. This algorithm uses a special form of transition diagram called a trie . A trie is a tree-structured transition diagram with distinct labels on the edges leading from a node to its children. Leaves of the trie represent recognized keywords. NOTE: A trie is a tree-structured transition diagram or finite automaton , do you see the power of tree structure here as I do Knuth, Morris, and Pratt presented an algorithm for recognizing a single keyword $b_1 b_2 \\dots b_n$(length of the word is n) in a text string. Here the trie is a transition diagram with n+ 1 states, 0 through n. State 0 is the initial state, and state n represents acceptance, that is, discovery of the keyword. From each state s from 0 through n - 1, there is a transition to state s + 1, labeled by symbol $b_{s+1}$ . For example, the trie for the keyword ababaa is: In order to process text strings rapidly and search those strings for a keyword, it is useful to define, for keyword $b_1 b_2 \\dots b_n$ and position s in that keyword (corresponding to state s of its trie), a failure function , f(s) , computed as in Fig. 3.19. The objective is that $b_1 b_2 \\dots b_{f(s)}$ is the longest proper prefix of $b_1 b_2 \\dots b_s$ that is also a suffix of $b_1 b_2 \\dots b_s$. The reason $f (s)$ is important is that if we are trying to match a text string for $b_1 b_2 \\dots b_n$, and we have matched the first s positions, but we then fail (i.e., the next position of the text string does not hold $b_{s+1}$ ), then $f (s)$ is the longest prefix of $b_1 b_2 \\dots b_n$ that could possibly match the text string up to the point we are at. Of course, the next character of the text string must be $b_{f (s)+1}$, or else we still have problems and must consider a yet shorter prefix, which will be $b_{f (f (s))}$. t = 0; f (1) = 0; for (s = 1; s < n; s + +) f while (t > 0 && b[s+1] ! = b[t+1]) t = f (t); if (b[s+1] == b[t+1]) { t = t + 1; f (s + 1) = t; } else f (s + 1) = 0; } Figure 3.19: Algorithm to compute the failure function for keyword $b_1 b_2 \\dots b_n$ As an example, the failure function for the trie constructed above for ababaa is: s 1 2 3 4 5 6 f(s) 0 0 1 2 3 1 For instance, states 3 and 1 represent prefixes aba and a , respectively. f (3) = 1 because a is the longest proper prefix of aba that is also a suffix of aba . Also, f (2) = 0 , because the longest proper prefix of ab that is also a suffix is the empty string. Aho and Corasick generalized the KMP algorithm to recognize any of a set of keywords in a text string. In this case, the trie is a true tree, with branching from the root. There is one state for every string that is a prefix (not necessarily proper) of any keyword. The parent of a state corresponding to string $b_1 b_2 \\dots b_n$ is the state that corresponds to $b_1 b_2 \\dots b_k$. A state is accepting if it corresponds to a complete keyword. For example, Fig. 3.21 shows the trie for the keywords he , she , his , and hers .","title":"3.4Recognition-of-Tokens"},{"location":"Chapter3-Lexical-Analysis/3.4Recognition-of-Tokens/#34-recognition-of-tokens","text":"NOTE: This chapter describes conversion from regular-expression patterns to transition diagrams by hand, which is intended to help reader to understand the finite automaton (finite state machine) widely used in engineering described in section 3.5. In essence, a transition diagram is a visual representation of a finite state machine. In the previous section we learned how to express patterns using regular expressions. Now, we must study how to take the patterns for all the needed tokens and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns. Our discussion will make use of the following running example. $$ \\begin{align} stmt \\to if \\; expr \\; then \\; stmt \\ | if \\; expr \\; then \\; stmt \\; else \\; stmt \\ | ? \\ expr \\to term \\; relop \\; term \\ | term \\ term \\to id \\ | number \\end{align} $$ Figure 3.10: A grammar for branching statements","title":"3.4 Recognition of Tokens"},{"location":"Chapter3-Lexical-Analysis/3.4Recognition-of-Tokens/#aho-corasick-algorithm","text":"NOTE: The main content of this chapter is passed, but in the exercise, the author introduce Aho-Corasick algorithm and KMP algorithm which are interesting. The following exercises, up to Exercise 3.4.12, introduce the Aho-Corasick algorithm for recognizing a collection of keywords in a text string in time proportional to the length of the text and the sum of the length of the keywords. This algorithm uses a special form of transition diagram called a trie . A trie is a tree-structured transition diagram with distinct labels on the edges leading from a node to its children. Leaves of the trie represent recognized keywords. NOTE: A trie is a tree-structured transition diagram or finite automaton , do you see the power of tree structure here as I do Knuth, Morris, and Pratt presented an algorithm for recognizing a single keyword $b_1 b_2 \\dots b_n$(length of the word is n) in a text string. Here the trie is a transition diagram with n+ 1 states, 0 through n. State 0 is the initial state, and state n represents acceptance, that is, discovery of the keyword. From each state s from 0 through n - 1, there is a transition to state s + 1, labeled by symbol $b_{s+1}$ . For example, the trie for the keyword ababaa is: In order to process text strings rapidly and search those strings for a keyword, it is useful to define, for keyword $b_1 b_2 \\dots b_n$ and position s in that keyword (corresponding to state s of its trie), a failure function , f(s) , computed as in Fig. 3.19. The objective is that $b_1 b_2 \\dots b_{f(s)}$ is the longest proper prefix of $b_1 b_2 \\dots b_s$ that is also a suffix of $b_1 b_2 \\dots b_s$. The reason $f (s)$ is important is that if we are trying to match a text string for $b_1 b_2 \\dots b_n$, and we have matched the first s positions, but we then fail (i.e., the next position of the text string does not hold $b_{s+1}$ ), then $f (s)$ is the longest prefix of $b_1 b_2 \\dots b_n$ that could possibly match the text string up to the point we are at. Of course, the next character of the text string must be $b_{f (s)+1}$, or else we still have problems and must consider a yet shorter prefix, which will be $b_{f (f (s))}$. t = 0; f (1) = 0; for (s = 1; s < n; s + +) f while (t > 0 && b[s+1] ! = b[t+1]) t = f (t); if (b[s+1] == b[t+1]) { t = t + 1; f (s + 1) = t; } else f (s + 1) = 0; } Figure 3.19: Algorithm to compute the failure function for keyword $b_1 b_2 \\dots b_n$ As an example, the failure function for the trie constructed above for ababaa is: s 1 2 3 4 5 6 f(s) 0 0 1 2 3 1 For instance, states 3 and 1 represent prefixes aba and a , respectively. f (3) = 1 because a is the longest proper prefix of aba that is also a suffix of aba . Also, f (2) = 0 , because the longest proper prefix of ab that is also a suffix is the empty string. Aho and Corasick generalized the KMP algorithm to recognize any of a set of keywords in a text string. In this case, the trie is a true tree, with branching from the root. There is one state for every string that is a prefix (not necessarily proper) of any keyword. The parent of a state corresponding to string $b_1 b_2 \\dots b_n$ is the state that corresponds to $b_1 b_2 \\dots b_k$. A state is accepting if it corresponds to a complete keyword. For example, Fig. 3.21 shows the trie for the keywords he , she , his , and hers .","title":"Aho-Corasick algorithm"},{"location":"Chapter3-Lexical-Analysis/3.6Finite-Automata/","text":"3.6 Finite Automata \uf0c1 We shall now discover how Lex turns its input program into a lexical analyzer. At the heart of the transition is the formalism known as finite automata . These are essentially graphs, like transition diagrams, with a few differences: Finite automata are recognizers ; they simply say \"yes\" or \"no\" about each possible input string. Finite automata come in two favors: (a) Nondeterministic finite automata (NFA) have no restrictions on the labels of their edges. A symbol can label several edges out of the same state, and $\\epsilon$, the empty string, is a possible label. (b) Deterministic finite automata (DFA) have, for each state, and for each symbol of its input alphabet exactly one edge with that symbol leaving that state. Both deterministic and nondeterministic finite automata are capable of recognizing the same languages. In fact these languages are exactly the same languages, called the regular languages, that regular expressions can describe. There is a small lacuna: as we defined them, regular expressions cannot describe the empty language, since we would never want to use this pattern in practice. However, finite automata can define the empty language. In the theory, $\\phi$ is treated as an additional regular expression for the sole purpose of defining the empty language. NOTE: In chapter 3.4 Recognition of Tokens, there are description as below A trie is a tree-structured transition diagram or finite automaton , It is known to us all that tree is graph. In first paragraph, the author also regard finite automata as graph. 3.6.1 Nondeterministic Finite Automata \uf0c1 A nondeterministic finite automaton (NFA) consists of: A finite set of states S . A set of input symbols $ \\Sigma $, the input alphabet . We assume that $\\epsilon$, which stands for the empty string, is never a member of $ \\Sigma $. A transition function that gives, for each state, and for each symbol in $ \\Sigma \\cup {\\epsilon}$ a set of next states . A state $s_0$ from $S$ that is distinguished as the start state (or initial state). A set of states $F$ , a subset of $S$ , that is distinguished as the accepting states (or final states ). We can represent either an NFA or DFA by a transition graph , where the nodes are states and the labeled edges represent the transition function. There is an edge labeled a from state $s$ to state $t$ if and only if $t$ is one of the next states for state $s$ and input $a$. This graph is very much like a transition diagram, except: a) The same symbol can label edges from one state to several different states, and b) An edge may be labeled by $\\epsilon$, the empty string, instead of, or in addition to, symbols from the input alphabet. Example 3.14 : The transition graph for an NFA recognizing the language of regular expression $(a|b)*abb$ is shown in Fig. 3.24. This abstract example, describing all strings of a's and b's ending in the particular string abb , will be used throughout this section. It is similar to regular expressions that describe languages of real interest, however. For instance, an expression describing all files whose name ends in .o is any * .o, where any stands for any printable character. NOTE: There are two edges labeled with a from state 0 while this is not allowed in DFA. Below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. Following our convention for transition diagrams, the double circle around state 3 indicates that this state is accepting. Notice that the only ways to get from the start state 0 to the accepting state is to follow some path that stays in state 0 for a while, then go es to states 1, 2, and 3 by reading abb from the input. Thus, the only strings getting to the accepting state are those that end in abb . 3.6.2 Transition Tables \uf0c1 We can also represent an NFA by a transition table , whose rows correspond to states, and whose columns correspond to the input symbols and $\\epsilon$. The entry for a given state and input is the value of the transition function applied to those arguments. If the transition function has no information about that state-input pair, we put $\\phi$ in the table for the pair. Example 3.15 : The transition table for the NFA of Fig. 3.24 is shown in Fig. 3.25. The transition table has the advantage that we can easily find the transitions on a given state and input. Its disadvantage is that it takes a lot of space, when the input alphabet is large, yet most states do not have any moves on most of the input symbols. NOTE: Transition diagrams and transition table correspond to two kind of data structure describing finite automata 3.6.4 Deterministic Finite Automata \uf0c1 A deterministic finite automaton (DFA) is a special case of an NFA where: There are no moves on input $\\epsilon$ , and For each state s and input symbol a , there is exactly one edge out of s labeled a . If we are using a transition table to represent a DFA, then each entry is a single state. We may therefore represent this state without the curly braces that we use to form sets. While the NFA is an abstract representation of an algorithm to recognize the strings of a certain language, the DFA is a simple, concrete algorithm for recognizing strings. It is fortunate indeed that every regular expression and every NFA can be converted to a DFA accepting the same language, because it is the DFA that we really implement or simulate when building lexical analyzers. The following algorithm shows how to apply a DFA to a string. Algorithm 3.18 : Simulating a DFA. INPUT : An input string x terminated by an end-of-file character eof . A DFA D with start state $s_0$, accepting states F , and transition function move . OUTPUT : Answer \"yes\" if D accepts x ;\"no\" otherwise. METHOD : Apply the algorithm in Fig. 3.27 to the input string x . The function move(s, c) gives the state to which there is an edge from state s on input c . The function nextChar returns the next character of the input string x . Example 3.19 : In Fig. 3.28 we see the transition graph of a DFA accepting the language $(a|b)*abb$ , the same as that accepted by the NFA of Fig. 3.24. Given the input string ababb , this DFA enters the sequence of states 0, 1, 2, 1, 2, 3 and returns \" yes.\"","title":"3.6Finite-Automata"},{"location":"Chapter3-Lexical-Analysis/3.6Finite-Automata/#36-finite-automata","text":"We shall now discover how Lex turns its input program into a lexical analyzer. At the heart of the transition is the formalism known as finite automata . These are essentially graphs, like transition diagrams, with a few differences: Finite automata are recognizers ; they simply say \"yes\" or \"no\" about each possible input string. Finite automata come in two favors: (a) Nondeterministic finite automata (NFA) have no restrictions on the labels of their edges. A symbol can label several edges out of the same state, and $\\epsilon$, the empty string, is a possible label. (b) Deterministic finite automata (DFA) have, for each state, and for each symbol of its input alphabet exactly one edge with that symbol leaving that state. Both deterministic and nondeterministic finite automata are capable of recognizing the same languages. In fact these languages are exactly the same languages, called the regular languages, that regular expressions can describe. There is a small lacuna: as we defined them, regular expressions cannot describe the empty language, since we would never want to use this pattern in practice. However, finite automata can define the empty language. In the theory, $\\phi$ is treated as an additional regular expression for the sole purpose of defining the empty language. NOTE: In chapter 3.4 Recognition of Tokens, there are description as below A trie is a tree-structured transition diagram or finite automaton , It is known to us all that tree is graph. In first paragraph, the author also regard finite automata as graph.","title":"3.6 Finite Automata"},{"location":"Chapter3-Lexical-Analysis/3.6Finite-Automata/#361-nondeterministic-finite-automata","text":"A nondeterministic finite automaton (NFA) consists of: A finite set of states S . A set of input symbols $ \\Sigma $, the input alphabet . We assume that $\\epsilon$, which stands for the empty string, is never a member of $ \\Sigma $. A transition function that gives, for each state, and for each symbol in $ \\Sigma \\cup {\\epsilon}$ a set of next states . A state $s_0$ from $S$ that is distinguished as the start state (or initial state). A set of states $F$ , a subset of $S$ , that is distinguished as the accepting states (or final states ). We can represent either an NFA or DFA by a transition graph , where the nodes are states and the labeled edges represent the transition function. There is an edge labeled a from state $s$ to state $t$ if and only if $t$ is one of the next states for state $s$ and input $a$. This graph is very much like a transition diagram, except: a) The same symbol can label edges from one state to several different states, and b) An edge may be labeled by $\\epsilon$, the empty string, instead of, or in addition to, symbols from the input alphabet. Example 3.14 : The transition graph for an NFA recognizing the language of regular expression $(a|b)*abb$ is shown in Fig. 3.24. This abstract example, describing all strings of a's and b's ending in the particular string abb , will be used throughout this section. It is similar to regular expressions that describe languages of real interest, however. For instance, an expression describing all files whose name ends in .o is any * .o, where any stands for any printable character. NOTE: There are two edges labeled with a from state 0 while this is not allowed in DFA. Below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality. Following our convention for transition diagrams, the double circle around state 3 indicates that this state is accepting. Notice that the only ways to get from the start state 0 to the accepting state is to follow some path that stays in state 0 for a while, then go es to states 1, 2, and 3 by reading abb from the input. Thus, the only strings getting to the accepting state are those that end in abb .","title":"3.6.1 Nondeterministic Finite Automata"},{"location":"Chapter3-Lexical-Analysis/3.6Finite-Automata/#362-transition-tables","text":"We can also represent an NFA by a transition table , whose rows correspond to states, and whose columns correspond to the input symbols and $\\epsilon$. The entry for a given state and input is the value of the transition function applied to those arguments. If the transition function has no information about that state-input pair, we put $\\phi$ in the table for the pair. Example 3.15 : The transition table for the NFA of Fig. 3.24 is shown in Fig. 3.25. The transition table has the advantage that we can easily find the transitions on a given state and input. Its disadvantage is that it takes a lot of space, when the input alphabet is large, yet most states do not have any moves on most of the input symbols. NOTE: Transition diagrams and transition table correspond to two kind of data structure describing finite automata","title":"3.6.2 Transition Tables"},{"location":"Chapter3-Lexical-Analysis/3.6Finite-Automata/#364-deterministic-finite-automata","text":"A deterministic finite automaton (DFA) is a special case of an NFA where: There are no moves on input $\\epsilon$ , and For each state s and input symbol a , there is exactly one edge out of s labeled a . If we are using a transition table to represent a DFA, then each entry is a single state. We may therefore represent this state without the curly braces that we use to form sets. While the NFA is an abstract representation of an algorithm to recognize the strings of a certain language, the DFA is a simple, concrete algorithm for recognizing strings. It is fortunate indeed that every regular expression and every NFA can be converted to a DFA accepting the same language, because it is the DFA that we really implement or simulate when building lexical analyzers. The following algorithm shows how to apply a DFA to a string. Algorithm 3.18 : Simulating a DFA. INPUT : An input string x terminated by an end-of-file character eof . A DFA D with start state $s_0$, accepting states F , and transition function move . OUTPUT : Answer \"yes\" if D accepts x ;\"no\" otherwise. METHOD : Apply the algorithm in Fig. 3.27 to the input string x . The function move(s, c) gives the state to which there is an edge from state s on input c . The function nextChar returns the next character of the input string x . Example 3.19 : In Fig. 3.28 we see the transition graph of a DFA accepting the language $(a|b)*abb$ , the same as that accepted by the NFA of Fig. 3.24. Given the input string ababb , this DFA enters the sequence of states 0, 1, 2, 1, 2, 3 and returns \" yes.\"","title":"3.6.4 Deterministic Finite Automata"},{"location":"Chapter3-Lexical-Analysis/3.7From-Regular-Expressions-to-Automata/","text":"3.7 From Regular Expressions to Automata \uf0c1 NOTE: What this chapter describe is mainly three algorithms Name Function Thompson's construction algorithm transforming a regular expression into an equivalent nondeterministic finite automaton (NFA) subset construction algorithm converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language DFA minimization transforming a given deterministic finite automaton (DFA) into an equivalent DFA that has a minimum number of states. Using the two algorithms above, a regular expression can be converted to the corresponding NFA or DFA. The regular expression is the notation of choice for describing lexical analyzers and other pattern-processing software, as was reflected in Section 3.5. However, implementation of that software requires the simulation of a DFA, as in Algorithm 3.18, or perhaps simulation of an NFA. Because an NFA often has a choice of move on an input symbol (as Fig. 3.24 does on input a from state 0) or on $\\epsilon$ (as Fig. 3.26 does from state 0), or even a choice of making a transition on $\\epsilon$ or on a real input symbol, its simulation is less straightforward than for a DFA. Thus often it is important to convert an NFA to a DFA that accepts the same language. 3.7.1 Conversion of an NFA to a DFA \uf0c1 NOTE: The subset construction explained in this book is too abstract to grasp, wikipedia's explanation about this algorithm is more intuitionistic. The general idea behind the subset construction is that each state of the constructed DFA corresponds to a set of NFA states. After reading input $a_1 a_2 \\dots a_n$, the DFA is in that state which corresponds to the set of states that the NFA can reach, from its start state, following paths labeled $a_1 a_2 \\dots a_n$. It is p ossible that the number of DFA states is exponential in the number of NFA states, which could lead to difficulties when we try to implement this DFA. However, part of the p ower of the automaton-based approach to lexical analysis is that for real languages, the NFA and DFA have approximately the same number of states, and the exponential behavior is not seen. Algorithm 3.20 : The subset construction of a DFA from an NFA. INPUT : An NFA N . OUTPUT : A DFA D accepting the same language as N . METHOD : Our algorithm constructs a transition table $D_{tran}$ for D . Each state of D is a set of NFA states, and we construct $D_{tran}$ so D will simulate \"in parallel\" all possible moves N can make on a given input string. Our first problem is to deal with $\\epsilon$-transitions of N properly. In Fig. 3.31 we see the definitions of several functions that describe basic computations on the states of N that are needed in the algorithm. Note that s is a single state of N , while T is a set of states of N . OPERATION DESCRIPTION $\\epsilon-closure(s)$ Set of NFA states reachable from NFA state s on $\\epsilon$-transitions alone. $\\epsilon-closure(T)$ Set of NFA states reachable from some NFA state s in set T on $\\epsilon$-transitions alone; $= \\cup _{s \\in T} {\\epsilon-closure(s)}$ move(T, a) Set of NFA states to which there is a transition on input symbol a from some state s in T . Figure 3.31: Operations on NFA states We must explore those sets of states that N can be in after seeing some input string. As a basis, before reading the first input symbol, N can be in any of the states of $\\epsilon-closure(s_0)$, where $s_0$ is its start state . For the induction, suppose that N can be in set of states T after reading input string x . If it next reads input a , then N can immediately go to any of the states in $move (T , a)$. However, after reading a , it may also make several $\\epsilon-transitions$; thus N could be in any state of $\\epsilon-closure(move(T, a))$ after reading input xa . Following these ideas, the construction of the set of D 's states, $D_{states}$, and its transition function $D_{tran}$, is shown in Fig. 3.32. The start state of D is $\\epsilon-closure(s_0)$, and the accepting states of D are all those sets of N 's states that include at least one accepting state of N . To complete our description of the subset construction, we need only to show how $\\epsilon-closure(T)$ is computed for any set of NFA states T . This process, shown in Fig. 3.33, is a straightforward search in a graph from a set of states. In this case, imagine that only the $\\epsilon$-labeled edges are available in the graph. 3.7.4 Construction of an NFA from a Regular Expression \uf0c1 We now give an algorithm for converting any regular expression to an NFA that defines the same language. The algorithm is syntax-directed , in the sense that it works recursively up the parse tree for the regular expression. For each sub expression the algorithm constructs an NFA with a single accepting state. NOTE: bottom-up NOTE: How to build a parse tree for a regular expression? Efficiently building a parse tree from a regular expression Algorithm 3.23 : The McNaughton-Yamada-Thompson algorithm to convert a regular expression to an NFA. INPUT : A regular expression r over alphabet $\\Sigma$. OUTPUT : An NFA N accepting L(r ) . METHOD: Begin by parsing r into its constituent subexpressions. The rules for constructing an NFA consist of basis rules for handling subexpressions with no operators, and inductive rules for constructing larger NFA's from the NFA's for the immediate subexpressions of a given expression. BASIS : For expression $\\epsilon$ construct the NFA Here, i is a new state, the start state of this NFA, and f is another new state, the accepting state for the NFA. For any subexpression a in $\\Sigma$, construct the NFA where again i and f are new states, the start and accepting states, respectively. Note that in both of the basis constructions, we construct a distinct NFA, with new states, for every occurrence of $\\epsilon$ or some a as a subexpression of r . INDUCTION : Suppose N (s) and N (t) are NFA's for regular expressions s and t , respectively. a) Suppose r = s|t . Then N (r ) , the NFA for r , is constructed as in Fig. 3.40. Here, i and f are new states, the start and accepting states of N (r ) , respectively. There are $\\epsilon$-transitions from i to the start states of N (s) and N (t) , and each of their accepting states have $\\epsilon$-transitions to the accepting state f . Note that the accepting states of N (s) and N (t) are not accepting in N (r) . Since any path from i to f must pass through either N (s) or N (t) exclusively, and since the label of that path is not changed by the $\\epsilon$'s leaving i or entering f , we conclude that N (r ) accepts $L(s) \\cup L(t)$, which is the same as L(r ) . That is, Fig. 3.40 is a correct construction for the union operator. b) Suppose r = st . Then construct N (r ) as in Fig. 3.41. The start state of N (s) becomes the start state of N (r ) , and the accepting state of N (t) is the only accepting state of N (r ) . The accepting state of N(s) and the start state of N (t) are merged into a single state, with all the transitions in or out of either state. A path from i to f in Fig. 3.41 must go first through N (s) , and therefore its label will begin with some string in L(s) . The path then continues through N (t) , so the path's label finishes with a string in L(t) . As we shall so on argue, accepting states never have edges out and start states never have edges in, so it is not possible for a path to re-enter N (s) after leaving it. Thus, N (r ) accepts exactly L(s)L(t) , and is a correct NFA for r = st . c) Suppose $r = s^ $. Then for r we construct the NFA N (r ) shown in Fig. 3.42. Here, i and f are new states, the start state and lone accepting state of N (r ) . To get from i to f , we can either follow the introduced path labeled $\\epsilon$, which takes care of the one string in $L(s)^0$ , or we can go to the start state of N(s) , through that NFA, then from its accepting state back to its start state zero or more times. These options allow N (r ) to accept all the strings in $L(s)^1$, $L(s)^2$, and so on, so the entire set of strings accepted by N (r ) is $L(s^ )$. d) Finally, suppose r = (s) . Then L(r ) = L(s) , and we can use the NFA N (s) as N (r ) . NOTE: Implementing a regular expression engine is far more complex than the algorithm described here. The method description in Algorithm 3.23 contains hints as to why the inductive construction works as it should. We shall not give a formal correctness proof, but we shall list several properties of the constructed NFA's, in addition to the all-important fact that N (r ) accepts language L(r ) . These properties are interesting in their own right, and helpful in making a formal proof. N (r ) has at most twice as many states as there are operators and operands in r . This bound follows from the fact that each step of the algorithm creates at most two new states. N (r ) has one start state and one accepting state . The accepting state has no outgoing transitions, and the start state has no incoming transitions. Each state of N (r ) other than the accepting state has either one outgoing transition on a symbol in $\\Sigma$ or two outgoing transitions, both on $\\epsilon$. Example 3.24 : Let us use Algorithm 3.23 to construct an NFA for $r = (a | b) ^*abb$. Figure 3.43 shows a parse tree for r that is analogous to the parse trees constructed for arithmetic expressions in Section 2.2.3. For subexpression $r_1$ , the first a , we construct the NFA: State numbers have been chosen for consistency with what follows. For $r_2$ we construct: We can now combine $N (r_1)$ and $N (r_2)$, using the construction of Fig. 3.40 to obtain the NFA for $r_3 =r_1|r_2$; this NFA is shown in Fig. 3.44. The NFA for $r_4= (r_3)$ is the same as that for $r_3$. The NFA for $r_5= (r_3)$ is then as shown in Fig. 3.45. We have used the construction in Fig. 3.42 to build this NFA from the NFA in Fig. 3.44. Now, consider subexpression $r_6$, which is another a . We use the basis construction for a again, but we must use new states. It is not permissible to reuse the NFA we constructed for $r_1$, even though $r_1$ and $r_6$ are the same expression. The NFA for $r_6$ is: To obtain the NFA for $r_7= r_5r_6$, we apply the construction of Fig. 3.41. We merge states 7 and $7^{'}$, yielding the NFA of Fig. 3.46. Continuing in this fashion with new NFA's for the two subexpressions b called $r_8$ and $r_{10}$, we eventually construct the NFA for tha$r = (a | b) ^*abb$ that we first met in Fig. 3.34.","title":"3.7From-Regular-Expressions-to-Automata"},{"location":"Chapter3-Lexical-Analysis/3.7From-Regular-Expressions-to-Automata/#37-from-regular-expressions-to-automata","text":"NOTE: What this chapter describe is mainly three algorithms Name Function Thompson's construction algorithm transforming a regular expression into an equivalent nondeterministic finite automaton (NFA) subset construction algorithm converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language DFA minimization transforming a given deterministic finite automaton (DFA) into an equivalent DFA that has a minimum number of states. Using the two algorithms above, a regular expression can be converted to the corresponding NFA or DFA. The regular expression is the notation of choice for describing lexical analyzers and other pattern-processing software, as was reflected in Section 3.5. However, implementation of that software requires the simulation of a DFA, as in Algorithm 3.18, or perhaps simulation of an NFA. Because an NFA often has a choice of move on an input symbol (as Fig. 3.24 does on input a from state 0) or on $\\epsilon$ (as Fig. 3.26 does from state 0), or even a choice of making a transition on $\\epsilon$ or on a real input symbol, its simulation is less straightforward than for a DFA. Thus often it is important to convert an NFA to a DFA that accepts the same language.","title":"3.7 From Regular Expressions to Automata"},{"location":"Chapter3-Lexical-Analysis/3.7From-Regular-Expressions-to-Automata/#371-conversion-of-an-nfa-to-a-dfa","text":"NOTE: The subset construction explained in this book is too abstract to grasp, wikipedia's explanation about this algorithm is more intuitionistic. The general idea behind the subset construction is that each state of the constructed DFA corresponds to a set of NFA states. After reading input $a_1 a_2 \\dots a_n$, the DFA is in that state which corresponds to the set of states that the NFA can reach, from its start state, following paths labeled $a_1 a_2 \\dots a_n$. It is p ossible that the number of DFA states is exponential in the number of NFA states, which could lead to difficulties when we try to implement this DFA. However, part of the p ower of the automaton-based approach to lexical analysis is that for real languages, the NFA and DFA have approximately the same number of states, and the exponential behavior is not seen. Algorithm 3.20 : The subset construction of a DFA from an NFA. INPUT : An NFA N . OUTPUT : A DFA D accepting the same language as N . METHOD : Our algorithm constructs a transition table $D_{tran}$ for D . Each state of D is a set of NFA states, and we construct $D_{tran}$ so D will simulate \"in parallel\" all possible moves N can make on a given input string. Our first problem is to deal with $\\epsilon$-transitions of N properly. In Fig. 3.31 we see the definitions of several functions that describe basic computations on the states of N that are needed in the algorithm. Note that s is a single state of N , while T is a set of states of N . OPERATION DESCRIPTION $\\epsilon-closure(s)$ Set of NFA states reachable from NFA state s on $\\epsilon$-transitions alone. $\\epsilon-closure(T)$ Set of NFA states reachable from some NFA state s in set T on $\\epsilon$-transitions alone; $= \\cup _{s \\in T} {\\epsilon-closure(s)}$ move(T, a) Set of NFA states to which there is a transition on input symbol a from some state s in T . Figure 3.31: Operations on NFA states We must explore those sets of states that N can be in after seeing some input string. As a basis, before reading the first input symbol, N can be in any of the states of $\\epsilon-closure(s_0)$, where $s_0$ is its start state . For the induction, suppose that N can be in set of states T after reading input string x . If it next reads input a , then N can immediately go to any of the states in $move (T , a)$. However, after reading a , it may also make several $\\epsilon-transitions$; thus N could be in any state of $\\epsilon-closure(move(T, a))$ after reading input xa . Following these ideas, the construction of the set of D 's states, $D_{states}$, and its transition function $D_{tran}$, is shown in Fig. 3.32. The start state of D is $\\epsilon-closure(s_0)$, and the accepting states of D are all those sets of N 's states that include at least one accepting state of N . To complete our description of the subset construction, we need only to show how $\\epsilon-closure(T)$ is computed for any set of NFA states T . This process, shown in Fig. 3.33, is a straightforward search in a graph from a set of states. In this case, imagine that only the $\\epsilon$-labeled edges are available in the graph.","title":"3.7.1 Conversion of an NFA to a DFA"},{"location":"Chapter3-Lexical-Analysis/3.7From-Regular-Expressions-to-Automata/#374-construction-of-an-nfa-from-a-regular-expression","text":"We now give an algorithm for converting any regular expression to an NFA that defines the same language. The algorithm is syntax-directed , in the sense that it works recursively up the parse tree for the regular expression. For each sub expression the algorithm constructs an NFA with a single accepting state. NOTE: bottom-up NOTE: How to build a parse tree for a regular expression? Efficiently building a parse tree from a regular expression Algorithm 3.23 : The McNaughton-Yamada-Thompson algorithm to convert a regular expression to an NFA. INPUT : A regular expression r over alphabet $\\Sigma$. OUTPUT : An NFA N accepting L(r ) . METHOD: Begin by parsing r into its constituent subexpressions. The rules for constructing an NFA consist of basis rules for handling subexpressions with no operators, and inductive rules for constructing larger NFA's from the NFA's for the immediate subexpressions of a given expression. BASIS : For expression $\\epsilon$ construct the NFA Here, i is a new state, the start state of this NFA, and f is another new state, the accepting state for the NFA. For any subexpression a in $\\Sigma$, construct the NFA where again i and f are new states, the start and accepting states, respectively. Note that in both of the basis constructions, we construct a distinct NFA, with new states, for every occurrence of $\\epsilon$ or some a as a subexpression of r . INDUCTION : Suppose N (s) and N (t) are NFA's for regular expressions s and t , respectively. a) Suppose r = s|t . Then N (r ) , the NFA for r , is constructed as in Fig. 3.40. Here, i and f are new states, the start and accepting states of N (r ) , respectively. There are $\\epsilon$-transitions from i to the start states of N (s) and N (t) , and each of their accepting states have $\\epsilon$-transitions to the accepting state f . Note that the accepting states of N (s) and N (t) are not accepting in N (r) . Since any path from i to f must pass through either N (s) or N (t) exclusively, and since the label of that path is not changed by the $\\epsilon$'s leaving i or entering f , we conclude that N (r ) accepts $L(s) \\cup L(t)$, which is the same as L(r ) . That is, Fig. 3.40 is a correct construction for the union operator. b) Suppose r = st . Then construct N (r ) as in Fig. 3.41. The start state of N (s) becomes the start state of N (r ) , and the accepting state of N (t) is the only accepting state of N (r ) . The accepting state of N(s) and the start state of N (t) are merged into a single state, with all the transitions in or out of either state. A path from i to f in Fig. 3.41 must go first through N (s) , and therefore its label will begin with some string in L(s) . The path then continues through N (t) , so the path's label finishes with a string in L(t) . As we shall so on argue, accepting states never have edges out and start states never have edges in, so it is not possible for a path to re-enter N (s) after leaving it. Thus, N (r ) accepts exactly L(s)L(t) , and is a correct NFA for r = st . c) Suppose $r = s^ $. Then for r we construct the NFA N (r ) shown in Fig. 3.42. Here, i and f are new states, the start state and lone accepting state of N (r ) . To get from i to f , we can either follow the introduced path labeled $\\epsilon$, which takes care of the one string in $L(s)^0$ , or we can go to the start state of N(s) , through that NFA, then from its accepting state back to its start state zero or more times. These options allow N (r ) to accept all the strings in $L(s)^1$, $L(s)^2$, and so on, so the entire set of strings accepted by N (r ) is $L(s^ )$. d) Finally, suppose r = (s) . Then L(r ) = L(s) , and we can use the NFA N (s) as N (r ) . NOTE: Implementing a regular expression engine is far more complex than the algorithm described here. The method description in Algorithm 3.23 contains hints as to why the inductive construction works as it should. We shall not give a formal correctness proof, but we shall list several properties of the constructed NFA's, in addition to the all-important fact that N (r ) accepts language L(r ) . These properties are interesting in their own right, and helpful in making a formal proof. N (r ) has at most twice as many states as there are operators and operands in r . This bound follows from the fact that each step of the algorithm creates at most two new states. N (r ) has one start state and one accepting state . The accepting state has no outgoing transitions, and the start state has no incoming transitions. Each state of N (r ) other than the accepting state has either one outgoing transition on a symbol in $\\Sigma$ or two outgoing transitions, both on $\\epsilon$. Example 3.24 : Let us use Algorithm 3.23 to construct an NFA for $r = (a | b) ^*abb$. Figure 3.43 shows a parse tree for r that is analogous to the parse trees constructed for arithmetic expressions in Section 2.2.3. For subexpression $r_1$ , the first a , we construct the NFA: State numbers have been chosen for consistency with what follows. For $r_2$ we construct: We can now combine $N (r_1)$ and $N (r_2)$, using the construction of Fig. 3.40 to obtain the NFA for $r_3 =r_1|r_2$; this NFA is shown in Fig. 3.44. The NFA for $r_4= (r_3)$ is the same as that for $r_3$. The NFA for $r_5= (r_3)$ is then as shown in Fig. 3.45. We have used the construction in Fig. 3.42 to build this NFA from the NFA in Fig. 3.44. Now, consider subexpression $r_6$, which is another a . We use the basis construction for a again, but we must use new states. It is not permissible to reuse the NFA we constructed for $r_1$, even though $r_1$ and $r_6$ are the same expression. The NFA for $r_6$ is: To obtain the NFA for $r_7= r_5r_6$, we apply the construction of Fig. 3.41. We merge states 7 and $7^{'}$, yielding the NFA of Fig. 3.46. Continuing in this fashion with new NFA's for the two subexpressions b called $r_8$ and $r_{10}$, we eventually construct the NFA for tha$r = (a | b) ^*abb$ that we first met in Fig. 3.34.","title":"3.7.4 Construction of an NFA from a Regular Expression"},{"location":"Chapter3-Lexical-Analysis/Lexical-Analysis/","text":"Lexical Analysis \uf0c1 In this chapter we show how to construct a lexical analyzer . To implement a lexical analyzer by hand, it helps to start with a diagram or other description for the lexemes of each token. We can then write code to identify each occurrence of each lexeme on the input and to return information about the token identified. NOTE: Manual implementation just to make it easier for the readers to understand how the lexical analyzer works, what should be focused on is how does it work automatically and how to implement it. We can also produce a lexical analyzer automatically by specifying the lexeme patterns to a lexical-analyzer generator and compiling those patterns into code that functions as a lexical analyzer . This approach makes it easier to modify a lexical analyzer , since we have only to rewrite the affected patterns, not the entire program. It also speeds up the process of implementing the lexical analyzer, since the programmer specifies the software at the very high level of patterns and relies on the generator to produce the detailed code. We shall introduce in Section 3.5 a lexical-analyzer generator called Lex (or Flex in a more recent embodiment). NOTE: In fact, the essence lies in how lex works. We begin the study of lexical-analyzer generators by introducing regular expressions, a convenient notation for specifying lexeme patterns. We show how this notation can be transformed, first into nondeterministic automata and then into deterministic automata. The latter two notations can be used as input to a \"driver,\" that is, code which simulates these automata and uses them as a guide to determining the next token. This driver and the specification of the automaton form the nucleus of the lexical analyzer. NOTE: Nucleus of the lexical analyzer is what we should focus on. NOTE: This chapter is mainly about algorithms of lexical analysis, I extend to include the algorithms relevant to lexical analysis in wikipedia which is listed in section wikipedia String","title":"Lexical-Analysis"},{"location":"Chapter3-Lexical-Analysis/Lexical-Analysis/#lexical-analysis","text":"In this chapter we show how to construct a lexical analyzer . To implement a lexical analyzer by hand, it helps to start with a diagram or other description for the lexemes of each token. We can then write code to identify each occurrence of each lexeme on the input and to return information about the token identified. NOTE: Manual implementation just to make it easier for the readers to understand how the lexical analyzer works, what should be focused on is how does it work automatically and how to implement it. We can also produce a lexical analyzer automatically by specifying the lexeme patterns to a lexical-analyzer generator and compiling those patterns into code that functions as a lexical analyzer . This approach makes it easier to modify a lexical analyzer , since we have only to rewrite the affected patterns, not the entire program. It also speeds up the process of implementing the lexical analyzer, since the programmer specifies the software at the very high level of patterns and relies on the generator to produce the detailed code. We shall introduce in Section 3.5 a lexical-analyzer generator called Lex (or Flex in a more recent embodiment). NOTE: In fact, the essence lies in how lex works. We begin the study of lexical-analyzer generators by introducing regular expressions, a convenient notation for specifying lexeme patterns. We show how this notation can be transformed, first into nondeterministic automata and then into deterministic automata. The latter two notations can be used as input to a \"driver,\" that is, code which simulates these automata and uses them as a guide to determining the next token. This driver and the specification of the automaton form the nucleus of the lexical analyzer. NOTE: Nucleus of the lexical analyzer is what we should focus on. NOTE: This chapter is mainly about algorithms of lexical analysis, I extend to include the algorithms relevant to lexical analysis in wikipedia which is listed in section wikipedia String","title":"Lexical Analysis"},{"location":"Chapter3-Lexical-Analysis/wikipedia-String/wikipedia-String(computer-science)/","text":"","title":"wikipedia-String(computer-science)"},{"location":"Chapter3-Lexical-Analysis/wikipedia-String/Regular-expression/wikipedia-Powerset-construction/","text":"Powerset construction \uf0c1 In the theory of computation and automata theory , the powerset construction or subset construction is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language . It is important in theory because it establishes that NFAs, despite their additional flexibility, are unable to recognize any language that cannot be recognized by some DFA. It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. However, if the NFA has n states, the resulting DFA may have up to $2^n$ states, an exponentially larger number, which sometimes makes the construction impractical for large NFAs. The construction, sometimes called the Rabin\u2013Scott powerset construction (or subset construction) to distinguish it from similar constructions for other types of automata, was first published by Michael O. Rabin and Dana Scott in 1959.[ 1] Intuition \uf0c1 To simulate the operation of a DFA on a given input string, one needs to keep track of a single state at any time: the state that the automaton will reach after seeing a prefix of the input. In contrast, to simulate an NFA, one needs to keep track of a set of states : all of the states that the automaton could reach after seeing the same prefix of the input, according to the nondeterministic choices made by the automaton. If, after a certain prefix of the input, a set S of states can be reached, then after the next input symbol x the set of reachable states is a deterministic function of S and x . Therefore, the sets of reachable NFA states play the same role in the NFA simulation as single DFA states play in the DFA simulation, and in fact the sets of NFA states appearing in this simulation may be re-interpreted as being states of a DFA.[ 2] NOTE: The difference between DFA and NFA can help to understand why a single state in DFA versus a set of states in NFA, below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.","title":"wikipedia-Powerset-construction"},{"location":"Chapter3-Lexical-Analysis/wikipedia-String/Regular-expression/wikipedia-Powerset-construction/#powerset-construction","text":"In the theory of computation and automata theory , the powerset construction or subset construction is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language . It is important in theory because it establishes that NFAs, despite their additional flexibility, are unable to recognize any language that cannot be recognized by some DFA. It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. However, if the NFA has n states, the resulting DFA may have up to $2^n$ states, an exponentially larger number, which sometimes makes the construction impractical for large NFAs. The construction, sometimes called the Rabin\u2013Scott powerset construction (or subset construction) to distinguish it from similar constructions for other types of automata, was first published by Michael O. Rabin and Dana Scott in 1959.[ 1]","title":"Powerset construction"},{"location":"Chapter3-Lexical-Analysis/wikipedia-String/Regular-expression/wikipedia-Powerset-construction/#intuition","text":"To simulate the operation of a DFA on a given input string, one needs to keep track of a single state at any time: the state that the automaton will reach after seeing a prefix of the input. In contrast, to simulate an NFA, one needs to keep track of a set of states : all of the states that the automaton could reach after seeing the same prefix of the input, according to the nondeterministic choices made by the automaton. If, after a certain prefix of the input, a set S of states can be reached, then after the next input symbol x the set of reachable states is a deterministic function of S and x . Therefore, the sets of reachable NFA states play the same role in the NFA simulation as single DFA states play in the DFA simulation, and in fact the sets of NFA states appearing in this simulation may be re-interpreted as being states of a DFA.[ 2] NOTE: The difference between DFA and NFA can help to understand why a single state in DFA versus a set of states in NFA, below is come from Finite-state machine : In a deterministic automaton, every state has exactly one transition for each possible input. In a non-deterministic automaton, an input can lead to one, more than one, or no transition for a given state. The powerset construction algorithm can transform any nondeterministic automaton into a (usually more complex) deterministic automaton with identical functionality.","title":"Intuition"},{"location":"Chapter3-Lexical-Analysis/wikipedia-String/string-searching/wikipedia-Aho-Corasick-algorithm/","text":"","title":"wikipedia-Aho-Corasick-algorithm"},{"location":"Chapter4-Syntax-Analysis/Syntax-Analysis/","text":"Chapter 4 Syntax Analysis \uf0c1 NOTE: This chapter is mainly about algorithms of parsing or syntax analysis, I extend it to include algorithms relevant to parsing in wikipedia which is listed in section Wikipedia Parsing algorithms","title":"Syntax-Analysis"},{"location":"Chapter4-Syntax-Analysis/Syntax-Analysis/#chapter-4-syntax-analysis","text":"NOTE: This chapter is mainly about algorithms of parsing or syntax analysis, I extend it to include algorithms relevant to parsing in wikipedia which is listed in section Wikipedia Parsing algorithms","title":"Chapter 4 Syntax Analysis"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/wikipedia-Parsing/","text":"Parsing Parsing \uf0c1","title":"wikipedia-Parsing"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/wikipedia-Parsing/#parsing","text":"","title":"Parsing"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/blog-Equation(expression)parser-with-precedence/","text":"Equation (expression) parser with precedence? \uf0c1 Simple Guide to Mathematical Expression Parsing \uf0c1","title":"blog-Equation(expression)parser-with-precedence"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/blog-Equation(expression)parser-with-precedence/#equation-expression-parser-with-precedence","text":"","title":"Equation (expression) parser with precedence?"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/blog-Equation(expression)parser-with-precedence/#simple-guide-to-mathematical-expression-parsing","text":"","title":"Simple Guide to Mathematical Expression Parsing"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/wikipedia-CYK-algorithm/","text":"CYK algorithm \uf0c1","title":"wikipedia-CYK-algorithm"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/wikipedia-CYK-algorithm/#cyk-algorithm","text":"","title":"CYK algorithm"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/wikipedia-LR-parser/","text":"","title":"wikipedia-LR-parser"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/wikipedia-Top-down-parsing/","text":"","title":"wikipedia-Top-down-parsing"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/Precedence/wikipedia-Operator-precedence-parser/","text":"","title":"wikipedia-Operator-precedence-parser"},{"location":"Chapter4-Syntax-Analysis/Wikipedia-Parsing-algorithms/Bottom-up/Precedence/wikipedia-Shunting-yard-algorithm/","text":"","title":"wikipedia-Shunting-yard-algorithm"}]}